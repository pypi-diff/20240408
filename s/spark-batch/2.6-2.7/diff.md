# Comparing `tmp/spark_batch-2.6-py3-none-any.whl.zip` & `tmp/spark_batch-2.7-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,9 +1,9 @@
-Zip file size: 402328 bytes, number of entries: 46
--rw-r--r--  2.0 unx        0 b- defN 24-Mar-25 06:07 spark_batch/__init__.py
+Zip file size: 402376 bytes, number of entries: 46
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-08 09:32 spark_batch/__init__.py
 -rw-r--r--  2.0 unx     5508 b- defN 24-Feb-07 09:17 spark_batch/run.py
 -rwxr-xr-x  2.0 unx     2562 b- defN 24-Feb-07 09:17 spark_batch/data/shp/BML_BADM_AS.dbf
 -rwxr-xr-x  2.0 unx      422 b- defN 24-Feb-07 09:17 spark_batch/data/shp/BML_BADM_AS.prj
 -rwxr-xr-x  2.0 unx      348 b- defN 24-Feb-07 09:17 spark_batch/data/shp/BML_BADM_AS.sbn
 -rwxr-xr-x  2.0 unx      132 b- defN 24-Feb-07 09:17 spark_batch/data/shp/BML_BADM_AS.sbx
 -rwxr-xr-x  2.0 unx   151044 b- defN 24-Feb-07 09:17 spark_batch/data/shp/BML_BADM_AS.shp
 -rwxr-xr-x  2.0 unx     1055 b- defN 24-Feb-07 09:17 spark_batch/data/shp/BML_BADM_AS.shp.xml
@@ -24,25 +24,25 @@
 -rwxr-xr-x  2.0 unx     4170 b- defN 24-Feb-07 09:17 spark_batch/data/shp2024/BML_HADM_AS.dbf
 -rwxr-xr-x  2.0 unx      516 b- defN 24-Feb-07 09:17 spark_batch/data/shp2024/BML_HADM_AS.sbn
 -rwxr-xr-x  2.0 unx      164 b- defN 24-Feb-07 09:17 spark_batch/data/shp2024/BML_HADM_AS.sbx
 -rwxr-xr-x  2.0 unx   136876 b- defN 24-Feb-07 09:17 spark_batch/data/shp2024/BML_HADM_AS.shp
 -rwxr-xr-x  2.0 unx     7908 b- defN 24-Feb-07 09:17 spark_batch/data/shp2024/BML_HADM_AS.shp.xml
 -rwxr-xr-x  2.0 unx      396 b- defN 24-Feb-07 09:17 spark_batch/data/shp2024/BML_HADM_AS.shx
 -rw-r--r--  2.0 unx        0 b- defN 24-Feb-07 09:17 spark_batch/lib/__init__.py
--rw-r--r--  2.0 unx     7885 b- defN 24-Feb-07 09:17 spark_batch/lib/csv_table_manager.py
+-rw-r--r--  2.0 unx     8050 b- defN 24-Apr-08 09:13 spark_batch/lib/csv_table_manager.py
 -rw-r--r--  2.0 unx    16092 b- defN 24-Feb-07 09:17 spark_batch/lib/delta_table_manager.py
--rw-r--r--  2.0 unx    22383 b- defN 24-Mar-25 04:10 spark_batch/lib/elt_manager.py
--rw-r--r--  2.0 unx     5521 b- defN 24-Mar-25 05:58 spark_batch/lib/json_table_manager.py
+-rw-r--r--  2.0 unx    22385 b- defN 24-Apr-08 09:31 spark_batch/lib/elt_manager.py
+-rw-r--r--  2.0 unx     5591 b- defN 24-Apr-08 09:02 spark_batch/lib/json_table_manager.py
 -rw-r--r--  2.0 unx     5256 b- defN 24-Feb-07 09:17 spark_batch/lib/mariadb_table_manager.py
 -rw-r--r--  2.0 unx     5525 b- defN 24-Feb-07 09:17 spark_batch/lib/oracle_table_manager.py
 -rw-r--r--  2.0 unx    10391 b- defN 24-Feb-07 09:17 spark_batch/lib/postgresql_table_manager.py
 -rw-r--r--  2.0 unx     2483 b- defN 24-Feb-07 09:17 spark_batch/lib/pxlogger.py
 -rw-r--r--  2.0 unx     5269 b- defN 24-Feb-07 09:17 spark_batch/lib/resource_manager.py
 -rw-r--r--  2.0 unx     2324 b- defN 24-Feb-07 09:17 spark_batch/lib/spark_session.py
 -rw-r--r--  2.0 unx     1860 b- defN 24-Feb-07 09:17 spark_batch/lib/udf_aes_cipher.py
 -rw-r--r--  2.0 unx     5709 b- defN 24-Feb-07 09:17 spark_batch/lib/udf_geo_converter.py
 -rw-r--r--  2.0 unx     4335 b- defN 24-Feb-07 09:17 spark_batch/lib/util.py
--rw-r--r--  2.0 unx      388 b- defN 24-Mar-25 06:07 spark_batch-2.6.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 24-Mar-25 06:07 spark_batch-2.6.dist-info/WHEEL
--rw-r--r--  2.0 unx       12 b- defN 24-Mar-25 06:07 spark_batch-2.6.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     4201 b- defN 24-Mar-25 06:07 spark_batch-2.6.dist-info/RECORD
-46 files, 671415 bytes uncompressed, 395512 bytes compressed:  41.1%
+-rw-r--r--  2.0 unx      388 b- defN 24-Apr-08 09:32 spark_batch-2.7.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-Apr-08 09:32 spark_batch-2.7.dist-info/WHEEL
+-rw-r--r--  2.0 unx       12 b- defN 24-Apr-08 09:32 spark_batch-2.7.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx     4201 b- defN 24-Apr-08 09:32 spark_batch-2.7.dist-info/RECORD
+46 files, 671652 bytes uncompressed, 395560 bytes compressed:  41.1%
```

## zipnote {}

```diff
@@ -120,20 +120,20 @@
 
 Filename: spark_batch/lib/udf_geo_converter.py
 Comment: 
 
 Filename: spark_batch/lib/util.py
 Comment: 
 
-Filename: spark_batch-2.6.dist-info/METADATA
+Filename: spark_batch-2.7.dist-info/METADATA
 Comment: 
 
-Filename: spark_batch-2.6.dist-info/WHEEL
+Filename: spark_batch-2.7.dist-info/WHEEL
 Comment: 
 
-Filename: spark_batch-2.6.dist-info/top_level.txt
+Filename: spark_batch-2.7.dist-info/top_level.txt
 Comment: 
 
-Filename: spark_batch-2.6.dist-info/RECORD
+Filename: spark_batch-2.7.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## spark_batch/lib/csv_table_manager.py

```diff
@@ -11,14 +11,15 @@
 
 :param spark: Spark 세션.
 :param default_bucket: Delta 테이블이 저장된 기본 S3 버킷.
 :param default_dpath: Delta 테이블의 기본 dpath 또는 하위 디렉토리.
 """
 
 class CSVTableManager:
+    # s3_endpoint, s3_access_key, s3_secret_key, s3_ssl_verify, archive_bucket, archive_dpath, bucket=topic, dpath=dpath)
     def __init__(self, spark, s3_endpoint, s3_access_key, s3_secret_key, s3_ssl_verify, archive_bucket, archive_dpath, bucket, dpath):
         self.spark = spark
         self.default_bucket = bucket
         self.default_dpath = dpath
         self.archive_bucket = archive_bucket
         self.archive_dpath = archive_dpath
         self.logger = CustomLogger("CSVTableManager")
@@ -111,21 +112,21 @@
             self.loadTables(tableNames, bucket=bucket, dpath=dpath, customSchemas=customSchemas, header=header, infer_schema=infer_schema, delemeter=delemeter, charset=charset)
 
         csv_df = self.spark.sql(query)
 
         return csv_df
 
     def archive(self, file_name, bucket=None, dpath=None):
-        self.logger.debug(("archive = ", file_name, bucket, dpath))
-                
         if bucket is None:
             bucket = self.default_bucket
         if dpath is None:
             dpath = self.default_dpath
 
+        self.logger.debug(("archive = ", file_name, bucket, dpath))
+
         if self.archive_bucket is None: # No Archiving
             return 
 
         try:
             # S3 버킷 내 파일 목록 가져오기
             objects = self.s3_client.list_objects_v2(Bucket=bucket, Prefix=dpath)
             file_list = [obj['Key'] for obj in objects.get('Contents', [])]
@@ -133,15 +134,15 @@
             # file_name 패턴에 매칭되는 파일 목록 필터링
             matching_files = [file[len(dpath) + 1:] for file in file_list if fnmatch.fnmatchcase(file[len(dpath) + 1:], file_name)]
             self.logger.debug(("archive matching files=", matching_files))
             # 각 파일을 복제하고 삭제
             for filename in matching_files:
 
                 #destination_key = f'{dpath}/archive/{source_key}'  # 변경 필요
-                destination_key = self._getArchivePath(filename, dpath)
+                destination_key = self._getArchivePath(filename)
                 source_key = filename if dpath is None else f'{dpath}/{filename}'
                 self.copy_and_delete(bucket, source_key, destination_key)
 
         except Exception as e:
             self.logger.error(("Error moving file to archive:", e))
 
     def copy_and_delete(self, bucket, source_key, destination_key):
@@ -160,14 +161,17 @@
         except Exception as e:
             self.logger.error(("Error moving file to archive:", e))
 
     def _getArchivePath(self, tableName, dpath=None) :
         # 아카이브 경로 초기화
         archive_path = ""
 
+        if dpath is None:
+            dpath = self.archive_dpath
+
         # dpath가 있으면 추가
         if dpath:
             archive_path += dpath + "/"
 
         # 현재 날짜 폴더 추가
         current_date = datetime.now().strftime("%Y%m%d")
         archive_path += current_date + "/"
```

## spark_batch/lib/elt_manager.py

```diff
@@ -233,32 +233,32 @@
                 self.logger.info(f"Source Loading Chunk : {sourceInfo} / seq={math.ceil(offset / chunk_size + 1)} offset={offset:,} chunk_size={chunk_read_size:,} / elipsed={timer.tab():.2f}")
 
                 self.logger.info(f"Target Saving Chunk : {targetInfo} / elipsed={timer.tab():.2f}")
 
                 offset += chunk_read_size
 
                 if self.source_tm.getType() in ["csv", "json"]:
-                    self.source_tm.archive(sourceTable)
                     source_df.cache()  # For servicing after archiving
+                    self.source_tm.archive(sourceTable)
 
                 if self.source_tm.getType() == "delta" and self.target_tm.getType() == "delta":  # Full loaded already
                     chunk_read_size = 0
 
             self.logger.info(f"Source Loading Count : {sourceInfo} ({offset:,})")
 
             target_df = self.target_tm.loadTable(targetTable)
             self.logger.info(f"Target Saving Count : {targetInfo} ({target_df.count():,})")
 
             valid = offset == target_df.count() + tot_cleaned_count - target_df_org_count
 
             self.logger.info(f"ETL/FL Done : [ {targetInfo} / {valid} ({offset:,}, {target_df.count() - target_df_org_count:,}, {tot_cleaned_count:,}) / {timer.elapsed():,.2f} ]")
             self._record(self.record_full_header, source_object, target_object, valid, offset, target_df.count(), tot_cleaned_count, None, timer.elapsed())
 
-            if self.source_tm.getType() in ["csv", "json"]:
-                source_df = self.source_tm.loadTable(sourceTable)
+            #if self.source_tm.getType() in ["csv", "json"]:
+            #    source_df = self.source_tm.loadTable(sourceTable)
 
             return (source_df, target_df, valid)
         
         except Exception as e1:
             self.logger.error(f"ETL/FL Error : {e1}")
             self.logger.error(f"ETL/FL Done : [ {targetInfo} / False (-1, -1, -1) / {timer.elapsed():,.2f} ]")
             self._record(self.record_full_header, source_object, target_object, False, -1, -1, -1, str(e1)[:255], timer.elapsed())
@@ -338,16 +338,16 @@
             target_read_size = self.target_tm.countTableCondition(target_condition, target_object)
             valid = source_read_size == target_read_size + cleaned_count
             self.logger.info(f"ETL/IC Done : [ {targetInfo} / {valid} ({source_read_size:,}, {target_read_size:,}, {cleaned_count:,}) / {timer.elapsed():,.2f} ]")
             self._record(self.record_inc_header, source_objects, target_object, valid, source_read_size, target_read_size, cleaned_count, None, timer.elapsed())
 
             if self.source_tm.getType() in ["csv", "json"]:
                 for sourceTable in source_objects:
-                    self.source_tm.archive(sourceTable)        
                     source_df.cache()  # For servicing after archiving
+                    self.source_tm.archive(sourceTable)        
 
 
             return (source_df, target_df, valid)
         
         except Exception as e1:
             self.logger.error(f"ETL/IC Error : {e1}")
             self.logger.error(f"ETL/IC Done : [ {targetInfo} / False (-1, -1, -1) / {timer.elapsed():,.2f} ]")
```

## spark_batch/lib/json_table_manager.py

```diff
@@ -91,14 +91,17 @@
         self.logger.debug(("archive = ", file_name, bucket, dpath))
                 
         sBucket = self._getBucket(bucket)
         sKey = self._getPath(file_name, dpath)
     
         archive_bucket = self.archive_bucket
         destination_key = self._getArchivePath(file_name, dpath)
+
+        if archive_bucket is None: # No Archiving
+            return
         
         try:
             # Copy the file to the archive bucket
             self.s3_client.copy_object(CopySource={'Bucket': sBucket, 'Key': sKey },
                                        Bucket=archive_bucket,
                                        Key=destination_key)
```

## Comparing `spark_batch-2.6.dist-info/RECORD` & `spark_batch-2.7.dist-info/RECORD`

 * *Files 10% similar despite different names*

```diff
@@ -23,24 +23,24 @@
 spark_batch/data/shp2024/BML_HADM_AS.dbf,sha256=axtEtnuA6e6OZMyw3gwawBQ3H_uTs9lYKeWJk6I752U,4170
 spark_batch/data/shp2024/BML_HADM_AS.sbn,sha256=x08C2Me5oA0JbqmE4bB-_D3Nstf_hBI2E_Wzh_Ryjho,516
 spark_batch/data/shp2024/BML_HADM_AS.sbx,sha256=qrnx1EymDl520IpEXjZJFpZEi470za-ezuHjvxoXXGo,164
 spark_batch/data/shp2024/BML_HADM_AS.shp,sha256=BmwmyOaT2W7XOCUbPwu0EGTcUIFLhuvkpMCLOImFGT0,136876
 spark_batch/data/shp2024/BML_HADM_AS.shp.xml,sha256=6Ef72zmvTgi2WI4CnsCYPBOIQcAjCNQ6egppFVxDHuA,7908
 spark_batch/data/shp2024/BML_HADM_AS.shx,sha256=XV3jn51r5xjUuOcC7NPDu4G6J_PqPaARso57dvzsiBg,396
 spark_batch/lib/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-spark_batch/lib/csv_table_manager.py,sha256=Wy-X0fgEeBTE5u66wT8qCZWNgFrjYfhv0WEN0TKly3g,7885
+spark_batch/lib/csv_table_manager.py,sha256=BSncpQrDGU4c2ek-LzfMBBOEgRq7vtrgPHcCqs0GytM,8050
 spark_batch/lib/delta_table_manager.py,sha256=ZMmxhCOsspmONsu8cDHW4xD3nRBKDEiQPiSlSjngKxo,16092
-spark_batch/lib/elt_manager.py,sha256=BTnoFy24k0kU5q6ihDLnbs2v-FQ6635SEM9x1gzkHxs,22383
-spark_batch/lib/json_table_manager.py,sha256=LMEXTToowrr5pJfdB3EkKhV_axQ-URNj_8cVOIoXdU0,5521
+spark_batch/lib/elt_manager.py,sha256=1HPFY5JEZtKY_FuCTAsGA16VLYzmKjC-ObCtALkZ1Ow,22385
+spark_batch/lib/json_table_manager.py,sha256=DUelfGO40Jf-9TuFwELL60rMuGofHs-f9GvVdarxhnM,5591
 spark_batch/lib/mariadb_table_manager.py,sha256=xTXi8yRKxlCv_i6gD-uvdF4b70rSbU1K6vigwpMgxwY,5256
 spark_batch/lib/oracle_table_manager.py,sha256=KbL8W1oqrGii1WrvF9qMUfUxvnvMArkL1mdDK8DgWQc,5525
 spark_batch/lib/postgresql_table_manager.py,sha256=3XlxvXT95sUmttO_Aiq4B7Q9XMWY0PnMKBubQKnf9Do,10391
 spark_batch/lib/pxlogger.py,sha256=wlu-0LfxaayGW1y2Wy5i6cDBh4GX-IaV--1ajtvLOJ4,2483
 spark_batch/lib/resource_manager.py,sha256=AtvGna8yawgNaLCgM_r_PKbdDD-kswJKgkVv4W6DqWc,5269
 spark_batch/lib/spark_session.py,sha256=Gv1xnnGFMERxnKNuWfX9frmcZh5sNx4AWk9rRgkHg7U,2324
 spark_batch/lib/udf_aes_cipher.py,sha256=_Ua9wlX7WDrAEZWfK11EUwCa3HExApzj9tXM3rMuHAs,1860
 spark_batch/lib/udf_geo_converter.py,sha256=NIZFgvZuvPDRaqwNDoFK4FrykgK9hbnRuxY6D6XjZ_k,5709
 spark_batch/lib/util.py,sha256=gI4r7OCcFQ35KpTLIHFVw36e5d-Absc6ufA3pHXddI8,4335
-spark_batch-2.6.dist-info/METADATA,sha256=g6sW7Qy3PYFfZKCxajwHhjNHZMgWHi7eaXZPiDCv4N0,388
-spark_batch-2.6.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
-spark_batch-2.6.dist-info/top_level.txt,sha256=4CXvktYZ-h_dAJ9jETz_09zaMrW0RBzckuDwWC4BGqM,12
-spark_batch-2.6.dist-info/RECORD,,
+spark_batch-2.7.dist-info/METADATA,sha256=gmW-ncCc1__Pnt11Q17X-XZlgKeOfSYboX07h517yRA,388
+spark_batch-2.7.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
+spark_batch-2.7.dist-info/top_level.txt,sha256=4CXvktYZ-h_dAJ9jETz_09zaMrW0RBzckuDwWC4BGqM,12
+spark_batch-2.7.dist-info/RECORD,,
```

