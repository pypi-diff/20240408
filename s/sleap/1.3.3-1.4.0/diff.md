# Comparing `tmp/sleap-1.3.3-py3-none-any.whl.zip` & `tmp/sleap-1.4.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,178 +1,178 @@
-Zip file size: 1178008 bytes, number of entries: 176
--rw-r--r--  2.0 unx      773 b- defN 23-Sep-15 22:12 sleap/__init__.py
--rw-r--r--  2.0 unx     4109 b- defN 23-Sep-15 22:12 sleap/diagnostic.py
--rw-r--r--  2.0 unx    63933 b- defN 23-Sep-15 22:12 sleap/instance.py
--rw-r--r--  2.0 unx     8316 b- defN 23-Sep-15 22:12 sleap/message.py
--rw-r--r--  2.0 unx     2206 b- defN 23-Sep-15 22:12 sleap/prefs.py
--rw-r--r--  2.0 unx     4653 b- defN 23-Sep-15 22:12 sleap/rangelist.py
--rw-r--r--  2.0 unx    41735 b- defN 23-Sep-15 22:12 sleap/skeleton.py
--rw-r--r--  2.0 unx    12005 b- defN 23-Sep-15 22:12 sleap/util.py
--rw-r--r--  2.0 unx      938 b- defN 23-Sep-15 22:12 sleap/version.py
--rw-r--r--  2.0 unx     1181 b- defN 23-Sep-15 22:12 sleap/config/colors.yaml
--rw-r--r--  2.0 unx      113 b- defN 23-Sep-15 22:12 sleap/config/head_type_form.yaml
--rw-r--r--  2.0 unx      479 b- defN 23-Sep-15 22:12 sleap/config/labeled_clip_form.yaml
--rw-r--r--  2.0 unx        0 b- defN 23-Sep-15 22:12 sleap/config/path_prefixes.yaml
--rw-r--r--  2.0 unx    22045 b- defN 23-Sep-15 22:12 sleap/config/pipeline_form.yaml
--rw-r--r--  2.0 unx      969 b- defN 23-Sep-15 22:12 sleap/config/shortcuts.yaml
--rw-r--r--  2.0 unx     4411 b- defN 23-Sep-15 22:12 sleap/config/suggestions.yaml
--rw-r--r--  2.0 unx    33738 b- defN 23-Sep-15 22:12 sleap/config/training_editor_form.yaml
--rw-r--r--  2.0 unx        0 b- defN 23-Sep-15 22:12 sleap/gui/__init__.py
--rw-r--r--  2.0 unx    59978 b- defN 23-Sep-15 22:12 sleap/gui/app.py
--rw-r--r--  2.0 unx   423849 b- defN 23-Sep-15 22:12 sleap/gui/background.png
--rw-r--r--  2.0 unx    10027 b- defN 23-Sep-15 22:12 sleap/gui/color.py
--rw-r--r--  2.0 unx   112097 b- defN 23-Sep-15 22:12 sleap/gui/commands.py
--rw-r--r--  2.0 unx    23106 b- defN 23-Sep-15 22:12 sleap/gui/dataviews.py
--rw-r--r--  2.0 unx   123167 b- defN 23-Sep-15 22:12 sleap/gui/icon.png
--rw-r--r--  2.0 unx     4428 b- defN 23-Sep-15 22:12 sleap/gui/shortcuts.py
--rw-r--r--  2.0 unx     6614 b- defN 23-Sep-15 22:12 sleap/gui/state.py
--rw-r--r--  2.0 unx    12294 b- defN 23-Sep-15 22:12 sleap/gui/suggestions.py
--rw-r--r--  2.0 unx     5490 b- defN 23-Sep-15 22:12 sleap/gui/web.py
--rw-r--r--  2.0 unx        0 b- defN 23-Sep-15 22:12 sleap/gui/dialogs/__init__.py
--rw-r--r--  2.0 unx     7869 b- defN 23-Sep-15 22:12 sleap/gui/dialogs/delete.py
--rw-r--r--  2.0 unx      857 b- defN 23-Sep-15 22:12 sleap/gui/dialogs/export_clip.py
--rw-r--r--  2.0 unx     3956 b- defN 23-Sep-15 22:12 sleap/gui/dialogs/filedialog.py
--rw-r--r--  2.0 unx    33224 b- defN 23-Sep-15 22:12 sleap/gui/dialogs/formbuilder.py
--rw-r--r--  2.0 unx    23258 b- defN 23-Sep-15 22:12 sleap/gui/dialogs/importvideos.py
--rw-r--r--  2.0 unx    20140 b- defN 23-Sep-15 22:12 sleap/gui/dialogs/merge.py
--rw-r--r--  2.0 unx      684 b- defN 23-Sep-15 22:12 sleap/gui/dialogs/message.py
--rw-r--r--  2.0 unx    12059 b- defN 23-Sep-15 22:12 sleap/gui/dialogs/metrics.py
--rw-r--r--  2.0 unx     7467 b- defN 23-Sep-15 22:12 sleap/gui/dialogs/missingfiles.py
--rw-r--r--  2.0 unx     1052 b- defN 23-Sep-15 22:12 sleap/gui/dialogs/query.py
--rw-r--r--  2.0 unx     3575 b- defN 23-Sep-15 22:12 sleap/gui/dialogs/shortcuts.py
--rw-r--r--  2.0 unx        0 b- defN 23-Sep-15 22:12 sleap/gui/learning/__init__.py
--rw-r--r--  2.0 unx    17308 b- defN 23-Sep-15 22:12 sleap/gui/learning/configs.py
--rw-r--r--  2.0 unx     5923 b- defN 23-Sep-15 22:12 sleap/gui/learning/datagen.py
--rw-r--r--  2.0 unx    51453 b- defN 23-Sep-15 22:12 sleap/gui/learning/dialog.py
--rw-r--r--  2.0 unx     7832 b- defN 23-Sep-15 22:12 sleap/gui/learning/receptivefield.py
--rw-r--r--  2.0 unx    30015 b- defN 23-Sep-15 22:12 sleap/gui/learning/runners.py
--rw-r--r--  2.0 unx     7803 b- defN 23-Sep-15 22:12 sleap/gui/learning/scopedkeydict.py
--rw-r--r--  2.0 unx        0 b- defN 23-Sep-15 22:12 sleap/gui/overlays/__init__.py
--rw-r--r--  2.0 unx     1457 b- defN 23-Sep-15 22:12 sleap/gui/overlays/anchors.py
--rw-r--r--  2.0 unx     7368 b- defN 23-Sep-15 22:12 sleap/gui/overlays/base.py
--rw-r--r--  2.0 unx     5400 b- defN 23-Sep-15 22:12 sleap/gui/overlays/confmaps.py
--rw-r--r--  2.0 unx     2033 b- defN 23-Sep-15 22:12 sleap/gui/overlays/instance.py
--rw-r--r--  2.0 unx    10117 b- defN 23-Sep-15 22:12 sleap/gui/overlays/pafs.py
--rw-r--r--  2.0 unx     8998 b- defN 23-Sep-15 22:12 sleap/gui/overlays/tracks.py
--rw-r--r--  2.0 unx        0 b- defN 23-Sep-15 22:12 sleap/gui/widgets/__init__.py
--rw-r--r--  2.0 unx    18522 b- defN 23-Sep-15 22:12 sleap/gui/widgets/docks.py
--rw-r--r--  2.0 unx     4567 b- defN 23-Sep-15 22:12 sleap/gui/widgets/imagedir.py
--rw-r--r--  2.0 unx    22348 b- defN 23-Sep-15 22:12 sleap/gui/widgets/monitor.py
--rw-r--r--  2.0 unx      745 b- defN 23-Sep-15 22:12 sleap/gui/widgets/mpl.py
--rw-r--r--  2.0 unx     3056 b- defN 23-Sep-15 22:12 sleap/gui/widgets/multicheck.py
--rw-r--r--  2.0 unx    43496 b- defN 23-Sep-15 22:12 sleap/gui/widgets/slider.py
--rw-r--r--  2.0 unx    22322 b- defN 23-Sep-15 22:12 sleap/gui/widgets/training_monitor.py
--rw-r--r--  2.0 unx    83393 b- defN 23-Sep-15 22:12 sleap/gui/widgets/video.py
--rw-r--r--  2.0 unx     3522 b- defN 23-Sep-15 22:12 sleap/gui/widgets/views.py
--rw-r--r--  2.0 unx        0 b- defN 23-Sep-15 22:12 sleap/info/__init__.py
--rw-r--r--  2.0 unx     8775 b- defN 23-Sep-15 22:12 sleap/info/align.py
--rw-r--r--  2.0 unx    25741 b- defN 23-Sep-15 22:12 sleap/info/feature_suggestions.py
--rw-r--r--  2.0 unx     4603 b- defN 23-Sep-15 22:12 sleap/info/labels.py
--rw-r--r--  2.0 unx    10493 b- defN 23-Sep-15 22:12 sleap/info/metrics.py
--rw-r--r--  2.0 unx     9118 b- defN 23-Sep-15 22:12 sleap/info/summary.py
--rw-r--r--  2.0 unx     1917 b- defN 23-Sep-15 22:12 sleap/info/trackcleaner.py
--rw-r--r--  2.0 unx    16626 b- defN 23-Sep-15 22:12 sleap/info/write_tracking_h5.py
--rw-r--r--  2.0 unx        0 b- defN 23-Sep-15 22:12 sleap/io/__init__.py
--rw-r--r--  2.0 unx     7174 b- defN 23-Sep-15 22:12 sleap/io/asyncvideo.py
--rw-r--r--  2.0 unx     6312 b- defN 23-Sep-15 22:12 sleap/io/convert.py
--rw-r--r--  2.0 unx   107724 b- defN 23-Sep-15 22:12 sleap/io/dataset.py
--rw-r--r--  2.0 unx     9189 b- defN 23-Sep-15 22:12 sleap/io/legacy.py
--rw-r--r--  2.0 unx     4844 b- defN 23-Sep-15 22:12 sleap/io/pathutils.py
--rw-r--r--  2.0 unx    55054 b- defN 23-Sep-15 22:12 sleap/io/video.py
--rw-r--r--  2.0 unx     3104 b- defN 23-Sep-15 22:12 sleap/io/videowriter.py
--rw-r--r--  2.0 unx    23792 b- defN 23-Sep-15 22:12 sleap/io/visuals.py
--rw-r--r--  2.0 unx       30 b- defN 23-Sep-15 22:12 sleap/io/format/__init__.py
--rw-r--r--  2.0 unx     2743 b- defN 23-Sep-15 22:12 sleap/io/format/adaptor.py
--rw-r--r--  2.0 unx    16437 b- defN 23-Sep-15 22:12 sleap/io/format/alphatracker.py
--rw-r--r--  2.0 unx     6871 b- defN 23-Sep-15 22:12 sleap/io/format/coco.py
--rw-r--r--  2.0 unx     1885 b- defN 23-Sep-15 22:12 sleap/io/format/csv.py
--rw-r--r--  2.0 unx    10721 b- defN 23-Sep-15 22:12 sleap/io/format/deeplabcut.py
--rw-r--r--  2.0 unx     2540 b- defN 23-Sep-15 22:12 sleap/io/format/deepposekit.py
--rw-r--r--  2.0 unx     3808 b- defN 23-Sep-15 22:12 sleap/io/format/dispatch.py
--rw-r--r--  2.0 unx     3241 b- defN 23-Sep-15 22:12 sleap/io/format/filehandle.py
--rw-r--r--  2.0 unx     1064 b- defN 23-Sep-15 22:12 sleap/io/format/genericjson.py
--rw-r--r--  2.0 unx    20757 b- defN 23-Sep-15 22:12 sleap/io/format/hdf5.py
--rw-r--r--  2.0 unx    21088 b- defN 23-Sep-15 22:12 sleap/io/format/labels_json.py
--rw-r--r--  2.0 unx     4109 b- defN 23-Sep-15 22:12 sleap/io/format/leap_matlab.py
--rw-r--r--  2.0 unx     6361 b- defN 23-Sep-15 22:12 sleap/io/format/main.py
--rw-r--r--  2.0 unx    14132 b- defN 23-Sep-15 22:12 sleap/io/format/ndx_pose.py
--rw-r--r--  2.0 unx    17846 b- defN 23-Sep-15 22:12 sleap/io/format/nix.py
--rw-r--r--  2.0 unx     4833 b- defN 23-Sep-15 22:12 sleap/io/format/sleap_analysis.py
--rw-r--r--  2.0 unx      965 b- defN 23-Sep-15 22:12 sleap/io/format/text.py
--rw-r--r--  2.0 unx      449 b- defN 23-Sep-15 22:12 sleap/nn/__init__.py
--rw-r--r--  2.0 unx    10013 b- defN 23-Sep-15 22:12 sleap/nn/callbacks.py
--rw-r--r--  2.0 unx    30099 b- defN 23-Sep-15 22:12 sleap/nn/evals.py
--rw-r--r--  2.0 unx    18212 b- defN 23-Sep-15 22:12 sleap/nn/heads.py
--rw-r--r--  2.0 unx     9177 b- defN 23-Sep-15 22:12 sleap/nn/identity.py
--rw-r--r--  2.0 unx   224133 b- defN 23-Sep-15 22:12 sleap/nn/inference.py
--rw-r--r--  2.0 unx     5673 b- defN 23-Sep-15 22:12 sleap/nn/losses.py
--rw-r--r--  2.0 unx    13278 b- defN 23-Sep-15 22:12 sleap/nn/model.py
--rw-r--r--  2.0 unx    74698 b- defN 23-Sep-15 22:12 sleap/nn/paf_grouping.py
--rw-r--r--  2.0 unx    25718 b- defN 23-Sep-15 22:12 sleap/nn/peak_finding.py
--rw-r--r--  2.0 unx     6885 b- defN 23-Sep-15 22:12 sleap/nn/system.py
--rw-r--r--  2.0 unx    54310 b- defN 23-Sep-15 22:12 sleap/nn/tracking.py
--rw-r--r--  2.0 unx    75381 b- defN 23-Sep-15 22:12 sleap/nn/training.py
--rw-r--r--  2.0 unx     5449 b- defN 23-Sep-15 22:12 sleap/nn/utils.py
--rw-r--r--  2.0 unx    11047 b- defN 23-Sep-15 22:12 sleap/nn/viz.py
--rw-r--r--  2.0 unx      681 b- defN 23-Sep-15 22:12 sleap/nn/architectures/__init__.py
--rw-r--r--  2.0 unx      752 b- defN 23-Sep-15 22:12 sleap/nn/architectures/common.py
--rw-r--r--  2.0 unx    27650 b- defN 23-Sep-15 22:12 sleap/nn/architectures/encoder_decoder.py
--rw-r--r--  2.0 unx    11993 b- defN 23-Sep-15 22:12 sleap/nn/architectures/hourglass.py
--rw-r--r--  2.0 unx    19077 b- defN 23-Sep-15 22:12 sleap/nn/architectures/hrnet.py
--rw-r--r--  2.0 unx     5326 b- defN 23-Sep-15 22:12 sleap/nn/architectures/leap.py
--rw-r--r--  2.0 unx     9359 b- defN 23-Sep-15 22:12 sleap/nn/architectures/pretrained_encoders.py
--rw-r--r--  2.0 unx    27951 b- defN 23-Sep-15 22:12 sleap/nn/architectures/resnet.py
--rw-r--r--  2.0 unx    11270 b- defN 23-Sep-15 22:12 sleap/nn/architectures/unet.py
--rw-r--r--  2.0 unx    12302 b- defN 23-Sep-15 22:12 sleap/nn/architectures/upsampling.py
--rw-r--r--  2.0 unx     1023 b- defN 23-Sep-15 22:12 sleap/nn/config/__init__.py
--rw-r--r--  2.0 unx     8736 b- defN 23-Sep-15 22:12 sleap/nn/config/data.py
--rw-r--r--  2.0 unx    33172 b- defN 23-Sep-15 22:12 sleap/nn/config/model.py
--rw-r--r--  2.0 unx    13792 b- defN 23-Sep-15 22:12 sleap/nn/config/optimization.py
--rw-r--r--  2.0 unx    11154 b- defN 23-Sep-15 22:12 sleap/nn/config/outputs.py
--rw-r--r--  2.0 unx     6239 b- defN 23-Sep-15 22:12 sleap/nn/config/training_job.py
--rw-r--r--  2.0 unx     2769 b- defN 23-Sep-15 22:12 sleap/nn/config/utils.py
--rw-r--r--  2.0 unx      456 b- defN 23-Sep-15 22:12 sleap/nn/data/__init__.py
--rw-r--r--  2.0 unx    16665 b- defN 23-Sep-15 22:12 sleap/nn/data/augmentation.py
--rw-r--r--  2.0 unx    23439 b- defN 23-Sep-15 22:12 sleap/nn/data/confidence_maps.py
--rw-r--r--  2.0 unx    12553 b- defN 23-Sep-15 22:12 sleap/nn/data/dataset_ops.py
--rw-r--r--  2.0 unx    14764 b- defN 23-Sep-15 22:12 sleap/nn/data/edge_maps.py
--rw-r--r--  2.0 unx     4974 b- defN 23-Sep-15 22:12 sleap/nn/data/general.py
--rw-r--r--  2.0 unx     3059 b- defN 23-Sep-15 22:12 sleap/nn/data/grouping.py
--rw-r--r--  2.0 unx     7857 b- defN 23-Sep-15 22:12 sleap/nn/data/identity.py
--rw-r--r--  2.0 unx    12356 b- defN 23-Sep-15 22:12 sleap/nn/data/inference.py
--rw-r--r--  2.0 unx     8058 b- defN 23-Sep-15 22:12 sleap/nn/data/instance_centroids.py
--rw-r--r--  2.0 unx    23401 b- defN 23-Sep-15 22:12 sleap/nn/data/instance_cropping.py
--rw-r--r--  2.0 unx    14260 b- defN 23-Sep-15 22:12 sleap/nn/data/normalization.py
--rw-r--r--  2.0 unx     3291 b- defN 23-Sep-15 22:12 sleap/nn/data/offset_regression.py
--rw-r--r--  2.0 unx    48997 b- defN 23-Sep-15 22:12 sleap/nn/data/pipelines.py
--rw-r--r--  2.0 unx    18846 b- defN 23-Sep-15 22:12 sleap/nn/data/providers.py
--rw-r--r--  2.0 unx    21216 b- defN 23-Sep-15 22:12 sleap/nn/data/resizing.py
--rw-r--r--  2.0 unx     9756 b- defN 23-Sep-15 22:12 sleap/nn/data/training.py
--rw-r--r--  2.0 unx     6936 b- defN 23-Sep-15 22:12 sleap/nn/data/utils.py
--rw-r--r--  2.0 unx        0 b- defN 23-Sep-15 22:12 sleap/nn/tracker/__init__.py
--rw-r--r--  2.0 unx    19176 b- defN 23-Sep-15 22:12 sleap/nn/tracker/components.py
--rw-r--r--  2.0 unx    24539 b- defN 23-Sep-15 22:12 sleap/nn/tracker/kalman.py
--rw-r--r--  2.0 unx    44761 b- defN 23-Sep-15 22:12 sleap/skeletons/bees.json
--rw-r--r--  2.0 unx    35218 b- defN 23-Sep-15 22:12 sleap/skeletons/flies13.json
--rw-r--r--  2.0 unx    24453 b- defN 23-Sep-15 22:12 sleap/skeletons/fly32.json
--rw-r--r--  2.0 unx    49861 b- defN 23-Sep-15 22:12 sleap/skeletons/gerbils.json
--rw-r--r--  2.0 unx    39954 b- defN 23-Sep-15 22:12 sleap/skeletons/mice_hc.json
--rw-r--r--  2.0 unx    16119 b- defN 23-Sep-15 22:12 sleap/skeletons/mice_of.json
--rwxr-xr-x  2.0 unx     4477 b- defN 23-Sep-15 22:12 sleap/training_profiles/baseline.centroid.json
--rw-r--r--  2.0 unx     4640 b- defN 23-Sep-15 22:12 sleap/training_profiles/baseline_large_rf.bottomup.json
--rw-r--r--  2.0 unx     4344 b- defN 23-Sep-15 22:12 sleap/training_profiles/baseline_large_rf.single.json
--rw-r--r--  2.0 unx     4381 b- defN 23-Sep-15 22:12 sleap/training_profiles/baseline_large_rf.topdown.json
--rw-r--r--  2.0 unx     4639 b- defN 23-Sep-15 22:12 sleap/training_profiles/baseline_medium_rf.bottomup.json
--rw-r--r--  2.0 unx     4343 b- defN 23-Sep-15 22:12 sleap/training_profiles/baseline_medium_rf.single.json
--rwxr-xr-x  2.0 unx     4522 b- defN 23-Sep-15 22:12 sleap/training_profiles/baseline_medium_rf.topdown.json
--rw-r--r--  2.0 unx     4552 b- defN 23-Sep-15 22:12 sleap/training_profiles/pretrained.bottomup.json
--rw-r--r--  2.0 unx     4259 b- defN 23-Sep-15 22:12 sleap/training_profiles/pretrained.centroid.json
--rw-r--r--  2.0 unx     4256 b- defN 23-Sep-15 22:12 sleap/training_profiles/pretrained.single.json
--rw-r--r--  2.0 unx     4293 b- defN 23-Sep-15 22:12 sleap/training_profiles/pretrained.topdown.json
--rw-r--r--  2.0 unx      556 b- defN 23-Sep-15 22:18 sleap-1.3.3.dist-info/AUTHORS
--rw-r--r--  2.0 unx     1748 b- defN 23-Sep-15 22:18 sleap-1.3.3.dist-info/LICENSE
--rw-r--r--  2.0 unx    17463 b- defN 23-Sep-15 22:18 sleap-1.3.3.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Sep-15 22:18 sleap-1.3.3.dist-info/WHEEL
--rw-r--r--  2.0 unx      326 b- defN 23-Sep-15 22:18 sleap-1.3.3.dist-info/entry_points.txt
--rw-r--r--  2.0 unx        6 b- defN 23-Sep-15 22:18 sleap-1.3.3.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx    14875 b- defN 23-Sep-15 22:18 sleap-1.3.3.dist-info/RECORD
-176 files, 3244875 bytes uncompressed, 1154878 bytes compressed:  64.4%
+Zip file size: 1180463 bytes, number of entries: 176
+-rw-r--r--  2.0 unx      773 b- defN 24-Apr-08 03:25 sleap/__init__.py
+-rw-r--r--  2.0 unx     4109 b- defN 24-Apr-08 03:25 sleap/diagnostic.py
+-rw-r--r--  2.0 unx    63933 b- defN 24-Apr-08 03:25 sleap/instance.py
+-rw-r--r--  2.0 unx     8316 b- defN 24-Apr-08 03:25 sleap/message.py
+-rw-r--r--  2.0 unx     2206 b- defN 24-Apr-08 03:25 sleap/prefs.py
+-rw-r--r--  2.0 unx     4653 b- defN 24-Apr-08 03:25 sleap/rangelist.py
+-rw-r--r--  2.0 unx    41735 b- defN 24-Apr-08 03:25 sleap/skeleton.py
+-rw-r--r--  2.0 unx    12005 b- defN 24-Apr-08 03:25 sleap/util.py
+-rw-r--r--  2.0 unx      938 b- defN 24-Apr-08 03:25 sleap/version.py
+-rw-r--r--  2.0 unx     1181 b- defN 24-Apr-08 03:25 sleap/config/colors.yaml
+-rw-r--r--  2.0 unx      113 b- defN 24-Apr-08 03:25 sleap/config/head_type_form.yaml
+-rw-r--r--  2.0 unx      582 b- defN 24-Apr-08 03:25 sleap/config/labeled_clip_form.yaml
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-08 03:25 sleap/config/path_prefixes.yaml
+-rw-r--r--  2.0 unx    22063 b- defN 24-Apr-08 03:25 sleap/config/pipeline_form.yaml
+-rw-r--r--  2.0 unx      969 b- defN 24-Apr-08 03:25 sleap/config/shortcuts.yaml
+-rw-r--r--  2.0 unx     4411 b- defN 24-Apr-08 03:25 sleap/config/suggestions.yaml
+-rw-r--r--  2.0 unx    33753 b- defN 24-Apr-08 03:25 sleap/config/training_editor_form.yaml
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-08 03:25 sleap/gui/__init__.py
+-rw-r--r--  2.0 unx    60981 b- defN 24-Apr-08 03:25 sleap/gui/app.py
+-rw-r--r--  2.0 unx   423849 b- defN 24-Apr-08 03:25 sleap/gui/background.png
+-rw-r--r--  2.0 unx    10027 b- defN 24-Apr-08 03:25 sleap/gui/color.py
+-rw-r--r--  2.0 unx   116429 b- defN 24-Apr-08 03:25 sleap/gui/commands.py
+-rw-r--r--  2.0 unx    23106 b- defN 24-Apr-08 03:25 sleap/gui/dataviews.py
+-rw-r--r--  2.0 unx   123167 b- defN 24-Apr-08 03:25 sleap/gui/icon.png
+-rw-r--r--  2.0 unx     4428 b- defN 24-Apr-08 03:25 sleap/gui/shortcuts.py
+-rw-r--r--  2.0 unx     6614 b- defN 24-Apr-08 03:25 sleap/gui/state.py
+-rw-r--r--  2.0 unx    12294 b- defN 24-Apr-08 03:25 sleap/gui/suggestions.py
+-rw-r--r--  2.0 unx     5490 b- defN 24-Apr-08 03:25 sleap/gui/web.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-08 03:25 sleap/gui/dialogs/__init__.py
+-rw-r--r--  2.0 unx     7869 b- defN 24-Apr-08 03:25 sleap/gui/dialogs/delete.py
+-rw-r--r--  2.0 unx      857 b- defN 24-Apr-08 03:25 sleap/gui/dialogs/export_clip.py
+-rw-r--r--  2.0 unx     3956 b- defN 24-Apr-08 03:25 sleap/gui/dialogs/filedialog.py
+-rw-r--r--  2.0 unx    33224 b- defN 24-Apr-08 03:25 sleap/gui/dialogs/formbuilder.py
+-rw-r--r--  2.0 unx    23258 b- defN 24-Apr-08 03:25 sleap/gui/dialogs/importvideos.py
+-rw-r--r--  2.0 unx    20140 b- defN 24-Apr-08 03:25 sleap/gui/dialogs/merge.py
+-rw-r--r--  2.0 unx      684 b- defN 24-Apr-08 03:25 sleap/gui/dialogs/message.py
+-rw-r--r--  2.0 unx    12059 b- defN 24-Apr-08 03:25 sleap/gui/dialogs/metrics.py
+-rw-r--r--  2.0 unx     7467 b- defN 24-Apr-08 03:25 sleap/gui/dialogs/missingfiles.py
+-rw-r--r--  2.0 unx     1052 b- defN 24-Apr-08 03:25 sleap/gui/dialogs/query.py
+-rw-r--r--  2.0 unx     3575 b- defN 24-Apr-08 03:25 sleap/gui/dialogs/shortcuts.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-08 03:25 sleap/gui/learning/__init__.py
+-rw-r--r--  2.0 unx    17308 b- defN 24-Apr-08 03:25 sleap/gui/learning/configs.py
+-rw-r--r--  2.0 unx     5923 b- defN 24-Apr-08 03:25 sleap/gui/learning/datagen.py
+-rw-r--r--  2.0 unx    52442 b- defN 24-Apr-08 03:25 sleap/gui/learning/dialog.py
+-rw-r--r--  2.0 unx     7832 b- defN 24-Apr-08 03:25 sleap/gui/learning/receptivefield.py
+-rw-r--r--  2.0 unx    30015 b- defN 24-Apr-08 03:25 sleap/gui/learning/runners.py
+-rw-r--r--  2.0 unx     7803 b- defN 24-Apr-08 03:25 sleap/gui/learning/scopedkeydict.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-08 03:25 sleap/gui/overlays/__init__.py
+-rw-r--r--  2.0 unx     1457 b- defN 24-Apr-08 03:25 sleap/gui/overlays/anchors.py
+-rw-r--r--  2.0 unx     7347 b- defN 24-Apr-08 03:25 sleap/gui/overlays/base.py
+-rw-r--r--  2.0 unx     5400 b- defN 24-Apr-08 03:25 sleap/gui/overlays/confmaps.py
+-rw-r--r--  2.0 unx     2033 b- defN 24-Apr-08 03:25 sleap/gui/overlays/instance.py
+-rw-r--r--  2.0 unx    10117 b- defN 24-Apr-08 03:25 sleap/gui/overlays/pafs.py
+-rw-r--r--  2.0 unx     8998 b- defN 24-Apr-08 03:25 sleap/gui/overlays/tracks.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-08 03:25 sleap/gui/widgets/__init__.py
+-rw-r--r--  2.0 unx    18522 b- defN 24-Apr-08 03:25 sleap/gui/widgets/docks.py
+-rw-r--r--  2.0 unx     4567 b- defN 24-Apr-08 03:25 sleap/gui/widgets/imagedir.py
+-rw-r--r--  2.0 unx    22348 b- defN 24-Apr-08 03:25 sleap/gui/widgets/monitor.py
+-rw-r--r--  2.0 unx      745 b- defN 24-Apr-08 03:25 sleap/gui/widgets/mpl.py
+-rw-r--r--  2.0 unx     3056 b- defN 24-Apr-08 03:25 sleap/gui/widgets/multicheck.py
+-rw-r--r--  2.0 unx    43496 b- defN 24-Apr-08 03:25 sleap/gui/widgets/slider.py
+-rw-r--r--  2.0 unx    22322 b- defN 24-Apr-08 03:25 sleap/gui/widgets/training_monitor.py
+-rw-r--r--  2.0 unx    83393 b- defN 24-Apr-08 03:25 sleap/gui/widgets/video.py
+-rw-r--r--  2.0 unx     3522 b- defN 24-Apr-08 03:25 sleap/gui/widgets/views.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-08 03:25 sleap/info/__init__.py
+-rw-r--r--  2.0 unx     8775 b- defN 24-Apr-08 03:25 sleap/info/align.py
+-rw-r--r--  2.0 unx    25741 b- defN 24-Apr-08 03:25 sleap/info/feature_suggestions.py
+-rw-r--r--  2.0 unx     4603 b- defN 24-Apr-08 03:25 sleap/info/labels.py
+-rw-r--r--  2.0 unx     9141 b- defN 24-Apr-08 03:25 sleap/info/metrics.py
+-rw-r--r--  2.0 unx     9118 b- defN 24-Apr-08 03:25 sleap/info/summary.py
+-rw-r--r--  2.0 unx     1917 b- defN 24-Apr-08 03:25 sleap/info/trackcleaner.py
+-rw-r--r--  2.0 unx    16626 b- defN 24-Apr-08 03:25 sleap/info/write_tracking_h5.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-08 03:25 sleap/io/__init__.py
+-rw-r--r--  2.0 unx     7174 b- defN 24-Apr-08 03:25 sleap/io/asyncvideo.py
+-rw-r--r--  2.0 unx     6312 b- defN 24-Apr-08 03:25 sleap/io/convert.py
+-rw-r--r--  2.0 unx   107724 b- defN 24-Apr-08 03:25 sleap/io/dataset.py
+-rw-r--r--  2.0 unx     9189 b- defN 24-Apr-08 03:25 sleap/io/legacy.py
+-rw-r--r--  2.0 unx     4844 b- defN 24-Apr-08 03:25 sleap/io/pathutils.py
+-rw-r--r--  2.0 unx    55471 b- defN 24-Apr-08 03:25 sleap/io/video.py
+-rw-r--r--  2.0 unx     3104 b- defN 24-Apr-08 03:25 sleap/io/videowriter.py
+-rw-r--r--  2.0 unx    24905 b- defN 24-Apr-08 03:25 sleap/io/visuals.py
+-rw-r--r--  2.0 unx       30 b- defN 24-Apr-08 03:25 sleap/io/format/__init__.py
+-rw-r--r--  2.0 unx     2743 b- defN 24-Apr-08 03:25 sleap/io/format/adaptor.py
+-rw-r--r--  2.0 unx    16437 b- defN 24-Apr-08 03:25 sleap/io/format/alphatracker.py
+-rw-r--r--  2.0 unx     6871 b- defN 24-Apr-08 03:25 sleap/io/format/coco.py
+-rw-r--r--  2.0 unx     1885 b- defN 24-Apr-08 03:25 sleap/io/format/csv.py
+-rw-r--r--  2.0 unx    12037 b- defN 24-Apr-08 03:25 sleap/io/format/deeplabcut.py
+-rw-r--r--  2.0 unx     2540 b- defN 24-Apr-08 03:25 sleap/io/format/deepposekit.py
+-rw-r--r--  2.0 unx     3808 b- defN 24-Apr-08 03:25 sleap/io/format/dispatch.py
+-rw-r--r--  2.0 unx     3241 b- defN 24-Apr-08 03:25 sleap/io/format/filehandle.py
+-rw-r--r--  2.0 unx     1064 b- defN 24-Apr-08 03:25 sleap/io/format/genericjson.py
+-rw-r--r--  2.0 unx    22389 b- defN 24-Apr-08 03:25 sleap/io/format/hdf5.py
+-rw-r--r--  2.0 unx    21088 b- defN 24-Apr-08 03:25 sleap/io/format/labels_json.py
+-rw-r--r--  2.0 unx     4109 b- defN 24-Apr-08 03:25 sleap/io/format/leap_matlab.py
+-rw-r--r--  2.0 unx     6361 b- defN 24-Apr-08 03:25 sleap/io/format/main.py
+-rw-r--r--  2.0 unx    14132 b- defN 24-Apr-08 03:25 sleap/io/format/ndx_pose.py
+-rw-r--r--  2.0 unx    17846 b- defN 24-Apr-08 03:25 sleap/io/format/nix.py
+-rw-r--r--  2.0 unx     4859 b- defN 24-Apr-08 03:25 sleap/io/format/sleap_analysis.py
+-rw-r--r--  2.0 unx      965 b- defN 24-Apr-08 03:25 sleap/io/format/text.py
+-rw-r--r--  2.0 unx      449 b- defN 24-Apr-08 03:25 sleap/nn/__init__.py
+-rw-r--r--  2.0 unx    10013 b- defN 24-Apr-08 03:25 sleap/nn/callbacks.py
+-rw-r--r--  2.0 unx    30099 b- defN 24-Apr-08 03:25 sleap/nn/evals.py
+-rw-r--r--  2.0 unx    18212 b- defN 24-Apr-08 03:25 sleap/nn/heads.py
+-rw-r--r--  2.0 unx     9177 b- defN 24-Apr-08 03:25 sleap/nn/identity.py
+-rw-r--r--  2.0 unx   226328 b- defN 24-Apr-08 03:25 sleap/nn/inference.py
+-rw-r--r--  2.0 unx     5673 b- defN 24-Apr-08 03:25 sleap/nn/losses.py
+-rw-r--r--  2.0 unx    13278 b- defN 24-Apr-08 03:25 sleap/nn/model.py
+-rw-r--r--  2.0 unx    74698 b- defN 24-Apr-08 03:25 sleap/nn/paf_grouping.py
+-rw-r--r--  2.0 unx    25718 b- defN 24-Apr-08 03:25 sleap/nn/peak_finding.py
+-rw-r--r--  2.0 unx     7640 b- defN 24-Apr-08 03:25 sleap/nn/system.py
+-rw-r--r--  2.0 unx    54310 b- defN 24-Apr-08 03:25 sleap/nn/tracking.py
+-rw-r--r--  2.0 unx    75381 b- defN 24-Apr-08 03:25 sleap/nn/training.py
+-rw-r--r--  2.0 unx     5449 b- defN 24-Apr-08 03:25 sleap/nn/utils.py
+-rw-r--r--  2.0 unx    11047 b- defN 24-Apr-08 03:25 sleap/nn/viz.py
+-rw-r--r--  2.0 unx      681 b- defN 24-Apr-08 03:25 sleap/nn/architectures/__init__.py
+-rw-r--r--  2.0 unx      752 b- defN 24-Apr-08 03:25 sleap/nn/architectures/common.py
+-rw-r--r--  2.0 unx    27650 b- defN 24-Apr-08 03:25 sleap/nn/architectures/encoder_decoder.py
+-rw-r--r--  2.0 unx    11993 b- defN 24-Apr-08 03:25 sleap/nn/architectures/hourglass.py
+-rw-r--r--  2.0 unx    19077 b- defN 24-Apr-08 03:25 sleap/nn/architectures/hrnet.py
+-rw-r--r--  2.0 unx     5326 b- defN 24-Apr-08 03:25 sleap/nn/architectures/leap.py
+-rw-r--r--  2.0 unx     9359 b- defN 24-Apr-08 03:25 sleap/nn/architectures/pretrained_encoders.py
+-rw-r--r--  2.0 unx    27951 b- defN 24-Apr-08 03:25 sleap/nn/architectures/resnet.py
+-rw-r--r--  2.0 unx    11270 b- defN 24-Apr-08 03:25 sleap/nn/architectures/unet.py
+-rw-r--r--  2.0 unx    12302 b- defN 24-Apr-08 03:25 sleap/nn/architectures/upsampling.py
+-rw-r--r--  2.0 unx     1023 b- defN 24-Apr-08 03:25 sleap/nn/config/__init__.py
+-rw-r--r--  2.0 unx     8736 b- defN 24-Apr-08 03:25 sleap/nn/config/data.py
+-rw-r--r--  2.0 unx    33172 b- defN 24-Apr-08 03:25 sleap/nn/config/model.py
+-rw-r--r--  2.0 unx    13792 b- defN 24-Apr-08 03:25 sleap/nn/config/optimization.py
+-rw-r--r--  2.0 unx    11154 b- defN 24-Apr-08 03:25 sleap/nn/config/outputs.py
+-rw-r--r--  2.0 unx     6239 b- defN 24-Apr-08 03:25 sleap/nn/config/training_job.py
+-rw-r--r--  2.0 unx     2769 b- defN 24-Apr-08 03:25 sleap/nn/config/utils.py
+-rw-r--r--  2.0 unx      456 b- defN 24-Apr-08 03:25 sleap/nn/data/__init__.py
+-rw-r--r--  2.0 unx    16325 b- defN 24-Apr-08 03:25 sleap/nn/data/augmentation.py
+-rw-r--r--  2.0 unx    23439 b- defN 24-Apr-08 03:25 sleap/nn/data/confidence_maps.py
+-rw-r--r--  2.0 unx    12553 b- defN 24-Apr-08 03:25 sleap/nn/data/dataset_ops.py
+-rw-r--r--  2.0 unx    14764 b- defN 24-Apr-08 03:25 sleap/nn/data/edge_maps.py
+-rw-r--r--  2.0 unx     4974 b- defN 24-Apr-08 03:25 sleap/nn/data/general.py
+-rw-r--r--  2.0 unx     3059 b- defN 24-Apr-08 03:25 sleap/nn/data/grouping.py
+-rw-r--r--  2.0 unx     7857 b- defN 24-Apr-08 03:25 sleap/nn/data/identity.py
+-rw-r--r--  2.0 unx    12356 b- defN 24-Apr-08 03:25 sleap/nn/data/inference.py
+-rw-r--r--  2.0 unx     8058 b- defN 24-Apr-08 03:25 sleap/nn/data/instance_centroids.py
+-rw-r--r--  2.0 unx    23401 b- defN 24-Apr-08 03:25 sleap/nn/data/instance_cropping.py
+-rw-r--r--  2.0 unx    14260 b- defN 24-Apr-08 03:25 sleap/nn/data/normalization.py
+-rw-r--r--  2.0 unx     3291 b- defN 24-Apr-08 03:25 sleap/nn/data/offset_regression.py
+-rw-r--r--  2.0 unx    49061 b- defN 24-Apr-08 03:25 sleap/nn/data/pipelines.py
+-rw-r--r--  2.0 unx    18800 b- defN 24-Apr-08 03:25 sleap/nn/data/providers.py
+-rw-r--r--  2.0 unx    21216 b- defN 24-Apr-08 03:25 sleap/nn/data/resizing.py
+-rw-r--r--  2.0 unx     9756 b- defN 24-Apr-08 03:25 sleap/nn/data/training.py
+-rw-r--r--  2.0 unx     6936 b- defN 24-Apr-08 03:25 sleap/nn/data/utils.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-08 03:25 sleap/nn/tracker/__init__.py
+-rw-r--r--  2.0 unx    19176 b- defN 24-Apr-08 03:25 sleap/nn/tracker/components.py
+-rw-r--r--  2.0 unx    24539 b- defN 24-Apr-08 03:25 sleap/nn/tracker/kalman.py
+-rw-r--r--  2.0 unx    44761 b- defN 24-Apr-08 03:25 sleap/skeletons/bees.json
+-rw-r--r--  2.0 unx    35218 b- defN 24-Apr-08 03:25 sleap/skeletons/flies13.json
+-rw-r--r--  2.0 unx    24453 b- defN 24-Apr-08 03:25 sleap/skeletons/fly32.json
+-rw-r--r--  2.0 unx    49861 b- defN 24-Apr-08 03:25 sleap/skeletons/gerbils.json
+-rw-r--r--  2.0 unx    39954 b- defN 24-Apr-08 03:25 sleap/skeletons/mice_hc.json
+-rw-r--r--  2.0 unx    16119 b- defN 24-Apr-08 03:25 sleap/skeletons/mice_of.json
+-rwxr-xr-x  2.0 unx     4477 b- defN 24-Apr-08 03:25 sleap/training_profiles/baseline.centroid.json
+-rw-r--r--  2.0 unx     4640 b- defN 24-Apr-08 03:25 sleap/training_profiles/baseline_large_rf.bottomup.json
+-rw-r--r--  2.0 unx     4344 b- defN 24-Apr-08 03:25 sleap/training_profiles/baseline_large_rf.single.json
+-rw-r--r--  2.0 unx     4381 b- defN 24-Apr-08 03:25 sleap/training_profiles/baseline_large_rf.topdown.json
+-rw-r--r--  2.0 unx     4639 b- defN 24-Apr-08 03:25 sleap/training_profiles/baseline_medium_rf.bottomup.json
+-rw-r--r--  2.0 unx     4343 b- defN 24-Apr-08 03:25 sleap/training_profiles/baseline_medium_rf.single.json
+-rwxr-xr-x  2.0 unx     4522 b- defN 24-Apr-08 03:25 sleap/training_profiles/baseline_medium_rf.topdown.json
+-rw-r--r--  2.0 unx     4552 b- defN 24-Apr-08 03:25 sleap/training_profiles/pretrained.bottomup.json
+-rw-r--r--  2.0 unx     4259 b- defN 24-Apr-08 03:25 sleap/training_profiles/pretrained.centroid.json
+-rw-r--r--  2.0 unx     4256 b- defN 24-Apr-08 03:25 sleap/training_profiles/pretrained.single.json
+-rw-r--r--  2.0 unx     4293 b- defN 24-Apr-08 03:25 sleap/training_profiles/pretrained.topdown.json
+-rw-r--r--  2.0 unx      645 b- defN 24-Apr-08 03:26 sleap-1.4.0.dist-info/AUTHORS
+-rw-r--r--  2.0 unx     1748 b- defN 24-Apr-08 03:26 sleap-1.4.0.dist-info/LICENSE
+-rw-r--r--  2.0 unx    17835 b- defN 24-Apr-08 03:26 sleap-1.4.0.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-Apr-08 03:26 sleap-1.4.0.dist-info/WHEEL
+-rw-r--r--  2.0 unx      326 b- defN 24-Apr-08 03:26 sleap-1.4.0.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx        6 b- defN 24-Apr-08 03:26 sleap-1.4.0.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx    14874 b- defN 24-Apr-08 03:26 sleap-1.4.0.dist-info/RECORD
+176 files, 3257554 bytes uncompressed, 1157333 bytes compressed:  64.5%
```

## zipnote {}

```diff
@@ -501,29 +501,29 @@
 
 Filename: sleap/training_profiles/pretrained.single.json
 Comment: 
 
 Filename: sleap/training_profiles/pretrained.topdown.json
 Comment: 
 
-Filename: sleap-1.3.3.dist-info/AUTHORS
+Filename: sleap-1.4.0.dist-info/AUTHORS
 Comment: 
 
-Filename: sleap-1.3.3.dist-info/LICENSE
+Filename: sleap-1.4.0.dist-info/LICENSE
 Comment: 
 
-Filename: sleap-1.3.3.dist-info/METADATA
+Filename: sleap-1.4.0.dist-info/METADATA
 Comment: 
 
-Filename: sleap-1.3.3.dist-info/WHEEL
+Filename: sleap-1.4.0.dist-info/WHEEL
 Comment: 
 
-Filename: sleap-1.3.3.dist-info/entry_points.txt
+Filename: sleap-1.4.0.dist-info/entry_points.txt
 Comment: 
 
-Filename: sleap-1.3.3.dist-info/top_level.txt
+Filename: sleap-1.4.0.dist-info/top_level.txt
 Comment: 
 
-Filename: sleap-1.3.3.dist-info/RECORD
+Filename: sleap-1.4.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## sleap/version.py

```diff
@@ -8,15 +8,15 @@
 the release should be created with a tag matching the version number set here.
 For example, if you set the version to X.Y.Z, then the tag should be "vX.Y.Z".
 
 Must be a semver string, "aN" should be appended for alpha releases.
 """
 
 
-__version__ = "1.3.3"
+__version__ = "1.4.0"
 
 
 def versions():
     """Print versions of SLEAP and other libraries."""
     import tensorflow as tf
     import numpy as np
     import platform
```

## sleap/config/labeled_clip_form.yaml

```diff
@@ -14,11 +14,15 @@
     type: double
     default: 1.0
     range: 0.1,1.0
   - name: use_gui_visuals
     label: Use GUI Visual Settings (colors, line widths)
     type: bool
     default: true
+  - name: background
+    label: Video Background
+    type: list
+    options: original,black,white,grey
   - name: open_when_done
     label: Open When Done Saving
     type: bool
     default: true
```

## sleap/config/pipeline_form.yaml

```diff
@@ -283,15 +283,15 @@
 
 inference:
 
 - name: _pipeline
   label: Training/Inference Pipeline Type
   type: stacked
   default: "multi-animal bottom-up "
-  options: "multi-animal bottom-up,multi-animal top-down,multi-animal bottom-up-id,multi-animal top-down-id,single animal,movenet-lightning,movenet-thunder,none"
+  options: "multi-animal bottom-up,multi-animal top-down,multi-animal bottom-up-id,multi-animal top-down-id,single animal,movenet-lightning,movenet-thunder,tracking-only"
 
   multi-animal bottom-up:
   - type: text
     text: '<b>Multi-Animal Bottom-Up Pipeline</b>:<br />
       This pipeline uses single model with two output heads:
       a "<u>confidence map</u>" head to predicts the
       nodes for an entire image and a "<u>part affinity field</u>" head to group
@@ -361,15 +361,15 @@
     text: '<b>MoveNet Thunder Pipeline</b>:<br />
       This pipeline uses a pretrained MoveNet Thunder model to predict the nodes
       for an entire image and then groups all of these nodes into a single
       instance. Thunder is intended for applications that require high accuracy.
       Note that this model is intended for human pose estimation. There is no
       support for videos containing more than one instance'
 
-  none:
+  tracking-only:
 
 - name: tracking.tracker
   label: Tracker (cross-frame identity) Method
   type: stacked
   default: none
   options: none,flow,simple
```

## sleap/config/training_editor_form.yaml

```diff
@@ -657,14 +657,15 @@
     over a larger number of examples at the cost of considerably more GPU memory,
     especially for larger sized images. Lower numbers may lead to overfitting, but
     may be beneficial to the optimization process when few but varied examples are
     available.
   label: Batch Size
   name: optimization.batch_size
   type: int
+  range: 1,512
 - default: 100
   help: Maximum number of epochs to train for. Training can be stopped manually or automatically if early stopping is enabled and a plateau is detected.
   label: Epochs
   name: optimization.epochs
   type: int
   range: 1,1000
 - default: 0.0001
```

## sleap/gui/app.py

```diff
@@ -45,14 +45,16 @@
 """
 
 
 import os
 import platform
 import random
 import re
+import traceback
+from logging import getLogger
 from pathlib import Path
 from typing import Callable, List, Optional, Tuple
 
 from qtpy import QtCore, QtGui
 from qtpy.QtCore import QEvent, Qt
 from qtpy.QtWidgets import QApplication, QMainWindow, QMessageBox
 
@@ -81,14 +83,17 @@
 from sleap.io.dataset import Labels
 from sleap.io.video import available_video_exts
 from sleap.prefs import prefs
 from sleap.skeleton import Skeleton
 from sleap.util import parse_uri_path
 
 
+logger = getLogger(__name__)
+
+
 class MainWindow(QMainWindow):
     """The SLEAP GUI application.
 
     Each project (`Labels` dataset) that you have loaded in the GUI will
     have its own `MainWindow` object.
 
     Attributes:
@@ -97,14 +102,15 @@
         state: Object that holds GUI state, e.g., current video, frame,
             whether to show node labels, etc.
     """
 
     def __init__(
         self,
         labels_path: Optional[str] = None,
+        labels: Optional[Labels] = None,
         reset: bool = False,
         no_usage_data: bool = False,
         *args,
         **kwargs,
     ):
         """Initialize the app.
 
@@ -114,15 +120,15 @@
             no_usage_data: If `True`, launch GUI without sharing usage data regardless
                 of stored preferences.
         """
         super(MainWindow, self).__init__(*args, **kwargs)
         self.setAcceptDrops(True)
 
         self.state = GuiState()
-        self.labels = Labels()
+        self.labels = labels or Labels()
 
         self.commands = CommandContext(
             state=self.state, app=self, update_callback=self.on_data_update
         )
 
         self.shortcuts = Shortcuts()
 
@@ -171,16 +177,18 @@
         if reset:
             print("Reseting GUI state and preferences...")
             prefs.reset_to_default()
         elif len(prefs["window state"]) > 0:
             print("Restoring GUI state...")
             self.restoreState(prefs["window state"])
 
-        if labels_path:
+        if labels_path is not None:
             self.commands.loadProjectFile(filename=labels_path)
+        elif labels is not None:
+            self.commands.loadLabelsObject(labels=labels)
         else:
             self.state["project_loaded"] = False
 
     def setWindowTitle(self, value):
         """Sets window title (if value is not None)."""
         if value is not None:
             super(MainWindow, self).setWindowTitle(
@@ -250,15 +258,14 @@
         mime_format = 'application/x-qt-windows-mime;value="FileName"'
         if mime_format in event.mimeData().formats():
             # This only returns the first filename if multiple files are dropped:
             filename = event.mimeData().data(mime_format).data().decode()
             event.acceptProposedAction()
 
     def dropEvent(self, event):
-
         # Parse filenames
         filenames = event.mimeData().data("text/uri-list").data().decode()
         filenames = [parse_uri_path(f.strip()) for f in filenames.strip().split("\n")]
 
         exts = [Path(f).suffix for f in filenames]
 
         if len(exts) == 1 and exts[0].lower() == ".slp":
@@ -1590,16 +1597,20 @@
             self.commands.completeInstanceNodes(instance)
 
     def _show_keyboard_shortcuts_window(self):
         """Shows gui for viewing/modifying keyboard shortucts."""
         ShortcutDialog().exec_()
 
 
-def main(args: Optional[list] = None):
-    """Starts new instance of app."""
+def create_sleap_label_parser():
+    """Creates parser for `sleap-label` command line arguments.
+
+    Returns:
+        argparse.ArgumentParser: The parser.
+    """
 
     import argparse
 
     parser = argparse.ArgumentParser()
     parser.add_argument(
         "labels_path", help="Path to labels file", type=str, default=None, nargs="?"
     )
@@ -1631,36 +1642,62 @@
         "--no-usage-data",
         help=("Launch the GUI without sharing usage data regardless of preferences."),
         action="store_const",
         const=True,
         default=False,
     )
 
+    return parser
+
+
+def create_app():
+    """Creates Qt application."""
+
+    app = QApplication([])
+    app.setApplicationName(f"SLEAP v{sleap.version.__version__}")
+    app.setWindowIcon(QtGui.QIcon(sleap.util.get_package_file("gui/icon.png")))
+
+    return app
+
+
+def main(args: Optional[list] = None, labels: Optional[Labels] = None):
+    """Starts new instance of app."""
+
+    parser = create_sleap_label_parser()
     args = parser.parse_args(args)
 
     if args.nonnative:
         os.environ["USE_NON_NATIVE_FILE"] = "1"
 
     if platform.system() == "Darwin":
         # TODO: Remove this workaround when we update to qtpy >= 5.15.
         # https://bugreports.qt.io/browse/QTBUG-87014
         # https://stackoverflow.com/q/64818879
         os.environ["QT_MAC_WANTS_LAYER"] = "1"
 
-    app = QApplication([])
-    app.setApplicationName(f"SLEAP v{sleap.version.__version__}")
-    app.setWindowIcon(QtGui.QIcon(sleap.util.get_package_file("gui/icon.png")))
+    app = create_app()
 
     window = MainWindow(
-        labels_path=args.labels_path, reset=args.reset, no_usage_data=args.no_usage_data
+        labels_path=args.labels_path,
+        labels=labels,
+        reset=args.reset,
+        no_usage_data=args.no_usage_data,
     )
     window.showMaximized()
 
     # Disable GPU in GUI process. This does not affect subprocesses.
-    sleap.use_cpu_only()
+    try:
+        sleap.use_cpu_only()
+    except RuntimeError:  # Visible devices cannot be modified after being initialized
+        logger.warning(
+            "Running processes on the GPU. Restarting your GUI should allow switching "
+            "back to CPU-only mode.\n"
+            "Received the following error when trying to switch back to CPU-only mode:"
+        )
+        traceback.print_exc()
 
     # Print versions.
     print()
     print("Software versions:")
     sleap.versions()
     print()
     print("Happy SLEAPing! :)")
```

## sleap/gui/commands.py

```diff
@@ -32,28 +32,27 @@
 import re
 import subprocess
 import sys
 import traceback
 from enum import Enum
 from glob import glob
 from pathlib import Path, PurePath
-from typing import Callable, Dict, Iterator, List, Optional, Tuple, Type
+from typing import Callable, Dict, Iterator, List, Optional, Tuple, Type, Union, cast
 
 import attr
 import cv2
 import numpy as np
 from qtpy import QtCore, QtGui, QtWidgets
 
 from sleap.gui.dialogs.delete import DeleteDialog
 from sleap.gui.dialogs.filedialog import FileDialog
 from sleap.gui.dialogs.importvideos import ImportVideos
 from sleap.gui.dialogs.merge import MergeDialog, ReplaceSkeletonTableDialog
 from sleap.gui.dialogs.message import MessageDialog
 from sleap.gui.dialogs.missingfiles import MissingFilesDialog
-from sleap.gui.dialogs.query import QueryDialog
 from sleap.gui.state import GuiState
 from sleap.gui.suggestions import VideoFrameSuggestions
 from sleap.instance import Instance, LabeledFrame, Point, PredictedInstance, Track
 from sleap.io.convert import default_analysis_filename
 from sleap.io.dataset import Labels
 from sleap.io.format.adaptor import Adaptor
 from sleap.io.format.csv import CSVAdaptor
@@ -256,24 +255,23 @@
 
         Returns:
             None.
 
         """
         self.execute(LoadLabelsObject, labels=labels, filename=filename)
 
-    def loadProjectFile(self, filename: str):
+    def loadProjectFile(self, filename: Union[str, Labels]):
         """Loads given labels file into GUI.
 
         Args:
-            filename: The path to the saved labels dataset. If None,
-                then don't do anything.
+            filename: The path to the saved labels dataset or the `Labels` object.
+                If None, then don't do anything.
 
         Returns:
             None
-
         """
         self.execute(LoadProjectFile, filename=filename)
 
     def openProject(self, filename: Optional[str] = None, first_open: bool = False):
         """Allows user to select and then open a saved project.
 
         Args:
@@ -643,17 +641,16 @@
 
         Args:
             labels: The `Labels` object to load.
             filename: The filename where this file is saved, if any.
 
         Returns:
             None.
-
         """
-        filename = params["filename"]
+        filename = params.get("filename", None)  # If called with just a Labels object
         labels: Labels = params["labels"]
 
         context.state["labels"] = labels
         context.state["filename"] = filename
 
         context.changestack_clear()
         context.app.color_manager.labels = context.labels
@@ -665,39 +662,40 @@
             context.state["skeleton"] = labels.skeletons[0]
 
         # Load first video
         if len(labels.videos):
             context.state["video"] = labels.videos[0]
 
         context.state["project_loaded"] = True
-        context.state["has_changes"] = params.get("changed_on_load", False)
+        context.state["has_changes"] = params.get("changed_on_load", False) or (
+            filename is None
+        )
 
         # This is not listed as an edit command since we want a clean changestack
         context.app.on_data_update([UpdateTopic.project, UpdateTopic.all])
 
 
 class LoadProjectFile(LoadLabelsObject):
     @staticmethod
     def ask(context: "CommandContext", params: dict):
         filename = params["filename"]
 
         if len(filename) == 0:
             return
 
-        gui_video_callback = Labels.make_gui_video_callback(
-            search_paths=[os.path.dirname(filename)], context=params
-        )
-
         has_loaded = False
         labels = None
-        if type(filename) == Labels:
+        if isinstance(filename, Labels):
             labels = filename
             filename = None
             has_loaded = True
         else:
+            gui_video_callback = Labels.make_gui_video_callback(
+                search_paths=[os.path.dirname(filename)], context=params
+            )
             try:
                 labels = Labels.load_file(filename, video_search=gui_video_callback)
                 has_loaded = True
             except ValueError as e:
                 print(e)
                 QtWidgets.QMessageBox(text=f"Unable to load {filename}.").exec_()
 
@@ -747,15 +745,14 @@
             params["filename"] = filename
         return True
 
 
 class ImportAlphaTracker(AppCommand):
     @staticmethod
     def do_action(context: "CommandContext", params: dict):
-
         video_path = params["video_path"] if "video_path" in params else None
 
         labels = Labels.load_alphatracker(
             filename=params["filename"],
             full_video=video_path,
         )
 
@@ -787,15 +784,14 @@
 
         return True
 
 
 class ImportNWB(AppCommand):
     @staticmethod
     def do_action(context: "CommandContext", params: dict):
-
         labels = Labels.load_nwb(filename=params["filename"])
 
         new_window = context.app.__class__()
         new_window.showMaximized()
         new_window.commands.loadLabelsObject(labels=labels)
 
     @staticmethod
@@ -820,15 +816,14 @@
 
         return True
 
 
 class ImportDeepPoseKit(AppCommand):
     @staticmethod
     def do_action(context: "CommandContext", params: dict):
-
         labels = Labels.from_deepposekit(
             filename=params["filename"],
             video_path=params["video_path"],
             skeleton_path=params["skeleton_path"],
         )
 
         new_window = context.app.__class__()
@@ -869,15 +864,14 @@
 
         return True
 
 
 class ImportLEAP(AppCommand):
     @staticmethod
     def do_action(context: "CommandContext", params: dict):
-
         labels = Labels.load_leap_matlab(
             filename=params["filename"],
         )
 
         new_window = context.app.__class__()
         new_window.showMaximized()
         new_window.commands.loadLabelsObject(labels=labels)
@@ -900,15 +894,14 @@
 
         return True
 
 
 class ImportCoco(AppCommand):
     @staticmethod
     def do_action(context: "CommandContext", params: dict):
-
         labels = Labels.load_coco(
             filename=params["filename"], img_dir=params["img_dir"], use_missing_gui=True
         )
 
         new_window = context.app.__class__()
         new_window.showMaximized()
         new_window.commands.loadLabelsObject(labels=labels)
@@ -932,15 +925,14 @@
 
         return True
 
 
 class ImportDeepLabCut(AppCommand):
     @staticmethod
     def do_action(context: "CommandContext", params: dict):
-
         labels = Labels.load_deeplabcut(filename=params["filename"])
 
         new_window = context.app.__class__()
         new_window.showMaximized()
         new_window.commands.loadLabelsObject(labels=labels)
 
     @staticmethod
@@ -1291,29 +1283,29 @@
         save_labeled_video(
             filename=params["filename"],
             labels=context.state["labels"],
             video=context.state["video"],
             frames=list(params["frames"]),
             fps=params["fps"],
             color_manager=params["color_manager"],
+            background=params["background"],
             show_edges=params["show edges"],
             edge_is_wedge=params["edge_is_wedge"],
             marker_size=params["marker size"],
             scale=params["scale"],
             crop_size_xy=params["crop"],
             gui_progress=True,
         )
 
         if params["open_when_done"]:
             # Open the file using default video playing app
             open_file(params["filename"])
 
     @staticmethod
     def ask(context: CommandContext, params: dict) -> bool:
-
         from sleap.gui.dialogs.export_clip import ExportClipDialog
 
         dialog = ExportClipDialog()
 
         # Set default fps from video (if video has fps attribute)
         dialog.form_widget.set_form_data(
             dict(fps=getattr(context.state["video"], "fps", 30))
@@ -1350,14 +1342,15 @@
         if len(filename) == 0:
             return False
 
         params["filename"] = filename
         params["fps"] = export_options["fps"]
         params["scale"] = export_options["scale"]
         params["open_when_done"] = export_options["open_when_done"]
+        params["background"] = export_options["background"]
 
         params["crop"] = None
 
         # Determine crop size relative to original size and scale
         # (crop size should be *final* output size, thus already scaled).
         w = int(context.state["video"].width * params["scale"])
         h = int(context.state["video"].height * params["scale"])
@@ -1580,15 +1573,14 @@
 
 
 class GoNextSuggestedFrame(NavCommand):
     seek_direction = 1
 
     @classmethod
     def do_action(cls, context: CommandContext, params: dict):
-
         next_suggestion_frame = context.labels.get_next_suggestion(
             context.state["video"], context.state["frame_idx"], cls.seek_direction
         )
         if next_suggestion_frame is not None:
             cls.go_to(
                 context, next_suggestion_frame.frame_idx, next_suggestion_frame.video
             )
@@ -1766,15 +1758,14 @@
 
 
 class ReplaceVideo(EditCommand):
     topics = [UpdateTopic.video, UpdateTopic.frame]
 
     @staticmethod
     def do_action(context: CommandContext, params: dict) -> bool:
-
         import_list = params["import_list"]
 
         for import_item, video in import_list:
             import_params = import_item["params"]
 
             # TODO: Will need to create a new backend if import has different extension.
             if (
@@ -1895,15 +1886,14 @@
     @staticmethod
     def ask(context: CommandContext, params: dict) -> bool:
         videos = context.labels.videos.copy()
         row_idxs = context.state["selected_batch_video"]
         video_file_names = []
         total_num_labeled_frames = 0
         for idx in row_idxs:
-
             video = videos[idx]
             if video is None:
                 return False
 
             # Count labeled frames for this video
             n = len(context.labels.find(video))
 
@@ -1940,15 +1930,14 @@
             new_skeleton = sk_list[0]
         return new_skeleton
 
     @staticmethod
     def compare_skeletons(
         skeleton: Skeleton, new_skeleton: Skeleton
     ) -> Tuple[List[str], List[str], List[str]]:
-
         delete_nodes = []
         add_nodes = []
         if skeleton.node_names != new_skeleton.node_names:
             # Compare skeletons
             base_nodes = skeleton.node_names
             new_nodes = new_skeleton.node_names
             delete_nodes = [node for node in base_nodes if node not in new_nodes]
@@ -2719,15 +2708,14 @@
 
 
 class GenerateSuggestions(EditCommand):
     topics = [UpdateTopic.suggestions]
 
     @classmethod
     def do_action(cls, context: CommandContext, params: dict):
-
         if len(context.labels.videos) == 0:
             print("Error: no videos to generate suggestions for")
             return
 
         # TODO: Progress bar
         win = MessageDialog(
             "Generating list of suggested frames... " "This may take a few minutes.",
@@ -2847,42 +2835,214 @@
 
         cls.do_with_signal(context, params)
 
 
 class AddInstance(EditCommand):
     topics = [UpdateTopic.frame, UpdateTopic.project_instances, UpdateTopic.suggestions]
 
-    @staticmethod
-    def get_previous_frame_index(context: CommandContext) -> Optional[int]:
-        frames = context.labels.frames(
-            context.state["video"],
-            from_frame_idx=context.state["frame_idx"],
-            reverse=True,
-        )
-
-        try:
-            next_idx = next(frames).frame_idx
-        except:
-            return
-
-        return next_idx
-
     @classmethod
     def do_action(cls, context: CommandContext, params: dict):
         copy_instance = params.get("copy_instance", None)
         init_method = params.get("init_method", "best")
         location = params.get("location", None)
         mark_complete = params.get("mark_complete", False)
 
         if context.state["labeled_frame"] is None:
             return
 
         if len(context.state["skeleton"]) == 0:
             return
 
+        (
+            copy_instance,
+            from_predicted,
+            from_prev_frame,
+        ) = AddInstance.find_instance_to_copy_from(
+            context, copy_instance=copy_instance, init_method=init_method
+        )
+
+        new_instance = AddInstance.create_new_instance(
+            context=context,
+            from_predicted=from_predicted,
+            copy_instance=copy_instance,
+            mark_complete=mark_complete,
+            init_method=init_method,
+            location=location,
+            from_prev_frame=from_prev_frame,
+        )
+
+        # Add the instance
+        context.labels.add_instance(context.state["labeled_frame"], new_instance)
+
+        if context.state["labeled_frame"] not in context.labels.labels:
+            context.labels.append(context.state["labeled_frame"])
+
+    @staticmethod
+    def create_new_instance(
+        context: CommandContext,
+        from_predicted: Optional[PredictedInstance],
+        copy_instance: Optional[Union[Instance, PredictedInstance]],
+        mark_complete: bool,
+        init_method: str,
+        location: Optional[QtCore.QPoint],
+        from_prev_frame: bool,
+    ) -> Instance:
+        """Create new instance."""
+
+        # Now create the new instance
+        new_instance = Instance(
+            skeleton=context.state["skeleton"],
+            from_predicted=from_predicted,
+            frame=context.state["labeled_frame"],
+        )
+
+        has_missing_nodes = AddInstance.set_visible_nodes(
+            context=context,
+            copy_instance=copy_instance,
+            new_instance=new_instance,
+            mark_complete=mark_complete,
+        )
+
+        if has_missing_nodes:
+            AddInstance.fill_missing_nodes(
+                context=context,
+                copy_instance=copy_instance,
+                init_method=init_method,
+                new_instance=new_instance,
+                location=location,
+            )
+
+        # If we're copying a predicted instance or from another frame, copy the track
+        if hasattr(copy_instance, "score") or from_prev_frame:
+            copy_instance = cast(Union[PredictedInstance, Instance], copy_instance)
+            new_instance.track = copy_instance.track
+
+        return new_instance
+
+    @staticmethod
+    def fill_missing_nodes(
+        context: CommandContext,
+        copy_instance: Optional[Union[Instance, PredictedInstance]],
+        init_method: str,
+        new_instance: Instance,
+        location: Optional[QtCore.QPoint],
+    ):
+        """Fill in missing nodes for new instance.
+
+        Args:
+            context: The command context.
+            copy_instance: The instance to copy from.
+            init_method: The initialization method.
+            new_instance: The new instance.
+            location: The location of the instance.
+
+        Returns:
+            None
+        """
+
+        # mark the node as not "visible" if we're copying from a predicted instance without this node
+        is_visible = copy_instance is None or (not hasattr(copy_instance, "score"))
+
+        if init_method == "force_directed":
+            AddMissingInstanceNodes.add_force_directed_nodes(
+                context=context,
+                instance=new_instance,
+                visible=is_visible,
+                center_point=location,
+            )
+        elif init_method == "random":
+            AddMissingInstanceNodes.add_random_nodes(
+                context=context, instance=new_instance, visible=is_visible
+            )
+        elif init_method == "template":
+            AddMissingInstanceNodes.add_nodes_from_template(
+                context=context,
+                instance=new_instance,
+                visible=is_visible,
+                center_point=location,
+            )
+        else:
+            AddMissingInstanceNodes.add_best_nodes(
+                context=context, instance=new_instance, visible=is_visible
+            )
+
+    @staticmethod
+    def set_visible_nodes(
+        context: CommandContext,
+        copy_instance: Optional[Union[Instance, PredictedInstance]],
+        new_instance: Instance,
+        mark_complete: bool,
+    ) -> bool:
+        """Sets visible nodes for new instance.
+
+        Args:
+            context: The command context.
+            copy_instance: The instance to copy from.
+            new_instance: The new instance.
+            mark_complete: Whether to mark the instance as complete.
+
+        Returns:
+            Whether the new instance has missing nodes.
+        """
+
+        if copy_instance is None:
+            return True
+
+        has_missing_nodes = False
+
+        # Calculate scale factor for getting new x and y values.
+        old_size_width = copy_instance.frame.video.shape[2]
+        old_size_height = copy_instance.frame.video.shape[1]
+        new_size_width = new_instance.frame.video.shape[2]
+        new_size_height = new_instance.frame.video.shape[1]
+        scale_width = new_size_width / old_size_width
+        scale_height = new_size_height / old_size_height
+
+        # Go through each node in skeleton.
+        for node in context.state["skeleton"].node_names:
+            # If we're copying from a skeleton that has this node.
+            if node in copy_instance and not copy_instance[node].isnan():
+                # Ensure x, y inside current frame, then copy x, y, and visible.
+                # We don't want to copy a PredictedPoint or score attribute.
+                x_old = copy_instance[node].x
+                y_old = copy_instance[node].y
+                x_new = x_old * scale_width
+                y_new = y_old * scale_height
+
+                new_instance[node] = Point(
+                    x=x_new,
+                    y=y_new,
+                    visible=copy_instance[node].visible,
+                    complete=mark_complete,
+                )
+            else:
+                has_missing_nodes = True
+
+        return has_missing_nodes
+
+    @staticmethod
+    def find_instance_to_copy_from(
+        context: CommandContext,
+        copy_instance: Optional[Union[Instance, PredictedInstance]],
+        init_method: bool,
+    ) -> Tuple[
+        Optional[Union[Instance, PredictedInstance]], Optional[PredictedInstance], bool
+    ]:
+        """Find instance to copy from.
+
+        Args:
+            context: The command context.
+            copy_instance: The `Instance` to copy from.
+            init_method: The initialization method.
+
+        Returns:
+            The instance to copy from, the predicted instance (if it is from a predicted
+            instance, else None), and whether it's from a previous frame.
+        """
+
         from_predicted = copy_instance
         from_prev_frame = False
 
         if init_method == "best" and copy_instance is None:
             selected_inst = context.state["instance"]
             if selected_inst is not None:
                 # If the user has selected an instance, copy that one.
@@ -2900,15 +3060,15 @@
                 from_predicted = copy_instance
 
         if (
             init_method == "best" and copy_instance is None
         ) or init_method == "prior_frame":
             # Otherwise, if there are instances in previous frames,
             # copy the points from one of those instances.
-            prev_idx = cls.get_previous_frame_index(context)
+            prev_idx = AddInstance.get_previous_frame_index(context)
 
             if prev_idx is not None:
                 prev_instances = context.labels.find(
                     context.state["video"], prev_idx, return_new=True
                 )[0].instances
                 if len(prev_instances) > len(context.state["labeled_frame"].instances):
                     # If more instances in previous frame than current, then use the
@@ -2925,79 +3085,34 @@
                     copy_instance = context.state["labeled_frame"].instances[-1]
                 elif len(prev_instances):
                     # Otherwise use the last instance added to previous frame.
                     copy_instance = prev_instances[-1]
                     from_prev_frame = True
 
         from_predicted = from_predicted if hasattr(from_predicted, "score") else None
+        from_predicted = cast(Optional[PredictedInstance], from_predicted)
 
-        # Now create the new instance
-        new_instance = Instance(
-            skeleton=context.state["skeleton"],
-            from_predicted=from_predicted,
-            frame=context.state["labeled_frame"],
-        )
-
-        has_missing_nodes = False
-
-        # go through each node in skeleton
-        for node in context.state["skeleton"].node_names:
-            # if we're copying from a skeleton that has this node
-            if (
-                copy_instance is not None
-                and node in copy_instance
-                and not copy_instance[node].isnan()
-            ):
-                # just copy x, y, and visible
-                # we don't want to copy a PredictedPoint or score attribute
-                new_instance[node] = Point(
-                    x=copy_instance[node].x,
-                    y=copy_instance[node].y,
-                    visible=copy_instance[node].visible,
-                    complete=mark_complete,
-                )
-            else:
-                has_missing_nodes = True
-
-        if has_missing_nodes:
-            # mark the node as not "visible" if we're copying from a predicted instance without this node
-            is_visible = copy_instance is None or (not hasattr(copy_instance, "score"))
+        return copy_instance, from_predicted, from_prev_frame
 
-            if init_method == "force_directed":
-                AddMissingInstanceNodes.add_force_directed_nodes(
-                    context=context,
-                    instance=new_instance,
-                    visible=is_visible,
-                    center_point=location,
-                )
-            elif init_method == "random":
-                AddMissingInstanceNodes.add_random_nodes(
-                    context=context, instance=new_instance, visible=is_visible
-                )
-            elif init_method == "template":
-                AddMissingInstanceNodes.add_nodes_from_template(
-                    context=context,
-                    instance=new_instance,
-                    visible=is_visible,
-                    center_point=location,
-                )
-            else:
-                AddMissingInstanceNodes.add_best_nodes(
-                    context=context, instance=new_instance, visible=is_visible
-                )
+    @staticmethod
+    def get_previous_frame_index(context: CommandContext) -> Optional[int]:
+        """Returns index of previous frame."""
 
-        # If we're copying a predicted instance or from another frame, copy the track
-        if hasattr(copy_instance, "score") or from_prev_frame:
-            new_instance.track = copy_instance.track
+        frames = context.labels.frames(
+            context.state["video"],
+            from_frame_idx=context.state["frame_idx"],
+            reverse=True,
+        )
 
-        # Add the instance
-        context.labels.add_instance(context.state["labeled_frame"], new_instance)
+        try:
+            next_idx = next(frames).frame_idx
+        except:
+            return
 
-        if context.state["labeled_frame"] not in context.labels.labels:
-            context.labels.append(context.state["labeled_frame"])
+        return next_idx
 
 
 class SetInstancePointLocations(EditCommand):
     """Sets locations for node(s) for an instance.
 
     Note: It's important that this command does *not* update the visual
     scene, since this would redraw the frame and create new visual objects.
```

## sleap/gui/learning/dialog.py

```diff
@@ -10,18 +10,17 @@
 
 import sleap
 from sleap import Labels, Video
 from sleap.gui.dialogs.filedialog import FileDialog
 from sleap.gui.dialogs.formbuilder import YamlFormWidget
 from sleap.gui.learning import runners, scopedkeydict, configs, datagen, receptivefield
 
-from typing import Dict, List, Optional, Text, Optional, cast
+from typing import Dict, List, Text, Optional, cast
 
 from qtpy import QtWidgets, QtCore
-
 import json
 
 # List of fields which should show list of skeleton nodes
 NODE_LIST_FIELDS = [
     "data.instance_cropping.center_on_part",
     "model.heads.centered_instance.anchor_part",
     "model.heads.centroid.anchor_part",
@@ -124,20 +123,33 @@
 
         self.tab_widget.addTab(self.pipeline_form_widget, tab_label)
         self.make_tabs()
 
         self.message_widget = QtWidgets.QLabel("")
 
         # Layout for entire dialog
-        layout = QtWidgets.QVBoxLayout()
-        layout.addWidget(self.tab_widget)
-        layout.addWidget(self.message_widget)
-        layout.addWidget(buttons_layout_widget)
+        content_widget = QtWidgets.QWidget()
+        content_layout = QtWidgets.QVBoxLayout(content_widget)
+
+        content_layout.addWidget(self.tab_widget)
+        content_layout.addWidget(self.message_widget)
+        content_layout.addWidget(buttons_layout_widget)
+
+        # Create the QScrollArea.
+        scroll_area = QtWidgets.QScrollArea()
+        scroll_area.setWidgetResizable(True)
+        scroll_area.setWidget(content_widget)
 
-        self.setLayout(layout)
+        scroll_area.setVerticalScrollBarPolicy(QtCore.Qt.ScrollBarAsNeeded)
+        scroll_area.setHorizontalScrollBarPolicy(QtCore.Qt.ScrollBarAsNeeded)
+
+        layout = QtWidgets.QVBoxLayout(self)
+        layout.addWidget(scroll_area)
+
+        self.adjust_initial_size()
 
         # Default to most recently trained pipeline (if there is one)
         self.set_default_pipeline_tab()
 
         # Connect functions to update pipeline tabs when pipeline changes
         self.pipeline_form_widget.updatePipeline.connect(self.set_pipeline)
         self.pipeline_form_widget.emitPipeline()
@@ -153,14 +165,28 @@
 
         # Connect button for previewing the training data
         if "_view_datagen" in self.pipeline_form_widget.buttons:
             self.pipeline_form_widget.buttons["_view_datagen"].clicked.connect(
                 self.view_datagen
             )
 
+    def adjust_initial_size(self):
+        # Get screen size
+        screen = QtWidgets.QDesktopWidget().screenGeometry()
+
+        max_width = 1860
+        max_height = 1150
+        margin = 0.10
+
+        # Calculate target width and height
+        target_width = min(screen.width() - screen.width() * margin, max_width)
+        target_height = min(screen.height() - screen.height() * margin, max_height)
+        # Set the dialog's dimensions
+        self.resize(target_width, target_height)
+
     def update_file_lists(self):
         self._cfg_getter.update()
         for tab in self.tabs.values():
             tab.update_file_list()
 
     @staticmethod
     def count_total_frames_for_selection_option(
```

## sleap/gui/overlays/base.py

```diff
@@ -67,15 +67,14 @@
             return
         for item in self.items:
             try:
                 self.player.scene.removeItem(item)
 
             except RuntimeError as e:  # Internal C++ object (PySide2.QtWidgets.QGraphicsPathItem) already deleted.
                 logger.debug(e)
-                pass
 
         # Stop tracking the items after they been removed from the scene
         self.items = []
 
     def redraw(self, video, frame_idx, *args, **kwargs):
         """Remove all items from the scene before adding new items to the scene.
```

## sleap/info/metrics.py

```diff
@@ -6,83 +6,14 @@
 from scipy.optimize import linear_sum_assignment
 from typing import Callable, List, Optional, Union, Tuple
 
 from sleap.instance import Instance, PredictedInstance
 from sleap.io.dataset import Labels
 
 
-def matched_instance_distances(
-    labels_gt: Labels,
-    labels_pr: Labels,
-    match_lists_function: Callable,
-    frame_range: Optional[range] = None,
-) -> Tuple[List[int], np.ndarray, np.ndarray, np.ndarray]:
-
-    """
-    Distances between ground truth and predicted nodes over a set of frames.
-
-    Args:
-        labels_gt: the `Labels` object with ground truth data
-        labels_pr: the `Labels` object with predicted data
-        match_lists_function: function for determining corresponding instances
-            Takes two lists of instances and returns "sorted" lists.
-        frame_range (optional): range of frames for which to compare data
-            If None, we compare every frame in labels_gt with corresponding
-            frame in labels_pr.
-    Returns:
-        Tuple:
-        * frame indices map: instance idx (for other matrices) -> frame idx
-        * distance matrix: (instances * nodes)
-        * ground truth points matrix: (instances * nodes * 2)
-        * predicted points matrix: (instances * nodes * 2)
-    """
-
-    frame_idxs = []
-    points_gt = []
-    points_pr = []
-    for lf_gt in labels_gt.find(labels_gt.videos[0]):
-        frame_idx = lf_gt.frame_idx
-
-        # Get instances from ground truth/predicted labels
-        instances_gt = lf_gt.instances
-        lfs_pr = labels_pr.find(labels_pr.videos[0], frame_idx=frame_idx)
-        if len(lfs_pr):
-            instances_pr = lfs_pr[0].instances
-        else:
-            instances_pr = []
-
-        # Sort ground truth and predicted instances.
-        # We'll then compare points between corresponding items in lists.
-        # We can use different "match" functions depending on what we want.
-        sorted_gt, sorted_pr = match_lists_function(instances_gt, instances_pr)
-
-        # Convert lists of instances to (instances, nodes, 2) matrices.
-        # This allows match_lists_function to return data as either
-        # a list of Instances or a (instances, nodes, 2) matrix.
-        if type(sorted_gt[0]) != np.ndarray:
-            sorted_gt = list_points_array(sorted_gt)
-        if type(sorted_pr[0]) != np.ndarray:
-            sorted_pr = list_points_array(sorted_pr)
-
-        points_gt.append(sorted_gt)
-        points_pr.append(sorted_pr)
-        frame_idxs.extend([frame_idx] * len(sorted_gt))
-
-    # Convert arrays to numpy matrixes
-    # instances * nodes * (x,y)
-    points_gt = np.concatenate(points_gt)
-    points_pr = np.concatenate(points_pr)
-
-    # Calculate distances between corresponding nodes for all corresponding
-    # ground truth and predicted instances.
-    D = np.linalg.norm(points_gt - points_pr, axis=2)
-
-    return frame_idxs, D, points_gt, points_pr
-
-
 def match_instance_lists(
     instances_a: List[Union[Instance, PredictedInstance]],
     instances_b: List[Union[Instance, PredictedInstance]],
     cost_function: Callable,
 ) -> Tuple[
     List[Union[Instance, PredictedInstance]], List[Union[Instance, PredictedInstance]]
 ]:
@@ -161,14 +92,83 @@
 
         # Add matrix of points to compare against ground truth instance
         best_points_array.append(closest_point_array)
 
     return instances_a, best_points_array
 
 
+def matched_instance_distances(
+    labels_gt: Labels,
+    labels_pr: Labels,
+    match_lists_function: Callable = match_instance_lists_nodewise,
+    frame_range: Optional[range] = None,
+) -> Tuple[List[int], np.ndarray, np.ndarray, np.ndarray]:
+
+    """
+    Distances between ground truth and predicted nodes over a set of frames.
+
+    Args:
+        labels_gt: the `Labels` object with ground truth data
+        labels_pr: the `Labels` object with predicted data
+        match_lists_function: function for determining corresponding instances
+            Takes two lists of instances and returns "sorted" lists.
+        frame_range (optional): range of frames for which to compare data
+            If None, we compare every frame in labels_gt with corresponding
+            frame in labels_pr.
+    Returns:
+        Tuple:
+        * frame indices map: instance idx (for other matrices) -> frame idx
+        * distance matrix: (instances * nodes)
+        * ground truth points matrix: (instances * nodes * 2)
+        * predicted points matrix: (instances * nodes * 2)
+    """
+
+    frame_idxs = []
+    points_gt = []
+    points_pr = []
+    for lf_gt in labels_gt.find(labels_gt.videos[0]):
+        frame_idx = lf_gt.frame_idx
+
+        # Get instances from ground truth/predicted labels
+        instances_gt = lf_gt.instances
+        lfs_pr = labels_pr.find(labels_pr.videos[0], frame_idx=frame_idx)
+        if len(lfs_pr):
+            instances_pr = lfs_pr[0].instances
+        else:
+            instances_pr = []
+
+        # Sort ground truth and predicted instances.
+        # We'll then compare points between corresponding items in lists.
+        # We can use different "match" functions depending on what we want.
+        sorted_gt, sorted_pr = match_lists_function(instances_gt, instances_pr)
+
+        # Convert lists of instances to (instances, nodes, 2) matrices.
+        # This allows match_lists_function to return data as either
+        # a list of Instances or a (instances, nodes, 2) matrix.
+        if type(sorted_gt[0]) != np.ndarray:
+            sorted_gt = list_points_array(sorted_gt)
+        if type(sorted_pr[0]) != np.ndarray:
+            sorted_pr = list_points_array(sorted_pr)
+
+        points_gt.append(sorted_gt)
+        points_pr.append(sorted_pr)
+        frame_idxs.extend([frame_idx] * len(sorted_gt))
+
+    # Convert arrays to numpy matrixes
+    # instances * nodes * (x,y)
+    points_gt = np.concatenate(points_gt)
+    points_pr = np.concatenate(points_pr)
+
+    # Calculate distances between corresponding nodes for all corresponding
+    # ground truth and predicted instances.
+    D = np.linalg.norm(points_gt - points_pr, axis=2)
+
+    return frame_idxs, D, points_gt, points_pr
+
+
 def point_dist(
     inst_a: Union[Instance, PredictedInstance],
     inst_b: Union[Instance, PredictedInstance],
 ) -> np.ndarray:
     """Given two instances, returns array of distances for corresponding nodes."""
 
     points_a = inst_a.points_array
@@ -234,50 +234,7 @@
     """Given an array of distances, returns number which are <= threshold."""
     return np.sum(dist_array[~np.isnan(dist_array)] <= thresh)
 
 
 def point_nonmatch_count(dist_array: np.ndarray, thresh: float = 5) -> int:
     """Given an array of distances, returns number which are not <= threshold."""
     return dist_array.shape[0] - point_match_count(dist_array, thresh)
-
-
-if __name__ == "__main__":
-
-    labels_gt = Labels.load_json("tests/data/json_format_v1/centered_pair.json")
-    labels_pr = Labels.load_json(
-        "tests/data/json_format_v2/centered_pair_predictions.json"
-    )
-
-    # OPTION 1
-
-    # Match each ground truth instance node to the closest corresponding node
-    # from any predicted instance in the same frame.
-
-    nodewise_matching_func = match_instance_lists_nodewise
-
-    # OPTION 2
-
-    # Match each ground truth instance to a distinct predicted instance:
-    # We want to maximize the number of "matching" points between instances,
-    # where "match" means the points are within some threshold distance.
-    # Note that each sorted list will be as long as the shorted input list.
-
-    instwise_matching_func = lambda gt_list, pr_list: match_instance_lists(
-        gt_list, pr_list, point_nonmatch_count
-    )
-
-    # PICK THE FUNCTION
-
-    inst_matching_func = nodewise_matching_func
-    # inst_matching_func = instwise_matching_func
-
-    # Calculate distances
-    frame_idxs, D, points_gt, points_pr = matched_instance_distances(
-        labels_gt, labels_pr, inst_matching_func
-    )
-
-    # Show mean difference for each node
-    node_names = labels_gt.skeletons[0].node_names
-
-    for node_idx, node_name in enumerate(node_names):
-        mean_d = np.nanmean(D[..., node_idx])
-        print(f"{node_name}\t\t{mean_d}")
```

## sleap/io/video.py

```diff
@@ -1114,16 +1114,17 @@
         """
         if np.isscalar(idxs):
             idxs = [idxs]
         return np.stack([self.get_frame(idx) for idx in idxs], axis=0)
 
     def get_frames_safely(self, idxs: Iterable[int]) -> Tuple[List[int], np.ndarray]:
         """Return list of frame indices and frames which were successfully loaded.
+        Args:
+            idxs: An iterable object that contains the indices of frames.
 
-        idxs: An iterable object that contains the indices of frames.
 
         Returns: A tuple of (frame indices, frames), where
             * frame indices is a subset of the specified idxs, and
             * frames has shape (len(frame indices), height, width, channels).
             If zero frames were loaded successfully, then frames is None.
         """
         frames = []
@@ -1438,27 +1439,39 @@
 
         with h5.File(path, "a") as f:
 
             if format:
 
                 def encode(img):
                     _, encoded = cv2.imencode("." + format, img)
-                    return np.squeeze(encoded)
+                    return np.squeeze(encoded).astype("int8")
+
+                # pad with zeroes to guarantee int8 type in hdf5 file
+                frames = []
+                for i in range(len(frame_numbers)):
+                    frames.append(encode(frame_data[i]))
+
+                max_frame_size = (
+                    max([len(x) if len(x) else 0 for x in frames]) if len(frames) else 0
+                )
 
-                dtype = h5.special_dtype(vlen=np.dtype("int8"))
                 dset = f.create_dataset(
-                    dataset + "/video", (len(frame_numbers),), dtype=dtype
+                    dataset + "/video",
+                    (len(frame_numbers), max_frame_size),
+                    dtype="int8",
+                    compression="gzip",
                 )
                 dset.attrs["format"] = format
                 dset.attrs["channels"] = self.channels
                 dset.attrs["height"] = self.height
                 dset.attrs["width"] = self.width
 
-                for i in range(len(frame_numbers)):
-                    dset[i] = encode(frame_data[i])
+                for i, frame in enumerate(frames):
+                    dset[i, 0 : len(frame)] = frame
+
             else:
                 f.create_dataset(
                     dataset + "/video",
                     data=frame_data,
                     compression="gzip",
                     compression_opts=9,
                 )
```

## sleap/io/visuals.py

```diff
@@ -23,28 +23,36 @@
 
 logger = logging.getLogger(__name__)
 
 # Object that signals shutdown
 _sentinel = object()
 
 
-def reader(out_q: Queue, video: Video, frames: List[int], scale: float = 1.0):
+def reader(
+    out_q: Queue,
+    video: Video,
+    frames: List[int],
+    scale: float = 1.0,
+    background: str = "original",
+):
     """Read frame images from video and send them into queue.
 
     Args:
         out_q: Queue to send (list of frame indexes, ndarray of frame images)
             for chunks of video.
         video: The `Video` object to read.
         frames: Full list frame indexes we want to read.
         scale: Output scale for frame images.
+        background: output video background. Either original, black, white, grey
 
     Returns:
         None.
     """
 
+    background = background.lower()
     cv2.setNumThreads(usable_cpu_count())
 
     total_count = len(frames)
     chunk_size = 64
     chunk_count = math.ceil(total_count / chunk_size)
 
     logger.info(f"Chunks: {chunk_count}, chunk size: {chunk_size}")
@@ -60,14 +68,24 @@
 
             t0 = perf_counter()
 
             # Safely load frames from video, skipping frames we can't load
             loaded_chunk_idxs, video_frame_images = video.get_frames_safely(
                 frames_idx_chunk
             )
+            if background != "original":
+                # fill the frame with the color
+                fill_values = {"black": 0, "grey": 127, "white": 255}
+                try:
+                    fill = fill_values[background]
+                except KeyError:
+                    raise ValueError(
+                        f"Invalid background color: {background}. Options include: {', '.join(fill_values.keys())}"
+                    )
+                video_frame_images = video_frame_images * 0 + fill
 
             if not loaded_chunk_idxs:
                 print(f"No frames could be loaded from chunk {chunk_i}")
                 i += 1
                 continue
 
             if scale != 1.0:
@@ -493,14 +511,15 @@
     filename: str,
     labels: Labels,
     video: Video,
     frames: List[int],
     fps: int = 15,
     scale: float = 1.0,
     crop_size_xy: Optional[Tuple[int, int]] = None,
+    background: str = "original",
     show_edges: bool = True,
     edge_is_wedge: bool = False,
     marker_size: int = 4,
     color_manager: Optional[ColorManager] = None,
     palette: str = "standard",
     distinctly_color: str = "instances",
     gui_progress: bool = False,
@@ -511,14 +530,15 @@
         filename: Output filename.
         labels: The dataset from which to get data.
         video: The source :class:`Video` we want to annotate.
         frames: List of frames to include in output video.
         fps: Frames per second for output video.
         scale: scale of image (so we can scale point locations to match)
         crop_size_xy: size of crop around instances, or None for full images
+        background: output video background. Either original, black, white, grey
         show_edges: whether to draw lines between nodes
         edge_is_wedge: whether to draw edges as wedges (draw as line if False)
         marker_size: Size of marker in pixels before scaling by `scale`
         color_manager: ColorManager object which determine what colors to use
             for what instance/node/edge
         palette: SLEAP color palette to use. Options include: "alphabet", "five+",
             "solarized", or "standard". Only used if `color_manager` is None.
@@ -533,15 +553,15 @@
 
     t0 = perf_counter()
 
     q1 = Queue(maxsize=10)
     q2 = Queue(maxsize=10)
     progress_queue = Queue()
 
-    thread_read = Thread(target=reader, args=(q1, video, frames, scale))
+    thread_read = Thread(target=reader, args=(q1, video, frames, scale, background))
     thread_mark = VideoMarkerThread(
         in_q=q1,
         out_q=q2,
         labels=labels,
         video_idx=labels.videos.index(video),
         scale=scale,
         show_edges=show_edges,
@@ -691,14 +711,23 @@
         type=str,
         default="instances",
         help=(
             "Specify how to color instances. Options include: 'instances', 'edges', "
             "and 'nodes' (default: 'nodes')"
         ),
     )
+    parser.add_argument(
+        "--background",
+        type=str,
+        default="original",
+        help=(
+            "Specify the type of background to be used to save the videos."
+            "Options for background: original, black, white and grey"
+        ),
+    )
     args = parser.parse_args(args=args)
     labels = Labels.load_file(
         args.data_path, video_search=[os.path.dirname(args.data_path)]
     )
 
     if args.video_index >= len(labels.videos):
         raise IndexError(f"There is no video with index {args.video_index}.")
@@ -726,14 +755,15 @@
         scale=args.scale,
         crop_size_xy=crop_size_xy,
         show_edges=args.show_edges > 0,
         edge_is_wedge=args.edge_is_wedge > 0,
         marker_size=args.marker_size,
         palette=args.palette,
         distinctly_color=args.distinctly_color,
+        background=args.background,
     )
 
     print(f"Video saved as: {filename}")
 
 
 # if __name__ == "__main__":
 #    main()
```

## sleap/io/format/deeplabcut.py

```diff
@@ -15,18 +15,18 @@
 import os
 import re
 import yaml
 
 import numpy as np
 import pandas as pd
 
-from typing import List, Optional
+from typing import List, Optional, Dict
 
 from sleap import Labels, Video, Skeleton
-from sleap.instance import Instance, LabeledFrame, Point
+from sleap.instance import Instance, LabeledFrame, Point, Track
 from sleap.util import find_files_by_suffix
 
 from .adaptor import Adaptor, SleapObjectType
 from .filehandle import FileHandle
 
 
 class LabelsDeepLabCutCsvAdaptor(Adaptor):
@@ -115,19 +115,20 @@
 
         if is_multianimal:
             # Reload with additional header rows if using new format.
             data = pd.read_csv(filename, header=[1, 2, 3])
 
             # Pull out animal and node names from the columns.
             start_col = 3 if is_new_format else 1
-            animal_names = []
+            tracks: Dict[str, Optional[Track]] = {}
             node_names = []
             for animal_name, node_name, _ in data.columns[start_col:][::2]:
-                if animal_name not in animal_names:
-                    animal_names.append(animal_name)
+                # Keep the starting frame index for each individual/track
+                if animal_name not in tracks.keys():
+                    tracks[animal_name] = None
                 if node_name not in node_names:
                     node_names.append(node_name)
 
         else:
             # Create the skeleton from the list of nodes in the csv file.
             # Note that DeepLabCut doesn't have edges, so these will need to be
             # added by user later.
@@ -173,31 +174,41 @@
                         f"Unable to determine frame index for image {img_files[i]}"
                     )
             else:
                 frame_idx = i
 
             instances = []
             if is_multianimal:
-                for animal_name in animal_names:
+                for animal_name in tracks.keys():
                     any_not_missing = False
                     # Get points for each node.
                     instance_points = dict()
                     for node in node_names:
-                        x, y = (
-                            data[(animal_name, node, "x")][i],
-                            data[(animal_name, node, "y")][i],
-                        )
+                        if (animal_name, node) in data.columns:
+                            x, y = (
+                                data[(animal_name, node, "x")][i],
+                                data[(animal_name, node, "y")][i],
+                            )
+                        else:
+                            x, y = np.nan, np.nan
                         instance_points[node] = Point(x, y)
                         if ~(np.isnan(x) and np.isnan(y)):
                             any_not_missing = True
 
                     if any_not_missing:
+                        # Create track
+                        if tracks[animal_name] is None:
+                            tracks[animal_name] = Track(spawned_on=i, name=animal_name)
                         # Create instance with points.
                         instances.append(
-                            Instance(skeleton=skeleton, points=instance_points)
+                            Instance(
+                                skeleton=skeleton,
+                                points=instance_points,
+                                track=tracks[animal_name],
+                            )
                         )
             else:
                 # Get points for each node.
                 any_not_missing = False
                 instance_points = dict()
                 for node in node_names:
                     x, y = data[(node, "x")][i], data[(node, "y")][i]
@@ -266,14 +277,16 @@
         # Load data from the YAML file
         project_data = yaml.load(file.text, Loader=yaml.SafeLoader)
 
         # Create skeleton which we'll use for each video
         skeleton = Skeleton()
         if project_data.get("multianimalbodyparts", False):
             skeleton.add_nodes(project_data["multianimalbodyparts"])
+            if "uniquebodyparts" in project_data:
+                skeleton.add_nodes(project_data["uniquebodyparts"])
         else:
             skeleton.add_nodes(project_data["bodyparts"])
 
         # Get subdirectories of videos and labeled data
         root_dir = os.path.dirname(filename)
         videos_dir = os.path.join(root_dir, "videos")
         labeled_data_dir = os.path.join(root_dir, "labeled-data")
@@ -294,21 +307,32 @@
             if csv_files:
                 csv_path = csv_files[0]
 
                 # Try to find a full video corresponding to this subdir.
                 # If subdirectory is foo, we look for foo.mp4 in videos dir.
 
                 shortname = os.path.split(data_subdir)[-1]
-                video_path = os.path.join(videos_dir, f"{shortname}.mp4")
+                video_path = None
+                if os.path.exists(videos_dir):
+                    with os.scandir(videos_dir) as file_iterator:
+                        for file in file_iterator:
+                            if not file.is_file():
+                                continue
+                            if os.path.splitext(file.name)[0] != shortname:
+                                continue
+                            video_path = os.path.join(videos_dir, file.name)
+                            break
 
-                if os.path.exists(video_path):
+                if video_path is not None and os.path.exists(video_path):
                     video = Video.from_filename(video_path)
                 else:
                     # When no video is found, the individual frame images
                     # stored in the labeled data subdir will be used.
+                    if video_path is None:
+                        video_path = os.path.join(videos_dir, f"{shortname}.mp4")
                     print(
                         f"Unable to find {video_path} so using individual frame images."
                     )
                     video = None
 
                 # Import the labeled fraems
                 labeled_frames.extend(
```

## sleap/io/format/hdf5.py

```diff
@@ -77,15 +77,18 @@
         video_search: Union[Callable, List[Text], None] = None,
         match_to: Optional[Labels] = None,
     ):
         f = file.file
 
         # Extract the Labels JSON metadata and create Labels object with just this
         # metadata.
-        dicts = json_loads(f.require_group("metadata").attrs["json"].tobytes().decode())
+        json = f.require_group("metadata").attrs["json"]
+        if not isinstance(json, str):
+            json = json.tobytes().decode()
+        dicts = json_loads(json)
 
         # These items are stored in separate lists because the metadata group got to be
         # too big.
         for key in ("videos", "tracks", "suggestions"):
             hdf5_key = f"{key}_json"
             if hdf5_key in f:
                 items = [json_loads(item_json) for item_json in f[hdf5_key]]
@@ -147,14 +150,53 @@
 
         # Shift the *non-predicted* points since these used to be saved with a gridline
         # coordinate system.
         if (format_id or 0) < 1.1:
             points_dset[:]["x"] -= 0.5
             points_dset[:]["y"] -= 0.5
 
+        def cast_as_compound(arr, dtype):
+            out = np.empty(shape=(len(arr),), dtype=dtype)
+            if out.size == 0:
+                return out
+            for i, (name, _) in enumerate(dtype):
+                out[name] = arr[:, i]
+            return out
+
+        # cast points, instances, and frames into complex dtype if not already
+        dtype_points = [("x", "<f8"), ("y", "<f8"), ("visible", "?"), ("complete", "?")]
+        if points_dset.dtype.kind != "V":
+            points_dset = cast_as_compound(points_dset, dtype_points)
+        if pred_points_dset.dtype.kind != "V":
+            pred_points_dset = cast_as_compound(pred_points_dset, dtype_points)
+
+        dtype_instances = [
+            ("instance_id", "<i8"),
+            ("instance_type", "u1"),
+            ("frame_id", "<u8"),
+            ("skeleton", "<u4"),
+            ("track", "<i4"),
+            ("from_predicted", "<i8"),
+            ("score", "<f4"),
+            ("point_id_start", "<u8"),
+            ("point_id_end", "<u8"),
+        ]
+        if instances_dset.dtype.kind != "V":
+            instances_dset = cast_as_compound(instances_dset, dtype_instances)
+
+        dtype_frames = [
+            ("frame_id", "<u8"),
+            ("video", "<u4"),
+            ("frame_idx", "<u8"),
+            ("instance_id_start", "<u8"),
+            ("instance_id_end", "<u8"),
+        ]
+        if frames_dset.dtype.kind != "V":
+            frames_dset = cast_as_compound(frames_dset, dtype_frames)
+
         # Rather than instantiate a bunch of Point\PredictedPoint objects, we will use
         # inplace numpy recarrays. This will save a lot of time and memory when reading
         # things in.
         points = PointArray(buf=points_dset, shape=len(points_dset))
 
         pred_points = PredictedPointArray(
             buf=pred_points_dset, shape=len(pred_points_dset)
@@ -279,17 +321,18 @@
 
             meta_group.attrs["format_id"] = cls.FORMAT_ID
 
             # If we are appending and there already exists JSON metadata
             if append and "json" in meta_group.attrs:
 
                 # Otherwise, we need to read the JSON and append to the lists
-                old_labels = labels_json.LabelsJsonAdaptor.from_json_data(
-                    meta_group.attrs["json"].tobytes().decode()
-                )
+                json = meta_group.attrs["json"]
+                if not isinstance(json, str):
+                    json = json.tobytes().decode()
+                old_labels = labels_json.LabelsJsonAdaptor.from_json_data(json)
 
                 # A function to join to list but only include new non-dupe entries
                 # from the right hand list.
                 def append_unique(old, new):
                     unique = []
                     for x in new:
                         try:
```

## sleap/io/format/sleap_analysis.py

```diff
@@ -74,15 +74,15 @@
 
         f = file.file
         tracks_matrix = f["tracks"][:].T
 
         # shape: frames * nodes * 2 * tracks
         frame_count, node_count, _, track_count = tracks_matrix.shape
 
-        if "track_names" in f:
+        if "track_names" in f and len(f["track_names"]):
             track_names_list = f["track_names"][:].T
             tracks = [Track(0, track_name.decode()) for track_name in track_names_list]
         else:
             tracks = [Track(0, f"track_{i}") for i in range(track_count)]
 
         if "node_names" in f:
             node_names_dset = f["node_names"][:].T
```

## sleap/nn/inference.py

```diff
@@ -1578,14 +1578,23 @@
         object_builder = Thread(target=_object_builder)
         object_builder.start()
 
         # Loop over batches.
         try:
             for ex in generator:
                 prediction_queue.put(ex)
+
+        except KeyError as e:
+            # Gracefully handle seeking errors by early termination.
+            if "Unable to load frame" in str(e):
+                pass  # TODO: Print warning obeying verbosity? (This code path is also
+                # called for interactive prediction where we don't want any spam.)
+            else:
+                raise
+
         finally:
             prediction_queue.put(None)
             object_builder.join()
 
         return predicted_frames
 
     def export_model(
@@ -2628,14 +2637,23 @@
         object_builder = Thread(target=_object_builder)
         object_builder.start()
 
         # Loop over batches.
         try:
             for ex in generator:
                 prediction_queue.put(ex)
+
+        except KeyError as e:
+            # Gracefully handle seeking errors by early termination.
+            if "Unable to load frame" in str(e):
+                pass  # TODO: Print warning obeying verbosity? (This code path is also
+                # called for interactive prediction where we don't want any spam.)
+            else:
+                raise
+
         finally:
             prediction_queue.put(None)
             object_builder.join()
 
         if self.tracker:
             self.tracker.final_pass(predicted_frames)
 
@@ -3261,14 +3279,23 @@
         object_builder = Thread(target=_object_builder)
         object_builder.start()
 
         # Loop over batches.
         try:
             for ex in generator:
                 prediction_queue.put(ex)
+
+        except KeyError as e:
+            # Gracefully handle seeking errors by early termination.
+            if "Unable to load frame" in str(e):
+                pass  # TODO: Print warning obeying verbosity? (This code path is also
+                # called for interactive prediction where we don't want any spam.)
+            else:
+                raise
+
         finally:
             prediction_queue.put(None)
             object_builder.join()
 
         if self.tracker:
             self.tracker.final_pass(predicted_frames)
 
@@ -3766,14 +3793,23 @@
         object_builder = Thread(target=_object_builder)
         object_builder.start()
 
         # Loop over batches.
         try:
             for ex in generator:
                 prediction_queue.put(ex)
+
+        except KeyError as e:
+            # Gracefully handle seeking errors by early termination.
+            if "Unable to load frame" in str(e):
+                pass  # TODO: Print warning obeying verbosity? (This code path is also
+                # called for interactive prediction where we don't want any spam.)
+            else:
+                raise
+
         finally:
             prediction_queue.put(None)
             object_builder.join()
 
         return predicted_frames
 
 
@@ -4453,14 +4489,23 @@
         object_builder = Thread(target=_object_builder)
         object_builder.start()
 
         # Loop over batches.
         try:
             for ex in generator:
                 prediction_queue.put(ex)
+
+        except KeyError as e:
+            # Gracefully handle seeking errors by early termination.
+            if "Unable to load frame" in str(e):
+                pass  # TODO: Print warning obeying verbosity? (This code path is also
+                # called for interactive prediction where we don't want any spam.)
+            else:
+                raise
+
         finally:
             prediction_queue.put(None)
             object_builder.join()
 
         return predicted_frames
 
     def export_model(
@@ -4730,14 +4775,23 @@
         object_builder = Thread(target=_object_builder)
         object_builder.start()
 
         # Loop over batches.
         try:
             for ex in generator:
                 prediction_queue.put(ex)
+
+        except KeyError as e:
+            # Gracefully handle seeking errors by early termination.
+            if "Unable to load frame" in str(e):
+                pass  # TODO: Print warning obeying verbosity? (This code path is also
+                # called for interactive prediction where we don't want any spam.)
+            else:
+                raise
+
         finally:
             prediction_queue.put(None)
             object_builder.join()
 
         return predicted_frames
 
 
@@ -4935,15 +4989,15 @@
     print("Args:")
     pprint(vars(args))
     print()
 
     export_model(
         args.models,
         args.export_path,
-        unrag_outputs=args.unrag,
+        unrag_outputs=(not args.ragged),
         max_instances=args.max_instances,
     )
 
 
 def _make_export_cli_parser() -> argparse.ArgumentParser:
     """Create argument parser for sleap-export CLI."""
 
@@ -4967,21 +5021,21 @@
         default="exported_model",
         help=(
             "Path to output directory where the frozen model will be exported to. "
             "Defaults to a folder named 'exported_model'."
         ),
     )
     parser.add_argument(
-        "-u",
-        "--unrag",
+        "-r",
+        "--ragged",
         action="store_true",
-        default=True,
+        default=False,
         help=(
-            "Convert ragged tensors into regular tensors with NaN padding. "
-            "Defaults to True."
+            "Keep tensors ragged if present. If ommited, convert ragged tensors"
+            " into regular tensors with NaN padding."
         ),
     )
     parser.add_argument(
         "-n",
         "--max_instances",
         type=int,
         help=(
```

## sleap/nn/system.py

```diff
@@ -44,25 +44,46 @@
         return available_gpus[0]
     else:
         raise ValueError("Multiple GPUs are available.")
 
 
 def use_cpu_only():
     """Hide GPUs from TensorFlow to ensure only the CPU is available."""
-    tf.config.set_visible_devices([], "GPU")
+    # RuntimeError: Visible devices cannot be modified after being initialized
+    try:
+        tf.config.set_visible_devices([], "GPU")
+    except RuntimeError as ex:
+        if (
+            len(ex.args) > 0
+            and ex.args[0]
+            == "Visible devices cannot be modified after being initialized"
+        ):
+            print(
+                "Failed to set visible GPU. Visible devices cannot be modified after being initialized."
+            )
 
 
 def use_gpu(device_ind: int):
     """Make a single GPU available to TensorFlow.
 
     Args:
         device_ind: Index of the GPU within the list of system GPUs.
     """
     gpus = get_all_gpus()
-    tf.config.set_visible_devices(gpus[device_ind], "GPU")
+    try:
+        tf.config.set_visible_devices(gpus[device_ind], "GPU")
+    except RuntimeError as ex:
+        if (
+            len(ex.args) > 0
+            and ex.args[0]
+            == "Visible devices cannot be modified after being initialized"
+        ):
+            print(
+                "Failed to set visible GPU. Visible devices cannot be modified after being initialized."
+            )
 
 
 def use_first_gpu():
     """Make only the first GPU available to TensorFlow."""
     use_gpu(0)
 
 
@@ -155,15 +176,15 @@
     gpus = get_available_gpus()
     all_gpus = get_all_gpus()
     if len(all_gpus) > 0:
         print(f"GPUs: {len(gpus)}/{len(all_gpus)} available")
         for gpu in all_gpus:
             print(f"  Device: {gpu.name}")
             print(f"         Available: {gpu in gpus}")
-            print(f"        Initalized: {is_initialized(gpu)}")
+            print(f"       Initialized: {is_initialized(gpu)}")
             print(
                 f"     Memory growth: {tf.config.experimental.get_memory_growth(gpu)}"
             )
 
     else:
         print("GPUs: None detected.")
```

## sleap/nn/data/augmentation.py

```diff
@@ -1,23 +1,15 @@
 """Transformers for applying data augmentation."""
 
-# Monkey patch for: https://github.com/aleju/imgaug/issues/537
-# TODO: Fix when PyPI/conda packages are available for version fencing.
-import numpy
-
-if hasattr(numpy.random, "_bit_generator"):
-    numpy.random.bit_generator = numpy.random._bit_generator
-
 import sleap
 import numpy as np
 import tensorflow as tf
 import attr
 from typing import List, Text, Optional
-import imgaug as ia
-import imgaug.augmenters as iaa
+import albumentations as A
 from sleap.nn.config import AugmentationConfig
 from sleap.nn.data.instance_cropping import crop_bboxes
 
 
 def flip_instances_lr(
     instances: tf.Tensor, img_width: int, symmetric_inds: Optional[tf.Tensor] = None
 ) -> tf.Tensor:
@@ -107,97 +99,109 @@
         instances = tf.tensor_scatter_nd_update(instances, subs1, pts2)
         instances = tf.tensor_scatter_nd_update(instances, subs2, pts1)
 
     return instances
 
 
 @attr.s(auto_attribs=True)
-class ImgaugAugmenter:
-    """Data transformer based on the `imgaug` library.
+class AlbumentationsAugmenter:
+    """Data transformer based on the `albumentations` library.
 
     This class can generate a `tf.data.Dataset` from an existing one that generates
     image and instance data. Element of the output dataset will have a set of
     augmentation transformations applied.
 
     Attributes:
-        augmenter: An instance of `imgaug.augmenters.Sequential` that will be applied to
+        augmenter: An instance of `albumentations.Compose` that will be applied to
             each element of the input dataset.
         image_key: Name of the example key where the image is stored. Defaults to
             "image".
         instances_key: Name of the example key where the instance points are stored.
             Defaults to "instances".
     """
 
-    augmenter: iaa.Sequential
+    augmenter: A.Compose
     image_key: str = "image"
     instances_key: str = "instances"
 
     @classmethod
     def from_config(
         cls,
         config: AugmentationConfig,
         image_key: Text = "image",
         instances_key: Text = "instances",
-    ) -> "ImgaugAugmenter":
+    ) -> "AlbumentationsAugmenter":
         """Create an augmenter from a set of configuration parameters.
 
         Args:
             config: An `AugmentationConfig` instance with the desired parameters.
             image_key: Name of the example key where the image is stored. Defaults to
                 "image".
             instances_key: Name of the example key where the instance points are stored.
                 Defaults to "instances".
 
         Returns:
-            An instance of `ImgaugAugmenter` with the specified augmentation
+            An instance of `AlbumentationsAugmenter` with the specified augmentation
             configuration.
         """
         aug_stack = []
         if config.rotate:
             aug_stack.append(
-                iaa.Affine(
-                    rotate=(config.rotation_min_angle, config.rotation_max_angle)
+                A.Rotate(
+                    limit=(config.rotation_min_angle, config.rotation_max_angle), p=1.0
                 )
             )
         if config.translate:
             aug_stack.append(
-                iaa.Affine(
+                A.Affine(
                     translate_px={
                         "x": (config.translate_min, config.translate_max),
                         "y": (config.translate_min, config.translate_max),
-                    }
+                    },
+                    p=1.0,
                 )
             )
         if config.scale:
-            aug_stack.append(iaa.Affine(scale=(config.scale_min, config.scale_max)))
-        if config.uniform_noise:
             aug_stack.append(
-                iaa.AddElementwise(
-                    value=(config.uniform_noise_min_val, config.uniform_noise_max_val)
-                )
+                A.Affine(scale=(config.scale_min, config.scale_max), p=1.0)
             )
+        if config.uniform_noise:
+
+            def uniform_noise(image, **kwargs):
+                return image + np.random.uniform(
+                    config.uniform_noise_min_val, config.uniform_noise_max_val
+                )
+
+            aug_stack.append(A.Lambda(image=uniform_noise))
         if config.gaussian_noise:
             aug_stack.append(
-                iaa.AdditiveGaussianNoise(
-                    loc=config.gaussian_noise_mean, scale=config.gaussian_noise_stddev
+                A.GaussNoise(
+                    mean=config.gaussian_noise_mean,
+                    var_limit=config.gaussian_noise_stddev,
                 )
             )
         if config.contrast:
             aug_stack.append(
-                iaa.GammaContrast(
-                    gamma=(config.contrast_min_gamma, config.contrast_max_gamma)
+                A.RandomGamma(
+                    gamma_limit=(config.contrast_min_gamma, config.contrast_max_gamma),
+                    p=1.0,
                 )
             )
         if config.brightness:
             aug_stack.append(
-                iaa.Add(value=(config.brightness_min_val, config.brightness_max_val))
+                A.RandomBrightness(
+                    limit=(config.brightness_min_val, config.brightness_max_val), p=1.0
+                )
             )
 
         return cls(
-            augmenter=iaa.Sequential(aug_stack),
+            augmenter=A.Compose(
+                aug_stack,
+                keypoint_params=A.KeypointParams(format="xy", remove_invisible=False),
+            ),
             image_key=image_key,
             instances_key=instances_key,
         )
 
     @property
     def input_keys(self) -> List[Text]:
         """Return the keys that incoming elements are expected to have."""
@@ -222,30 +226,24 @@
         Notes:
             The "scale" key in examples are not modified when scaling augmentation is
             applied.
         """
         # Define augmentation function to map over each sample.
         def py_augment(image, instances):
             """Local processing function that will not be autographed."""
-            # Ensure that the transformations applied to all data within this
-            # example are kept consistent.
-            aug_det = self.augmenter.to_deterministic()
-
-            # Augment the image.
-            aug_img = aug_det.augment_image(image.numpy())
-
-            # This will get converted to a rank 3 tensor (n_instances, n_nodes, 2).
-            aug_instances = np.full_like(instances, np.nan)
-
-            # Augment each set of points for each instance.
-            for i, instance in enumerate(instances):
-                kps = ia.KeypointsOnImage.from_xy_array(
-                    instance.numpy(), tuple(image.shape)
-                )
-                aug_instances[i] = aug_det.augment_keypoints(kps).to_xy_array()
+            # Convert to numpy arrays.
+            img = image.numpy()
+            kps = instances.numpy()
+            original_shape = kps.shape
+            kps = kps.reshape(-1, 2)
+
+            # Augment.
+            augmented = self.augmenter(image=img, keypoints=kps)
+            aug_img = augmented["image"]
+            aug_instances = np.array(augmented["keypoints"]).reshape(original_shape)
 
             return aug_img, aug_instances
 
         def augment(frame_data):
             """Wrap local processing function for dataset mapping."""
             image, instances = tf.py_function(
                 py_augment,
@@ -254,15 +252,14 @@
             )
             image.set_shape(frame_data["image"].get_shape())
             instances.set_shape(frame_data["instances"].get_shape())
             frame_data.update({"image": image, "instances": instances})
             return frame_data
 
         # Apply the augmentation to each element.
-        # Note: We map sequentially since imgaug gets slower with tf.data parallelism.
         output_ds = input_ds.map(augment)
 
         return output_ds
 
 
 @attr.s(auto_attribs=True)
 class RandomCropper:
```

## sleap/nn/data/pipelines.py

```diff
@@ -14,15 +14,15 @@
 import time
 from typing import Sequence, Text, Optional, List, Tuple, Union, TypeVar, Dict
 
 import sleap
 from sleap.nn.data.providers import LabelsReader, VideoReader
 from sleap.nn.data.augmentation import (
     AugmentationConfig,
-    ImgaugAugmenter,
+    AlbumentationsAugmenter,
     RandomCropper,
     RandomFlipper,
 )
 from sleap.nn.data.normalization import Normalizer
 from sleap.nn.data.resizing import Resizer, PointsRescaler, SizeMatcher
 from sleap.nn.data.instance_centroids import InstanceCentroidFinder
 from sleap.nn.data.instance_cropping import InstanceCropper, PredictedInstanceCropper
@@ -64,15 +64,15 @@
     SingleInstanceConfmapsHead,
     OffsetRefinementHead,
 )
 
 
 PROVIDERS = (LabelsReader, VideoReader)
 TRANSFORMERS = (
-    ImgaugAugmenter,
+    AlbumentationsAugmenter,
     RandomCropper,
     Normalizer,
     Resizer,
     SizeMatcher,
     InstanceCentroidFinder,
     InstanceCropper,
     MultiConfidenceMapGenerator,
@@ -402,15 +402,15 @@
             )
 
         if self.optimization_config.augmentation_config.random_flip:
             pipeline += RandomFlipper.from_skeleton(
                 self.data_config.labels.skeletons[0],
                 horizontal=self.optimization_config.augmentation_config.flip_horizontal,
             )
-        pipeline += ImgaugAugmenter.from_config(
+        pipeline += AlbumentationsAugmenter.from_config(
             self.optimization_config.augmentation_config
         )
         if self.optimization_config.augmentation_config.random_crop:
             pipeline += RandomCropper(
                 crop_height=self.optimization_config.augmentation_config.random_crop_height,
                 crop_width=self.optimization_config.augmentation_config.random_crop_width,
             )
@@ -546,15 +546,15 @@
                 provider=data_provider,
             )
         if self.optimization_config.augmentation_config.random_flip:
             pipeline += RandomFlipper.from_skeleton(
                 self.data_config.labels.skeletons[0],
                 horizontal=self.optimization_config.augmentation_config.flip_horizontal,
             )
-        pipeline += ImgaugAugmenter.from_config(
+        pipeline += AlbumentationsAugmenter.from_config(
             self.optimization_config.augmentation_config
         )
         if self.optimization_config.augmentation_config.random_crop:
             pipeline += RandomCropper(
                 crop_height=self.optimization_config.augmentation_config.random_crop_height,
                 crop_width=self.optimization_config.augmentation_config.random_crop_width,
             )
@@ -709,15 +709,15 @@
                 provider=data_provider,
             )
         if self.optimization_config.augmentation_config.random_flip:
             pipeline += RandomFlipper.from_skeleton(
                 self.data_config.labels.skeletons[0],
                 horizontal=self.optimization_config.augmentation_config.flip_horizontal,
             )
-        pipeline += ImgaugAugmenter.from_config(
+        pipeline += AlbumentationsAugmenter.from_config(
             self.optimization_config.augmentation_config
         )
         pipeline += Normalizer.from_config(self.data_config.preprocessing)
         pipeline += Resizer.from_config(self.data_config.preprocessing)
         pipeline += InstanceCentroidFinder.from_config(
             self.data_config.instance_cropping,
             skeletons=self.data_config.labels.skeletons,
@@ -859,15 +859,15 @@
             )
         aug_config = self.optimization_config.augmentation_config
         if aug_config.random_flip:
             pipeline += RandomFlipper.from_skeleton(
                 self.data_config.labels.skeletons[0],
                 horizontal=aug_config.flip_horizontal,
             )
-        pipeline += ImgaugAugmenter.from_config(aug_config)
+        pipeline += AlbumentationsAugmenter.from_config(aug_config)
         if aug_config.random_crop:
             pipeline += RandomCropper(
                 crop_height=aug_config.random_crop_height,
                 crop_width=aug_config.random_crop_width,
             )
         pipeline += Normalizer.from_config(self.data_config.preprocessing)
         pipeline += Resizer.from_config(self.data_config.preprocessing)
@@ -1024,15 +1024,15 @@
         aug_config = self.optimization_config.augmentation_config
         if aug_config.random_flip:
             pipeline += RandomFlipper.from_skeleton(
                 self.data_config.labels.skeletons[0],
                 horizontal=aug_config.flip_horizontal,
             )
 
-        pipeline += ImgaugAugmenter.from_config(aug_config)
+        pipeline += AlbumentationsAugmenter.from_config(aug_config)
         if aug_config.random_crop:
             pipeline += RandomCropper(
                 crop_height=aug_config.random_crop_height,
                 crop_width=aug_config.random_crop_width,
             )
         pipeline += Normalizer.from_config(self.data_config.preprocessing)
         pipeline += Resizer.from_config(self.data_config.preprocessing)
@@ -1182,15 +1182,15 @@
                 shuffle=True, buffer_size=self.optimization_config.shuffle_buffer_size
             )
         if self.data_config.preprocessing.resize_and_pad_to_target:
             pipeline += SizeMatcher.from_config(
                 config=self.data_config.preprocessing,
                 provider=data_provider,
             )
-        pipeline += ImgaugAugmenter.from_config(
+        pipeline += AlbumentationsAugmenter.from_config(
             self.optimization_config.augmentation_config
         )
         pipeline += Normalizer.from_config(self.data_config.preprocessing)
         pipeline += Resizer.from_config(self.data_config.preprocessing)
 
         pipeline += ClassVectorGenerator()
         pipeline += InstanceCentroidFinder.from_config(
```

## sleap/nn/data/providers.py

```diff
@@ -390,17 +390,15 @@
                     (x_scale, y_scale) of the example. This is always (1.0, 1.0) when
                     the images are initially read, but may be modified downstream in
                     order to keep track of scaling operations. This is especially
                     important to keep track of changes to the aspect ratio of the image
                     grid in order to properly map points to image coordinates.
         """
         # Grab an image to test for the dtype.
-        test_image = tf.convert_to_tensor(
-            self.video.get_frame(self.video.last_frame_idx)
-        )
+        test_image = tf.convert_to_tensor(self.video.get_frame(0))
         image_dtype = test_image.dtype
 
         def py_fetch_frame(ind):
             """Local function that will not be autographed."""
             frame_ind = int(ind.numpy())
             raw_image = self.video.get_frame(frame_ind)
             raw_image_size = np.array(raw_image.shape).astype("int32")
```

## Comparing `sleap-1.3.3.dist-info/LICENSE` & `sleap-1.4.0.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `sleap-1.3.3.dist-info/METADATA` & `sleap-1.4.0.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,32 +1,31 @@
 Metadata-Version: 2.1
 Name: sleap
-Version: 1.3.3
+Version: 1.4.0
 Summary: SLEAP (Social LEAP Estimates Animal Poses) is a deep learning framework for animal pose tracking.
 Home-page: https://sleap.ai
 Author: Talmo Pereira
 Author-email: talmo@salk.edu
 License: BSD 3-Clause License
 Project-URL: Documentation, https://sleap.ai/
 Project-URL: Bug Tracker, https://github.com/talmolab/sleap/issues
 Project-URL: Source Code, https://github.com/talmolab/sleap
 Keywords: deep learning,pose estimation,tracking,neuroscience
 Requires-Python: >=3.6
 Description-Content-Type: text/x-rst
 License-File: LICENSE
 License-File: AUTHORS
 Requires-Dist: imgstore (<0.3.0)
-Requires-Dist: ndx-pose
 Requires-Dist: nixio (>=1.5.3)
 Requires-Dist: qimage2ndarray
 Requires-Dist: segmentation-models
 Requires-Dist: pynwb (>=2.3.3)
 Requires-Dist: tensorflow-macos (==2.9.2) ; sys_platform == "darwin" and platform_machine == "arm64"
 Requires-Dist: tensorflow-metal (==0.5.0) ; sys_platform == "darwin" and platform_machine == "arm64"
-Requires-Dist: h5py (<3.2) ; sys_platform == "win32"
+Requires-Dist: tensorflow-hub (==0.12.0) ; sys_platform == "darwin" and platform_machine == "arm64"
 Provides-Extra: conda_dev
 Requires-Dist: pytest ; extra == 'conda_dev'
 Requires-Dist: pytest-qt (>=4.0.0) ; extra == 'conda_dev'
 Requires-Dist: pytest-cov (<=3.0.0) ; extra == 'conda_dev'
 Requires-Dist: pytest-xvfb ; extra == 'conda_dev'
 Requires-Dist: ipython ; extra == 'conda_dev'
 Requires-Dist: sphinx ; extra == 'conda_dev'
@@ -53,15 +52,15 @@
 Provides-Extra: dev
 Requires-Dist: attrs (<=21.4.0,>=21.2.0) ; extra == 'dev'
 Requires-Dist: cattrs (==1.1.1) ; extra == 'dev'
 Requires-Dist: jsmin ; extra == 'dev'
 Requires-Dist: jsonpickle (==1.2) ; extra == 'dev'
 Requires-Dist: networkx ; extra == 'dev'
 Requires-Dist: numpy (<1.23.0,>=1.19.5) ; extra == 'dev'
-Requires-Dist: opencv-python (<=4.6.0,>=4.2.0) ; extra == 'dev'
+Requires-Dist: opencv-python (<=4.7.0,>=4.2.0) ; extra == 'dev'
 Requires-Dist: pandas ; extra == 'dev'
 Requires-Dist: pillow (<=8.4.0,>=8.3.1) ; extra == 'dev'
 Requires-Dist: psutil ; extra == 'dev'
 Requires-Dist: pykalman (==0.9.5) ; extra == 'dev'
 Requires-Dist: pyyaml ; extra == 'dev'
 Requires-Dist: pyzmq ; extra == 'dev'
 Requires-Dist: qtpy (>=2.0.1) ; extra == 'dev'
@@ -69,14 +68,16 @@
 Requires-Dist: imgaug (==0.4.0) ; extra == 'dev'
 Requires-Dist: scipy (<=1.9.0,>=1.4.1) ; extra == 'dev'
 Requires-Dist: scikit-image ; extra == 'dev'
 Requires-Dist: scikit-learn (==1.0.*) ; extra == 'dev'
 Requires-Dist: scikit-video ; extra == 'dev'
 Requires-Dist: seaborn ; extra == 'dev'
 Requires-Dist: tensorflow-hub (<=0.14.0) ; extra == 'dev'
+Requires-Dist: albumentations ; extra == 'dev'
+Requires-Dist: ndx-pose ; extra == 'dev'
 Requires-Dist: urllib3 (<2.0) ; extra == 'dev'
 Requires-Dist: protobuf (<3.20) ; extra == 'dev'
 Requires-Dist: pytest ; extra == 'dev'
 Requires-Dist: pytest-qt (>=4.0.0) ; extra == 'dev'
 Requires-Dist: pytest-cov (<=3.0.0) ; extra == 'dev'
 Requires-Dist: pytest-xvfb ; extra == 'dev'
 Requires-Dist: ipython ; extra == 'dev'
@@ -107,15 +108,15 @@
 Provides-Extra: jupyter
 Requires-Dist: attrs (<=21.4.0,>=21.2.0) ; extra == 'jupyter'
 Requires-Dist: cattrs (==1.1.1) ; extra == 'jupyter'
 Requires-Dist: jsmin ; extra == 'jupyter'
 Requires-Dist: jsonpickle (==1.2) ; extra == 'jupyter'
 Requires-Dist: networkx ; extra == 'jupyter'
 Requires-Dist: numpy (<1.23.0,>=1.19.5) ; extra == 'jupyter'
-Requires-Dist: opencv-python (<=4.6.0,>=4.2.0) ; extra == 'jupyter'
+Requires-Dist: opencv-python (<=4.7.0,>=4.2.0) ; extra == 'jupyter'
 Requires-Dist: pandas ; extra == 'jupyter'
 Requires-Dist: pillow (<=8.4.0,>=8.3.1) ; extra == 'jupyter'
 Requires-Dist: psutil ; extra == 'jupyter'
 Requires-Dist: pykalman (==0.9.5) ; extra == 'jupyter'
 Requires-Dist: pyyaml ; extra == 'jupyter'
 Requires-Dist: pyzmq ; extra == 'jupyter'
 Requires-Dist: qtpy (>=2.0.1) ; extra == 'jupyter'
@@ -123,14 +124,16 @@
 Requires-Dist: imgaug (==0.4.0) ; extra == 'jupyter'
 Requires-Dist: scipy (<=1.9.0,>=1.4.1) ; extra == 'jupyter'
 Requires-Dist: scikit-image ; extra == 'jupyter'
 Requires-Dist: scikit-learn (==1.0.*) ; extra == 'jupyter'
 Requires-Dist: scikit-video ; extra == 'jupyter'
 Requires-Dist: seaborn ; extra == 'jupyter'
 Requires-Dist: tensorflow-hub (<=0.14.0) ; extra == 'jupyter'
+Requires-Dist: albumentations ; extra == 'jupyter'
+Requires-Dist: ndx-pose ; extra == 'jupyter'
 Requires-Dist: urllib3 (<2.0) ; extra == 'jupyter'
 Requires-Dist: protobuf (<3.20) ; extra == 'jupyter'
 Requires-Dist: ipykernel ; extra == 'jupyter'
 Requires-Dist: ipywidgets ; extra == 'jupyter'
 Requires-Dist: jupyterlab ; extra == 'jupyter'
 Requires-Dist: PySide2 (<=5.14.1,>=5.13.2) ; (platform_machine != "arm64") and extra == 'jupyter'
 Requires-Dist: tensorflow (<2.9,>=2.6.3) ; (platform_machine != "arm64") and extra == 'jupyter'
@@ -142,15 +145,15 @@
 Provides-Extra: pypi
 Requires-Dist: attrs (<=21.4.0,>=21.2.0) ; extra == 'pypi'
 Requires-Dist: cattrs (==1.1.1) ; extra == 'pypi'
 Requires-Dist: jsmin ; extra == 'pypi'
 Requires-Dist: jsonpickle (==1.2) ; extra == 'pypi'
 Requires-Dist: networkx ; extra == 'pypi'
 Requires-Dist: numpy (<1.23.0,>=1.19.5) ; extra == 'pypi'
-Requires-Dist: opencv-python (<=4.6.0,>=4.2.0) ; extra == 'pypi'
+Requires-Dist: opencv-python (<=4.7.0,>=4.2.0) ; extra == 'pypi'
 Requires-Dist: pandas ; extra == 'pypi'
 Requires-Dist: pillow (<=8.4.0,>=8.3.1) ; extra == 'pypi'
 Requires-Dist: psutil ; extra == 'pypi'
 Requires-Dist: pykalman (==0.9.5) ; extra == 'pypi'
 Requires-Dist: pyyaml ; extra == 'pypi'
 Requires-Dist: pyzmq ; extra == 'pypi'
 Requires-Dist: qtpy (>=2.0.1) ; extra == 'pypi'
@@ -158,14 +161,16 @@
 Requires-Dist: imgaug (==0.4.0) ; extra == 'pypi'
 Requires-Dist: scipy (<=1.9.0,>=1.4.1) ; extra == 'pypi'
 Requires-Dist: scikit-image ; extra == 'pypi'
 Requires-Dist: scikit-learn (==1.0.*) ; extra == 'pypi'
 Requires-Dist: scikit-video ; extra == 'pypi'
 Requires-Dist: seaborn ; extra == 'pypi'
 Requires-Dist: tensorflow-hub (<=0.14.0) ; extra == 'pypi'
+Requires-Dist: albumentations ; extra == 'pypi'
+Requires-Dist: ndx-pose ; extra == 'pypi'
 Requires-Dist: urllib3 (<2.0) ; extra == 'pypi'
 Requires-Dist: protobuf (<3.20) ; extra == 'pypi'
 Requires-Dist: PySide2 (<=5.14.1,>=5.13.2) ; (platform_machine != "arm64") and extra == 'pypi'
 Requires-Dist: tensorflow (<2.9,>=2.6.3) ; (platform_machine != "arm64") and extra == 'pypi'
 Requires-Dist: python-rapidjson ; (sys_platform != "win32") and extra == 'pypi'
 Requires-Dist: PySide6 ; (sys_platform == "darwin" and platform_machine == "arm64") and extra == 'pypi'
 Requires-Dist: tensorflow-macos (==2.9.2) ; (sys_platform == "darwin" and platform_machine == "arm64") and extra == 'pypi'
@@ -254,15 +259,15 @@
 
 See the docs for `full installation instructions <https://sleap.ai/installation.html>`_.
 
 Learn to SLEAP
 --------------
 - **Learn step-by-step**: `Tutorial <https://sleap.ai/tutorials/tutorial.html>`_
 - **Learn more advanced usage**: `Guides <https://sleap.ai/guides/>`__ and `Notebooks <https://sleap.ai/notebooks/>`__
-- **Learn by watching**: `MIT CBMM Tutorial <https://cbmm.mit.edu/video/decoding-animal-behavior-through-pose-tracking>`_
+- **Learn by watching**: `ABL:AOC 2023 Workshop <https://www.youtube.com/watch?v=BfW-HgeDfMI>`_ and `MIT CBMM Tutorial <https://cbmm.mit.edu/video/decoding-animal-behavior-through-pose-tracking>`_
 - **Learn by reading**: `Paper (Pereira et al., Nature Methods, 2022) <https://www.nature.com/articles/s41592-022-01426-1>`__ and `Review on behavioral quantification (Pereira et al., Nature Neuroscience, 2020) <https://rdcu.be/caH3H>`_
 - **Learn from others**: `Discussions on Github <https://github.com/talmolab/sleap/discussions>`_
 
 
 References
 -----------
 SLEAP is the successor to the single-animal pose estimation software `LEAP <https://github.com/talmo/leap>`_ (`Pereira et al., Nature Methods, 2019 <https://www.nature.com/articles/s41592-018-0234-5>`_).
```

## Comparing `sleap-1.3.3.dist-info/RECORD` & `sleap-1.4.0.dist-info/RECORD`

 * *Files 6% similar despite different names*

```diff
@@ -2,28 +2,28 @@
 sleap/diagnostic.py,sha256=CBBOyzzxQSiAsoBoNd-VRNYXbJ6iT1j2yHJUbZUW9lw,4109
 sleap/instance.py,sha256=jBS125nfYenO2tuEu0yRgm01a5o1d3-TXSEiZvtQod8,63933
 sleap/message.py,sha256=2F1D5f9mh9uApPBzg42pY9HRdtIa2ul6vOYCXJlFChY,8316
 sleap/prefs.py,sha256=GNm0h7Xq0AABTgNIf_8ZeHUYdTyGnKfD5fDR9ASukzw,2206
 sleap/rangelist.py,sha256=IRchDqjlHha0kVNWrBod7u9IQyIFPLQeVLMBrwna5co,4653
 sleap/skeleton.py,sha256=ZBhI7RQZZwNKRbPxe_9pS7WbwlPRXUAxATB6g_QP1ZY,41735
 sleap/util.py,sha256=yVyFXj0k8VnBw4OvCOnMY_J_NCvfnnUU-MMv4ZNbj94,12005
-sleap/version.py,sha256=Hkf5AwF7n37YV_jgDUfb_Aamyq7RdLaRzviR1vXHvZ0,938
+sleap/version.py,sha256=s5xFjbqy2qg7Q_1xtmQmmzYr6A7q9tsVadQMo2YR8SQ,938
 sleap/config/colors.yaml,sha256=BaiHZCClXMZ0QILjIjdoTEluBcjmmEobq1TO3vSrY6Y,1181
 sleap/config/head_type_form.yaml,sha256=ZAUKcNvKF26iJN6kvOeyt7IbCdoZjpgVA_kzYLr-z4c,113
-sleap/config/labeled_clip_form.yaml,sha256=DEpC_yE1JxpcXDoTdOrOng9PvvMYCX6n31Dl5cY2PxU,479
+sleap/config/labeled_clip_form.yaml,sha256=Gum8bj85TBw1Ce98TNrfjhl6sj7am7mysJ3FBqCj0-w,582
 sleap/config/path_prefixes.yaml,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-sleap/config/pipeline_form.yaml,sha256=UtmTmTzR9KrBB5VQ8JQ03blBKZ-fGNfpq0-ELTtlPbU,22045
+sleap/config/pipeline_form.yaml,sha256=dTEenr7W8CX5vXA6kqwirU_0l4dOI-jSPy4a4ssWsMc,22063
 sleap/config/shortcuts.yaml,sha256=SvtoDUIeOhNoKiqovUSI-ARRBRC_A3gjFT0F3HB5EUk,969
 sleap/config/suggestions.yaml,sha256=dLDsoxPZj55GK5i3sm6eGnh76Drl9_Wwnegc_8_Gs9E,4411
-sleap/config/training_editor_form.yaml,sha256=OlkZWyqfY9V5cHougSZ4GeRpBp75MVp3qktR9NN4LTg,33738
+sleap/config/training_editor_form.yaml,sha256=3RGXd9npv2KrltxrM01q7PkG8IhOad8K3zo7GhYXIH0,33753
 sleap/gui/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-sleap/gui/app.py,sha256=WYgAkicbFAUaP6ywh2YMI01Zl7JkjWCTOgJ6S-ZGZo4,59978
+sleap/gui/app.py,sha256=dUu7oRJfQl5NLDZftSiBfgPIkj377loJy-53Mu-e2IY,60981
 sleap/gui/background.png,sha256=iAS3i60E-235QrGDGW6gMenj2cNmTDuA3GAQrLHR6qQ,423849
 sleap/gui/color.py,sha256=4RnWvnERM6KAZYA2K5-4jHyFtItL_YZXH93mhFiwBw8,10027
-sleap/gui/commands.py,sha256=rLCdWTfT0ZaNG8Cje8k2StXQnKqBb446311QDwuMl8w,112097
+sleap/gui/commands.py,sha256=JNj2ZE86iGhtQTcB0GodPU_nJhBVMQY1sPpX2wTsVsc,116429
 sleap/gui/dataviews.py,sha256=dcDdjehU9E7B_FLj4z1y-lUD-PdBS24BecHCvUGB6ww,23106
 sleap/gui/icon.png,sha256=AREY4hITAGkrgUm4ozzl1iMaA3KjaFzynr20aB4txYc,123167
 sleap/gui/shortcuts.py,sha256=HpeAAfuSQbbVBjIQj8AOYQC6q8VMBNYbrssB0vdYyvw,4428
 sleap/gui/state.py,sha256=uwyTcqvFUjelEB1wRoN9TVqpXugug4cm9JeFJkY6wIA,6614
 sleap/gui/suggestions.py,sha256=NKca44wGtdLi8dpWLawEJ88xgUtBCeMS0a60tW7-GqE,12294
 sleap/gui/web.py,sha256=iMVzmTGAdcs83vbXyrtCBqOOqCGe2BpzT5INjgrBnb8,5490
 sleap/gui/dialogs/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
@@ -37,21 +37,21 @@
 sleap/gui/dialogs/metrics.py,sha256=2F9Koha-Q7Nwbq2nUCdT1q3HKrbhgQXAxgDJRDbd8e4,12059
 sleap/gui/dialogs/missingfiles.py,sha256=beUHwtLk-IuYmnsRlXsNo07m_HZddv1yRGhR6E5-1lc,7467
 sleap/gui/dialogs/query.py,sha256=9e9J3DDfk1iNq3nQDtRnf4Xqt4G5BJapo5bhqNEQvKk,1052
 sleap/gui/dialogs/shortcuts.py,sha256=65HM30co5AHlS8blA1N461PJ6PAgXdBUMUyX4T_gC_0,3575
 sleap/gui/learning/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 sleap/gui/learning/configs.py,sha256=679-PCk6u5z51uyJi1ufBQJuTocMgg1Tp7gmKvW5_XI,17308
 sleap/gui/learning/datagen.py,sha256=Zay27f8jKXQDno1rs4RVjLYFKfiiBTn6Gs7x1yOXmYU,5923
-sleap/gui/learning/dialog.py,sha256=IduRiEhW01SfsL6E3pRyUDoEM_aeoJldGeVY5aH9p9s,51453
+sleap/gui/learning/dialog.py,sha256=n6TK_s3FeMDmpgQ7ts9_rnzawbrpB1uH7brTB8UwmCY,52442
 sleap/gui/learning/receptivefield.py,sha256=P_D37TGvxuJ8gGM_3yc-lEiaKbmblkvnDFcFzdPa7tU,7832
 sleap/gui/learning/runners.py,sha256=YZ3MjvbnGTvuLFO4_xQ1kTaH_ChsONl3Xrawu7CKxZA,30015
 sleap/gui/learning/scopedkeydict.py,sha256=Dubamb8cMgOuu9N5wqV0zcaEclXU1JbpfWTbj_DdoTI,7803
 sleap/gui/overlays/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 sleap/gui/overlays/anchors.py,sha256=j4Sw5p33O5O7Atv-4yntD8EMgw4lTXwHd2QHMarQ2Pk,1457
-sleap/gui/overlays/base.py,sha256=b5cRwyifQo54Gylb1NMZ3ZKvTe3yn__6B8ckPcU6Xmc,7368
+sleap/gui/overlays/base.py,sha256=N2jYCNoCbFnQ9OMpqM1xRSJ0pIY1gJ7g-FWmpfp1kCE,7347
 sleap/gui/overlays/confmaps.py,sha256=8jlpbLR1hqLK2G5MA0ZRixUavWQ7l9FJPRLQC0m-KFI,5400
 sleap/gui/overlays/instance.py,sha256=EQuG4GjGiaQvo1mZ65A4-Qf9-Lohle2esfh-VEO2Eco,2033
 sleap/gui/overlays/pafs.py,sha256=H5jpv80op-Nbaumb7NLuBpNtpDT2Zzax2e_U2Cg2Y2w,10117
 sleap/gui/overlays/tracks.py,sha256=r7kp5oELO4GBQLCGhOM7NrFonEP6nbl_uUw0Vu7yIMU,8998
 sleap/gui/widgets/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 sleap/gui/widgets/docks.py,sha256=GlpAKjNsf3971cWWwh1UsFsKtp749j7TCc5Jl5mpZis,18522
 sleap/gui/widgets/imagedir.py,sha256=m94tQWebea0VvfJmMKqzmESQR6T_j8RbLh7WcKnXx-Q,4567
@@ -62,56 +62,56 @@
 sleap/gui/widgets/training_monitor.py,sha256=BBuMPPGkKF1m9NjGYAXkShxCxuWVmeVUsohuwWHWzPw,22322
 sleap/gui/widgets/video.py,sha256=ogLCFmsWxW98-o3sutikzRF1M88Tt2l5aRP2mLznBcs,83393
 sleap/gui/widgets/views.py,sha256=b6QGAMuP5vR1OhJVufxpTHBIez6SEn4_SnTK6GKjnrM,3522
 sleap/info/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 sleap/info/align.py,sha256=jRIk04oKvUzLeR5dps-DsHYhzVLDVtSHmwPkxtG8Cwc,8775
 sleap/info/feature_suggestions.py,sha256=kzmymC5AkVaLh8BmarC8FJWwndsuAHujyPX4-FfK8iw,25741
 sleap/info/labels.py,sha256=K9mcA0M29FRRkGBBG_5iKpTSHr-qSR0AlFNDxoSX5n0,4603
-sleap/info/metrics.py,sha256=kef5_8WUWBGgSpIpBVAzuygIEsFE_Jb39_GMNyI7v6Y,10493
+sleap/info/metrics.py,sha256=AYVzG3Qu78iSilQJhVKs8N5P1NPEzog2Auluog2fTS0,9141
 sleap/info/summary.py,sha256=LFGLit_8wr1jFkLR22aIehcE843nFdaGA801EFo1hrY,9118
 sleap/info/trackcleaner.py,sha256=2Wt7CNfHJAxsYTyrESuaEzYg4IQiLK0aSyxZDAjwlV0,1917
 sleap/info/write_tracking_h5.py,sha256=hrqouA-_8hePpQL1CFF91Sl0Sp8D8IJFjaLBigTXaJQ,16626
 sleap/io/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 sleap/io/asyncvideo.py,sha256=buZxmOmbd_1qGtxShCfYkqQbivamwa88chQHwaGS3Rk,7174
 sleap/io/convert.py,sha256=XpRCeGtvhyHik2blKsi2tRzMUBmjLMnY7KN-8HXOqa0,6312
 sleap/io/dataset.py,sha256=6MpCsw1icT8LG3-rS92HtbqjMGWUMMp_pjQX30pWAFI,107724
 sleap/io/legacy.py,sha256=FakeTpHTrXXooeof5TXpvEb5WzVVozrkThhIcqh0pbU,9189
 sleap/io/pathutils.py,sha256=zH0q7uKw2TOt0kkMXbkSrdBbdLy3YgVZZLPqvNiEArs,4844
-sleap/io/video.py,sha256=nfs16B7lxW6AwLHoDfLcaBJHf_-FNo8X6OJf0JTOj6g,55054
+sleap/io/video.py,sha256=7K4l8gkpkujlMG75F-LQsIj64FwWrgQspjaTPFvqz1c,55471
 sleap/io/videowriter.py,sha256=PPK1-VZh6i1DTG0fuGHeNCxKuiW53iKzpkbqPlVdRXQ,3104
-sleap/io/visuals.py,sha256=7p8Vcm7GXz0UUzdSYo0v_8CUR1y3RbGaz7ClbO0h8-M,23792
+sleap/io/visuals.py,sha256=r1r9sPYkergIeusO9vns-Lk-Y1GSKbUxD3RtA-dTj5Y,24905
 sleap/io/format/__init__.py,sha256=WQff2hrcfsy9MeU7-s7b2qQ6R5EEJULeciE1uZXQPHI,30
 sleap/io/format/adaptor.py,sha256=apMLb4fIAgv9h-T0po7bIsF2xZALGspJ26rT9mtDo8c,2743
 sleap/io/format/alphatracker.py,sha256=qOVQuI0ufbLNTvDpqyE--DAcuwRk8cM3PmgMzKEHJyY,16437
 sleap/io/format/coco.py,sha256=0LHS1DLo2K4aSCl5or2dpzdFbJ9Sei08vgQzYs1aMeQ,6871
 sleap/io/format/csv.py,sha256=41hhn5fe69qKo4MZ1IP7zhELpDjrIKS5hI8ZCtwyOyE,1885
-sleap/io/format/deeplabcut.py,sha256=xzRZHii6WJJefMCX0sZQeMtUJ7-rtQPfMencZcLbabk,10721
+sleap/io/format/deeplabcut.py,sha256=CumGBpcOX5btlzUq0a9JGb2GEwKXGAcMX8BIQM2_efA,12037
 sleap/io/format/deepposekit.py,sha256=w27efvl2WwyC43SNi5bME_XWyVL3NVD0_5Qkesbx0Yo,2540
 sleap/io/format/dispatch.py,sha256=kbjY_IUkQGI0cllO1PAvzQs4NgG-H8xLqfgmMQll8bo,3808
 sleap/io/format/filehandle.py,sha256=vbNHaJUYZSEUytxjg_ZnSUwtZ3TdrUqF1QJtvw8Ml70,3241
 sleap/io/format/genericjson.py,sha256=7A0hnyE3P1voK3Bs_eGsHIs4uGuT2QBF_FAWg3UWyeQ,1064
-sleap/io/format/hdf5.py,sha256=r1pcN72akjccAjn2dE9QgNf1XStBW8A7ci6Cj9tsma0,20757
+sleap/io/format/hdf5.py,sha256=NfFyLE9zbL7juAezmeoCmY3wuiezKdiOshpouS7HkcA,22389
 sleap/io/format/labels_json.py,sha256=yUob6VQ02igygP9sQzMueK_24_wfoTZJKvlxNR9Nd-A,21088
 sleap/io/format/leap_matlab.py,sha256=gxlxVwoCAf-34Wom7CC0jajHm5rzawIZNndz26IywSs,4109
 sleap/io/format/main.py,sha256=pK5TmF84L9uXGNjhiVJqUhl06qpuP21dXaqT2xa_S0w,6361
 sleap/io/format/ndx_pose.py,sha256=3_PzFK_heS_kxlvSt-2W9yrhoz61U8Q6xQftITS5ghA,14132
 sleap/io/format/nix.py,sha256=8UJ3fb3dSczCEbvKACyTJ9jmN-Fp94T0Wcs4P869IPI,17846
-sleap/io/format/sleap_analysis.py,sha256=Nu3Pe0d5fAW7q8vgeA-EMF7yr778gdNdfUq_DpzA4jE,4833
+sleap/io/format/sleap_analysis.py,sha256=spVVMR_HShtELpNpPv8hu-XbASQYjJ14t5396IrpK-8,4859
 sleap/io/format/text.py,sha256=spCfU-gPi59gD2UqS8HPpAHVmL6ySfdA7O3GQcuMgVQ,965
 sleap/nn/__init__.py,sha256=Zfur1NR6bgDogkYPPoVr-WRLnkaijjXlbUpMbBj0rSU,449
 sleap/nn/callbacks.py,sha256=DcosXRseS0dlF8Zzfw92BY-_ZwzzYKxQ1QP80Z5lQsE,10013
 sleap/nn/evals.py,sha256=Z8eao1uPKibmvSU9L_TX2nT7dadWWiNbcly34sO3A0M,30099
 sleap/nn/heads.py,sha256=iPWFzG-OQ5-MrSQSbxk2ulv5pIvs8l_SYGsHY_y6lAo,18212
 sleap/nn/identity.py,sha256=v0pcsI5enNIeU7fhm-3lpTSdoF7O7vsJpCHYRmt7oeU,9177
-sleap/nn/inference.py,sha256=D4o6P6xlPcCC98HxHGyCO-I6CuIPqp1_6phBe0AkMag,224133
+sleap/nn/inference.py,sha256=TomlybQdEBAP7GIzIfa6QOHdPPKNU3CrXK2quDpCI2g,226328
 sleap/nn/losses.py,sha256=WSy2MIpDMtKnIk1oQDT7RR4nd6O04B1JWhbHMx6VrUg,5673
 sleap/nn/model.py,sha256=kk2_KTVuqJZt3eYQQD32IHne6kFgdZJEJYOc6yyWFGY,13278
 sleap/nn/paf_grouping.py,sha256=dWkAgzCAwMJTebPmWtH0Ar3tmpDrzNramoeeqrSUOds,74698
 sleap/nn/peak_finding.py,sha256=r1tV_Ad6_e2uZr0iJET5-EseTmvCzDcZK1e-fY6J7cs,25718
-sleap/nn/system.py,sha256=UNAILmpVmorvy-21SaA0JlUMcz9Q3vZ0f4du65RwcDw,6885
+sleap/nn/system.py,sha256=BuMCxWfx9vrBv-tUK2mZCug42X92S4j5tKZIHdbP0XY,7640
 sleap/nn/tracking.py,sha256=nO1csljKfndL6SFAEyWt7MOd0xlVUA9WPglqNiB9P7A,54310
 sleap/nn/training.py,sha256=NjpQO-UcTFxQ40wBb9q0Gfz5YlLE15NU6oYI1Aoxf-w,75381
 sleap/nn/utils.py,sha256=k23RLWoN-2YRY1ox3riTf3DDFe01cy6LShIQtv6Kstg,5449
 sleap/nn/viz.py,sha256=StlAOJkSI8JsG6rguEvXjG6HaK7-LU6BFnSyWZkISmQ,11047
 sleap/nn/architectures/__init__.py,sha256=QidpLoeWyUjdrC_GzjZLCijRBMJKgYqjTGwURsN8utI,681
 sleap/nn/architectures/common.py,sha256=xtlEcQgWu026vJbMJaxiTj7L6X3BK4wRb7_44I_1f00,752
 sleap/nn/architectures/encoder_decoder.py,sha256=4Pi7JX_-sFH0N8pOM9OH6zuKMM1kUaSbriVfmIn4gaY,27650
@@ -126,28 +126,28 @@
 sleap/nn/config/data.py,sha256=IxeYeUTE54OZNd3rHIj48z2gB-eaZ6msQa8GF6udEb0,8736
 sleap/nn/config/model.py,sha256=XH_kORjW_MW8aBYfxyjMLMyE8sBjDqAWylSywoWp_2U,33172
 sleap/nn/config/optimization.py,sha256=d_cxZBSfqGLx2AcGDn9uoskiXYnvZr5KMIuSpSldCcQ,13792
 sleap/nn/config/outputs.py,sha256=HgGkfQS1s4hNzhnUsU-c2C55NxYH9km8qKrLfZMqxCc,11154
 sleap/nn/config/training_job.py,sha256=8UGHJIcYt6S2hR6Yjb3q5BvRPs1_EISJoi9_-jaTWZ0,6239
 sleap/nn/config/utils.py,sha256=7ajEhZ9NVqDV_THXvUK27DZkb3TYvMNpWT5m-fWHnU0,2769
 sleap/nn/data/__init__.py,sha256=K5nkQaATa-mpqGfrkAbe4Os8Yo2IAk2p5sGybOcHIzQ,456
-sleap/nn/data/augmentation.py,sha256=BOEGXQ__JFWxSBkLymWeWJz2ht2Mhl0aeeQXpr2H5Kw,16665
+sleap/nn/data/augmentation.py,sha256=eqvDJFJuE1Wu4QxXpG3anrDIpQMxzqG9NIp1LL-emIg,16325
 sleap/nn/data/confidence_maps.py,sha256=nHebCWNIyAuJrFexGEepal4zbhbJ_JxkQbcHIHG59cQ,23439
 sleap/nn/data/dataset_ops.py,sha256=Lp3Sb3I-3VCPP1eZ_a5drwngumygiUBGAW5Rv7AX8Sw,12553
 sleap/nn/data/edge_maps.py,sha256=bDm_EzeuhqH6fVyaqAv8YPFjYLqDoTAGFmaWiWUaoRg,14764
 sleap/nn/data/general.py,sha256=7QWwmiWDrCuY6bpklf_5TTs0QI9dwSi0KK65Tw_Q5LM,4974
 sleap/nn/data/grouping.py,sha256=ibPfNNRCniwi3GKoh7o3fCbOtBpHev-uHr1lL-fmtgw,3059
 sleap/nn/data/identity.py,sha256=EKQLxRLFsRZsB1DUeTZ8irKMD9r6d5nkyCpV-4Add_A,7857
 sleap/nn/data/inference.py,sha256=uRlreubnJGZAK3ZP8CpuZJdelI3XAUrHon1Xdrph8IQ,12356
 sleap/nn/data/instance_centroids.py,sha256=Gp0mqaznlhR_XIthTxm_Q0MhKhC_qFYJA1KgDoatYi8,8058
 sleap/nn/data/instance_cropping.py,sha256=dd3KF7bMf2xUOG4X-JrUp9_KP7nK0smyF5Ff877jnCg,23401
 sleap/nn/data/normalization.py,sha256=oUDBXDQ0NhiBKQvSOk5-muFsqhxi4iWV04BpbUFIrVg,14260
 sleap/nn/data/offset_regression.py,sha256=uD9ycIGINA2apf-lC-a-tiamzkRvl6rSpE3SUyPxfEg,3291
-sleap/nn/data/pipelines.py,sha256=STerS26uRjJrHn7Rhcp6-GLW4_7Rxce2NAfvKdjHPdY,48997
-sleap/nn/data/providers.py,sha256=FlZ0j_EpcNJV923N15q0cvtzPrj0pFZAtew3u6jpYR8,18846
+sleap/nn/data/pipelines.py,sha256=7ae4cEhWDmJfjEOplPA3TXIzbM6qVGKFvjZ61p6OHko,49061
+sleap/nn/data/providers.py,sha256=QmvWX2yX4_1wqDXNEWccLC2zlWuMXhr6ZvrXcagLnvM,18800
 sleap/nn/data/resizing.py,sha256=EoC2irG0d7P0zO41lbHxn-wk0ohw4JVJ7KpSL98wOlw,21216
 sleap/nn/data/training.py,sha256=dHJWmB80PLyGmj2_WeN5JU3g5cuDfs8du0A6uDYqB8A,9756
 sleap/nn/data/utils.py,sha256=yVB25z5-5CE1kWKL6ZGKPkOzlL9ORKFVWbo_5jklRiA,6936
 sleap/nn/tracker/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 sleap/nn/tracker/components.py,sha256=w2vt-Y9ltOTyAP9KopxFX17rcwT3NSlYVnyWofs_Bj0,19176
 sleap/nn/tracker/kalman.py,sha256=3eWbI2gsPdDvhr5vAgVBnAVXekDO51YSX1-pbznwwgQ,24539
 sleap/skeletons/bees.json,sha256=t-8wqpRsM8X1HUTs1Sau3_6T4srLyl5OAH-G1Avi0tQ,44761
@@ -163,14 +163,14 @@
 sleap/training_profiles/baseline_medium_rf.bottomup.json,sha256=vHMi34K0nvQML2EVMUaOeNTCtdtGjXWL8IqQ5Ycu510,4639
 sleap/training_profiles/baseline_medium_rf.single.json,sha256=hWhZfs8H1SdjXMrumah3pgVPYCD43Eyzf9G2l-3Zs3M,4343
 sleap/training_profiles/baseline_medium_rf.topdown.json,sha256=Z0zsO27uLi8O17bGE6Hm-MAuSibZG73wWoPZs2UFuJ4,4522
 sleap/training_profiles/pretrained.bottomup.json,sha256=9fBTY_Lp5T9HarNX-6ULZoLD0XIi5rZ0_z6OTXnsSxY,4552
 sleap/training_profiles/pretrained.centroid.json,sha256=k65SwJK0GQV1-IbA1kntFddznEB2BZNb_AAWmxt1WTI,4259
 sleap/training_profiles/pretrained.single.json,sha256=0KoeuagELfeTjJFMYlHmXqTRCVfy1fnIOLJSjvpZ-uY,4256
 sleap/training_profiles/pretrained.topdown.json,sha256=oQMNb_zShzPXnSnNC8uVKrNyteJ9lJ5tBEIbIEVqBm4,4293
-sleap-1.3.3.dist-info/AUTHORS,sha256=MPCP1ZBcj52nss4-bgDPMCsR2N43CKoQPKjJIjW45mI,556
-sleap-1.3.3.dist-info/LICENSE,sha256=n3feSVEF4UPtYD9NcHgW2Y9hx-nU-JLZ3bSkE_mvRrA,1748
-sleap-1.3.3.dist-info/METADATA,sha256=hZHTDFWUd1vJvRg7tDErAI9PFFM96qRlt7t5zBUCrPc,17463
-sleap-1.3.3.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
-sleap-1.3.3.dist-info/entry_points.txt,sha256=2S0y0zfDrATdxApJ-oNRHOeCiyjCc4NEc5Wi2qMtPwU,326
-sleap-1.3.3.dist-info/top_level.txt,sha256=Au4QsAQYBEAnoWobrly9pR5Iy7GuOW-kJEn2Ame-F0U,6
-sleap-1.3.3.dist-info/RECORD,,
+sleap-1.4.0.dist-info/AUTHORS,sha256=_DKLQBc-g5WmLby_tojMnAEocX6RF-wSLkeo3q6cSu8,645
+sleap-1.4.0.dist-info/LICENSE,sha256=n3feSVEF4UPtYD9NcHgW2Y9hx-nU-JLZ3bSkE_mvRrA,1748
+sleap-1.4.0.dist-info/METADATA,sha256=Co4OHib4Cc3g_oCYiHkZZiAvM5YE2XbnFOESdYrHtHM,17835
+sleap-1.4.0.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
+sleap-1.4.0.dist-info/entry_points.txt,sha256=2S0y0zfDrATdxApJ-oNRHOeCiyjCc4NEc5Wi2qMtPwU,326
+sleap-1.4.0.dist-info/top_level.txt,sha256=Au4QsAQYBEAnoWobrly9pR5Iy7GuOW-kJEn2Ame-F0U,6
+sleap-1.4.0.dist-info/RECORD,,
```

