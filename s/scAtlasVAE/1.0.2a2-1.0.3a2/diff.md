# Comparing `tmp/scAtlasVAE-1.0.2a2-py3-none-any.whl.zip` & `tmp/scAtlasVAE-1.0.3a2-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,36 +1,36 @@
-Zip file size: 56235 bytes, number of entries: 34
+Zip file size: 56491 bytes, number of entries: 34
 -rw-r--r--  2.0 unx      890 b- defN 24-Jan-04 15:25 scatlasvae/__init__.py
 -rw-r--r--  2.0 unx     1420 b- defN 23-Nov-22 14:20 scatlasvae/_metadata.py
--rw-r--r--  2.0 unx       19 b- defN 24-Mar-20 06:50 scatlasvae/_version.py
+-rw-r--r--  2.0 unx       19 b- defN 24-Apr-08 05:42 scatlasvae/_version.py
 -rw-r--r--  2.0 unx       74 b- defN 24-Feb-14 07:00 scatlasvae/data/__init__.py
 -rw-r--r--  2.0 unx     1549 b- defN 24-Feb-14 07:03 scatlasvae/data/_dataloader.py
 -rw-r--r--  2.0 unx        0 b- defN 23-Dec-04 13:38 scatlasvae/externals/__init__.py
 -rw-r--r--  2.0 unx     1924 b- defN 23-Dec-04 13:38 scatlasvae/externals/_trvae_mmd_loss.py
 -rw-r--r--  2.0 unx       86 b- defN 23-Nov-24 03:53 scatlasvae/model/__init__.py
--rw-r--r--  2.0 unx    81136 b- defN 24-Mar-20 06:50 scatlasvae/model/_gex_model.py
+-rw-r--r--  2.0 unx    81038 b- defN 24-Apr-08 05:34 scatlasvae/model/_gex_model.py
 -rw-r--r--  2.0 unx    31170 b- defN 24-Mar-15 04:57 scatlasvae/model/_primitives.py
 -rw-r--r--  2.0 unx       41 b- defN 23-Nov-22 14:14 scatlasvae/pipeline/__init__.py
 -rw-r--r--  2.0 unx     8777 b- defN 23-Dec-04 13:45 scatlasvae/pipeline/_pipeline.py
 -rw-r--r--  2.0 unx      127 b- defN 23-Nov-22 14:24 scatlasvae/preprocessing/__init__.py
 -rw-r--r--  2.0 unx     3301 b- defN 23-Nov-22 14:10 scatlasvae/preprocessing/_infercnv.py
 -rw-r--r--  2.0 unx    35547 b- defN 24-Jan-02 15:36 scatlasvae/preprocessing/_preprocess.py
 -rw-r--r--  2.0 unx       78 b- defN 24-Jan-05 06:06 scatlasvae/tools/__init__.py
 -rw-r--r--  2.0 unx     3828 b- defN 24-Feb-01 13:09 scatlasvae/tools/_alignments.py
 -rw-r--r--  2.0 unx     6932 b- defN 24-Jan-05 06:06 scatlasvae/tools/_umap.py
 -rw-r--r--  2.0 unx      162 b- defN 24-Jan-04 15:28 scatlasvae/utils/__init__.py
 -rw-r--r--  2.0 unx      439 b- defN 23-Nov-22 14:22 scatlasvae/utils/_compat.py
 -rw-r--r--  2.0 unx     2356 b- defN 23-Nov-22 14:12 scatlasvae/utils/_decorators.py
 -rw-r--r--  2.0 unx     4339 b- defN 24-Jan-04 15:29 scatlasvae/utils/_definitions.py
 -rw-r--r--  2.0 unx    10679 b- defN 23-Nov-22 14:21 scatlasvae/utils/_distributions.py
--rw-r--r--  2.0 unx     4791 b- defN 24-Feb-21 13:28 scatlasvae/utils/_logger.py
--rw-r--r--  2.0 unx     8020 b- defN 23-Dec-04 13:37 scatlasvae/utils/_loss.py
+-rw-r--r--  2.0 unx     4791 b- defN 24-Mar-21 12:50 scatlasvae/utils/_logger.py
+-rw-r--r--  2.0 unx     7995 b- defN 24-Mar-22 07:27 scatlasvae/utils/_loss.py
 -rw-r--r--  2.0 unx     5680 b- defN 23-Nov-22 14:18 scatlasvae/utils/_parallelizer.py
 -rw-r--r--  2.0 unx     1081 b- defN 23-Nov-22 14:12 scatlasvae/utils/_tensor_utils.py
--rw-r--r--  2.0 unx     6569 b- defN 24-Jan-10 03:40 scatlasvae/utils/_utilities.py
--rw-r--r--  2.0 unx     1522 b- defN 24-Mar-20 06:51 scAtlasVAE-1.0.2a2.dist-info/LICENSE
--rw-r--r--  2.0 unx      876 b- defN 24-Mar-20 06:51 scAtlasVAE-1.0.2a2.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 24-Mar-20 06:51 scAtlasVAE-1.0.2a2.dist-info/WHEEL
--rw-r--r--  2.0 unx       39 b- defN 24-Mar-20 06:51 scAtlasVAE-1.0.2a2.dist-info/dependency_links.txt
--rw-r--r--  2.0 unx       11 b- defN 24-Mar-20 06:51 scAtlasVAE-1.0.2a2.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     2940 b- defN 24-Mar-20 06:51 scAtlasVAE-1.0.2a2.dist-info/RECORD
-34 files, 226495 bytes uncompressed, 51477 bytes compressed:  77.3%
+-rw-r--r--  2.0 unx     6611 b- defN 24-Mar-22 04:24 scatlasvae/utils/_utilities.py
+-rw-r--r--  2.0 unx     1522 b- defN 24-Apr-08 06:04 scAtlasVAE-1.0.3a2.dist-info/LICENSE
+-rw-r--r--  2.0 unx     1355 b- defN 24-Apr-08 06:04 scAtlasVAE-1.0.3a2.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-Apr-08 06:04 scAtlasVAE-1.0.3a2.dist-info/WHEEL
+-rw-r--r--  2.0 unx       39 b- defN 24-Apr-08 06:04 scAtlasVAE-1.0.3a2.dist-info/dependency_links.txt
+-rw-r--r--  2.0 unx       11 b- defN 24-Apr-08 06:04 scAtlasVAE-1.0.3a2.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     2941 b- defN 24-Apr-08 06:04 scAtlasVAE-1.0.3a2.dist-info/RECORD
+34 files, 226894 bytes uncompressed, 51733 bytes compressed:  77.2%
```

## zipnote {}

```diff
@@ -78,26 +78,26 @@
 
 Filename: scatlasvae/utils/_tensor_utils.py
 Comment: 
 
 Filename: scatlasvae/utils/_utilities.py
 Comment: 
 
-Filename: scAtlasVAE-1.0.2a2.dist-info/LICENSE
+Filename: scAtlasVAE-1.0.3a2.dist-info/LICENSE
 Comment: 
 
-Filename: scAtlasVAE-1.0.2a2.dist-info/METADATA
+Filename: scAtlasVAE-1.0.3a2.dist-info/METADATA
 Comment: 
 
-Filename: scAtlasVAE-1.0.2a2.dist-info/WHEEL
+Filename: scAtlasVAE-1.0.3a2.dist-info/WHEEL
 Comment: 
 
-Filename: scAtlasVAE-1.0.2a2.dist-info/dependency_links.txt
+Filename: scAtlasVAE-1.0.3a2.dist-info/dependency_links.txt
 Comment: 
 
-Filename: scAtlasVAE-1.0.2a2.dist-info/top_level.txt
+Filename: scAtlasVAE-1.0.3a2.dist-info/top_level.txt
 Comment: 
 
-Filename: scAtlasVAE-1.0.2a2.dist-info/RECORD
+Filename: scAtlasVAE-1.0.3a2.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## scatlasvae/_version.py

```diff
@@ -1 +1 @@
-version = '1.0.2a2'
+version = '1.0.3a2'
```

## scatlasvae/model/_gex_model.py

```diff
@@ -104,15 +104,15 @@
        log_variational: bool = True,
        total_variational: bool = False,
        bias: bool = True,
        use_batch_norm: bool = True,
        use_layer_norm: bool = False,
        batch_hidden_dim: int = 8,
        batch_embedding: Literal["embedding", "onehot"] = "embedding",
-       reconstruction_method: Literal['mse', 'zg', 'zinb'] = 'zinb',
+       reconstruction_method: Literal['mse', 'zg', 'zinb', 'nb'] = 'zinb',
        constrain_n_label: bool = True,
        constrain_n_batch: bool = True,
        constrain_latent_method: Literal['mse', 'normal'] = 'mse',
        constrain_latent_embedding: bool = False,
        constrain_latent_key: str = 'X_gex',
        encode_libsize: bool = False,
        decode_libsize: bool = True,
@@ -288,15 +288,15 @@
         )
 
         self.px_rna_rate_decoder = nn.Linear(self.n_hidden, self.in_dim)
         self.px_rna_scale_decoder = nn.Sequential(
             nn.Linear(self.n_hidden, self.in_dim),
             nn.Softmax(dim=-1)
         )
-        self.px_rna_dropout_decoder = nn.Linear(self.n_hidden, self.in_dim)
+        self.px_rna_dropout_decoder = Linear(self.n_hidden, self.in_dim, init='final')
 
         if self.n_label > 0:
             self.fc = nn.Sequential(
                 nn.Linear(self.n_latent, self.n_label)
             )
 
         if self.n_additional_label is not None:
@@ -675,69 +675,71 @@
             additional_batch_categories = [np.array(x.codes) for x in self.additional_batch_category]
 
 
         if self.constrain_latent_embedding and self.constrain_latent_key in self.adata.obsm.keys():
             P = self.adata.obsm[self.constrain_latent_key]
             if additional_batch_categories is not None:
                 if batch_categories is not None and label_categories is not None and additional_label_categories is not None:
-                    _dataset = list(zip(X, P, batch_categories, label_categories, *additional_label_categories, *additional_batch_categories))
+                    _dataset = list(zip(P, batch_categories, label_categories, *additional_label_categories, *additional_batch_categories))
                 elif batch_categories is not None and label_categories is not None:
-                    _dataset = list(zip(X, P, batch_categories, label_categories, *additional_batch_categories))
+                    _dataset = list(zip(P, batch_categories, label_categories, *additional_batch_categories))
                 elif batch_categories is not None:
-                    _dataset = list(zip(X, P, batch_categories, *additional_batch_categories))
+                    _dataset = list(zip(P, batch_categories, *additional_batch_categories))
                 elif label_categories is not None:
-                    _dataset = list(zip(X, P, label_categories, *additional_batch_categories))
+                    _dataset = list(zip(P, label_categories, *additional_batch_categories))
                 else:
-                    _dataset = list(zip(X, P, *additional_batch_categories))
+                    _dataset = list(zip(P, *additional_batch_categories))
             else:
                 if batch_categories is not None and label_categories is not None and additional_label_categories is not None:
-                    _dataset = list(zip(X, P, batch_categories, label_categories, *additional_label_categories))
+                    _dataset = list(zip(P, batch_categories, label_categories, *additional_label_categories))
                 elif batch_categories is not None and label_categories is not None:
-                    _dataset = list(zip(X, P, batch_categories, label_categories))
+                    _dataset = list(zip(P, batch_categories, label_categories))
                 elif batch_categories is not None:
-                    _dataset = list(zip(X, P, batch_categories))
+                    _dataset = list(zip(P, batch_categories))
                 elif label_categories is not None:
-                    _dataset = list(zip(X, P, label_categories))
+                    _dataset = list(zip(P, label_categories))
                 else:
-                    _dataset = list(zip(X, P))
+                    _dataset = list(zip(P))
         else:
             if additional_batch_categories is not None:
                 if batch_categories is not None and label_categories is not None and additional_label_categories is not None:
-                    _dataset = list(zip(X, batch_categories, label_categories, *additional_label_categories, *additional_batch_categories))
+                    _dataset = list(zip(batch_categories, label_categories, *additional_label_categories, *additional_batch_categories))
                 elif batch_categories is not None and label_categories is not None:
-                    _dataset = list(zip(X, batch_categories, label_categories, *additional_batch_categories))
+                    _dataset = list(zip(batch_categories, label_categories, *additional_batch_categories))
                 elif batch_categories is not None:
-                    _dataset = list(zip(X, batch_categories, *additional_batch_categories))
+                    _dataset = list(zip(batch_categories, *additional_batch_categories))
                 elif label_categories is not None:
-                    _dataset = list(zip(X, label_categories, *additional_batch_categories))
+                    _dataset = list(zip(label_categories, *additional_batch_categories))
                 else:
-                    _dataset = list(zip(X, *additional_batch_categories))
+                    _dataset = list(zip(*additional_batch_categories))
             else:
                 if batch_categories is not None and label_categories is not None and additional_label_categories is not None:
-                    _dataset = list(zip(X, batch_categories, label_categories, *additional_label_categories))
+                    _dataset = list(zip(batch_categories, label_categories, *additional_label_categories))
                 elif batch_categories is not None and label_categories is not None:
-                    _dataset = list(zip(X, batch_categories, label_categories))
+                    _dataset = list(zip(batch_categories, label_categories))
                 elif batch_categories is not None:
-                    _dataset = list(zip(X, batch_categories))
+                    _dataset = list(zip(batch_categories))
                 elif label_categories is not None:
-                    _dataset = list(zip(X, label_categories))
+                    _dataset = list(zip(label_categories))
                 else:
                     _dataset = list(X)
 
-
+        
         _shuffle_indices = list(range(len(_dataset)))
         np.random.shuffle(_shuffle_indices)
-        self._dataset = np.array([_dataset[i] for i in _shuffle_indices])
+        self._dataset = np.array([_dataset[i] for i in _shuffle_indices], dtype=object)
 
         self._shuffle_indices = np.array(
             [x for x, _ in sorted(zip(range(len(_dataset)), _shuffle_indices), key=lambda x: x[1])]
         )
 
         self._shuffled_indices_inverse = _shuffle_indices
 
+        mt("Finished initializing dataset into memory")
+
 
     def as_dataloader(
         self,
         subset_indices: Union[torch.tensor, np.ndarray] = None,
         n_per_batch: int = 128,
         train_test_split: bool = False,
         random_seed: bool = 42,
@@ -775,15 +777,15 @@
     
 
     def encode(self, X: torch.Tensor, batch_index: torch.Tensor = None, eps: float = 1e-4):
         # Encode for hidden space
         # if batch_index is not None and self.inject_batch:
         #    X = torch.hstack([X, batch_index])
         libsize = torch.log(X.sum(1))
-        if self.reconstruction_method == 'zinb':
+        if self.reconstruction_method == 'zinb' or self.reconstruction_method == 'nb':
             if self.total_variational:
                 X = self._normalize_data(X, after=1e4, copy=True)
             if self.log_variational:
                 X = torch.log(1+X)
         q = self.encoder.encode(torch.hstack([X,libsize.unsqueeze(1)])) if self.encode_libsize else self.encoder.encode(X)
         q_mu = self.z_mean_fc(q)
         q_var = torch.exp(self.z_var_fc(q)) + eps
@@ -838,15 +840,15 @@
             px_rna_rate = self.px_rna_rate_decoder(px) ## In logits
         elif self.dispersion == "gene-batch":
             px_rna_rate = F.linear(one_hot(batch_index, self.n_batch), self.px_rate)
         elif self.dispersion == "gene":
             px_rna_rate = self.px_rate
 
         px_rna_dropout = self.px_rna_dropout_decoder(px)  ## In logits
-
+        
         R = dict(
             h = h,
             px = px,
             px_rna_scale_orig = px_rna_scale,
             px_rna_scale = px_rna_scale_final,
             px_rna_rate = px_rna_rate,
             px_rna_dropout = px_rna_dropout
@@ -879,14 +881,21 @@
             reconstruction_loss = LossFunction.zinb_reconstruction_loss(
                 X,
                 mu = R['px_rna_scale'],
                 theta = R['px_rna_rate'].exp(),
                 gate_logits = R['px_rna_dropout'],
                 reduction = reduction
             )
+        elif self.reconstruction_method == 'nb':
+            reconstruction_loss = LossFunction.nb_reconstruction_loss(
+                X,
+                mu = R['px_rna_scale'],
+                theta = R['px_rna_rate'].exp(),
+                reduction = reduction
+            )
         elif self.reconstruction_method == 'zg':
             reconstruction_loss = LossFunction.zi_gaussian_reconstruction_loss(
                 X,
                 mean=R['px_rna_scale'],
                 variance=R['px_rna_rate'].exp(),
                 gate_logits=R['px_rna_dropout'],
                 reduction=reduction
@@ -895,14 +904,16 @@
             X_norm = self._normalize_data(X, after=1e4)
             X_norm = torch.log(X_norm + 1)
             reconstruction_loss = nn.functional.mse_loss(
                 X_norm,
                 R['px_rna_scale'],
                 reduction=reduction
             )
+        else:
+            raise ValueError(f"reconstruction_method {self.reconstruction_method} is not supported")
 
         if self.n_label > 0:
             criterion = nn.CrossEntropyLoss(weight=self.label_category_weight)
 
             prediction = self.fc(H['z'])
 
             if self.new_adata_code and self.new_adata_code in label_index:
@@ -945,27 +956,27 @@
                 mmd_loss = self.mmd_loss(
                     H['q_mu'],
                     batch_index.detach().cpu().numpy(),
                     dim=1
                 )
             elif self.mmd_key == 'additional_batch':
                 for i in range(len(self.additional_batch_keys)):
-                    mmd_loss_st += self.mmd_loss(
+                    mmd_loss += self.mmd_loss(
                         H['q_mu'], 
                         additional_batch_index[i].detach().cpu().numpy(),
                         dim=1
                     )
             elif self.mmd_key == 'both':
-                mmd_loss_st = self.mmd_loss(
+                mmd_loss = self.mmd_loss(
                     H['q_mu'], 
                     batch_index.detach().cpu().numpy(),
                     dim=1
                 )
                 for i in range(len(self.additional_batch_keys)):
-                    mmd_loss_st += self.hierarchical_mmd_loss_2(
+                    mmd_loss += self.hierarchical_mmd_loss_2(
                         H['q_mu'], 
                         batch_index.detach().cpu().numpy(),
                         additional_batch_index[i].detach().cpu().numpy(),
                         dim=1
                     )
 
         loss_record = {
@@ -1030,14 +1041,15 @@
 
     def fit(self,
         max_epoch: Optional[int] = None,
         n_per_batch:int = 128,
         kl_weight: float = 1.,
         pred_weight: float = 1.,
         mmd_weight: float = 1.,
+        gate_weight: float = 1.,
         constrain_weight: float = 1.,
         optimizer_parameters: Iterable = None,
         validation_split: float = .2,
         lr: bool = 5e-5,
         lr_schedule: bool = False,
         lr_factor: float = 0.6,
         lr_patience: int = 30,
@@ -1117,31 +1129,35 @@
         }
 
         epoch_total_loss_list = []
         epoch_reconstruction_loss_list = []
         epoch_kldiv_loss_list = []
         epoch_prediction_loss_list = []
         epoch_mmd_loss_list = []
+        epoch_gate_loss_list = []
 
 
         for epoch in range(1, max_epoch+1):
             self._trained = True
             pbar.desc = "Epoch {}".format(epoch)
             epoch_total_loss = 0
             epoch_reconstruction_loss = 0
             epoch_kldiv_loss = 0
             epoch_prediction_loss = 0
             epoch_mmd_loss = 0
+            epoch_gate_loss = 0
+
             X_train, X_test = self.as_dataloader(
                 n_per_batch=n_per_batch,
                 train_test_split = True,
                 validation_split = validation_split,
                 random_seed=random_seed,
                 subset_indices=subset_indices
             )
+            
             if self.n_label > 0 and epoch+1 == max_epoch - pred_last_n_epoch:
                 mt("saving transcriptome only state dict")
                 self.gene_only_state_dict = deepcopy(self.state_dict())
                 if  pred_last_n_epoch_fconly:
                     optimizer = optim.AdamW(chain(self.att.parameters(), self.fc.parameters()), lr, weight_decay=weight_decay)
 
             for b, X in enumerate(X_train):
@@ -1161,27 +1177,32 @@
 
                 reconstruction_loss = L['reconstruction_loss']
                 prediction_loss = pred_weight * L['prediction_loss']
                 additional_prediction_loss = pred_weight * L['additional_prediction_loss']
                 kldiv_loss = kl_weight * L['kldiv_loss']
                 mmd_loss = mmd_weight * L['mmd_loss']
 
+                avg_gate_loss = gate_weight * torch.sigmoid(R['px_rna_dropout']).sum(dim=1).mean()
+
                 avg_reconstruction_loss = reconstruction_loss.sum()  / n_per_batch
                 avg_kldiv_loss = kldiv_loss.sum()  / n_per_batch
                 avg_mmd_loss = mmd_loss / n_per_batch
 
                 epoch_reconstruction_loss += avg_reconstruction_loss.item()
                 epoch_kldiv_loss += avg_kldiv_loss.item()
+                epoch_mmd_loss += avg_mmd_loss.item()
+                epoch_gate_loss += avg_gate_loss.item()
+                
                 if self.n_label > 0:
                     epoch_prediction_loss += prediction_loss.sum().item()
 
                 if epoch > max_epoch - pred_last_n_epoch:
-                    loss = avg_reconstruction_loss + avg_kldiv_loss + avg_mmd_loss + (prediction_loss.sum() + additional_prediction_loss.sum()) / (len(self.n_additional_label) if self.n_additional_label is not None else 0 + 1)
+                    loss = avg_reconstruction_loss + avg_kldiv_loss + avg_mmd_loss + (prediction_loss.sum() + additional_prediction_loss.sum()) / (len(self.n_additional_label) if self.n_additional_label is not None else 0 + 1) + avg_gate_loss
                 else:
-                    loss = avg_reconstruction_loss + avg_kldiv_loss + avg_mmd_loss
+                    loss = avg_reconstruction_loss + avg_kldiv_loss + avg_mmd_loss + avg_gate_loss
 
                 if self.constrain_latent_embedding:
                     loss += constrain_weight * L['latent_constrain_loss']
 
                 epoch_total_loss += loss.item()
                 optimizer.zero_grad()
                 loss.backward()
@@ -1203,29 +1224,31 @@
                 'mmd': '{:.2e}'.format(loss_record["epoch_mmd_loss"]),
             })
             epoch_total_loss_list.append(epoch_total_loss)
             epoch_reconstruction_loss_list.append(epoch_reconstruction_loss)
             epoch_kldiv_loss_list.append(epoch_kldiv_loss)
             epoch_prediction_loss_list.append(epoch_prediction_loss)
             epoch_mmd_loss_list.append(epoch_mmd_loss)
+            epoch_gate_loss_list.append(epoch_gate_loss)
             pbar.update(1)
             if n_epochs_kl_warmup:
                 kl_weight = min( kl_weight + kl_warmup_gradient, kl_weight_max)
             random_seed += 1
         if current_score < best_score:
             mt("restoring state dict with best performance")
             self.load_state_dict(best_state_dict)
         pbar.close()
         self.trained_state_dict = deepcopy(self.state_dict())
         return dict(
             epoch_total_loss_list=epoch_total_loss_list,   
             epoch_reconstruction_loss_list=epoch_reconstruction_loss_list,
             epoch_kldiv_loss_list=epoch_kldiv_loss_list,
             epoch_prediction_loss_list=epoch_prediction_loss_list,
-            epoch_mmd_loss_list=epoch_mmd_loss_list
+            epoch_mmd_loss_list=epoch_mmd_loss_list,
+            epoch_gate_loss_list=epoch_gate_loss_list
         )
     
 
     @torch.no_grad()
     def predict_labels(
         self,
         n_per_batch: int = 128,
@@ -1250,15 +1273,15 @@
             x = self._dataset[x.cpu().numpy()]
             batch_index = None
             label_index = None
             if self.n_batch > 0 or self.n_label > 0:
                 if not isinstance(x, Iterable) and len(x) > 1:
                     raise ValueError()
                 if self.n_batch > 0 and self.n_label > 0:
-                    X, batch_index, label_index = get_k_elements(x,0), get_k_elements(x,1), get_k_elements(X,2)
+                    X, batch_index, label_index = get_k_elements(x,0), get_k_elements(x,1), get_k_elements(batch_data,2)
                 elif self.n_batch > 0:
                     X, batch_index = get_k_elements(x,0), get_k_elements(x,1)
                 elif self.n_label > 0:
                     X, label_index = get_k_elements(x,0), get_k_elements(x,1)
 
             if self.n_label > 0:
                 label_index = torch.tensor(label_index)
@@ -1546,140 +1569,127 @@
             reference_adata.obsm[use_rep],
             reference_adata.obsm['X_umap'],
             reference_adata.obsm[use_rep],
             method=method,
 
         )
 
-    def _prepare_batch(self, X):
-                P = None
-                X = self._dataset[X.cpu().numpy()]
-                batch_index, label_index, additional_label_index, additional_batch_index = None, None, None, None
-                if self.n_batch > 0 or self.n_label > 0:
-                    if not isinstance(X, Iterable) and len(X) > 1:
-                        raise ValueError()
-                    if self.n_additional_batch_ is not None:
-                        if self.n_batch > 0 and self.n_label > 0 and self.n_additional_label is not None:
-                            if self.constrain_latent_embedding:
-                                X, P, batch_index, label_index, additional_label_index, additional_batch_index = (
-                                    get_k_elements(X,0),
-                                    get_k_elements(X,1),
-                                    get_k_elements(X,2),
-                                    get_k_elements(X,3),
-                                    get_elements(X,4, len(self.n_additional_label)),
-                                    get_last_k_elements(X,4+len(self.n_additional_label))
-                                )
-                            else:
-                                X, batch_index, label_index, additional_label_index, additional_batch_index = (
-                                    get_k_elements(X,0),
-                                    get_k_elements(X,1),
-                                    get_k_elements(X,2),
-                                    get_elements(X,3, len(self.n_additional_label)),
-                                    get_last_k_elements(X,3+len(self.n_additional_label))
-                                )
-                            additional_label_index = list(np.vstack(additional_label_index).T.astype(int))
-                        elif self.n_batch > 0 and self.n_label > 0:
-                            if self.constrain_latent_embedding:
-                                X, P, batch_index, label_index, additional_batch_index = (
-                                    get_k_elements(X,0),
-                                    get_k_elements(X,1),
-                                    get_k_elements(X,2),
-                                    get_k_elements(X,3),
-                                    get_last_k_elements(X,4)
-                                )
-                            else:
-                                X, batch_index, label_index, additional_batch_index = (
-                                    get_k_elements(X,0),
-                                    get_k_elements(X,1),
-                                    get_k_elements(X,2),
-                                    get_last_k_elements(X,3)
-                                )
-                        elif self.n_batch > 0:
-                            if self.constrain_latent_embedding:
-                                X, P, batch_index, additional_batch_index = (
-                                    get_k_elements(X,0),
-                                    get_k_elements(X,1),
-                                    get_k_elements(X,2),
-                                    get_last_k_elements(X,3)
-                                )
-                            else:
-                                X, batch_index, additional_batch_index = get_k_elements(X,0), get_k_elements(X,1),  get_last_k_elements(X,2)
-                        elif self.n_label > 0:
-                            if self.constrain_latent_embedding:
-                                X, P, label_index, additional_batch_index = get_k_elements(X,0), get_k_elements(X,1), get_k_elements(X,2), get_last_k_elements(X,3)
-                            else:
-                                X, label_index, additional_batch_index = get_k_elements(X,0), get_k_elements(X,1), get_last_k_elements(X,2)
-                        additional_batch_index = list(np.vstack(additional_batch_index).T.astype(int))
+    def _prepare_batch(self, batch_indices):
+        P = None
+        batch_data = self._dataset[batch_indices.cpu().numpy()]
+        X = self.adata.X[batch_indices.cpu().numpy()]
+        batch_index, label_index, additional_label_index, additional_batch_index = None, None, None, None
+        if self.n_batch > 0 or self.n_label > 0:
+            if not (isinstance(batch_data, Iterable) and len(batch_data) > 1):
+                raise ValueError("batch_data is not iterable or has only one element")
+            if self.n_additional_batch_ is not None:
+                if self.n_batch > 0 and self.n_label > 0 and self.n_additional_label is not None:
+                    if self.constrain_latent_embedding:
+                        P, batch_index, label_index, additional_label_index, additional_batch_index = (
+                            get_k_elements(batch_data,0),
+                            get_k_elements(batch_data,1),
+                            get_k_elements(batch_data,2),
+                            get_elements(batch_data,3, len(self.n_additional_label)),
+                            get_last_k_elements(batch_data,3+len(self.n_additional_label))
+                        )
                     else:
-                        if self.n_batch > 0 and self.n_label > 0 and self.n_additional_label is not None:
-                            if self.constrain_latent_embedding:
-                                X, P, batch_index, label_index, additional_label_index = (
-                                    get_k_elements(X,0),
-                                    get_k_elements(X,1),
-                                    get_k_elements(X,2),
-                                    get_k_elements(X,3),
-                                    get_last_k_elements(X,4)
-                                )
-                            else:
-                                X, batch_index, label_index, additional_label_index = (
-                                    get_k_elements(X,0),
-                                    get_k_elements(X,1),
-                                    get_k_elements(X,2),
-                                    get_last_k_elements(X,3)
-                                )
-                            additional_label_index = list(np.vstack(additional_label_index).T.astype(int))
-                        elif self.n_batch > 0 and self.n_label > 0:
-                            if self.constrain_latent_embedding:
-                                X, P, batch_index, label_index = get_k_elements(X,0), get_k_elements(X,1), get_k_elements(X,2), get_k_elements(X,3)
-                            else:
-                                X, batch_index, label_index = get_k_elements(X,0), get_k_elements(X,1), get_k_elements(X,2)
-                        elif self.n_batch > 0:
-                            if self.constrain_latent_embedding:
-                                X, P, batch_index = get_k_elements(X,0), get_k_elements(X,1),  get_k_elements(X,2)
-                            else:
-                                X, batch_index = get_k_elements(X,0), get_k_elements(X,1)
-                        elif self.n_label > 0:
-                            if self.constrain_latent_embedding:
-                                X, P, label_index = get_k_elements(X,0), get_k_elements(X,1), get_k_elements(X,2)
-                            else:
-                                X, label_index = get_k_elements(X,0), get_k_elements(X,1)
-
-
-                X = torch.tensor(np.vstack(list(map(lambda x: x.toarray() if issparse(x) else x, X))))
-                if self.constrain_latent_embedding:
-                    P = torch.tensor(np.vstack(P)).type(torch.FloatTensor).to(self.device)
-
-                if self.n_label > 0:
-                    label_index = torch.tensor(label_index)
-                    if not isinstance(label_index, torch.FloatTensor):
-                        label_index = label_index.type(torch.FloatTensor)
-                    label_index = label_index.to(self.device).unsqueeze(1)
-
-                if self.n_batch > 0:
-                    batch_index = torch.tensor(batch_index)
-                    if not isinstance(batch_index, torch.FloatTensor):
-                        batch_index = batch_index.type(torch.FloatTensor)
-                    batch_index = batch_index.to(self.device).unsqueeze(1)
-
-                if self.n_additional_label is not None:
-                    for i in range(len(additional_label_index)):
-                        additional_label_index[i] = torch.tensor(additional_label_index[i])
-                        if not isinstance(additional_label_index[i], torch.FloatTensor):
-                            additional_label_index[i] = additional_label_index[i].type(torch.FloatTensor)
-                        additional_label_index[i] = additional_label_index[i].to(self.device).unsqueeze(1)
-
-                if self.n_additional_batch_ is not None:
-                    for i in range(len(additional_batch_index)):
-                        additional_batch_index[i] = torch.tensor(additional_batch_index[i])
-                        if not isinstance(additional_batch_index[i], torch.FloatTensor):
-                            additional_batch_index[i] = additional_batch_index[i].type(torch.FloatTensor)
-                        additional_batch_index[i] = additional_batch_index[i].to(self.device).unsqueeze(1)
-
-                if not isinstance(X, torch.FloatTensor):
-                    X = X.type(torch.FloatTensor)
-                if P is not None and not isinstance(P, torch.FloatTensor):
-                    P = P.type(torch.FloatTensor)
-                    P = P.to(self.device)
-                X = X.to(self.device)
+                        batch_index, label_index, additional_label_index, additional_batch_index = (
+                            get_k_elements(batch_data,0),
+                            get_k_elements(batch_data,1),
+                            get_elements(batch_data,2, len(self.n_additional_label)),
+                            get_last_k_elements(batch_data,2+len(self.n_additional_label))
+                        )
+                    additional_label_index = list(np.vstack(additional_label_index).T.astype(int))
+                elif self.n_batch > 0 and self.n_label > 0:
+                    if self.constrain_latent_embedding:
+                        P, batch_index, label_index, additional_batch_index = (
+                            get_k_elements(batch_data,0),
+                            get_k_elements(batch_data,1),
+                            get_k_elements(batch_data,2),
+                            get_last_k_elements(batch_data,3)
+                        )
+                    else:
+                        batch_index, label_index, additional_batch_index = (
+                            get_k_elements(batch_data,0),
+                            get_k_elements(batch_data,1),
+                            get_last_k_elements(batch_data,2)
+                        )
+                elif self.n_batch > 0:
+                    if self.constrain_latent_embedding:
+                        P, batch_index, additional_batch_index = (
+                            get_k_elements(batch_data,0),
+                            get_k_elements(batch_data,1),
+                            get_last_k_elements(batch_data,2)
+                        )
+                    else:
+                        batch_index, additional_batch_index = get_k_elements(batch_data,0), get_last_k_elements(batch_data,1)
+                elif self.n_label > 0:
+                    if self.constrain_latent_embedding:
+                        P, label_index, additional_batch_index = get_k_elements(batch_data,0), get_k_elements(batch_data,1), get_last_k_elements(batch_data,2)
+                    else:
+                        label_index, additional_batch_index = get_k_elements(batch_data,0), get_last_k_elements(batch_data,2)
+                additional_batch_index = list(np.vstack(additional_batch_index).T.astype(int))
+            else:
+                if self.n_batch > 0 and self.n_label > 0 and self.n_additional_label is not None:
+                    if self.constrain_latent_embedding:
+                        P, batch_index, label_index, additional_label_index = (
+                            get_k_elements(batch_data,0),
+                            get_k_elements(batch_data,1),
+                            get_k_elements(batch_data,2),
+                            get_last_k_elements(batch_data,3)
+                        )
+                    else:
+                        batch_index, label_index, additional_label_index = (
+                            get_k_elements(batch_data,0),
+                            get_k_elements(batch_data,1),
+                            get_last_k_elements(batch_data,2)
+                        )
+                    additional_label_index = list(np.vstack(additional_label_index).T.astype(int))
+                elif self.n_batch > 0 and self.n_label > 0:
+                    if self.constrain_latent_embedding:
+                        P, batch_index, label_index = get_k_elements(batch_data,0), get_k_elements(batch_data,1), get_k_elements(batch_data,2)
+                    else:
+                        batch_index, label_index = get_k_elements(batch_data,0), get_k_elements(batch_data,1)
+                elif self.n_batch > 0:
+                    if self.constrain_latent_embedding:
+                        P, batch_index = get_k_elements(batch_data,0), get_k_elements(batch_data,1)
+                    else:
+                        batch_index = get_k_elements(batch_data,0)
+                elif self.n_label > 0:
+                    if self.constrain_latent_embedding:
+                        P, label_index = get_k_elements(batch_data,0), get_k_elements(batch_data,1)
+                    else:
+                        label_index = get_k_elements(batch_data,0)
 
-                lib_size = X.sum(1).to(self.device)
-                return X, P, batch_index, label_index, additional_label_index, additional_batch_index, lib_size
+        X = torch.tensor(X.toarray() if issparse(X) else X)
+        if self.constrain_latent_embedding:
+            P = torch.tensor(np.vstack(P)).type(torch.FloatTensor).to(self.device)
+        if self.n_label > 0:
+            label_index = torch.tensor(label_index)
+            if not isinstance(label_index, torch.FloatTensor):
+                label_index = label_index.type(torch.FloatTensor)
+            label_index = label_index.to(self.device).unsqueeze(1)
+        if self.n_batch > 0:
+            batch_index = torch.tensor(batch_index)
+            if not isinstance(batch_index, torch.FloatTensor):
+                batch_index = batch_index.type(torch.FloatTensor)
+            batch_index = batch_index.to(self.device).unsqueeze(1)
+        if self.n_additional_label is not None:
+            for i in range(len(additional_label_index)):
+                additional_label_index[i] = torch.tensor(additional_label_index[i])
+                if not isinstance(additional_label_index[i], torch.FloatTensor):
+                    additional_label_index[i] = additional_label_index[i].type(torch.FloatTensor)
+                additional_label_index[i] = additional_label_index[i].to(self.device).unsqueeze(1)
+        if self.n_additional_batch_ is not None:
+            for i in range(len(additional_batch_index)):
+                additional_batch_index[i] = torch.tensor(additional_batch_index[i])
+                if not isinstance(additional_batch_index[i], torch.FloatTensor):
+                    additional_batch_index[i] = additional_batch_index[i].type(torch.FloatTensor)
+                additional_batch_index[i] = additional_batch_index[i].to(self.device).unsqueeze(1)
+        if not isinstance(X, torch.FloatTensor):
+            X = X.type(torch.FloatTensor)
+        if P is not None and not isinstance(P, torch.FloatTensor):
+            P = P.type(torch.FloatTensor)
+            P = P.to(self.device)
+        X = X.to(self.device)
+        lib_size = X.sum(1).to(self.device)
+        return X, P, batch_index, label_index, additional_label_index, additional_batch_index, lib_size
```

## scatlasvae/utils/_loss.py

```diff
@@ -69,24 +69,24 @@
         return F.mse_loss(recon_x, x, reduction = reduction)
 
     @staticmethod
     def bce(recon_x:   torch.tensor, 
             x:         torch.tensor, 
             reduction: str = "sum"):
         """
-        The reconstruction error in the form of mse
+        The error in the form of bce
         """
-        return F.binary_cross_entropy(recon_x, x)
+        return F.binary_cross_entropy(recon_x, x, reduction=reduction)
 
     @staticmethod
     def vae_mse(recon_x: torch.tensor, 
                 x:       torch.tensor, 
                 mu:      torch.tensor, 
                 var:     torch.tensor, 
-                kld_weight: float = 0.5):
+        ):
         """
         The KL-divergence of the latent probability z
         KL(q || p) = -∫ q(z) log [ p(z) / q(z) ] 
                 = -E[log p(z) - log q(z)] 
         """
         MSE = F.mse_loss(recon_x, x, reduction = "sum")
         KLD = torch.mean(-0.5 * torch.sum(1 + var - mu ** 2 - var.exp(), dim = 1), dim = 0)
```

## scatlasvae/utils/_utilities.py

```diff
@@ -97,16 +97,20 @@
         indices = np.argwhere(np.array(adata.obs[key] == k)).flatten()
         if len(indices) > 0:
             indices = np.random.choice(indices, v, replace=False)
             all_indices.append(indices)
     all_indices = np.hstack(all_indices)
     return adata[all_indices]
 
+
 def exists(x):
-    return x != None
+    return x is not None
+
+def absent(x):
+    return x is None
 
 
 def print_version():
     print(Colors.YELLOW)
     print('Python VERSION:{}\n'.format(Colors.NC), sys.version)
     print(Colors.YELLOW)
     print('PyTorch VERSION:{}\n'.format(Colors.NC), torch.__version__)
```

## Comparing `scAtlasVAE-1.0.2a2.dist-info/LICENSE` & `scAtlasVAE-1.0.3a2.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `scAtlasVAE-1.0.2a2.dist-info/RECORD` & `scAtlasVAE-1.0.3a2.dist-info/RECORD`

 * *Files 16% similar despite different names*

```diff
@@ -1,16 +1,16 @@
 scatlasvae/__init__.py,sha256=RLkNezvb5hYNMAZAYwsR3WXz6tNiM2mhceuvU2m1V9o,890
 scatlasvae/_metadata.py,sha256=YKuaQknWHvdKviwVf3XcxuIX9xEV7raH_hMSxTXpyKg,1420
-scatlasvae/_version.py,sha256=tH3m5lLetyjfWm32yOVNe-i2l5kyHXa60Cp6-5OPOjc,19
+scatlasvae/_version.py,sha256=C6kT7nMnFDtMNhMZLv0hgNqp7uESTCEBV6g3a9e__ZA,19
 scatlasvae/data/__init__.py,sha256=DRGPdhwDwndVwF029g9ITqgrxSiX0kTdm_6wZshyRgQ,74
 scatlasvae/data/_dataloader.py,sha256=3z02t6ufCxjZvQRYNXfKYE8Pl2sXzyGFSTWGqPsV6IU,1549
 scatlasvae/externals/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 scatlasvae/externals/_trvae_mmd_loss.py,sha256=kNRkIDk4Aid1vb9dEmXii0F6HRhLYEkar3k_KfRyjk4,1924
 scatlasvae/model/__init__.py,sha256=y_Vu4dtOU0crMGYbg4in9cqFw3W71AhtNs8dMd-Hd4Y,86
-scatlasvae/model/_gex_model.py,sha256=IHbCGANc6a_xLlrhPvZNtb_KLaBVWM7n1SsDU06wtag,81136
+scatlasvae/model/_gex_model.py,sha256=LCj8_N0SRQXfuGL3WRoV_InOC4lq0-J-G2tJyJpIyJQ,81038
 scatlasvae/model/_primitives.py,sha256=i0sQY4rU5eaMuswzcvZHVz6JHpsRrVvbVp2u2QemdsY,31170
 scatlasvae/pipeline/__init__.py,sha256=Gj_IP3ZGzWXcQcwf9wVJN9sxCL3LBik_GkKe77bQ4uw,41
 scatlasvae/pipeline/_pipeline.py,sha256=dvEfsFxeLskGPDg1rIUZE8USIxsqo-CC7gCVwDb8Bnc,8777
 scatlasvae/preprocessing/__init__.py,sha256=LJWHQgT8w93fDVVa5e-5DFv0UEph3wv-P6Oqeb4xOP8,127
 scatlasvae/preprocessing/_infercnv.py,sha256=uuVTDVFsqD_HsIfGtB9x-J8nQc1jyM07pk2SGlX-Fi0,3301
 scatlasvae/preprocessing/_preprocess.py,sha256=nES1QPOx1ee2lTM-W_qjEyly0IAoQ8SZd-3vI3urCM0,35547
 scatlasvae/tools/__init__.py,sha256=eyRiflnSSm25qdZo5cmqECvdSX8TLPYLr4MBgrhEqog,78
@@ -18,17 +18,17 @@
 scatlasvae/tools/_umap.py,sha256=WbhTd4q0ev3ZHGjsx8cvMkLq8VFD1V6qlIMkZg1NGcw,6932
 scatlasvae/utils/__init__.py,sha256=4TrTG_fljOG5Yuw7MEkw9OZJJA7CAoKrciDluHLhCxk,162
 scatlasvae/utils/_compat.py,sha256=5c7_RZ7UzkCS5nTQHFNUoAWVgNqCBmJqtdh8By57KVo,439
 scatlasvae/utils/_decorators.py,sha256=f4wXwLojOqiEi2LvXKOHqNZw9HtXMaKQm-kGLX9fELQ,2356
 scatlasvae/utils/_definitions.py,sha256=5CXPI1ikSx7FInKhZ-YwnvvuCIpa8f6OP2ddKZAl-qQ,4339
 scatlasvae/utils/_distributions.py,sha256=-Ige2YaFToBf9R68LS57g-TsdUK4s4Cm6zPC_EZBmq4,10679
 scatlasvae/utils/_logger.py,sha256=05cqq-6jTFyY13vrJhNWlu2Ryt-DXrbz6NpWGgVomQ0,4791
-scatlasvae/utils/_loss.py,sha256=dVFYGNOVBEOJRL9lpT_pwiBYXGJfDwaGgBrUo3J7lY4,8020
+scatlasvae/utils/_loss.py,sha256=DkaxjFBwmt-U435HfoXL06zWlai_Psm1dj-5B-j6vRQ,7995
 scatlasvae/utils/_parallelizer.py,sha256=BAX_FMAlifrbhp356te-GFuHIYUO5aDChlFMGM_qY1Y,5680
 scatlasvae/utils/_tensor_utils.py,sha256=RSwy0czCGgGFdk8mvQplK_DL4VN0zA-M81f3zFfSdCU,1081
-scatlasvae/utils/_utilities.py,sha256=AJqDXydETJxt3DchMcY9_SrpN-XbT-VqOO9AjeRo6dE,6569
-scAtlasVAE-1.0.2a2.dist-info/LICENSE,sha256=iIPK3YfcATgHwXIjFIPvNF0FUR7vP-S2mOJmlv5H0lI,1522
-scAtlasVAE-1.0.2a2.dist-info/METADATA,sha256=Xz7scurXeNmwFu8ZagORD0UN-mZr7iuKEKKJBJuFbr0,876
-scAtlasVAE-1.0.2a2.dist-info/WHEEL,sha256=Xo9-1PvkuimrydujYJAjF7pCkriuXBpUPEjma1nZyJ0,92
-scAtlasVAE-1.0.2a2.dist-info/dependency_links.txt,sha256=ruee1cEBfajPwjguNMor_ix7qykV-lvgrkqssMcjBXQ,39
-scAtlasVAE-1.0.2a2.dist-info/top_level.txt,sha256=CEiwZxZLLn_PrRCJEi6VHH2UvdtCzoB4i9WeEGf5IMY,11
-scAtlasVAE-1.0.2a2.dist-info/RECORD,,
+scatlasvae/utils/_utilities.py,sha256=L-V1XnxsBsCHgaFCZlQ2Wyt6NzeeZuTWCQPumDYkQd0,6611
+scAtlasVAE-1.0.3a2.dist-info/LICENSE,sha256=iIPK3YfcATgHwXIjFIPvNF0FUR7vP-S2mOJmlv5H0lI,1522
+scAtlasVAE-1.0.3a2.dist-info/METADATA,sha256=2F276QVwkJsrZL6F9086CcT5RLVp50LWuBgSrgpGhRE,1355
+scAtlasVAE-1.0.3a2.dist-info/WHEEL,sha256=Xo9-1PvkuimrydujYJAjF7pCkriuXBpUPEjma1nZyJ0,92
+scAtlasVAE-1.0.3a2.dist-info/dependency_links.txt,sha256=ruee1cEBfajPwjguNMor_ix7qykV-lvgrkqssMcjBXQ,39
+scAtlasVAE-1.0.3a2.dist-info/top_level.txt,sha256=CEiwZxZLLn_PrRCJEi6VHH2UvdtCzoB4i9WeEGf5IMY,11
+scAtlasVAE-1.0.3a2.dist-info/RECORD,,
```

