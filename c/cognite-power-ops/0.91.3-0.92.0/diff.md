# Comparing `tmp/cognite_power_ops-0.91.3.tar.gz` & `tmp/cognite_power_ops-0.92.0.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "cognite_power_ops-0.91.3.tar", max compression
+gzip compressed data, was "cognite_power_ops-0.92.0.tar", max compression
```

## Comparing `cognite_power_ops-0.91.3.tar` & `cognite_power_ops-0.92.0.tar`

### file list

```diff
@@ -1,710 +1,748 @@
--rw-r--r--   0        0        0    10758 2024-03-21 07:08:04.146300 cognite_power_ops-0.91.3/LICENSE
--rw-r--r--   0        0        0     3322 2024-03-21 07:08:04.146300 cognite_power_ops-0.91.3/README.md
--rw-r--r--   0        0        0      150 2024-03-21 07:08:04.146300 cognite_power_ops-0.91.3/cognite/powerops/__init__.py
--rw-r--r--   0        0        0       23 2024-03-21 07:08:04.146300 cognite_power_ops-0.91.3/cognite/powerops/_version.py
--rw-r--r--   0        0        0     5273 2024-03-21 07:08:04.146300 cognite_power_ops-0.91.3/cognite/powerops/cdf_labels.py
--rw-r--r--   0        0        0     8779 2024-03-21 07:08:04.146300 cognite_power_ops-0.91.3/cognite/powerops/cli.py
--rw-r--r--   0        0        0       74 2024-03-21 07:08:04.146300 cognite_power_ops-0.91.3/cognite/powerops/client/__init__.py
--rw-r--r--   0        0        0      105 2024-03-21 07:08:04.146300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/afrr_bid/__init__.py
--rw-r--r--   0        0        0        0 2024-03-21 07:08:04.146300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/afrr_bid/_api/__init__.py
--rw-r--r--   0        0        0    31739 2024-03-21 07:08:04.146300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/afrr_bid/_api/_core.py
--rw-r--r--   0        0        0    26235 2024-03-21 07:08:04.146300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/afrr_bid/_api/alert.py
--rw-r--r--   0        0        0     1492 2024-03-21 07:08:04.146300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/afrr_bid/_api/alert_query.py
--rw-r--r--   0        0        0    27110 2024-03-21 07:08:04.146300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/afrr_bid/_api/bid_document.py
--rw-r--r--   0        0        0     2041 2024-03-21 07:08:04.150300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/afrr_bid/_api/bid_document_alerts.py
--rw-r--r--   0        0        0     2039 2024-03-21 07:08:04.150300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/afrr_bid/_api/bid_document_bids.py
--rw-r--r--   0        0        0     5980 2024-03-21 07:08:04.150300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/afrr_bid/_api/bid_document_query.py
--rw-r--r--   0        0        0    16844 2024-03-21 07:08:04.150300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/afrr_bid/_api/bid_method.py
--rw-r--r--   0        0        0     1513 2024-03-21 07:08:04.150300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/afrr_bid/_api/bid_method_query.py
--rw-r--r--   0        0        0    27874 2024-03-21 07:08:04.150300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/afrr_bid/_api/bid_row.py
--rw-r--r--   0        0        0     1971 2024-03-21 07:08:04.150300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/afrr_bid/_api/bid_row_alerts.py
--rw-r--r--   0        0        0     5339 2024-03-21 07:08:04.150300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/afrr_bid/_api/bid_row_query.py
--rw-r--r--   0        0        0    16131 2024-03-21 07:08:04.150300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/afrr_bid/_api/price_area.py
--rw-r--r--   0        0        0    25401 2024-03-21 07:08:04.150300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/afrr_bid/_api/price_area_activation_price_down.py
--rw-r--r--   0        0        0    25313 2024-03-21 07:08:04.150300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/afrr_bid/_api/price_area_activation_price_up.py
--rw-r--r--   0        0        0    25313 2024-03-21 07:08:04.150300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/afrr_bid/_api/price_area_capacity_price_down.py
--rw-r--r--   0        0        0    25195 2024-03-21 07:08:04.150300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/afrr_bid/_api/price_area_capacity_price_up.py
--rw-r--r--   0        0        0    25692 2024-03-21 07:08:04.150300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/afrr_bid/_api/price_area_own_capacity_allocation_down.py
--rw-r--r--   0        0        0    25604 2024-03-21 07:08:04.150300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/afrr_bid/_api/price_area_own_capacity_allocation_up.py
--rw-r--r--   0        0        0     1513 2024-03-21 07:08:04.150300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/afrr_bid/_api/price_area_query.py
--rw-r--r--   0        0        0    25330 2024-03-21 07:08:04.150300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/afrr_bid/_api/price_area_relative_activation.py
--rw-r--r--   0        0        0    25780 2024-03-21 07:08:04.150300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/afrr_bid/_api/price_area_total_capacity_allocation_down.py
--rw-r--r--   0        0        0    25692 2024-03-21 07:08:04.150300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/afrr_bid/_api/price_area_total_capacity_allocation_up.py
--rw-r--r--   0        0        0     9172 2024-03-21 07:08:04.150300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/afrr_bid/_api_client.py
--rw-r--r--   0        0        0     2624 2024-03-21 07:08:04.150300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/afrr_bid/data_classes/__init__.py
--rw-r--r--   0        0        0    16179 2024-03-21 07:08:04.150300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/afrr_bid/data_classes/_alert.py
--rw-r--r--   0        0        0    19431 2024-03-21 07:08:04.150300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/afrr_bid/data_classes/_bid_document.py
--rw-r--r--   0        0        0     8148 2024-03-21 07:08:04.150300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/afrr_bid/data_classes/_bid_method.py
--rw-r--r--   0        0        0    23764 2024-03-21 07:08:04.150300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/afrr_bid/data_classes/_bid_row.py
--rw-r--r--   0        0        0    21842 2024-03-21 07:08:04.150300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/afrr_bid/data_classes/_core.py
--rw-r--r--   0        0        0     9006 2024-03-21 07:08:04.150300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/afrr_bid/data_classes/_price_area.py
--rw-r--r--   0        0        0      109 2024-03-21 07:08:04.150300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/__init__.py
--rw-r--r--   0        0        0        0 2024-03-21 07:08:04.150300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/__init__.py
--rw-r--r--   0        0        0    31954 2024-03-21 07:08:04.150300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/_core.py
--rw-r--r--   0        0        0    16860 2024-03-21 07:08:04.150300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/bid_method.py
--rw-r--r--   0        0        0     1511 2024-03-21 07:08:04.150300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/bid_method_query.py
--rw-r--r--   0        0        0    26226 2024-03-21 07:08:04.150300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/generator.py
--rw-r--r--   0        0        0    15705 2024-03-21 07:08:04.150300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/generator_efficiency_curve.py
--rw-r--r--   0        0        0     1625 2024-03-21 07:08:04.150300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/generator_efficiency_curve_query.py
--rw-r--r--   0        0        0    28005 2024-03-21 07:08:04.150300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/generator_is_available_time_series.py
--rw-r--r--   0        0        0     4541 2024-03-21 07:08:04.150300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/generator_query.py
--rw-r--r--   0        0        0    27596 2024-03-21 07:08:04.150300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/generator_start_stop_cost.py
--rw-r--r--   0        0        0     2197 2024-03-21 07:08:04.150300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/generator_turbine_curves.py
--rw-r--r--   0        0        0    32967 2024-03-21 07:08:04.150300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/plant.py
--rw-r--r--   0        0        0    30394 2024-03-21 07:08:04.150300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/plant_feeding_fee_time_series.py
--rw-r--r--   0        0        0     1998 2024-03-21 07:08:04.150300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/plant_generators.py
--rw-r--r--   0        0        0    30394 2024-03-21 07:08:04.150300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/plant_head_direct_time_series.py
--rw-r--r--   0        0        0    30394 2024-03-21 07:08:04.154300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/plant_inlet_level_time_series.py
--rw-r--r--   0        0        0    30438 2024-03-21 07:08:04.154300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/plant_outlet_level_time_series.py
--rw-r--r--   0        0        0    30100 2024-03-21 07:08:04.154300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/plant_p_max_time_series.py
--rw-r--r--   0        0        0    30100 2024-03-21 07:08:04.154300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/plant_p_min_time_series.py
--rw-r--r--   0        0        0     5508 2024-03-21 07:08:04.154300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/plant_query.py
--rw-r--r--   0        0        0    30394 2024-03-21 07:08:04.154300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/plant_water_value_time_series.py
--rw-r--r--   0        0        0    27060 2024-03-21 07:08:04.154300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/price_area.py
--rw-r--r--   0        0        0    27668 2024-03-21 07:08:04.154300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/price_area_activation_price_down.py
--rw-r--r--   0        0        0    27580 2024-03-21 07:08:04.154300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/price_area_activation_price_up.py
--rw-r--r--   0        0        0    27580 2024-03-21 07:08:04.154300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/price_area_capacity_price_down.py
--rw-r--r--   0        0        0    27462 2024-03-21 07:08:04.154300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/price_area_capacity_price_up.py
--rw-r--r--   0        0        0    27374 2024-03-21 07:08:04.154300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/price_area_day_ahead_price.py
--rw-r--r--   0        0        0    27739 2024-03-21 07:08:04.154300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/price_area_main_scenario_day_ahead.py
--rw-r--r--   0        0        0    27959 2024-03-21 07:08:04.154300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/price_area_own_capacity_allocation_down.py
--rw-r--r--   0        0        0    27871 2024-03-21 07:08:04.154300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/price_area_own_capacity_allocation_up.py
--rw-r--r--   0        0        0     2011 2024-03-21 07:08:04.154300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/price_area_plants.py
--rw-r--r--   0        0        0     6273 2024-03-21 07:08:04.154300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/price_area_query.py
--rw-r--r--   0        0        0    27597 2024-03-21 07:08:04.154300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/price_area_relative_activation.py
--rw-r--r--   0        0        0    28047 2024-03-21 07:08:04.154300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/price_area_total_capacity_allocation_down.py
--rw-r--r--   0        0        0    27959 2024-03-21 07:08:04.154300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/price_area_total_capacity_allocation_up.py
--rw-r--r--   0        0        0     2095 2024-03-21 07:08:04.154300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/price_area_watercourses.py
--rw-r--r--   0        0        0    20054 2024-03-21 07:08:04.154300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/reservoir.py
--rw-r--r--   0        0        0     1510 2024-03-21 07:08:04.154300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/reservoir_query.py
--rw-r--r--   0        0        0    15561 2024-03-21 07:08:04.154300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/turbine_efficiency_curve.py
--rw-r--r--   0        0        0     1615 2024-03-21 07:08:04.154300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/turbine_efficiency_curve_query.py
--rw-r--r--   0        0        0    22125 2024-03-21 07:08:04.154300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/watercourse.py
--rw-r--r--   0        0        0     2026 2024-03-21 07:08:04.154300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/watercourse_plants.py
--rw-r--r--   0        0        0    26508 2024-03-21 07:08:04.154300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/watercourse_production_obligation.py
--rw-r--r--   0        0        0     3023 2024-03-21 07:08:04.154300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/watercourse_query.py
--rw-r--r--   0        0        0    10107 2024-03-21 07:08:04.154300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api_client.py
--rw-r--r--   0        0        0     4780 2024-03-21 07:08:04.154300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/data_classes/__init__.py
--rw-r--r--   0        0        0     8028 2024-03-21 07:08:04.154300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/data_classes/_bid_method.py
--rw-r--r--   0        0        0    21842 2024-03-21 07:08:04.154300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/data_classes/_core.py
--rw-r--r--   0        0        0    19886 2024-03-21 07:08:04.154300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/data_classes/_generator.py
--rw-r--r--   0        0        0     9775 2024-03-21 07:08:04.154300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/data_classes/_generator_efficiency_curve.py
--rw-r--r--   0        0        0    32081 2024-03-21 07:08:04.154300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/data_classes/_plant.py
--rw-r--r--   0        0        0    33420 2024-03-21 07:08:04.154300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/data_classes/_price_area.py
--rw-r--r--   0        0        0    10116 2024-03-21 07:08:04.154300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/data_classes/_reservoir.py
--rw-r--r--   0        0        0     9645 2024-03-21 07:08:04.154300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/data_classes/_turbine_efficiency_curve.py
--rw-r--r--   0        0        0    13593 2024-03-21 07:08:04.154300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/data_classes/_watercourse.py
--rw-r--r--   0        0        0      113 2024-03-21 07:08:04.154300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/cogshop1/__init__.py
--rw-r--r--   0        0        0        0 2024-03-21 07:08:04.154300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/cogshop1/_api/__init__.py
--rw-r--r--   0        0        0     8201 2024-03-21 07:08:04.154300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/cogshop1/_api/_core.py
--rw-r--r--   0        0        0    14521 2024-03-21 07:08:04.154300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/cogshop1/_api/case.py
--rw-r--r--   0        0        0     7486 2024-03-21 07:08:04.154300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/cogshop1/_api/commands_config.py
--rw-r--r--   0        0        0     9780 2024-03-21 07:08:04.154300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/cogshop1/_api/file_ref.py
--rw-r--r--   0        0        0    15960 2024-03-21 07:08:04.154300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/cogshop1/_api/mapping.py
--rw-r--r--   0        0        0    17869 2024-03-21 07:08:04.154300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/cogshop1/_api/model_template.py
--rw-r--r--   0        0        0    11367 2024-03-21 07:08:04.154300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/cogshop1/_api/processing_log.py
--rw-r--r--   0        0        0    19744 2024-03-21 07:08:04.154300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/cogshop1/_api/scenario.py
--rw-r--r--   0        0        0    10871 2024-03-21 07:08:04.154300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/cogshop1/_api/transformation.py
--rw-r--r--   0        0        0     3003 2024-03-21 07:08:04.154300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/cogshop1/_api_client.py
--rw-r--r--   0        0        0     2524 2024-03-21 07:08:04.154300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/cogshop1/data_classes/__init__.py
--rw-r--r--   0        0        0     4596 2024-03-21 07:08:04.154300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/cogshop1/data_classes/_case.py
--rw-r--r--   0        0        0     2226 2024-03-21 07:08:04.154300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/cogshop1/data_classes/_commands_config.py
--rw-r--r--   0        0        0     5824 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/cogshop1/data_classes/_core.py
--rw-r--r--   0        0        0     2393 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/cogshop1/data_classes/_file_ref.py
--rw-r--r--   0        0        0     4586 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/cogshop1/data_classes/_mapping.py
--rw-r--r--   0        0        0     5174 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/cogshop1/data_classes/_model_template.py
--rw-r--r--   0        0        0     2819 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/cogshop1/data_classes/_processing_log.py
--rw-r--r--   0        0        0     6951 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/cogshop1/data_classes/_scenario.py
--rw-r--r--   0        0        0     2621 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/cogshop1/data_classes/_transformation.py
--rw-r--r--   0        0        0      118 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/__init__.py
--rw-r--r--   0        0        0        0 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/__init__.py
--rw-r--r--   0        0        0    32202 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/_core.py
--rw-r--r--   0        0        0    26333 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/alert.py
--rw-r--r--   0        0        0     1497 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/alert_query.py
--rw-r--r--   0        0        0    23471 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/basic_bid_matrix.py
--rw-r--r--   0        0        0     2115 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/basic_bid_matrix_alerts.py
--rw-r--r--   0        0        0     4257 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/basic_bid_matrix_query.py
--rw-r--r--   0        0        0    29138 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/bid_document.py
--rw-r--r--   0        0        0     2059 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/bid_document_alerts.py
--rw-r--r--   0        0        0     2106 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/bid_document_partials.py
--rw-r--r--   0        0        0     8683 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/bid_document_query.py
--rw-r--r--   0        0        0    22831 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/bid_matrix.py
--rw-r--r--   0        0        0     2032 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/bid_matrix_alerts.py
--rw-r--r--   0        0        0     4208 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/bid_matrix_query.py
--rw-r--r--   0        0        0    16942 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/bid_method.py
--rw-r--r--   0        0        0     1518 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/bid_method_query.py
--rw-r--r--   0        0        0    24946 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/multi_scenario_matrix.py
--rw-r--r--   0        0        0     2185 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/multi_scenario_matrix_alerts.py
--rw-r--r--   0        0        0     6202 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/multi_scenario_matrix_query.py
--rw-r--r--   0        0        0     2410 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/multi_scenario_matrix_scenario_results.py
--rw-r--r--   0        0        0    19843 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/price_area.py
--rw-r--r--   0        0        0    25798 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/price_area_main_scenario.py
--rw-r--r--   0        0        0    25883 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/price_area_price_scenarios.py
--rw-r--r--   0        0        0     2565 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/price_area_query.py
--rw-r--r--   0        0        0    20327 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/shop_multi_scenario_method.py
--rw-r--r--   0        0        0     2415 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/shop_multi_scenario_method_price_scenarios.py
--rw-r--r--   0        0        0     3285 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/shop_multi_scenario_method_query.py
--rw-r--r--   0        0        0    18030 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/shop_price_scenario.py
--rw-r--r--   0        0        0    24799 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/shop_price_scenario_price.py
--rw-r--r--   0        0        0     1559 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/shop_price_scenario_query.py
--rw-r--r--   0        0        0    15855 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/shop_price_scenario_result.py
--rw-r--r--   0        0        0    24935 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/shop_price_scenario_result_price.py
--rw-r--r--   0        0        0    25177 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/shop_price_scenario_result_production.py
--rw-r--r--   0        0        0     2729 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/shop_price_scenario_result_query.py
--rw-r--r--   0        0        0    18424 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/water_value_based_method.py
--rw-r--r--   0        0        0     1618 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/water_value_based_method_query.py
--rw-r--r--   0        0        0    11190 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api_client.py
--rw-r--r--   0        0        0     7367 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/data_classes/__init__.py
--rw-r--r--   0        0        0    16179 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/data_classes/_alert.py
--rw-r--r--   0        0        0    14603 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/data_classes/_basic_bid_matrix.py
--rw-r--r--   0        0        0    24068 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/data_classes/_bid_document.py
--rw-r--r--   0        0        0    14961 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/data_classes/_bid_matrix.py
--rw-r--r--   0        0        0     8042 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/data_classes/_bid_method.py
--rw-r--r--   0        0        0    21842 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/data_classes/_core.py
--rw-r--r--   0        0        0    17009 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/data_classes/_multi_scenario_matrix.py
--rw-r--r--   0        0        0    14474 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/data_classes/_price_area.py
--rw-r--r--   0        0        0    11922 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/data_classes/_shop_multi_scenario_method.py
--rw-r--r--   0        0        0     9352 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/data_classes/_shop_price_scenario.py
--rw-r--r--   0        0        0    13240 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/data_classes/_shop_price_scenario_result.py
--rw-r--r--   0        0        0     8909 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/data_classes/_water_value_based_method.py
--rw-r--r--   0        0        0      123 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/__init__.py
--rw-r--r--   0        0        0        0 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/__init__.py
--rw-r--r--   0        0        0    39672 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/_core.py
--rw-r--r--   0        0        0    26319 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/alert.py
--rw-r--r--   0        0        0     1486 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/alert_query.py
--rw-r--r--   0        0        0    24174 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/basic_bid_matrix.py
--rw-r--r--   0        0        0     2111 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/basic_bid_matrix_alerts.py
--rw-r--r--   0        0        0     4272 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/basic_bid_matrix_query.py
--rw-r--r--   0        0        0    17147 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_calculation_task.py
--rw-r--r--   0        0        0     3516 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_calculation_task_query.py
--rw-r--r--   0        0        0     9245 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_configuration.py
--rw-r--r--   0        0        0     2681 2024-03-21 07:08:04.158300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_configuration_query.py
--rw-r--r--   0        0        0    23945 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_configuration_shop.py
--rw-r--r--   0        0        0     2278 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_configuration_shop_plants_shop.py
--rw-r--r--   0        0        0     9424 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_configuration_shop_query.py
--rw-r--r--   0        0        0     2368 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_configuration_shop_watercourses_shop.py
--rw-r--r--   0        0        0    12634 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_configuration_water.py
--rw-r--r--   0        0        0     2215 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_configuration_water_plants.py
--rw-r--r--   0        0        0     9300 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_configuration_water_query.py
--rw-r--r--   0        0        0     2305 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_configuration_water_watercourses.py
--rw-r--r--   0        0        0    27757 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_document_afrr.py
--rw-r--r--   0        0        0     2124 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_document_afrr_alerts.py
--rw-r--r--   0        0        0     2122 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_document_afrr_bids.py
--rw-r--r--   0        0        0     6040 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_document_afrr_query.py
--rw-r--r--   0        0        0    30214 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_document_day_ahead.py
--rw-r--r--   0        0        0     2193 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_document_day_ahead_alerts.py
--rw-r--r--   0        0        0     2240 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_document_day_ahead_partials.py
--rw-r--r--   0        0        0     8876 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_document_day_ahead_query.py
--rw-r--r--   0        0        0    22586 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_matrix.py
--rw-r--r--   0        0        0     2028 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_matrix_alerts.py
--rw-r--r--   0        0        0     3015 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_matrix_query.py
--rw-r--r--   0        0        0    22970 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_matrix_raw.py
--rw-r--r--   0        0        0     2082 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_matrix_raw_alerts.py
--rw-r--r--   0        0        0     3035 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_matrix_raw_query.py
--rw-r--r--   0        0        0    16928 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_method.py
--rw-r--r--   0        0        0    17429 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_method_afrr.py
--rw-r--r--   0        0        0     1528 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_method_afrr_query.py
--rw-r--r--   0        0        0    17665 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_method_custom.py
--rw-r--r--   0        0        0     1538 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_method_custom_query.py
--rw-r--r--   0        0        0    17944 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_method_day_ahead.py
--rw-r--r--   0        0        0     1549 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_method_day_ahead_query.py
--rw-r--r--   0        0        0     1507 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_method_query.py
--rw-r--r--   0        0        0    27374 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_method_shop_multi_scenario.py
--rw-r--r--   0        0        0     2389 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_method_shop_multi_scenario_price_scenarios.py
--rw-r--r--   0        0        0     3208 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_method_shop_multi_scenario_query.py
--rw-r--r--   0        0        0     2357 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_method_shop_multi_scenario_scenarios.py
--rw-r--r--   0        0        0    18166 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_method_water_value.py
--rw-r--r--   0        0        0     1559 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_method_water_value_query.py
--rw-r--r--   0        0        0    27966 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_row.py
--rw-r--r--   0        0        0     1985 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_row_alerts.py
--rw-r--r--   0        0        0     5347 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_row_query.py
--rw-r--r--   0        0        0    16447 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/case.py
--rw-r--r--   0        0        0     2467 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/case_query.py
--rw-r--r--   0        0        0    15388 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/commands.py
--rw-r--r--   0        0        0     1500 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/commands_query.py
--rw-r--r--   0        0        0    24304 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/custom_bid_matrix.py
--rw-r--r--   0        0        0     2125 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/custom_bid_matrix_alerts.py
--rw-r--r--   0        0        0     4275 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/custom_bid_matrix_query.py
--rw-r--r--   0        0        0    27876 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/generator.py
--rw-r--r--   0        0        0    15759 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/generator_efficiency_curve.py
--rw-r--r--   0        0        0     1621 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/generator_efficiency_curve_query.py
--rw-r--r--   0        0        0    28667 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/generator_is_available_time_series.py
--rw-r--r--   0        0        0     4539 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/generator_query.py
--rw-r--r--   0        0        0    28258 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/generator_start_stop_cost.py
--rw-r--r--   0        0        0     2209 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/generator_turbine_curves.py
--rw-r--r--   0        0        0    20120 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/mapping.py
--rw-r--r--   0        0        0     1496 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/mapping_query.py
--rw-r--r--   0        0        0    25885 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/mapping_timeseries.py
--rw-r--r--   0        0        0    31011 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/market_configuration.py
--rw-r--r--   0        0        0     1557 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/market_configuration_query.py
--rw-r--r--   0        0        0    21807 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/model_template.py
--rw-r--r--   0        0        0     2150 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/model_template_base_mappings.py
--rw-r--r--   0        0        0     4372 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/model_template_query.py
--rw-r--r--   0        0        0    25665 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/multi_scenario_matrix.py
--rw-r--r--   0        0        0     2181 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/multi_scenario_matrix_alerts.py
--rw-r--r--   0        0        0     6184 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/multi_scenario_matrix_query.py
--rw-r--r--   0        0        0    26051 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/multi_scenario_matrix_raw.py
--rw-r--r--   0        0        0     2235 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/multi_scenario_matrix_raw_alerts.py
--rw-r--r--   0        0        0     6292 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/multi_scenario_matrix_raw_query.py
--rw-r--r--   0        0        0     2421 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/multi_scenario_matrix_raw_shop_results.py
--rw-r--r--   0        0        0     2339 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/multi_scenario_matrix_scenario_results.py
--rw-r--r--   0        0        0     2279 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/multi_scenario_matrix_shop_results.py
--rw-r--r--   0        0        0    27485 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/partial_post_processing_input.py
--rw-r--r--   0        0        0     2477 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/partial_post_processing_input_partial_bid_matrices_raw.py
--rw-r--r--   0        0        0     4659 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/partial_post_processing_input_query.py
--rw-r--r--   0        0        0    28130 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/partial_post_processing_output.py
--rw-r--r--   0        0        0     2305 2024-03-21 07:08:04.162300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/partial_post_processing_output_alerts.py
--rw-r--r--   0        0        0     2398 2024-03-21 07:08:04.166300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/partial_post_processing_output_partial_matrices.py
--rw-r--r--   0        0        0     6251 2024-03-21 07:08:04.166300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/partial_post_processing_output_query.py
--rw-r--r--   0        0        0    33043 2024-03-21 07:08:04.166300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/plant.py
--rw-r--r--   0        0        0    30474 2024-03-21 07:08:04.166300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/plant_feeding_fee_time_series.py
--rw-r--r--   0        0        0     2010 2024-03-21 07:08:04.166300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/plant_generators.py
--rw-r--r--   0        0        0    30474 2024-03-21 07:08:04.166300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/plant_head_direct_time_series.py
--rw-r--r--   0        0        0    30474 2024-03-21 07:08:04.166300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/plant_inlet_level_time_series.py
--rw-r--r--   0        0        0    30518 2024-03-21 07:08:04.166300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/plant_outlet_level_time_series.py
--rw-r--r--   0        0        0    30180 2024-03-21 07:08:04.166300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/plant_p_max_time_series.py
--rw-r--r--   0        0        0    30180 2024-03-21 07:08:04.166300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/plant_p_min_time_series.py
--rw-r--r--   0        0        0     5506 2024-03-21 07:08:04.166300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/plant_query.py
--rw-r--r--   0        0        0    20179 2024-03-21 07:08:04.166300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/plant_shop.py
--rw-r--r--   0        0        0     1507 2024-03-21 07:08:04.166300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/plant_shop_query.py
--rw-r--r--   0        0        0    30474 2024-03-21 07:08:04.166300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/plant_water_value_time_series.py
--rw-r--r--   0        0        0    27942 2024-03-21 07:08:04.166300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/preprocessor_input.py
--rw-r--r--   0        0        0     2560 2024-03-21 07:08:04.166300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/preprocessor_input_query.py
--rw-r--r--   0        0        0    26860 2024-03-21 07:08:04.166300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/preprocessor_output.py
--rw-r--r--   0        0        0     2153 2024-03-21 07:08:04.166300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/preprocessor_output_alerts.py
--rw-r--r--   0        0        0     5417 2024-03-21 07:08:04.166300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/preprocessor_output_query.py
--rw-r--r--   0        0        0    18429 2024-03-21 07:08:04.166300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/price_area.py
--rw-r--r--   0        0        0    20589 2024-03-21 07:08:04.166300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/price_area_afrr.py
--rw-r--r--   0        0        0    26201 2024-03-21 07:08:04.166300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/price_area_afrr_activation_price_down.py
--rw-r--r--   0        0        0    26113 2024-03-21 07:08:04.166300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/price_area_afrr_activation_price_up.py
--rw-r--r--   0        0        0    26113 2024-03-21 07:08:04.166300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/price_area_afrr_capacity_price_down.py
--rw-r--r--   0        0        0    25995 2024-03-21 07:08:04.166300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/price_area_afrr_capacity_price_up.py
--rw-r--r--   0        0        0    26492 2024-03-21 07:08:04.166300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/price_area_afrr_own_capacity_allocation_down.py
--rw-r--r--   0        0        0    26404 2024-03-21 07:08:04.166300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/price_area_afrr_own_capacity_allocation_up.py
--rw-r--r--   0        0        0     1528 2024-03-21 07:08:04.166300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/price_area_afrr_query.py
--rw-r--r--   0        0        0    26130 2024-03-21 07:08:04.166300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/price_area_afrr_relative_activation.py
--rw-r--r--   0        0        0    26580 2024-03-21 07:08:04.166300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/price_area_afrr_total_capacity_allocation_down.py
--rw-r--r--   0        0        0    26492 2024-03-21 07:08:04.166300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/price_area_afrr_total_capacity_allocation_up.py
--rw-r--r--   0        0        0    21294 2024-03-21 07:08:04.166300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/price_area_asset.py
--rw-r--r--   0        0        0     2103 2024-03-21 07:08:04.166300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/price_area_asset_plants.py
--rw-r--r--   0        0        0     4606 2024-03-21 07:08:04.166300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/price_area_asset_query.py
--rw-r--r--   0        0        0     2193 2024-03-21 07:08:04.166300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/price_area_asset_watercourses.py
--rw-r--r--   0        0        0    21052 2024-03-21 07:08:04.166300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/price_area_day_ahead.py
--rw-r--r--   0        0        0     2711 2024-03-21 07:08:04.166300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/price_area_day_ahead_query.py
--rw-r--r--   0        0        0     1507 2024-03-21 07:08:04.166300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/price_area_query.py
--rw-r--r--   0        0        0    14625 2024-03-21 07:08:04.166300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/price_prod_case.py
--rw-r--r--   0        0        0    24517 2024-03-21 07:08:04.166300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/price_prod_case_price.py
--rw-r--r--   0        0        0    24759 2024-03-21 07:08:04.166300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/price_prod_case_production.py
--rw-r--r--   0        0        0     2490 2024-03-21 07:08:04.166300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/price_prod_case_query.py
--rw-r--r--   0        0        0    17508 2024-03-21 07:08:04.166300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/price_scenario.py
--rw-r--r--   0        0        0     1527 2024-03-21 07:08:04.166300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/price_scenario_query.py
--rw-r--r--   0        0        0    24894 2024-03-21 07:08:04.166300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/price_scenario_timeseries.py
--rw-r--r--   0        0        0    20122 2024-03-21 07:08:04.166300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/reservoir.py
--rw-r--r--   0        0        0     1506 2024-03-21 07:08:04.166300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/reservoir_query.py
--rw-r--r--   0        0        0    21809 2024-03-21 07:08:04.166300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/scenario.py
--rw-r--r--   0        0        0     2076 2024-03-21 07:08:04.166300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/scenario_mappings_override.py
--rw-r--r--   0        0        0     5500 2024-03-21 07:08:04.166300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/scenario_query.py
--rw-r--r--   0        0        0    33208 2024-03-21 07:08:04.166300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/scenario_raw.py
--rw-r--r--   0        0        0     2131 2024-03-21 07:08:04.166300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/scenario_raw_mappings_override.py
--rw-r--r--   0        0        0     4385 2024-03-21 07:08:04.166300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/scenario_raw_query.py
--rw-r--r--   0        0        0    31589 2024-03-21 07:08:04.166300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/shop_partial_bid_calculation_input.py
--rw-r--r--   0        0        0     2360 2024-03-21 07:08:04.166300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/shop_partial_bid_calculation_input_alerts.py
--rw-r--r--   0        0        0     6071 2024-03-21 07:08:04.166300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/shop_partial_bid_calculation_input_query.py
--rw-r--r--   0        0        0     2596 2024-03-21 07:08:04.166300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/shop_partial_bid_calculation_input_shop_result_price_prod.py
--rw-r--r--   0        0        0     2437 2024-03-21 07:08:04.166300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/shop_partial_bid_calculation_input_shop_results.py
--rw-r--r--   0        0        0    28954 2024-03-21 07:08:04.166300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/shop_partial_bid_calculation_output.py
--rw-r--r--   0        0        0     2374 2024-03-21 07:08:04.166300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/shop_partial_bid_calculation_output_alerts.py
--rw-r--r--   0        0        0     5850 2024-03-21 07:08:04.166300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/shop_partial_bid_calculation_output_query.py
--rw-r--r--   0        0        0    15649 2024-03-21 07:08:04.166300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/shop_result.py
--rw-r--r--   0        0        0     2041 2024-03-21 07:08:04.166300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/shop_result_alerts.py
--rw-r--r--   0        0        0    25018 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/shop_result_output_timeseries.py
--rw-r--r--   0        0        0    24818 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/shop_result_price.py
--rw-r--r--   0        0        0    18577 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/shop_result_price_prod.py
--rw-r--r--   0        0        0     2193 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/shop_result_price_prod_alerts.py
--rw-r--r--   0        0        0    25755 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/shop_result_price_prod_output_timeseries.py
--rw-r--r--   0        0        0     2389 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/shop_result_price_prod_production_timeseries.py
--rw-r--r--   0        0        0     7683 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/shop_result_price_prod_query.py
--rw-r--r--   0        0        0    25060 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/shop_result_production.py
--rw-r--r--   0        0        0     4167 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/shop_result_query.py
--rw-r--r--   0        0        0    21373 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/shop_time_series.py
--rw-r--r--   0        0        0     1533 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/shop_time_series_query.py
--rw-r--r--   0        0        0    26305 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/shop_time_series_timeseries.py
--rw-r--r--   0        0        0    26033 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/shop_trigger_input.py
--rw-r--r--   0        0        0     2555 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/shop_trigger_input_query.py
--rw-r--r--   0        0        0    26958 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/shop_trigger_output.py
--rw-r--r--   0        0        0     2152 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/shop_trigger_output_alerts.py
--rw-r--r--   0        0        0     5516 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/shop_trigger_output_query.py
--rw-r--r--   0        0        0    30710 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/task_dispatcher_shop_input.py
--rw-r--r--   0        0        0     2751 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/task_dispatcher_shop_input_query.py
--rw-r--r--   0        0        0    28882 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/task_dispatcher_shop_output.py
--rw-r--r--   0        0        0     2263 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/task_dispatcher_shop_output_alerts.py
--rw-r--r--   0        0        0     2615 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/task_dispatcher_shop_output_partial_bid_calculations.py
--rw-r--r--   0        0        0     2481 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/task_dispatcher_shop_output_preprocessor_calculations.py
--rw-r--r--   0        0        0     8359 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/task_dispatcher_shop_output_query.py
--rw-r--r--   0        0        0    27353 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/task_dispatcher_water_input.py
--rw-r--r--   0        0        0     2761 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/task_dispatcher_water_input_query.py
--rw-r--r--   0        0        0    27998 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/task_dispatcher_water_output.py
--rw-r--r--   0        0        0     2277 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/task_dispatcher_water_output_alerts.py
--rw-r--r--   0        0        0     2617 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/task_dispatcher_water_output_bid_calculation_tasks.py
--rw-r--r--   0        0        0     6469 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/task_dispatcher_water_output_query.py
--rw-r--r--   0        0        0    26901 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/total_bid_matrix_calculation_input.py
--rw-r--r--   0        0        0     2476 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/total_bid_matrix_calculation_input_partial_bid_matrices.py
--rw-r--r--   0        0        0     3269 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/total_bid_matrix_calculation_input_query.py
--rw-r--r--   0        0        0    28910 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/total_bid_matrix_calculation_output.py
--rw-r--r--   0        0        0     2374 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/total_bid_matrix_calculation_output_alerts.py
--rw-r--r--   0        0        0     5816 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/total_bid_matrix_calculation_output_query.py
--rw-r--r--   0        0        0    15615 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/turbine_efficiency_curve.py
--rw-r--r--   0        0        0     1611 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/turbine_efficiency_curve_query.py
--rw-r--r--   0        0        0    26497 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/water_partial_bid_calculation_input.py
--rw-r--r--   0        0        0     2841 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/water_partial_bid_calculation_input_query.py
--rw-r--r--   0        0        0    29268 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/water_partial_bid_calculation_output.py
--rw-r--r--   0        0        0     2388 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/water_partial_bid_calculation_output_alerts.py
--rw-r--r--   0        0        0     5933 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/water_partial_bid_calculation_output_query.py
--rw-r--r--   0        0        0    23775 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/watercourse.py
--rw-r--r--   0        0        0     2038 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/watercourse_plants.py
--rw-r--r--   0        0        0    27164 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/watercourse_production_obligation.py
--rw-r--r--   0        0        0     3021 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/watercourse_query.py
--rw-r--r--   0        0        0    20859 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/watercourse_shop.py
--rw-r--r--   0        0        0     1537 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/watercourse_shop_query.py
--rw-r--r--   0        0        0    39687 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api_client.py
--rw-r--r--   0        0        0    39170 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/__init__.py
--rw-r--r--   0        0        0    16187 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_alert.py
--rw-r--r--   0        0        0    15658 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_basic_bid_matrix.py
--rw-r--r--   0        0        0    14108 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_bid_calculation_task.py
--rw-r--r--   0        0        0    11328 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_bid_configuration.py
--rw-r--r--   0        0        0    21582 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_bid_configuration_shop.py
--rw-r--r--   0        0        0    19986 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_bid_configuration_water.py
--rw-r--r--   0        0        0    19771 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_bid_document_afrr.py
--rw-r--r--   0        0        0    24622 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_bid_document_day_ahead.py
--rw-r--r--   0        0        0    13659 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_bid_matrix.py
--rw-r--r--   0        0        0    13022 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_bid_matrix_raw.py
--rw-r--r--   0        0        0     8032 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_bid_method.py
--rw-r--r--   0        0        0     8451 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_bid_method_afrr.py
--rw-r--r--   0        0        0     8400 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_bid_method_custom.py
--rw-r--r--   0        0        0     8491 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_bid_method_day_ahead.py
--rw-r--r--   0        0        0    16391 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_bid_method_shop_multi_scenario.py
--rw-r--r--   0        0        0     8809 2024-03-21 07:08:04.170300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_bid_method_water_value.py
--rw-r--r--   0        0        0    23791 2024-03-21 07:08:04.174300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_bid_row.py
--rw-r--r--   0        0        0    15843 2024-03-21 07:08:04.174300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_case.py
--rw-r--r--   0        0        0     7771 2024-03-21 07:08:04.174300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_commands.py
--rw-r--r--   0        0        0    21844 2024-03-21 07:08:04.174300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_core.py
--rw-r--r--   0        0        0    15535 2024-03-21 07:08:04.174300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_custom_bid_matrix.py
--rw-r--r--   0        0        0    20607 2024-03-21 07:08:04.174300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_generator.py
--rw-r--r--   0        0        0     9783 2024-03-21 07:08:04.174300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_generator_efficiency_curve.py
--rw-r--r--   0        0        0    12070 2024-03-21 07:08:04.174300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_mapping.py
--rw-r--r--   0        0        0    16839 2024-03-21 07:08:04.174300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_market_configuration.py
--rw-r--r--   0        0        0    16939 2024-03-21 07:08:04.174300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_model_template.py
--rw-r--r--   0        0        0    18207 2024-03-21 07:08:04.174300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_multi_scenario_matrix.py
--rw-r--r--   0        0        0    18186 2024-03-21 07:08:04.174300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_multi_scenario_matrix_raw.py
--rw-r--r--   0        0        0    18751 2024-03-21 07:08:04.174300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_partial_post_processing_input.py
--rw-r--r--   0        0        0    19241 2024-03-21 07:08:04.174300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_partial_post_processing_output.py
--rw-r--r--   0        0        0    32106 2024-03-21 07:08:04.174300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_plant.py
--rw-r--r--   0        0        0    10099 2024-03-21 07:08:04.174300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_plant_shop.py
--rw-r--r--   0        0        0    17302 2024-03-21 07:08:04.174300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_preprocessor_input.py
--rw-r--r--   0        0        0    18795 2024-03-21 07:08:04.174300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_preprocessor_output.py
--rw-r--r--   0        0        0     9116 2024-03-21 07:08:04.174300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_price_area.py
--rw-r--r--   0        0        0    21574 2024-03-21 07:08:04.174300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_price_area_afrr.py
--rw-r--r--   0        0        0    12748 2024-03-21 07:08:04.174300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_price_area_asset.py
--rw-r--r--   0        0        0    10034 2024-03-21 07:08:04.174300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_price_area_day_ahead.py
--rw-r--r--   0        0        0    11733 2024-03-21 07:08:04.174300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_price_prod_case.py
--rw-r--r--   0        0        0     7104 2024-03-21 07:08:04.174300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_price_scenario.py
--rw-r--r--   0        0        0    10082 2024-03-21 07:08:04.174300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_reservoir.py
--rw-r--r--   0        0        0    16790 2024-03-21 07:08:04.174300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_scenario.py
--rw-r--r--   0        0        0    19282 2024-03-21 07:08:04.174300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_scenario_raw.py
--rw-r--r--   0        0        0    24177 2024-03-21 07:08:04.174300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_shop_partial_bid_calculation_input.py
--rw-r--r--   0        0        0    20739 2024-03-21 07:08:04.174300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_shop_partial_bid_calculation_output.py
--rw-r--r--   0        0        0    15849 2024-03-21 07:08:04.174300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_shop_result.py
--rw-r--r--   0        0        0    21030 2024-03-21 07:08:04.174300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_shop_result_price_prod.py
--rw-r--r--   0        0        0    12379 2024-03-21 07:08:04.174300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_shop_time_series.py
--rw-r--r--   0        0        0    16345 2024-03-21 07:08:04.174300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_shop_trigger_input.py
--rw-r--r--   0        0        0    19223 2024-03-21 07:08:04.174300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_shop_trigger_output.py
--rw-r--r--   0        0        0    19606 2024-03-21 07:08:04.174300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_task_dispatcher_shop_input.py
--rw-r--r--   0        0        0    22391 2024-03-21 07:08:04.174300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_task_dispatcher_shop_output.py
--rw-r--r--   0        0        0    17566 2024-03-21 07:08:04.174300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_task_dispatcher_water_input.py
--rw-r--r--   0        0        0    19626 2024-03-21 07:08:04.174300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_task_dispatcher_water_output.py
--rw-r--r--   0        0        0    15671 2024-03-21 07:08:04.174300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_total_bid_matrix_calculation_input.py
--rw-r--r--   0        0        0    20410 2024-03-21 07:08:04.174300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_total_bid_matrix_calculation_output.py
--rw-r--r--   0        0        0     9653 2024-03-21 07:08:04.174300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_turbine_efficiency_curve.py
--rw-r--r--   0        0        0    16847 2024-03-21 07:08:04.174300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_water_partial_bid_calculation_input.py
--rw-r--r--   0        0        0    20859 2024-03-21 07:08:04.174300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_water_partial_bid_calculation_output.py
--rw-r--r--   0        0        0    14302 2024-03-21 07:08:04.174300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_watercourse.py
--rw-r--r--   0        0        0    10509 2024-03-21 07:08:04.174300 cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_watercourse_shop.py
--rw-r--r--   0        0        0     1301 2024-03-21 07:08:04.174300 cognite_power_ops-0.91.3/cognite/powerops/client/_logger.py
--rw-r--r--   0        0        0      139 2024-03-21 07:08:04.174300 cognite_power_ops-0.91.3/cognite/powerops/client/data_classes.py
--rw-r--r--   0        0        0     1322 2024-03-21 07:08:04.174300 cognite_power_ops-0.91.3/cognite/powerops/client/data_set_api.py
--rw-r--r--   0        0        0     6750 2024-03-21 07:08:04.174300 cognite_power_ops-0.91.3/cognite/powerops/client/powerops_client.py
--rw-r--r--   0        0        0       62 2024-03-21 07:08:04.174300 cognite_power_ops-0.91.3/cognite/powerops/client/shop/__init__.py
--rw-r--r--   0        0        0      885 2024-03-21 07:08:04.174300 cognite_power_ops-0.91.3/cognite/powerops/client/shop/_api_client.py
--rw-r--r--   0        0        0        0 2024-03-21 07:08:04.174300 cognite_power_ops-0.91.3/cognite/powerops/client/shop/api/__init__.py
--rw-r--r--   0        0        0     8367 2024-03-21 07:08:04.174300 cognite_power_ops-0.91.3/cognite/powerops/client/shop/api/dayahead_trigger_api.py
--rw-r--r--   0        0        0     4797 2024-03-21 07:08:04.174300 cognite_power_ops-0.91.3/cognite/powerops/client/shop/api/shop_result_files_api.py
--rw-r--r--   0        0        0     4432 2024-03-21 07:08:04.174300 cognite_power_ops-0.91.3/cognite/powerops/client/shop/api/shop_results_api.py
--rw-r--r--   0        0        0     9593 2024-03-21 07:08:04.174300 cognite_power_ops-0.91.3/cognite/powerops/client/shop/api/shop_run_api.py
--rw-r--r--   0        0        0      600 2024-03-21 07:08:04.174300 cognite_power_ops-0.91.3/cognite/powerops/client/shop/data_classes/__init__.py
--rw-r--r--   0        0        0     4114 2024-03-21 07:08:04.174300 cognite_power_ops-0.91.3/cognite/powerops/client/shop/data_classes/case.py
--rw-r--r--   0        0        0     9057 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/client/shop/data_classes/dayahead_trigger.py
--rw-r--r--   0        0        0     2720 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/client/shop/data_classes/helpers.py
--rw-r--r--   0        0        0      603 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/client/shop/data_classes/plotting.py
--rw-r--r--   0        0        0     6164 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/client/shop/data_classes/shop_result_files.py
--rw-r--r--   0        0        0     6607 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/client/shop/data_classes/shop_results.py
--rw-r--r--   0        0        0    10210 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/client/shop/data_classes/shop_results_compare.py
--rw-r--r--   0        0        0     3021 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/client/shop/data_classes/shop_run.py
--rw-r--r--   0        0        0     3743 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/client/shop/data_classes/shop_run_event.py
--rw-r--r--   0        0        0     4374 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/client/shop/shop_case.py
--rw-r--r--   0        0        0     1101 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/client/shop/shop_file_reference.py
--rw-r--r--   0        0        0    13466 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/client/shop/shop_run.py
--rw-r--r--   0        0        0    15419 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/client/shop/shop_run_api.py
--rw-r--r--   0        0        0     3885 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/client/shop/shop_run_filter.py
--rw-r--r--   0        0        0      563 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/client/shop/utils.py
--rw-r--r--   0        0        0      947 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/cognite_modules/_system.yaml
--rw-r--r--   0        0        0     1892 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/config.dev.yaml
--rw-r--r--   0        0        0      729 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/AFRRBid.datamodel.yaml
--rw-r--r--   0        0        0     1095 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/Asset.datamodel.yaml
--rw-r--r--   0        0        0     1680 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/DayAheadBid.datamodel.yaml
--rw-r--r--   0        0        0      100 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/affr_space.space.yaml
--rw-r--r--   0        0        0       92 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/asset.space.yaml
--rw-r--r--   0        0        0      367 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/containers/AFRRBid-BidMethod.container.yaml
--rw-r--r--   0        0        0     2102 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/containers/AFRRBid-BidRow.container.yaml
--rw-r--r--   0        0        0      750 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/containers/asset-EfficiencyCurve.container.yaml
--rw-r--r--   0        0        0     1380 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/containers/base-Alert.container.yaml
--rw-r--r--   0        0        0      857 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/containers/base-Asset.container.yaml
--rw-r--r--   0        0        0     1038 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/containers/base-BidDocument.container.yaml
--rw-r--r--   0        0        0      502 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/containers/base-BidMethod.container.yaml
--rw-r--r--   0        0        0     1408 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/containers/base-Generator.container.yaml
--rw-r--r--   0        0        0     2281 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/containers/base-Plant.container.yaml
--rw-r--r--   0        0        0     2375 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/containers/base-PriceArea.container.yaml
--rw-r--r--   0        0        0      556 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/containers/base-Watercourse.container.yaml
--rw-r--r--   0        0        0      466 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/containers/dayAheadBids-BidDocument.container.yaml
--rw-r--r--   0        0        0      767 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/containers/dayAheadBids-BidMatrix.container.yaml
--rw-r--r--   0        0        0      371 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/containers/dayAheadBids-BidMethod.container.yaml
--rw-r--r--   0        0        0      532 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/containers/dayAheadBids-MultiScenarioMatrix.container.yaml
--rw-r--r--   0        0        0      618 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/containers/dayAheadBids-SHOPMultiScenario.container.yaml
--rw-r--r--   0        0        0      416 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/containers/dayAheadBids-SHOPPriceScenario.container.yaml
--rw-r--r--   0        0        0      490 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/containers/dayAheadBids-SHOPPriceScenarioResults.container.yaml
--rw-r--r--   0        0        0      113 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/dayahead_space.space.yaml
--rw-r--r--   0        0        0      117 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/instance.space.yaml
--rw-r--r--   0        0        0     1515 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/node_types/power-ops-types.powerops_nodes.yaml
--rw-r--r--   0        0        0      116 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/shared.space.yaml
--rw-r--r--   0        0        0       81 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/types.space.yaml
--rw-r--r--   0        0        0     1379 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/views/AFRRBid-BidDocument.view.yaml
--rw-r--r--   0        0        0      606 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/views/AFRRBid-BidMethod.view.yaml
--rw-r--r--   0        0        0     3917 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/views/AFRRBid-BidRow.view.yaml
--rw-r--r--   0        0        0     2786 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/views/AFRRBid-PriceArea.view.yaml
--rw-r--r--   0        0        0     2618 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/views/asset-Generator.view.yaml
--rw-r--r--   0        0        0     1138 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/views/asset-GeneratorEfficiency.view.yaml
--rw-r--r--   0        0        0     5199 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/views/asset-Plant.view.yaml
--rw-r--r--   0        0        0     5204 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/views/asset-PriceArea.view.yaml
--rw-r--r--   0        0        0     1060 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/views/asset-Reservoir.view.yaml
--rw-r--r--   0        0        0     1121 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/views/asset-TurbineEfficiency.view.yaml
--rw-r--r--   0        0        0     1887 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/views/asset-Watercourse.view.yaml
--rw-r--r--   0        0        0     2558 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/views/baseBids-Alert.view.yaml
--rw-r--r--   0        0        0     2374 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/views/baseBids-BidDocument.view.yaml
--rw-r--r--   0        0        0      858 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/views/baseBids-BidMethod.view.yaml
--rw-r--r--   0        0        0      579 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/views/dayAheadBids-BasicBidMatrix.view.yaml
--rw-r--r--   0        0        0     1945 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/views/dayAheadBids-BidDocument.view.yaml
--rw-r--r--   0        0        0     1845 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/views/dayAheadBids-BidMatrix.view.yaml
--rw-r--r--   0        0        0      723 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/views/dayAheadBids-BidMethod.view.yaml
--rw-r--r--   0        0        0      700 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/views/dayAheadBids-CustomBidMatrix.view.yaml
--rw-r--r--   0        0        0      711 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/views/dayAheadBids-CustomBidMethod.view.yaml
--rw-r--r--   0        0        0     1015 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/views/dayAheadBids-MultiScenarioMatrix.view.yaml
--rw-r--r--   0        0        0     1479 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/views/dayAheadBids-PriceArea.view.yaml
--rw-r--r--   0        0        0     1312 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/views/dayAheadBids-SHOPMultiScenarioMethod.view.yaml
--rw-r--r--   0        0        0      666 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/views/dayAheadBids-SHOPPriceScenario.view.yaml
--rw-r--r--   0        0        0     1230 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/views/dayAheadBids-SHOPPriceScenarioResult.view.yaml
--rw-r--r--   0        0        0      605 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/views/dayAheadBids-WaterValueBasedMethod.view.yaml
--rw-r--r--   0        0        0     7771 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/all_PowerOps.datamodel.yaml
--rw-r--r--   0        0        0     1402 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/compute_DayAheadBenchmarking.datamodel.yaml
--rw-r--r--   0        0        0     2995 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/compute_SHOPBasedDayAhead.datamodel.yaml
--rw-r--r--   0        0        0     2892 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/compute_TotalBidCalculation.datamodel.yaml
--rw-r--r--   0        0        0     2068 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/compute_WaterValueBasedDayAhead.datamodel.yaml
--rw-r--r--   0        0        0     2400 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/config_DayAheadConfiguration.datamodel.yaml
--rw-r--r--   0        0        0     1377 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/containers/Alert.container.yaml
--rw-r--r--   0        0        0      854 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/containers/Asset.container.yaml
--rw-r--r--   0        0        0      839 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/containers/BidConfiguration.container.yaml
--rw-r--r--   0        0        0     1035 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/containers/BidDocument.container.yaml
--rw-r--r--   0        0        0      474 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/containers/BidDocumentDayAhead.container.yaml
--rw-r--r--   0        0        0      873 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/containers/BidMatrix.container.yaml
--rw-r--r--   0        0        0     1217 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/containers/BidMethod.container.yaml
--rw-r--r--   0        0        0      374 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/containers/BidMethodAFRR.container.yaml
--rw-r--r--   0        0        0     2079 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/containers/BidRow.container.yaml
--rw-r--r--   0        0        0     1705 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/containers/Case.container.yaml
--rw-r--r--   0        0        0      278 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/containers/CommandsConfig.container.yaml
--rw-r--r--   0        0        0      754 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/containers/EfficiencyCurve.container.yaml
--rw-r--r--   0        0        0     2105 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/containers/FunctionData.container.yaml
--rw-r--r--   0        0        0     1752 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/containers/FunctionMetadata.container.yaml
--rw-r--r--   0        0        0     1402 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/containers/Generator.container.yaml
--rw-r--r--   0        0        0     1180 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/containers/Mapping.container.yaml
--rw-r--r--   0        0        0     2075 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/containers/MarketConfiguration.container.yaml
--rw-r--r--   0        0        0     1424 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/containers/ModelTemplate.container.yaml
--rw-r--r--   0        0        0     2275 2024-03-21 07:08:04.178300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/containers/Plant.container.yaml
--rw-r--r--   0        0        0     2389 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/containers/PriceArea.container.yaml
--rw-r--r--   0        0        0      503 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/containers/SHOPObjectiveValue.container.yaml
--rw-r--r--   0        0        0     2659 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/containers/SHOPResult.container.yaml
--rw-r--r--   0        0        0     1103 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/containers/SHOPTimeSeries.container.yaml
--rw-r--r--   0        0        0      980 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/containers/Scenario.container.yaml
--rw-r--r--   0        0        0      550 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/containers/Watercourse.container.yaml
--rw-r--r--   0        0        0      660 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/frontend_AFRRBid.datamodel.yaml
--rw-r--r--   0        0        0      998 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/frontend_Asset.datamodel.yaml
--rw-r--r--   0        0        0     2054 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/frontend_DayAheadBid.datamodel.yaml
--rw-r--r--   0        0        0      117 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/instance.space.yaml
--rw-r--r--   0        0        0      103 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/models.space.yaml
--rw-r--r--   0        0        0     6649 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/node_types/power-ops-types.powerops_nodes.yaml
--rw-r--r--   0        0        0      730 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/product_CogShop.datamodel.yaml
--rw-r--r--   0        0        0       90 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/type.space.yaml
--rw-r--r--   0        0        0     2653 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/Alert.view.yaml
--rw-r--r--   0        0        0     1465 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/BidCalculationTask.view.yaml
--rw-r--r--   0        0        0     3984 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/BidRow.view.yaml
--rw-r--r--   0        0        0     2821 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/MarketConfiguration.view.yaml
--rw-r--r--   0        0        0     1158 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/assets/1-interface.PowerAsset.view.yaml
--rw-r--r--   0        0        0     1104 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/assets/1-interface.PriceArea.view.yaml
--rw-r--r--   0        0        0     2262 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/assets/Generator.view.yaml
--rw-r--r--   0        0        0     1268 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/assets/GeneratorEfficiency.view.yaml
--rw-r--r--   0        0        0     4025 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/assets/Plant.view.yaml
--rw-r--r--   0        0        0      695 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/assets/PlantShop.view.yaml
--rw-r--r--   0        0        0     2778 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/assets/PriceAreaAFRR.view.yaml
--rw-r--r--   0        0        0     1320 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/assets/PriceAreaAsset.view.yaml
--rw-r--r--   0        0        0      951 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/assets/PriceAreaDayAhead.view.yaml
--rw-r--r--   0        0        0      380 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/assets/Reservoir.view.yaml
--rw-r--r--   0        0        0     1253 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/assets/TurbineEfficiency.view.yaml
--rw-r--r--   0        0        0     1571 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/assets/Watercourse.view.yaml
--rw-r--r--   0        0        0      708 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/assets/WatercourseShop.view.yaml
--rw-r--r--   0        0        0      802 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/bid_configuration/1-interface.BidConfiguration.view.yaml
--rw-r--r--   0        0        0     2033 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/bid_configuration/BidConfigurationAFRR.view.yaml
--rw-r--r--   0        0        0     2475 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/bid_configuration/BidConfigurationSHOP.view.yaml
--rw-r--r--   0        0        0     2237 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/bid_configuration/BidConfigurationWater.view.yaml
--rw-r--r--   0        0        0     2226 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/bid_document/1-interface.BidDocument.view.yaml
--rw-r--r--   0        0        0     1441 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/bid_document/BidDocumentAFRR.view.yaml
--rw-r--r--   0        0        0     1947 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/bid_document/BidDocumentDayAhead.view.yaml
--rw-r--r--   0        0        0     1867 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/bid_document/BidDocumentDayAheadSimple.view.yaml
--rw-r--r--   0        0        0     1624 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/bid_matrix/1-interface.BidMatrix.view.yaml
--rw-r--r--   0        0        0      731 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/bid_matrix/1-interface.BidMatrixRaw.view.yaml
--rw-r--r--   0        0        0     1063 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/bid_matrix/BasicBidMatrix.view.yaml
--rw-r--r--   0        0        0     1073 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/bid_matrix/BasicBidMatrixRaw.view.yaml
--rw-r--r--   0        0        0     1190 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/bid_matrix/CustomBidMatrix.view.yaml
--rw-r--r--   0        0        0     1532 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/bid_matrix/MultiScenarioMatrix.view.yaml
--rw-r--r--   0        0        0     1491 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/bid_matrix/MultiScenarioMatrixRaw.view.yaml
--rw-r--r--   0        0        0      576 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/bid_method/1.interface.BidMethod.view.yaml
--rw-r--r--   0        0        0      668 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/bid_method/1.interface.BidMethodDayAhead.view.yaml
--rw-r--r--   0        0        0      784 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/bid_method/BidMethodAFRR.view.yaml
--rw-r--r--   0        0        0      657 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/bid_method/BidMethodCustom.view.yaml
--rw-r--r--   0        0        0     1876 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/bid_method/BidMethodSHOPMultiScenario.view.yaml
--rw-r--r--   0        0        0      651 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/bid_method/BidMethodWaterValue.view.yaml
--rw-r--r--   0        0        0     2495 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/cogshop/Case.view.yaml
--rw-r--r--   0        0        0      806 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/cogshop/Commands.view.yaml
--rw-r--r--   0        0        0     1754 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/cogshop/Mapping.view.yaml
--rw-r--r--   0        0        0     2537 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/cogshop/ModelTemplate.view.yaml
--rw-r--r--   0        0        0     1906 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/cogshop/Scenario.view.yaml
--rw-r--r--   0        0        0     1379 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_inputs/1-interface.FunctionInput.view.yaml
--rw-r--r--   0        0        0     1051 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_inputs/BenchmarkingCollectorInput.view.yaml
--rw-r--r--   0        0        0     1436 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_inputs/PartialPostProcessingInput.view.yaml
--rw-r--r--   0        0        0     1477 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_inputs/PreprocessorInput.view.yaml
--rw-r--r--   0        0        0     2453 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_inputs/ShopPartialBidCalculationInput.view.yaml
--rw-r--r--   0        0        0     1276 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_inputs/ShopTriggerInput.view.yaml
--rw-r--r--   0        0        0     1067 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_inputs/TaskDispatcherBenchmarkingInput.view.yaml
--rw-r--r--   0        0        0     1667 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_inputs/TaskDispatcherShopInput.view.yaml
--rw-r--r--   0        0        0     1253 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_inputs/TaskDispatcherWaterInput.view.yaml
--rw-r--r--   0        0        0     1146 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_inputs/TotalBidMatrixCalculationInput.view.yaml
--rw-r--r--   0        0        0     1070 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_inputs/WaterPartialBidCalculationInput.view.yaml
--rw-r--r--   0        0        0     1730 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_outputs/1-interface.FunctionOutput.view.yaml
--rw-r--r--   0        0        0     1043 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_outputs/BenchmarkingCollectorOutput.view.yaml
--rw-r--r--   0        0        0     1478 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_outputs/PartiallPostProcessingOutput.view.yaml
--rw-r--r--   0        0        0     1415 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_outputs/PreprocessorOutput.view.yaml
--rw-r--r--   0        0        0     1364 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_outputs/SHOPTriggerOutput.view.yaml
--rw-r--r--   0        0        0     1520 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_outputs/ShopPartialBidCalculationOutput.view.yaml
--rw-r--r--   0        0        0     1072 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_outputs/TaskDispatcherBenchmarkingOutput.view.yaml
--rw-r--r--   0        0        0     1968 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_outputs/TaskDispatcherShopOutput.view.yaml
--rw-r--r--   0        0        0     1497 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_outputs/TaskDispatcherWaterOutput.view.yaml
--rw-r--r--   0        0        0     1418 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_outputs/TotalBidMatrixCalculationOutput.view.yaml
--rw-r--r--   0        0        0     1387 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_outputs/WaterPartialBidCalculationOutput.view.yaml
--rw-r--r--   0        0        0     2779 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/shop_result/1-interface.SHOPResult.view.yaml
--rw-r--r--   0        0        0     1354 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/shop_result/PriceProdCase.view.yaml
--rw-r--r--   0        0        0     1676 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/shop_result/SHOPResultPriceProd.view.yaml
--rw-r--r--   0        0        0     1576 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/shop_result/SHOPTimeSeries.view.yaml
--rw-r--r--   0        0        0     1107 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/shop_result/ShopObjectiveValue.view.yaml
--rw-r--r--   0        0        0        0 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/prerun_transformations/__init__.py
--rw-r--r--   0        0        0    34098 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/prerun_transformations/transformations.py
--rw-r--r--   0        0        0      278 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/resync/__init__.py
--rw-r--r--   0        0        0     1792 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/resync/config/__init__.py
--rw-r--r--   0        0        0    12764 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/resync/config/_main.py
--rw-r--r--   0        0        0      595 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/resync/config/_settings.py
--rw-r--r--   0        0        0    12408 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/resync/config/_shared.py
--rw-r--r--   0        0        0       75 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/resync/config/cogshop/__init__.py
--rw-r--r--   0        0        0      720 2024-03-21 07:08:04.182300 cognite_power_ops-0.91.3/cognite/powerops/resync/config/cogshop/shop_file_config.py
--rw-r--r--   0        0        0      815 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/resync/config/market/__init__.py
--rw-r--r--   0        0        0     5486 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/resync/config/market/_core.py
--rw-r--r--   0        0        0     2106 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/resync/config/market/benchmarking.py
--rw-r--r--   0        0        0     2105 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/resync/config/market/dayahead.py
--rw-r--r--   0        0        0      961 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/resync/config/market/market.py
--rw-r--r--   0        0        0    10827 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/resync/config/market/rkom.py
--rw-r--r--   0        0        0      417 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/resync/config/production/__init__.py
--rw-r--r--   0        0        0      830 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/resync/config/production/connections.py
--rw-r--r--   0        0        0     1414 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/resync/config/production/generator.py
--rw-r--r--   0        0        0     5675 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/resync/config/production/plant.py
--rw-r--r--   0        0        0     2868 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/resync/config/production/watercourse.py
--rw-r--r--   0        0        0      244 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/resync/core/__init__.py
--rw-r--r--   0        0        0     9185 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/resync/core/cdf.py
--rw-r--r--   0        0        0      442 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/resync/core/echo.py
--rw-r--r--   0        0        0    27692 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/resync/core/main.py
--rw-r--r--   0        0        0     2563 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/resync/core/transform.py
--rw-r--r--   0        0        0     2161 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/resync/core/validation.py
--rw-r--r--   0        0        0      474 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/resync/diff/__init__.py
--rw-r--r--   0        0        0     6774 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/resync/diff/core.py
--rw-r--r--   0        0        0    12631 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/resync/diff/data_classes.py
--rw-r--r--   0        0        0      424 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/resync/models/__init__.py
--rw-r--r--   0        0        0       82 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/resync/models/_shared_v1_v2/__init__.py
--rw-r--r--   0        0        0      811 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/resync/models/_shared_v1_v2/_to_instances.py
--rw-r--r--   0        0        0    10495 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/resync/models/_shared_v1_v2/cogshop_model.py
--rw-r--r--   0        0        0     1143 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/resync/models/_shared_v1_v2/market_model.py
--rw-r--r--   0        0        0     8337 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/resync/models/_shared_v1_v2/production_model.py
--rw-r--r--   0        0        0      788 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/resync/models/base/__init__.py
--rw-r--r--   0        0        0    11195 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/resync/models/base/asset_model.py
--rw-r--r--   0        0        0     9602 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/resync/models/base/asset_type.py
--rw-r--r--   0        0        0     5251 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/resync/models/base/cdf_resources.py
--rw-r--r--   0        0        0    11641 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/resync/models/base/data_model.py
--rw-r--r--   0        0        0     1896 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/resync/models/base/dms_models.py
--rw-r--r--   0        0        0      592 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/resync/models/base/graph_ql.py
--rw-r--r--   0        0        0      148 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/resync/models/base/helpers.py
--rw-r--r--   0        0        0     6668 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/resync/models/base/model.py
--rw-r--r--   0        0        0     1705 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/resync/models/base/resource_type.py
--rw-r--r--   0        0        0     1302 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/resync/models/v1/__init__.py
--rw-r--r--   0        0        0     8141 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/resync/models/v1/cogshop.py
--rw-r--r--   0        0        0      243 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/resync/models/v1/config_to_model/__init__.py
--rw-r--r--   0        0        0     8940 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/resync/models/v1/config_to_model/to_cogshop_model.py
--rw-r--r--   0        0        0    15631 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/resync/models/v1/config_to_model/to_market_model.py
--rw-r--r--   0        0        0     9726 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/resync/models/v1/config_to_model/to_production_model.py
--rw-r--r--   0        0        0      491 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/resync/models/v1/graphql_schemas/__init__.py
--rw-r--r--   0        0        0      832 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/resync/models/v1/graphql_schemas/cogshop1.graphql
--rw-r--r--   0        0        0     3499 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/resync/models/v1/market/__init__.py
--rw-r--r--   0        0        0     1291 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/resync/models/v1/market/base.py
--rw-r--r--   0        0        0     3184 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/resync/models/v1/market/benchmark.py
--rw-r--r--   0        0        0     2405 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/resync/models/v1/market/dayahead.py
--rw-r--r--   0        0        0     2797 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/resync/models/v1/market/rkom.py
--rw-r--r--   0        0        0     7801 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/resync/models/v1/production.py
--rw-r--r--   0        0        0      395 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/resync/models/v2/__init__.py
--rw-r--r--   0        0        0        0 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/resync/models/v2/config_to_model/__init__.py
--rw-r--r--   0        0        0     8873 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/resync/models/v2/config_to_model/to_powerasset_model.py
--rw-r--r--   0        0        0    27181 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/resync/models/v2/powerops_models.py
--rw-r--r--   0        0        0     7631 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/resync/models/v2/production_dm.py
--rw-r--r--   0        0        0      150 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/resync/time_series_mapping/__init__.py
--rw-r--r--   0        0        0     2149 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/resync/time_series_mapping/mapping.py
--rw-r--r--   0        0        0     2456 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/resync/time_series_mapping/static_mapping.py
--rw-r--r--   0        0        0        0 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/resync/v2/__init__.py
--rw-r--r--   0        0        0      528 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/resync/v2/main.py
--rw-r--r--   0        0        0    17911 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/resync/v2/shop_to_assets.py
--rw-r--r--   0        0        0     4524 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/resync/validation.py
--rw-r--r--   0        0        0        0 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/utils/__init__.py
--rw-r--r--   0        0        0      160 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/utils/cdf/__init__.py
--rw-r--r--   0        0        0     1389 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/utils/cdf/_cdf_auth.py
--rw-r--r--   0        0        0     3169 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/utils/cdf/_settings.py
--rw-r--r--   0        0        0     7517 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/utils/cdf/calls.py
--rw-r--r--   0        0        0    10874 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/utils/cdf/extraction_pipelines.py
--rw-r--r--   0        0        0      970 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/utils/cdf/resource_creation.py
--rw-r--r--   0        0        0     2540 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/utils/lookup.py
--rw-r--r--   0        0        0     1097 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/utils/navigation.py
--rw-r--r--   0        0        0      514 2024-03-21 07:08:04.186300 cognite_power_ops-0.91.3/cognite/powerops/utils/require.py
--rw-r--r--   0        0        0      227 2024-03-21 07:08:04.190300 cognite_power_ops-0.91.3/cognite/powerops/utils/retry/__init__.py
--rw-r--r--   0        0        0     5619 2024-03-21 07:08:04.190300 cognite_power_ops-0.91.3/cognite/powerops/utils/retry/api.py
--rw-r--r--   0        0        0     7828 2024-03-21 07:08:04.190300 cognite_power_ops-0.91.3/cognite/powerops/utils/serialization.py
--rw-r--r--   0        0        0     3667 2024-03-21 07:08:04.190300 cognite_power_ops-0.91.3/cognite/powerops/utils/time.py
--rw-r--r--   0        0        0     3775 2024-03-21 07:08:04.190300 cognite_power_ops-0.91.3/pyproject.toml
--rw-r--r--   0        0        0     5516 1970-01-01 00:00:00.000000 cognite_power_ops-0.91.3/PKG-INFO
+-rw-r--r--   0        0        0    10758 2024-04-08 12:08:46.272464 cognite_power_ops-0.92.0/LICENSE
+-rw-r--r--   0        0        0     3322 2024-04-08 12:08:46.272464 cognite_power_ops-0.92.0/README.md
+-rw-r--r--   0        0        0      150 2024-04-08 12:08:46.272464 cognite_power_ops-0.92.0/cognite/powerops/__init__.py
+-rw-r--r--   0        0        0       23 2024-04-08 12:08:46.272464 cognite_power_ops-0.92.0/cognite/powerops/_version.py
+-rw-r--r--   0        0        0     5273 2024-04-08 12:08:46.272464 cognite_power_ops-0.92.0/cognite/powerops/cdf_labels.py
+-rw-r--r--   0        0        0     8779 2024-04-08 12:08:46.272464 cognite_power_ops-0.92.0/cognite/powerops/cli.py
+-rw-r--r--   0        0        0       74 2024-04-08 12:08:46.272464 cognite_power_ops-0.92.0/cognite/powerops/client/__init__.py
+-rw-r--r--   0        0        0      105 2024-04-08 12:08:46.272464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/afrr_bid/__init__.py
+-rw-r--r--   0        0        0        0 2024-04-08 12:08:46.272464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/afrr_bid/_api/__init__.py
+-rw-r--r--   0        0        0    32021 2024-04-08 12:08:46.272464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/afrr_bid/_api/_core.py
+-rw-r--r--   0        0        0    26235 2024-04-08 12:08:46.272464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/afrr_bid/_api/alert.py
+-rw-r--r--   0        0        0     1492 2024-04-08 12:08:46.272464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/afrr_bid/_api/alert_query.py
+-rw-r--r--   0        0        0    27110 2024-04-08 12:08:46.272464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/afrr_bid/_api/bid_document.py
+-rw-r--r--   0        0        0     2041 2024-04-08 12:08:46.272464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/afrr_bid/_api/bid_document_alerts.py
+-rw-r--r--   0        0        0     2039 2024-04-08 12:08:46.272464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/afrr_bid/_api/bid_document_bids.py
+-rw-r--r--   0        0        0    11759 2024-04-08 12:08:46.272464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/afrr_bid/_api/bid_document_query.py
+-rw-r--r--   0        0        0    16844 2024-04-08 12:08:46.272464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/afrr_bid/_api/bid_method.py
+-rw-r--r--   0        0        0     1513 2024-04-08 12:08:46.272464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/afrr_bid/_api/bid_method_query.py
+-rw-r--r--   0        0        0    27874 2024-04-08 12:08:46.272464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/afrr_bid/_api/bid_row.py
+-rw-r--r--   0        0        0     1971 2024-04-08 12:08:46.272464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/afrr_bid/_api/bid_row_alerts.py
+-rw-r--r--   0        0        0     8302 2024-04-08 12:08:46.272464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/afrr_bid/_api/bid_row_query.py
+-rw-r--r--   0        0        0    16131 2024-04-08 12:08:46.272464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/afrr_bid/_api/price_area.py
+-rw-r--r--   0        0        0    25401 2024-04-08 12:08:46.276464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/afrr_bid/_api/price_area_activation_price_down.py
+-rw-r--r--   0        0        0    25313 2024-04-08 12:08:46.276464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/afrr_bid/_api/price_area_activation_price_up.py
+-rw-r--r--   0        0        0    25313 2024-04-08 12:08:46.276464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/afrr_bid/_api/price_area_capacity_price_down.py
+-rw-r--r--   0        0        0    25195 2024-04-08 12:08:46.276464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/afrr_bid/_api/price_area_capacity_price_up.py
+-rw-r--r--   0        0        0    25692 2024-04-08 12:08:46.276464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/afrr_bid/_api/price_area_own_capacity_allocation_down.py
+-rw-r--r--   0        0        0    25604 2024-04-08 12:08:46.276464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/afrr_bid/_api/price_area_own_capacity_allocation_up.py
+-rw-r--r--   0        0        0     1513 2024-04-08 12:08:46.276464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/afrr_bid/_api/price_area_query.py
+-rw-r--r--   0        0        0    25330 2024-04-08 12:08:46.276464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/afrr_bid/_api/price_area_relative_activation.py
+-rw-r--r--   0        0        0    25780 2024-04-08 12:08:46.276464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/afrr_bid/_api/price_area_total_capacity_allocation_down.py
+-rw-r--r--   0        0        0    25692 2024-04-08 12:08:46.276464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/afrr_bid/_api/price_area_total_capacity_allocation_up.py
+-rw-r--r--   0        0        0     9211 2024-04-08 12:08:46.276464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/afrr_bid/_api_client.py
+-rw-r--r--   0        0        0     2624 2024-04-08 12:08:46.276464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/afrr_bid/data_classes/__init__.py
+-rw-r--r--   0        0        0    16179 2024-04-08 12:08:46.276464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/afrr_bid/data_classes/_alert.py
+-rw-r--r--   0        0        0    19431 2024-04-08 12:08:46.276464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/afrr_bid/data_classes/_bid_document.py
+-rw-r--r--   0        0        0     8148 2024-04-08 12:08:46.276464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/afrr_bid/data_classes/_bid_method.py
+-rw-r--r--   0        0        0    23764 2024-04-08 12:08:46.276464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/afrr_bid/data_classes/_bid_row.py
+-rw-r--r--   0        0        0    22140 2024-04-08 12:08:46.276464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/afrr_bid/data_classes/_core.py
+-rw-r--r--   0        0        0     9006 2024-04-08 12:08:46.276464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/afrr_bid/data_classes/_price_area.py
+-rw-r--r--   0        0        0      109 2024-04-08 12:08:46.276464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/__init__.py
+-rw-r--r--   0        0        0        0 2024-04-08 12:08:46.276464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/__init__.py
+-rw-r--r--   0        0        0    32236 2024-04-08 12:08:46.276464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/_core.py
+-rw-r--r--   0        0        0    16860 2024-04-08 12:08:46.276464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/bid_method.py
+-rw-r--r--   0        0        0     1511 2024-04-08 12:08:46.276464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/bid_method_query.py
+-rw-r--r--   0        0        0    26226 2024-04-08 12:08:46.276464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/generator.py
+-rw-r--r--   0        0        0    15705 2024-04-08 12:08:46.276464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/generator_efficiency_curve.py
+-rw-r--r--   0        0        0     1625 2024-04-08 12:08:46.276464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/generator_efficiency_curve_query.py
+-rw-r--r--   0        0        0    28005 2024-04-08 12:08:46.276464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/generator_is_available_time_series.py
+-rw-r--r--   0        0        0     5828 2024-04-08 12:08:46.276464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/generator_query.py
+-rw-r--r--   0        0        0    27596 2024-04-08 12:08:46.276464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/generator_start_stop_cost.py
+-rw-r--r--   0        0        0     2197 2024-04-08 12:08:46.276464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/generator_turbine_curves.py
+-rw-r--r--   0        0        0    32967 2024-04-08 12:08:46.276464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/plant.py
+-rw-r--r--   0        0        0    30394 2024-04-08 12:08:46.276464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/plant_feeding_fee_time_series.py
+-rw-r--r--   0        0        0     1998 2024-04-08 12:08:46.276464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/plant_generators.py
+-rw-r--r--   0        0        0    30394 2024-04-08 12:08:46.276464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/plant_head_direct_time_series.py
+-rw-r--r--   0        0        0    30394 2024-04-08 12:08:46.276464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/plant_inlet_level_time_series.py
+-rw-r--r--   0        0        0    30438 2024-04-08 12:08:46.276464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/plant_outlet_level_time_series.py
+-rw-r--r--   0        0        0    30100 2024-04-08 12:08:46.276464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/plant_p_max_time_series.py
+-rw-r--r--   0        0        0    30100 2024-04-08 12:08:46.276464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/plant_p_min_time_series.py
+-rw-r--r--   0        0        0     8084 2024-04-08 12:08:46.276464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/plant_query.py
+-rw-r--r--   0        0        0    30394 2024-04-08 12:08:46.276464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/plant_water_value_time_series.py
+-rw-r--r--   0        0        0    27060 2024-04-08 12:08:46.276464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/price_area.py
+-rw-r--r--   0        0        0    27668 2024-04-08 12:08:46.276464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/price_area_activation_price_down.py
+-rw-r--r--   0        0        0    27580 2024-04-08 12:08:46.276464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/price_area_activation_price_up.py
+-rw-r--r--   0        0        0    27580 2024-04-08 12:08:46.276464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/price_area_capacity_price_down.py
+-rw-r--r--   0        0        0    27462 2024-04-08 12:08:46.276464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/price_area_capacity_price_up.py
+-rw-r--r--   0        0        0    27374 2024-04-08 12:08:46.276464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/price_area_day_ahead_price.py
+-rw-r--r--   0        0        0    27739 2024-04-08 12:08:46.276464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/price_area_main_scenario_day_ahead.py
+-rw-r--r--   0        0        0    27959 2024-04-08 12:08:46.280464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/price_area_own_capacity_allocation_down.py
+-rw-r--r--   0        0        0    27871 2024-04-08 12:08:46.280464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/price_area_own_capacity_allocation_up.py
+-rw-r--r--   0        0        0     2011 2024-04-08 12:08:46.280464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/price_area_plants.py
+-rw-r--r--   0        0        0    11709 2024-04-08 12:08:46.280464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/price_area_query.py
+-rw-r--r--   0        0        0    27597 2024-04-08 12:08:46.280464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/price_area_relative_activation.py
+-rw-r--r--   0        0        0    28047 2024-04-08 12:08:46.280464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/price_area_total_capacity_allocation_down.py
+-rw-r--r--   0        0        0    27959 2024-04-08 12:08:46.280464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/price_area_total_capacity_allocation_up.py
+-rw-r--r--   0        0        0     2095 2024-04-08 12:08:46.280464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/price_area_watercourses.py
+-rw-r--r--   0        0        0    20054 2024-04-08 12:08:46.280464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/reservoir.py
+-rw-r--r--   0        0        0     1510 2024-04-08 12:08:46.280464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/reservoir_query.py
+-rw-r--r--   0        0        0    15561 2024-04-08 12:08:46.280464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/turbine_efficiency_curve.py
+-rw-r--r--   0        0        0     1615 2024-04-08 12:08:46.280464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/turbine_efficiency_curve_query.py
+-rw-r--r--   0        0        0    22125 2024-04-08 12:08:46.280464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/watercourse.py
+-rw-r--r--   0        0        0     2026 2024-04-08 12:08:46.280464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/watercourse_plants.py
+-rw-r--r--   0        0        0    26508 2024-04-08 12:08:46.280464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/watercourse_production_obligation.py
+-rw-r--r--   0        0        0     6635 2024-04-08 12:08:46.280464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/watercourse_query.py
+-rw-r--r--   0        0        0    10150 2024-04-08 12:08:46.280464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api_client.py
+-rw-r--r--   0        0        0     4780 2024-04-08 12:08:46.280464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/data_classes/__init__.py
+-rw-r--r--   0        0        0     8028 2024-04-08 12:08:46.280464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/data_classes/_bid_method.py
+-rw-r--r--   0        0        0    22140 2024-04-08 12:08:46.280464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/data_classes/_core.py
+-rw-r--r--   0        0        0    19886 2024-04-08 12:08:46.280464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/data_classes/_generator.py
+-rw-r--r--   0        0        0     9775 2024-04-08 12:08:46.280464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/data_classes/_generator_efficiency_curve.py
+-rw-r--r--   0        0        0    32081 2024-04-08 12:08:46.280464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/data_classes/_plant.py
+-rw-r--r--   0        0        0    33420 2024-04-08 12:08:46.280464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/data_classes/_price_area.py
+-rw-r--r--   0        0        0    10116 2024-04-08 12:08:46.280464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/data_classes/_reservoir.py
+-rw-r--r--   0        0        0     9645 2024-04-08 12:08:46.280464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/data_classes/_turbine_efficiency_curve.py
+-rw-r--r--   0        0        0    13593 2024-04-08 12:08:46.280464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/data_classes/_watercourse.py
+-rw-r--r--   0        0        0      113 2024-04-08 12:08:46.280464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/cogshop1/__init__.py
+-rw-r--r--   0        0        0        0 2024-04-08 12:08:46.280464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/cogshop1/_api/__init__.py
+-rw-r--r--   0        0        0     8201 2024-04-08 12:08:46.280464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/cogshop1/_api/_core.py
+-rw-r--r--   0        0        0    14521 2024-04-08 12:08:46.280464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/cogshop1/_api/case.py
+-rw-r--r--   0        0        0     7486 2024-04-08 12:08:46.280464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/cogshop1/_api/commands_config.py
+-rw-r--r--   0        0        0     9780 2024-04-08 12:08:46.280464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/cogshop1/_api/file_ref.py
+-rw-r--r--   0        0        0    15960 2024-04-08 12:08:46.280464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/cogshop1/_api/mapping.py
+-rw-r--r--   0        0        0    17869 2024-04-08 12:08:46.280464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/cogshop1/_api/model_template.py
+-rw-r--r--   0        0        0    11367 2024-04-08 12:08:46.280464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/cogshop1/_api/processing_log.py
+-rw-r--r--   0        0        0    19744 2024-04-08 12:08:46.280464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/cogshop1/_api/scenario.py
+-rw-r--r--   0        0        0    10871 2024-04-08 12:08:46.280464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/cogshop1/_api/transformation.py
+-rw-r--r--   0        0        0     3003 2024-04-08 12:08:46.280464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/cogshop1/_api_client.py
+-rw-r--r--   0        0        0     2524 2024-04-08 12:08:46.280464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/cogshop1/data_classes/__init__.py
+-rw-r--r--   0        0        0     4596 2024-04-08 12:08:46.280464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/cogshop1/data_classes/_case.py
+-rw-r--r--   0        0        0     2226 2024-04-08 12:08:46.280464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/cogshop1/data_classes/_commands_config.py
+-rw-r--r--   0        0        0     5824 2024-04-08 12:08:46.280464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/cogshop1/data_classes/_core.py
+-rw-r--r--   0        0        0     2393 2024-04-08 12:08:46.280464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/cogshop1/data_classes/_file_ref.py
+-rw-r--r--   0        0        0     4586 2024-04-08 12:08:46.280464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/cogshop1/data_classes/_mapping.py
+-rw-r--r--   0        0        0     5174 2024-04-08 12:08:46.280464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/cogshop1/data_classes/_model_template.py
+-rw-r--r--   0        0        0     2819 2024-04-08 12:08:46.280464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/cogshop1/data_classes/_processing_log.py
+-rw-r--r--   0        0        0     6951 2024-04-08 12:08:46.280464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/cogshop1/data_classes/_scenario.py
+-rw-r--r--   0        0        0     2621 2024-04-08 12:08:46.280464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/cogshop1/data_classes/_transformation.py
+-rw-r--r--   0        0        0      118 2024-04-08 12:08:46.280464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/__init__.py
+-rw-r--r--   0        0        0        0 2024-04-08 12:08:46.280464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/__init__.py
+-rw-r--r--   0        0        0    32484 2024-04-08 12:08:46.280464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/_core.py
+-rw-r--r--   0        0        0    26333 2024-04-08 12:08:46.280464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/alert.py
+-rw-r--r--   0        0        0     1497 2024-04-08 12:08:46.280464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/alert_query.py
+-rw-r--r--   0        0        0    23471 2024-04-08 12:08:46.280464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/basic_bid_matrix.py
+-rw-r--r--   0        0        0     2115 2024-04-08 12:08:46.280464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/basic_bid_matrix_alerts.py
+-rw-r--r--   0        0        0     7177 2024-04-08 12:08:46.280464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/basic_bid_matrix_query.py
+-rw-r--r--   0        0        0    29138 2024-04-08 12:08:46.280464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/bid_document.py
+-rw-r--r--   0        0        0     2059 2024-04-08 12:08:46.280464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/bid_document_alerts.py
+-rw-r--r--   0        0        0     2106 2024-04-08 12:08:46.280464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/bid_document_partials.py
+-rw-r--r--   0        0        0    13622 2024-04-08 12:08:46.280464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/bid_document_query.py
+-rw-r--r--   0        0        0    22831 2024-04-08 12:08:46.280464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/bid_matrix.py
+-rw-r--r--   0        0        0     2032 2024-04-08 12:08:46.280464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/bid_matrix_alerts.py
+-rw-r--r--   0        0        0     7128 2024-04-08 12:08:46.280464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/bid_matrix_query.py
+-rw-r--r--   0        0        0    16942 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/bid_method.py
+-rw-r--r--   0        0        0     1518 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/bid_method_query.py
+-rw-r--r--   0        0        0    24946 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/multi_scenario_matrix.py
+-rw-r--r--   0        0        0     2185 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/multi_scenario_matrix_alerts.py
+-rw-r--r--   0        0        0    10308 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/multi_scenario_matrix_query.py
+-rw-r--r--   0        0        0     2410 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/multi_scenario_matrix_scenario_results.py
+-rw-r--r--   0        0        0    19843 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/price_area.py
+-rw-r--r--   0        0        0    25798 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/price_area_main_scenario.py
+-rw-r--r--   0        0        0    25883 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/price_area_price_scenarios.py
+-rw-r--r--   0        0        0     2613 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/price_area_query.py
+-rw-r--r--   0        0        0    20327 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/shop_multi_scenario_method.py
+-rw-r--r--   0        0        0     2415 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/shop_multi_scenario_method_price_scenarios.py
+-rw-r--r--   0        0        0     4483 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/shop_multi_scenario_method_query.py
+-rw-r--r--   0        0        0    18030 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/shop_price_scenario.py
+-rw-r--r--   0        0        0    24799 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/shop_price_scenario_price.py
+-rw-r--r--   0        0        0     1559 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/shop_price_scenario_query.py
+-rw-r--r--   0        0        0    15855 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/shop_price_scenario_result.py
+-rw-r--r--   0        0        0    24935 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/shop_price_scenario_result_price.py
+-rw-r--r--   0        0        0    25177 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/shop_price_scenario_result_production.py
+-rw-r--r--   0        0        0     2777 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/shop_price_scenario_result_query.py
+-rw-r--r--   0        0        0    18424 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/water_value_based_method.py
+-rw-r--r--   0        0        0     1618 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/water_value_based_method_query.py
+-rw-r--r--   0        0        0    11242 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api_client.py
+-rw-r--r--   0        0        0     7367 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/data_classes/__init__.py
+-rw-r--r--   0        0        0    16179 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/data_classes/_alert.py
+-rw-r--r--   0        0        0    14603 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/data_classes/_basic_bid_matrix.py
+-rw-r--r--   0        0        0    24068 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/data_classes/_bid_document.py
+-rw-r--r--   0        0        0    14961 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/data_classes/_bid_matrix.py
+-rw-r--r--   0        0        0     8042 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/data_classes/_bid_method.py
+-rw-r--r--   0        0        0    22140 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/data_classes/_core.py
+-rw-r--r--   0        0        0    17009 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/data_classes/_multi_scenario_matrix.py
+-rw-r--r--   0        0        0    14474 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/data_classes/_price_area.py
+-rw-r--r--   0        0        0    11922 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/data_classes/_shop_multi_scenario_method.py
+-rw-r--r--   0        0        0     9352 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/data_classes/_shop_price_scenario.py
+-rw-r--r--   0        0        0    13240 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/data_classes/_shop_price_scenario_result.py
+-rw-r--r--   0        0        0     8909 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/data_classes/_water_value_based_method.py
+-rw-r--r--   0        0        0      123 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/__init__.py
+-rw-r--r--   0        0        0        0 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/__init__.py
+-rw-r--r--   0        0        0    39603 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/_core.py
+-rw-r--r--   0        0        0    27908 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/alert.py
+-rw-r--r--   0        0        0     1486 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/alert_query.py
+-rw-r--r--   0        0        0    24174 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/basic_bid_matrix.py
+-rw-r--r--   0        0        0     2111 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/basic_bid_matrix_alerts.py
+-rw-r--r--   0        0        0     4272 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/basic_bid_matrix_query.py
+-rw-r--r--   0        0        0    17147 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_calculation_task.py
+-rw-r--r--   0        0        0     3516 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_calculation_task_query.py
+-rw-r--r--   0        0        0    21579 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_configuration.py
+-rw-r--r--   0        0        0     2311 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_configuration_partials.py
+-rw-r--r--   0        0        0     7519 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_configuration_query.py
+-rw-r--r--   0        0        0    23945 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_configuration_shop.py
+-rw-r--r--   0        0        0     2278 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_configuration_shop_plants_shop.py
+-rw-r--r--   0        0        0     9424 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_configuration_shop_query.py
+-rw-r--r--   0        0        0     2368 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_configuration_shop_watercourses_shop.py
+-rw-r--r--   0        0        0    12634 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_configuration_water.py
+-rw-r--r--   0        0        0     2215 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_configuration_water_plants.py
+-rw-r--r--   0        0        0     9300 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_configuration_water_query.py
+-rw-r--r--   0        0        0     2305 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_configuration_water_watercourses.py
+-rw-r--r--   0        0        0    27089 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_document.py
+-rw-r--r--   0        0        0    29386 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_document_afrr.py
+-rw-r--r--   0        0        0     2129 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_document_afrr_alerts.py
+-rw-r--r--   0        0        0     2127 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_document_afrr_bids.py
+-rw-r--r--   0        0        0    11574 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_document_afrr_query.py
+-rw-r--r--   0        0        0     2060 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_document_alerts.py
+-rw-r--r--   0        0        0    31049 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_document_day_ahead.py
+-rw-r--r--   0        0        0     2198 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_document_day_ahead_alerts.py
+-rw-r--r--   0        0        0     2245 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_document_day_ahead_partials.py
+-rw-r--r--   0        0        0    12425 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_document_day_ahead_query.py
+-rw-r--r--   0        0        0     6172 2024-04-08 12:08:46.284464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_document_query.py
+-rw-r--r--   0        0        0    20879 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_matrix.py
+-rw-r--r--   0        0        0     2033 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_matrix_alerts.py
+-rw-r--r--   0        0        0     8924 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_matrix_query.py
+-rw-r--r--   0        0        0    22970 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_matrix_raw.py
+-rw-r--r--   0        0        0     2082 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_matrix_raw_alerts.py
+-rw-r--r--   0        0        0     3035 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_matrix_raw_query.py
+-rw-r--r--   0        0        0    16928 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_method.py
+-rw-r--r--   0        0        0    17429 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_method_afrr.py
+-rw-r--r--   0        0        0     1528 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_method_afrr_query.py
+-rw-r--r--   0        0        0    17665 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_method_custom.py
+-rw-r--r--   0        0        0     1538 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_method_custom_query.py
+-rw-r--r--   0        0        0    17944 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_method_day_ahead.py
+-rw-r--r--   0        0        0     1549 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_method_day_ahead_query.py
+-rw-r--r--   0        0        0     1507 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_method_query.py
+-rw-r--r--   0        0        0    27374 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_method_shop_multi_scenario.py
+-rw-r--r--   0        0        0     2389 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_method_shop_multi_scenario_price_scenarios.py
+-rw-r--r--   0        0        0     3208 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_method_shop_multi_scenario_query.py
+-rw-r--r--   0        0        0     2357 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_method_shop_multi_scenario_scenarios.py
+-rw-r--r--   0        0        0    18166 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_method_water_value.py
+-rw-r--r--   0        0        0     1559 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_method_water_value_query.py
+-rw-r--r--   0        0        0    25006 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_row.py
+-rw-r--r--   0        0        0     1990 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_row_alerts.py
+-rw-r--r--   0        0        0     8643 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_row_query.py
+-rw-r--r--   0        0        0    16543 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/case.py
+-rw-r--r--   0        0        0     2515 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/case_query.py
+-rw-r--r--   0        0        0    16713 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/commands.py
+-rw-r--r--   0        0        0     1500 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/commands_query.py
+-rw-r--r--   0        0        0    24304 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/custom_bid_matrix.py
+-rw-r--r--   0        0        0     2125 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/custom_bid_matrix_alerts.py
+-rw-r--r--   0        0        0     4275 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/custom_bid_matrix_query.py
+-rw-r--r--   0        0        0    22960 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/function_input.py
+-rw-r--r--   0        0        0     1527 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/function_input_query.py
+-rw-r--r--   0        0        0    24555 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/function_output.py
+-rw-r--r--   0        0        0     2102 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/function_output_alerts.py
+-rw-r--r--   0        0        0     6190 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/function_output_query.py
+-rw-r--r--   0        0        0    30519 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/generator.py
+-rw-r--r--   0        0        0    29640 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/generator_availability_time_series.py
+-rw-r--r--   0        0        0    15759 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/generator_efficiency_curve.py
+-rw-r--r--   0        0        0     1621 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/generator_efficiency_curve_query.py
+-rw-r--r--   0        0        0    28667 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/generator_is_available_time_series.py
+-rw-r--r--   0        0        0     6000 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/generator_query.py
+-rw-r--r--   0        0        0    29214 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/generator_start_stop_cost.py
+-rw-r--r--   0        0        0     2209 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/generator_turbine_curves.py
+-rw-r--r--   0        0        0     2279 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/generator_turbine_efficiency_curves.py
+-rw-r--r--   0        0        0    20120 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/mapping.py
+-rw-r--r--   0        0        0     1496 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/mapping_query.py
+-rw-r--r--   0        0        0    25885 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/mapping_timeseries.py
+-rw-r--r--   0        0        0    31011 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/market_configuration.py
+-rw-r--r--   0        0        0     1557 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/market_configuration_query.py
+-rw-r--r--   0        0        0    23916 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/model_template.py
+-rw-r--r--   0        0        0     2155 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/model_template_base_mappings.py
+-rw-r--r--   0        0        0     4819 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/model_template_query.py
+-rw-r--r--   0        0        0    25665 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/multi_scenario_matrix.py
+-rw-r--r--   0        0        0     2181 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/multi_scenario_matrix_alerts.py
+-rw-r--r--   0        0        0     6184 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/multi_scenario_matrix_query.py
+-rw-r--r--   0        0        0    26051 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/multi_scenario_matrix_raw.py
+-rw-r--r--   0        0        0     2235 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/multi_scenario_matrix_raw_alerts.py
+-rw-r--r--   0        0        0     6292 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/multi_scenario_matrix_raw_query.py
+-rw-r--r--   0        0        0     2421 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/multi_scenario_matrix_raw_shop_results.py
+-rw-r--r--   0        0        0     2339 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/multi_scenario_matrix_scenario_results.py
+-rw-r--r--   0        0        0     2279 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/multi_scenario_matrix_shop_results.py
+-rw-r--r--   0        0        0    20680 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/partial_bid_configuration.py
+-rw-r--r--   0        0        0     1616 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/partial_bid_configuration_query.py
+-rw-r--r--   0        0        0    28470 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/partial_bid_matrix_calculation_input.py
+-rw-r--r--   0        0        0     2898 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/partial_bid_matrix_calculation_input_query.py
+-rw-r--r--   0        0        0    30528 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/partial_bid_matrix_calculation_output.py
+-rw-r--r--   0        0        0     2407 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/partial_bid_matrix_calculation_output_alerts.py
+-rw-r--r--   0        0        0    10594 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/partial_bid_matrix_calculation_output_query.py
+-rw-r--r--   0        0        0    27485 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/partial_post_processing_input.py
+-rw-r--r--   0        0        0     2477 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/partial_post_processing_input_partial_bid_matrices_raw.py
+-rw-r--r--   0        0        0     4659 2024-04-08 12:08:46.288464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/partial_post_processing_input_query.py
+-rw-r--r--   0        0        0    28130 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/partial_post_processing_output.py
+-rw-r--r--   0        0        0     2305 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/partial_post_processing_output_alerts.py
+-rw-r--r--   0        0        0     2398 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/partial_post_processing_output_partial_matrices.py
+-rw-r--r--   0        0        0     6251 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/partial_post_processing_output_query.py
+-rw-r--r--   0        0        0    33312 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/plant.py
+-rw-r--r--   0        0        0    30645 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/plant_feeding_fee_time_series.py
+-rw-r--r--   0        0        0     2015 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/plant_generators.py
+-rw-r--r--   0        0        0    30645 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/plant_head_direct_time_series.py
+-rw-r--r--   0        0        0    30645 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/plant_inlet_level_time_series.py
+-rw-r--r--   0        0        0    30689 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/plant_outlet_level_time_series.py
+-rw-r--r--   0        0        0    30180 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/plant_p_max_time_series.py
+-rw-r--r--   0        0        0    30180 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/plant_p_min_time_series.py
+-rw-r--r--   0        0        0    30777 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/plant_production_max_time_series.py
+-rw-r--r--   0        0        0    30777 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/plant_production_min_time_series.py
+-rw-r--r--   0        0        0     6243 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/plant_query.py
+-rw-r--r--   0        0        0    20179 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/plant_shop.py
+-rw-r--r--   0        0        0     1507 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/plant_shop_query.py
+-rw-r--r--   0        0        0    30645 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/plant_water_value_time_series.py
+-rw-r--r--   0        0        0    21879 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/power_asset.py
+-rw-r--r--   0        0        0     1512 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/power_asset_query.py
+-rw-r--r--   0        0        0    28054 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/preprocessor_input.py
+-rw-r--r--   0        0        0     2608 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/preprocessor_input_query.py
+-rw-r--r--   0        0        0    26880 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/preprocessor_output.py
+-rw-r--r--   0        0        0     2158 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/preprocessor_output_alerts.py
+-rw-r--r--   0        0        0     8658 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/preprocessor_output_query.py
+-rw-r--r--   0        0        0    23269 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/price_area.py
+-rw-r--r--   0        0        0    25429 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/price_area_afrr.py
+-rw-r--r--   0        0        0    27965 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/price_area_afrr_activation_price_down.py
+-rw-r--r--   0        0        0    27877 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/price_area_afrr_activation_price_up.py
+-rw-r--r--   0        0        0    27877 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/price_area_afrr_capacity_price_down.py
+-rw-r--r--   0        0        0    27759 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/price_area_afrr_capacity_price_up.py
+-rw-r--r--   0        0        0    28256 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/price_area_afrr_own_capacity_allocation_down.py
+-rw-r--r--   0        0        0    28168 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/price_area_afrr_own_capacity_allocation_up.py
+-rw-r--r--   0        0        0     1528 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/price_area_afrr_query.py
+-rw-r--r--   0        0        0    27894 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/price_area_afrr_relative_activation.py
+-rw-r--r--   0        0        0    28344 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/price_area_afrr_total_capacity_allocation_down.py
+-rw-r--r--   0        0        0    28256 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/price_area_afrr_total_capacity_allocation_up.py
+-rw-r--r--   0        0        0    21294 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/price_area_asset.py
+-rw-r--r--   0        0        0     2103 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/price_area_asset_plants.py
+-rw-r--r--   0        0        0     4606 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/price_area_asset_query.py
+-rw-r--r--   0        0        0     2193 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/price_area_asset_watercourses.py
+-rw-r--r--   0        0        0    21052 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/price_area_day_ahead.py
+-rw-r--r--   0        0        0     2711 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/price_area_day_ahead_query.py
+-rw-r--r--   0        0        0     1507 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/price_area_query.py
+-rw-r--r--   0        0        0    14625 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/price_prod_case.py
+-rw-r--r--   0        0        0    24517 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/price_prod_case_price.py
+-rw-r--r--   0        0        0    24759 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/price_prod_case_production.py
+-rw-r--r--   0        0        0     2490 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/price_prod_case_query.py
+-rw-r--r--   0        0        0    14906 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/price_production.py
+-rw-r--r--   0        0        0    24609 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/price_production_price.py
+-rw-r--r--   0        0        0    24851 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/price_production_production.py
+-rw-r--r--   0        0        0     2623 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/price_production_query.py
+-rw-r--r--   0        0        0    17508 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/price_scenario.py
+-rw-r--r--   0        0        0     1527 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/price_scenario_query.py
+-rw-r--r--   0        0        0    24894 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/price_scenario_timeseries.py
+-rw-r--r--   0        0        0    20122 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/reservoir.py
+-rw-r--r--   0        0        0     1506 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/reservoir_query.py
+-rw-r--r--   0        0        0    21829 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/scenario.py
+-rw-r--r--   0        0        0     2081 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/scenario_mappings_override.py
+-rw-r--r--   0        0        0     7322 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/scenario_query.py
+-rw-r--r--   0        0        0    33208 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/scenario_raw.py
+-rw-r--r--   0        0        0     2131 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/scenario_raw_mappings_override.py
+-rw-r--r--   0        0        0     4385 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/scenario_raw_query.py
+-rw-r--r--   0        0        0    25372 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/scenario_set.py
+-rw-r--r--   0        0        0     4823 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/scenario_set_query.py
+-rw-r--r--   0        0        0     2136 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/scenario_set_shop_scenarios.py
+-rw-r--r--   0        0        0    24190 2024-04-08 12:08:46.292464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/shop_based_partial_bid_configuration.py
+-rw-r--r--   0        0        0     3881 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/shop_based_partial_bid_configuration_query.py
+-rw-r--r--   0        0        0    31589 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/shop_partial_bid_calculation_input.py
+-rw-r--r--   0        0        0     2360 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/shop_partial_bid_calculation_input_alerts.py
+-rw-r--r--   0        0        0     6071 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/shop_partial_bid_calculation_input_query.py
+-rw-r--r--   0        0        0     2596 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/shop_partial_bid_calculation_input_shop_result_price_prod.py
+-rw-r--r--   0        0        0     2437 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/shop_partial_bid_calculation_input_shop_results.py
+-rw-r--r--   0        0        0    28954 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/shop_partial_bid_calculation_output.py
+-rw-r--r--   0        0        0     2374 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/shop_partial_bid_calculation_output_alerts.py
+-rw-r--r--   0        0        0     5850 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/shop_partial_bid_calculation_output_query.py
+-rw-r--r--   0        0        0    32251 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/shop_partial_bid_matrix_calculation_input.py
+-rw-r--r--   0        0        0     2612 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/shop_partial_bid_matrix_calculation_input_price_production.py
+-rw-r--r--   0        0        0     7626 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/shop_partial_bid_matrix_calculation_input_query.py
+-rw-r--r--   0        0        0    16401 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/shop_result.py
+-rw-r--r--   0        0        0     2046 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/shop_result_alerts.py
+-rw-r--r--   0        0        0     2205 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/shop_result_output_timeseries.py
+-rw-r--r--   0        0        0    24818 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/shop_result_price.py
+-rw-r--r--   0        0        0    18577 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/shop_result_price_prod.py
+-rw-r--r--   0        0        0     2193 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/shop_result_price_prod_alerts.py
+-rw-r--r--   0        0        0    25755 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/shop_result_price_prod_output_timeseries.py
+-rw-r--r--   0        0        0     2389 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/shop_result_price_prod_production_timeseries.py
+-rw-r--r--   0        0        0     7683 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/shop_result_price_prod_query.py
+-rw-r--r--   0        0        0    25060 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/shop_result_production.py
+-rw-r--r--   0        0        0    11003 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/shop_result_query.py
+-rw-r--r--   0        0        0    21373 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/shop_time_series.py
+-rw-r--r--   0        0        0     1533 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/shop_time_series_query.py
+-rw-r--r--   0        0        0    26305 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/shop_time_series_timeseries.py
+-rw-r--r--   0        0        0    27179 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/shop_trigger_input.py
+-rw-r--r--   0        0        0     3678 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/shop_trigger_input_query.py
+-rw-r--r--   0        0        0    26978 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/shop_trigger_output.py
+-rw-r--r--   0        0        0     2157 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/shop_trigger_output_alerts.py
+-rw-r--r--   0        0        0     8757 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/shop_trigger_output_query.py
+-rw-r--r--   0        0        0    26617 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/task_dispatcher_input.py
+-rw-r--r--   0        0        0     2719 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/task_dispatcher_input_query.py
+-rw-r--r--   0        0        0    27156 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/task_dispatcher_output.py
+-rw-r--r--   0        0        0     2199 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/task_dispatcher_output_alerts.py
+-rw-r--r--   0        0        0     2334 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/task_dispatcher_output_process_sub_tasks.py
+-rw-r--r--   0        0        0    11543 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/task_dispatcher_output_query.py
+-rw-r--r--   0        0        0    30710 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/task_dispatcher_shop_input.py
+-rw-r--r--   0        0        0     2751 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/task_dispatcher_shop_input_query.py
+-rw-r--r--   0        0        0    28882 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/task_dispatcher_shop_output.py
+-rw-r--r--   0        0        0     2263 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/task_dispatcher_shop_output_alerts.py
+-rw-r--r--   0        0        0     2615 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/task_dispatcher_shop_output_partial_bid_calculations.py
+-rw-r--r--   0        0        0     2481 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/task_dispatcher_shop_output_preprocessor_calculations.py
+-rw-r--r--   0        0        0     8359 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/task_dispatcher_shop_output_query.py
+-rw-r--r--   0        0        0    27353 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/task_dispatcher_water_input.py
+-rw-r--r--   0        0        0     2761 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/task_dispatcher_water_input_query.py
+-rw-r--r--   0        0        0    27998 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/task_dispatcher_water_output.py
+-rw-r--r--   0        0        0     2277 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/task_dispatcher_water_output_alerts.py
+-rw-r--r--   0        0        0     2617 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/task_dispatcher_water_output_bid_calculation_tasks.py
+-rw-r--r--   0        0        0     6469 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/task_dispatcher_water_output_query.py
+-rw-r--r--   0        0        0    29841 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/total_bid_matrix_calculation_input.py
+-rw-r--r--   0        0        0     2481 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/total_bid_matrix_calculation_input_partial_bid_matrices.py
+-rw-r--r--   0        0        0     6340 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/total_bid_matrix_calculation_input_query.py
+-rw-r--r--   0        0        0    28930 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/total_bid_matrix_calculation_output.py
+-rw-r--r--   0        0        0     2379 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/total_bid_matrix_calculation_output_alerts.py
+-rw-r--r--   0        0        0     9057 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/total_bid_matrix_calculation_output_query.py
+-rw-r--r--   0        0        0    15615 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/turbine_efficiency_curve.py
+-rw-r--r--   0        0        0     1611 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/turbine_efficiency_curve_query.py
+-rw-r--r--   0        0        0    26497 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/water_partial_bid_calculation_input.py
+-rw-r--r--   0        0        0     2841 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/water_partial_bid_calculation_input_query.py
+-rw-r--r--   0        0        0    29268 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/water_partial_bid_calculation_output.py
+-rw-r--r--   0        0        0     2388 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/water_partial_bid_calculation_output_alerts.py
+-rw-r--r--   0        0        0     5933 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/water_partial_bid_calculation_output_query.py
+-rw-r--r--   0        0        0    23906 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/water_value_based_partial_bid_configuration.py
+-rw-r--r--   0        0        0     2768 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/water_value_based_partial_bid_configuration_query.py
+-rw-r--r--   0        0        0    32045 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/water_value_based_partial_bid_matrix_calculation_input.py
+-rw-r--r--   0        0        0     4452 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/water_value_based_partial_bid_matrix_calculation_input_query.py
+-rw-r--r--   0        0        0    23775 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/watercourse.py
+-rw-r--r--   0        0        0     2038 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/watercourse_plants.py
+-rw-r--r--   0        0        0    27164 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/watercourse_production_obligation.py
+-rw-r--r--   0        0        0     3021 2024-04-08 12:08:46.296464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/watercourse_query.py
+-rw-r--r--   0        0        0    20859 2024-04-08 12:08:46.300464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/watercourse_shop.py
+-rw-r--r--   0        0        0     1537 2024-04-08 12:08:46.300464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/watercourse_shop_query.py
+-rw-r--r--   0        0        0    38763 2024-04-08 12:08:46.300464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api_client.py
+-rw-r--r--   0        0        0    29040 2024-04-08 12:08:46.300464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/__init__.py
+-rw-r--r--   0        0        0    17412 2024-04-08 12:08:46.300464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_alert.py
+-rw-r--r--   0        0        0    15658 2024-04-08 12:08:46.300464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_basic_bid_matrix.py
+-rw-r--r--   0        0        0    14108 2024-04-08 12:08:46.300464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_bid_calculation_task.py
+-rw-r--r--   0        0        0    17019 2024-04-08 12:08:46.300464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_bid_configuration.py
+-rw-r--r--   0        0        0    21582 2024-04-08 12:08:46.300464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_bid_configuration_shop.py
+-rw-r--r--   0        0        0    19986 2024-04-08 12:08:46.300464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_bid_configuration_water.py
+-rw-r--r--   0        0        0    16649 2024-04-08 12:08:46.300464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_bid_document.py
+-rw-r--r--   0        0        0    20003 2024-04-08 12:08:46.300464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_bid_document_afrr.py
+-rw-r--r--   0        0        0    23271 2024-04-08 12:08:46.300464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_bid_document_day_ahead.py
+-rw-r--r--   0        0        0    16839 2024-04-08 12:08:46.300464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_bid_matrix.py
+-rw-r--r--   0        0        0    13022 2024-04-08 12:08:46.300464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_bid_matrix_raw.py
+-rw-r--r--   0        0        0     8032 2024-04-08 12:08:46.300464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_bid_method.py
+-rw-r--r--   0        0        0     8451 2024-04-08 12:08:46.300464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_bid_method_afrr.py
+-rw-r--r--   0        0        0     8400 2024-04-08 12:08:46.300464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_bid_method_custom.py
+-rw-r--r--   0        0        0     8491 2024-04-08 12:08:46.300464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_bid_method_day_ahead.py
+-rw-r--r--   0        0        0    16391 2024-04-08 12:08:46.300464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_bid_method_shop_multi_scenario.py
+-rw-r--r--   0        0        0     8809 2024-04-08 12:08:46.300464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_bid_method_water_value.py
+-rw-r--r--   0        0        0    21959 2024-04-08 12:08:46.300464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_bid_row.py
+-rw-r--r--   0        0        0    16041 2024-04-08 12:08:46.300464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_case.py
+-rw-r--r--   0        0        0     8628 2024-04-08 12:08:46.300464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_commands.py
+-rw-r--r--   0        0        0    22147 2024-04-08 12:08:46.300464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_core.py
+-rw-r--r--   0        0        0    15535 2024-04-08 12:08:46.300464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_custom_bid_matrix.py
+-rw-r--r--   0        0        0    12028 2024-04-08 12:08:46.300464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_function_input.py
+-rw-r--r--   0        0        0    13753 2024-04-08 12:08:46.300464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_function_output.py
+-rw-r--r--   0        0        0    23059 2024-04-08 12:08:46.300464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_generator.py
+-rw-r--r--   0        0        0     9803 2024-04-08 12:08:46.300464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_generator_efficiency_curve.py
+-rw-r--r--   0        0        0    12090 2024-04-08 12:08:46.300464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_mapping.py
+-rw-r--r--   0        0        0    16859 2024-04-08 12:08:46.300464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_market_configuration.py
+-rw-r--r--   0        0        0    15969 2024-04-08 12:08:46.300464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_model_template.py
+-rw-r--r--   0        0        0    18207 2024-04-08 12:08:46.300464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_multi_scenario_matrix.py
+-rw-r--r--   0        0        0    18186 2024-04-08 12:08:46.300464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_multi_scenario_matrix_raw.py
+-rw-r--r--   0        0        0    10633 2024-04-08 12:08:46.300464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_partial_bid_configuration.py
+-rw-r--r--   0        0        0    17356 2024-04-08 12:08:46.300464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_partial_bid_matrix_calculation_input.py
+-rw-r--r--   0        0        0    23277 2024-04-08 12:08:46.300464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_partial_bid_matrix_calculation_output.py
+-rw-r--r--   0        0        0    18751 2024-04-08 12:08:46.300464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_partial_post_processing_input.py
+-rw-r--r--   0        0        0    19241 2024-04-08 12:08:46.300464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_partial_post_processing_output.py
+-rw-r--r--   0        0        0    28315 2024-04-08 12:08:46.300464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_plant.py
+-rw-r--r--   0        0        0    10099 2024-04-08 12:08:46.300464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_plant_shop.py
+-rw-r--r--   0        0        0    11171 2024-04-08 12:08:46.300464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_power_asset.py
+-rw-r--r--   0        0        0    17069 2024-04-08 12:08:46.300464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_preprocessor_input.py
+-rw-r--r--   0        0        0    18191 2024-04-08 12:08:46.300464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_preprocessor_output.py
+-rw-r--r--   0        0        0    11837 2024-04-08 12:08:46.300464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_price_area.py
+-rw-r--r--   0        0        0    24344 2024-04-08 12:08:46.300464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_price_area_afrr.py
+-rw-r--r--   0        0        0    12748 2024-04-08 12:08:46.300464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_price_area_asset.py
+-rw-r--r--   0        0        0    10034 2024-04-08 12:08:46.300464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_price_area_day_ahead.py
+-rw-r--r--   0        0        0    11733 2024-04-08 12:08:46.300464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_price_prod_case.py
+-rw-r--r--   0        0        0    12347 2024-04-08 12:08:46.300464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_price_production.py
+-rw-r--r--   0        0        0     7104 2024-04-08 12:08:46.300464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_price_scenario.py
+-rw-r--r--   0        0        0    10082 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_reservoir.py
+-rw-r--r--   0        0        0    16805 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_scenario.py
+-rw-r--r--   0        0        0    19282 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_scenario_raw.py
+-rw-r--r--   0        0        0    15285 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_scenario_set.py
+-rw-r--r--   0        0        0    17124 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_shop_based_partial_bid_configuration.py
+-rw-r--r--   0        0        0    24177 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_shop_partial_bid_calculation_input.py
+-rw-r--r--   0        0        0    20739 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_shop_partial_bid_calculation_output.py
+-rw-r--r--   0        0        0    23907 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_shop_partial_bid_matrix_calculation_input.py
+-rw-r--r--   0        0        0    16460 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_shop_result.py
+-rw-r--r--   0        0        0    21030 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_shop_result_price_prod.py
+-rw-r--r--   0        0        0    12399 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_shop_time_series.py
+-rw-r--r--   0        0        0    19134 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_shop_trigger_input.py
+-rw-r--r--   0        0        0    18619 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_shop_trigger_output.py
+-rw-r--r--   0        0        0    16755 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_task_dispatcher_input.py
+-rw-r--r--   0        0        0    18205 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_task_dispatcher_output.py
+-rw-r--r--   0        0        0    19606 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_task_dispatcher_shop_input.py
+-rw-r--r--   0        0        0    22391 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_task_dispatcher_shop_output.py
+-rw-r--r--   0        0        0    17566 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_task_dispatcher_water_input.py
+-rw-r--r--   0        0        0    19626 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_task_dispatcher_water_output.py
+-rw-r--r--   0        0        0    19627 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_total_bid_matrix_calculation_input.py
+-rw-r--r--   0        0        0    19819 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_total_bid_matrix_calculation_output.py
+-rw-r--r--   0        0        0     9673 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_turbine_efficiency_curve.py
+-rw-r--r--   0        0        0    16847 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_water_partial_bid_calculation_input.py
+-rw-r--r--   0        0        0    20859 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_water_partial_bid_calculation_output.py
+-rw-r--r--   0        0        0    14092 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_water_value_based_partial_bid_configuration.py
+-rw-r--r--   0        0        0    22429 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_water_value_based_partial_bid_matrix_calculation_input.py
+-rw-r--r--   0        0        0    14302 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_watercourse.py
+-rw-r--r--   0        0        0    10509 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_watercourse_shop.py
+-rw-r--r--   0        0        0     1301 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/client/_logger.py
+-rw-r--r--   0        0        0      139 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/client/data_classes.py
+-rw-r--r--   0        0        0     1322 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/client/data_set_api.py
+-rw-r--r--   0        0        0     6750 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/client/powerops_client.py
+-rw-r--r--   0        0        0       62 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/client/shop/__init__.py
+-rw-r--r--   0        0        0      885 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/client/shop/_api_client.py
+-rw-r--r--   0        0        0        0 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/client/shop/api/__init__.py
+-rw-r--r--   0        0        0     8367 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/client/shop/api/dayahead_trigger_api.py
+-rw-r--r--   0        0        0     4797 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/client/shop/api/shop_result_files_api.py
+-rw-r--r--   0        0        0     4432 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/client/shop/api/shop_results_api.py
+-rw-r--r--   0        0        0     9593 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/client/shop/api/shop_run_api.py
+-rw-r--r--   0        0        0      600 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/client/shop/data_classes/__init__.py
+-rw-r--r--   0        0        0     4114 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/client/shop/data_classes/case.py
+-rw-r--r--   0        0        0     9057 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/client/shop/data_classes/dayahead_trigger.py
+-rw-r--r--   0        0        0     2720 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/client/shop/data_classes/helpers.py
+-rw-r--r--   0        0        0      603 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/client/shop/data_classes/plotting.py
+-rw-r--r--   0        0        0     6164 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/client/shop/data_classes/shop_result_files.py
+-rw-r--r--   0        0        0     6607 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/client/shop/data_classes/shop_results.py
+-rw-r--r--   0        0        0    10210 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/client/shop/data_classes/shop_results_compare.py
+-rw-r--r--   0        0        0     3021 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/client/shop/data_classes/shop_run.py
+-rw-r--r--   0        0        0     3743 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/client/shop/data_classes/shop_run_event.py
+-rw-r--r--   0        0        0     4374 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/client/shop/shop_case.py
+-rw-r--r--   0        0        0     1101 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/client/shop/shop_file_reference.py
+-rw-r--r--   0        0        0    13466 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/client/shop/shop_run.py
+-rw-r--r--   0        0        0    15419 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/client/shop/shop_run_api.py
+-rw-r--r--   0        0        0     3885 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/client/shop/shop_run_filter.py
+-rw-r--r--   0        0        0      563 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/client/shop/utils.py
+-rw-r--r--   0        0        0      947 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/cognite_modules/_system.yaml
+-rw-r--r--   0        0        0     1949 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/config.dev.yaml
+-rw-r--r--   0        0        0      729 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/AFRRBid.datamodel.yaml
+-rw-r--r--   0        0        0     1095 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/Asset.datamodel.yaml
+-rw-r--r--   0        0        0     1680 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/DayAheadBid.datamodel.yaml
+-rw-r--r--   0        0        0      100 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/affr_space.space.yaml
+-rw-r--r--   0        0        0       92 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/asset.space.yaml
+-rw-r--r--   0        0        0      367 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/containers/AFRRBid-BidMethod.container.yaml
+-rw-r--r--   0        0        0     2102 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/containers/AFRRBid-BidRow.container.yaml
+-rw-r--r--   0        0        0      750 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/containers/asset-EfficiencyCurve.container.yaml
+-rw-r--r--   0        0        0     1380 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/containers/base-Alert.container.yaml
+-rw-r--r--   0        0        0      857 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/containers/base-Asset.container.yaml
+-rw-r--r--   0        0        0     1038 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/containers/base-BidDocument.container.yaml
+-rw-r--r--   0        0        0      502 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/containers/base-BidMethod.container.yaml
+-rw-r--r--   0        0        0     1408 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/containers/base-Generator.container.yaml
+-rw-r--r--   0        0        0     2281 2024-04-08 12:08:46.304464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/containers/base-Plant.container.yaml
+-rw-r--r--   0        0        0     2375 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/containers/base-PriceArea.container.yaml
+-rw-r--r--   0        0        0      556 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/containers/base-Watercourse.container.yaml
+-rw-r--r--   0        0        0      466 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/containers/dayAheadBids-BidDocument.container.yaml
+-rw-r--r--   0        0        0      767 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/containers/dayAheadBids-BidMatrix.container.yaml
+-rw-r--r--   0        0        0      371 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/containers/dayAheadBids-BidMethod.container.yaml
+-rw-r--r--   0        0        0      532 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/containers/dayAheadBids-MultiScenarioMatrix.container.yaml
+-rw-r--r--   0        0        0      618 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/containers/dayAheadBids-SHOPMultiScenario.container.yaml
+-rw-r--r--   0        0        0      416 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/containers/dayAheadBids-SHOPPriceScenario.container.yaml
+-rw-r--r--   0        0        0      490 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/containers/dayAheadBids-SHOPPriceScenarioResults.container.yaml
+-rw-r--r--   0        0        0      113 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/dayahead_space.space.yaml
+-rw-r--r--   0        0        0      117 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/instance.space.yaml
+-rw-r--r--   0        0        0     1515 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/node_types/power-ops-types.powerops_nodes.yaml
+-rw-r--r--   0        0        0      116 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/shared.space.yaml
+-rw-r--r--   0        0        0       81 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/types.space.yaml
+-rw-r--r--   0        0        0     1379 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/views/AFRRBid-BidDocument.view.yaml
+-rw-r--r--   0        0        0      606 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/views/AFRRBid-BidMethod.view.yaml
+-rw-r--r--   0        0        0     3917 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/views/AFRRBid-BidRow.view.yaml
+-rw-r--r--   0        0        0     2786 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/views/AFRRBid-PriceArea.view.yaml
+-rw-r--r--   0        0        0     2618 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/views/asset-Generator.view.yaml
+-rw-r--r--   0        0        0     1138 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/views/asset-GeneratorEfficiency.view.yaml
+-rw-r--r--   0        0        0     5199 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/views/asset-Plant.view.yaml
+-rw-r--r--   0        0        0     5204 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/views/asset-PriceArea.view.yaml
+-rw-r--r--   0        0        0     1060 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/views/asset-Reservoir.view.yaml
+-rw-r--r--   0        0        0     1121 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/views/asset-TurbineEfficiency.view.yaml
+-rw-r--r--   0        0        0     1887 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/views/asset-Watercourse.view.yaml
+-rw-r--r--   0        0        0     2558 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/views/baseBids-Alert.view.yaml
+-rw-r--r--   0        0        0     2374 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/views/baseBids-BidDocument.view.yaml
+-rw-r--r--   0        0        0      858 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/views/baseBids-BidMethod.view.yaml
+-rw-r--r--   0        0        0      579 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/views/dayAheadBids-BasicBidMatrix.view.yaml
+-rw-r--r--   0        0        0     1945 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/views/dayAheadBids-BidDocument.view.yaml
+-rw-r--r--   0        0        0     1845 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/views/dayAheadBids-BidMatrix.view.yaml
+-rw-r--r--   0        0        0      723 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/views/dayAheadBids-BidMethod.view.yaml
+-rw-r--r--   0        0        0      700 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/views/dayAheadBids-CustomBidMatrix.view.yaml
+-rw-r--r--   0        0        0      711 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/views/dayAheadBids-CustomBidMethod.view.yaml
+-rw-r--r--   0        0        0     1015 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/views/dayAheadBids-MultiScenarioMatrix.view.yaml
+-rw-r--r--   0        0        0     1479 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/views/dayAheadBids-PriceArea.view.yaml
+-rw-r--r--   0        0        0     1312 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/views/dayAheadBids-SHOPMultiScenarioMethod.view.yaml
+-rw-r--r--   0        0        0      666 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/views/dayAheadBids-SHOPPriceScenario.view.yaml
+-rw-r--r--   0        0        0     1230 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/views/dayAheadBids-SHOPPriceScenarioResult.view.yaml
+-rw-r--r--   0        0        0      605 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/views/dayAheadBids-WaterValueBasedMethod.view.yaml
+-rw-r--r--   0        0        0     4327 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/all_DayAhead.datamodel.yaml
+-rw-r--r--   0        0        0     5304 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/all_PowerOps.datamodel.yaml
+-rw-r--r--   0        0        0     2017 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/compute_DayAheadBenchmarking.datamodel.yaml
+-rw-r--r--   0        0        0     3218 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/compute_SHOPBasedDayAhead.datamodel.yaml
+-rw-r--r--   0        0        0     2423 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/compute_TotalBidMatrixCalculation.datamodel.yaml
+-rw-r--r--   0        0        0     2421 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/compute_WaterValueBasedDayAhead.datamodel.yaml
+-rw-r--r--   0        0        0     1898 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/config_DayAheadConfiguration.datamodel.yaml
+-rw-r--r--   0        0        0     1559 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/containers/Alert.container.yaml
+-rw-r--r--   0        0        0     1042 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/containers/Asset.container.yaml
+-rw-r--r--   0        0        0      619 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/containers/BidConfiguration.container.yaml
+-rw-r--r--   0        0        0     1259 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/containers/BidDocument.container.yaml
+-rw-r--r--   0        0        0      494 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/containers/BidDocumentDayAhead.container.yaml
+-rw-r--r--   0        0        0     1089 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/containers/BidMatrix.container.yaml
+-rw-r--r--   0        0        0     1658 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/containers/BidRow.container.yaml
+-rw-r--r--   0        0        0     1715 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/containers/Case.container.yaml
+-rw-r--r--   0        0        0      467 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/containers/CommandsConfig.container.yaml
+-rw-r--r--   0        0        0      754 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/containers/EfficiencyCurve.container.yaml
+-rw-r--r--   0        0        0     2249 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/containers/FunctionData.container.yaml
+-rw-r--r--   0        0        0     1752 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/containers/FunctionMetadata.container.yaml
+-rw-r--r--   0        0        0     1039 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/containers/Generator.container.yaml
+-rw-r--r--   0        0        0     1180 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/containers/Mapping.container.yaml
+-rw-r--r--   0        0        0     2075 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/containers/MarketConfiguration.container.yaml
+-rw-r--r--   0        0        0     1453 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/containers/ModelTemplate.container.yaml
+-rw-r--r--   0        0        0      755 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/containers/PartialBidConfiguration.container.yaml
+-rw-r--r--   0        0        0      261 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/containers/PartialShopBased.container.yaml
+-rw-r--r--   0        0        0     2383 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/containers/Plant.container.yaml
+-rw-r--r--   0        0        0     2389 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/containers/PriceArea.container.yaml
+-rw-r--r--   0        0        0      549 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/containers/PriceProduction.container.yaml
+-rw-r--r--   0        0        0      503 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/containers/SHOPObjectiveValue.container.yaml
+-rw-r--r--   0        0        0     1110 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/containers/SHOPResult.container.yaml
+-rw-r--r--   0        0        0     1103 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/containers/SHOPTimeSeries.container.yaml
+-rw-r--r--   0        0        0      980 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/containers/Scenario.container.yaml
+-rw-r--r--   0        0        0      926 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/containers/ScenarioSet.container.yaml
+-rw-r--r--   0        0        0      861 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/frontend_AFRRBid.datamodel.yaml
+-rw-r--r--   0        0        0      782 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/frontend_Asset.datamodel.yaml
+-rw-r--r--   0        0        0     1927 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/frontend_DayAheadBid.datamodel.yaml
+-rw-r--r--   0        0        0      117 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/instance.space.yaml
+-rw-r--r--   0        0        0      103 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/models.space.yaml
+-rw-r--r--   0        0        0     5245 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/node_types/power-ops-types.powerops_nodes.yaml
+-rw-r--r--   0        0        0      725 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/product_CogShop.datamodel.yaml
+-rw-r--r--   0        0        0       90 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/type.space.yaml
+-rw-r--r--   0        0        0     2898 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/Alert.view.yaml
+-rw-r--r--   0        0        0     3687 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/BidRow.view.yaml
+-rw-r--r--   0        0        0     2821 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/MarketConfiguration.view.yaml
+-rw-r--r--   0        0        0     1369 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/assets/1-interface.PowerAsset.view.yaml
+-rw-r--r--   0        0        0      909 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/assets/1-interface.PriceArea.view.yaml
+-rw-r--r--   0        0        0     2316 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/assets/Generator.view.yaml
+-rw-r--r--   0        0        0     1268 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/assets/GeneratorEfficiency.view.yaml
+-rw-r--r--   0        0        0     4181 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/assets/Plant.view.yaml
+-rw-r--r--   0        0        0     2778 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/assets/PriceAreaAFRR.view.yaml
+-rw-r--r--   0        0        0     1253 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/assets/TurbineEfficiency.view.yaml
+-rw-r--r--   0        0        0     1119 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/bid_configuration/1-interface.PartialBidConfiguration.view.yaml
+-rw-r--r--   0        0        0     1969 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/bid_configuration/BidConfiguration.view.yaml
+-rw-r--r--   0        0        0     1441 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/bid_configuration/ShopBasedPartialBidConfiguration.view.yaml
+-rw-r--r--   0        0        0     4984 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/bid_configuration/WaterValueBasedPartialBidConfiguration.view.yaml
+-rw-r--r--   0        0        0     2479 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/bid_document/1-interface.BidDocument.view.yaml
+-rw-r--r--   0        0        0     1441 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/bid_document/BidDocumentAFRR.view.yaml
+-rw-r--r--   0        0        0     1677 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/bid_document/BidDocumentDayAhead.view.yaml
+-rw-r--r--   0        0        0     1553 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/bid_document/BidDocumentDayAheadSimple.view.yaml
+-rw-r--r--   0        0        0     1690 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/bid_matrix/BidMatrix.view.yaml
+-rw-r--r--   0        0        0     2495 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/cogshop/Case.view.yaml
+-rw-r--r--   0        0        0     1011 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/cogshop/Commands.view.yaml
+-rw-r--r--   0        0        0     1754 2024-04-08 12:08:46.308464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/cogshop/Mapping.view.yaml
+-rw-r--r--   0        0        0     2568 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/cogshop/ModelTemplate.view.yaml
+-rw-r--r--   0        0        0     1906 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/cogshop/Scenario.view.yaml
+-rw-r--r--   0        0        0     1783 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/cogshop/ScenarioSet.view.yaml
+-rw-r--r--   0        0        0     1528 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_inputs/1-interface.FunctionInput.view.yaml
+-rw-r--r--   0        0        0     1357 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_inputs/1-interface.PartialBidMatrixCalculationInput.view.yaml
+-rw-r--r--   0        0        0     1051 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_inputs/BenchmarkingCollectorInput.view.yaml
+-rw-r--r--   0        0        0     1487 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_inputs/PreprocessorInput.view.yaml
+-rw-r--r--   0        0        0     1667 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_inputs/ShopPartialBidMatrixCalculationInput.view.yaml
+-rw-r--r--   0        0        0     1642 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_inputs/ShopTriggerInput.view.yaml
+-rw-r--r--   0        0        0     1067 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_inputs/TaskDispatcherBenchmarkingInput.view.yaml
+-rw-r--r--   0        0        0     1233 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_inputs/TaskDispatcherInput.view.yaml
+-rw-r--r--   0        0        0     1666 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_inputs/TotalBidMatrixCalculationInput.view.yaml
+-rw-r--r--   0        0        0     1256 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_inputs/WaterPartialBidMatrixCalculationInput.view.yaml
+-rw-r--r--   0        0        0     1878 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_outputs/1-interface.FunctionOutput.view.yaml
+-rw-r--r--   0        0        0     1043 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_outputs/BenchmarkingCollectorOutput.view.yaml
+-rw-r--r--   0        0        0     1680 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_outputs/PartialBidMatrixCalculationOutput.view.yaml
+-rw-r--r--   0        0        0     1415 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_outputs/PreprocessorOutput.view.yaml
+-rw-r--r--   0        0        0     1364 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_outputs/SHOPTriggerOutput.view.yaml
+-rw-r--r--   0        0        0     1072 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_outputs/TaskDispatcherBenchmarkingOutput.view.yaml
+-rw-r--r--   0        0        0     1478 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_outputs/TaskDispatcherOutput.view.yaml
+-rw-r--r--   0        0        0     1418 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_outputs/TotalBidMatrixCalculationOutput.view.yaml
+-rw-r--r--   0        0        0     1215 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/shop_result/PriceProduction.view.yaml
+-rw-r--r--   0        0        0     2862 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/shop_result/SHOPResult.view.yaml
+-rw-r--r--   0        0        0     1576 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/shop_result/SHOPTimeSeries.view.yaml
+-rw-r--r--   0        0        0     1107 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/shop_result/ShopObjectiveValue.view.yaml
+-rw-r--r--   0        0        0        0 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/prerun_transformations/__init__.py
+-rw-r--r--   0        0        0    34098 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/prerun_transformations/transformations.py
+-rw-r--r--   0        0        0      278 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/resync/__init__.py
+-rw-r--r--   0        0        0     1792 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/resync/config/__init__.py
+-rw-r--r--   0        0        0    12764 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/resync/config/_main.py
+-rw-r--r--   0        0        0      595 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/resync/config/_settings.py
+-rw-r--r--   0        0        0    12408 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/resync/config/_shared.py
+-rw-r--r--   0        0        0       75 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/resync/config/cogshop/__init__.py
+-rw-r--r--   0        0        0      720 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/resync/config/cogshop/shop_file_config.py
+-rw-r--r--   0        0        0      815 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/resync/config/market/__init__.py
+-rw-r--r--   0        0        0     5486 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/resync/config/market/_core.py
+-rw-r--r--   0        0        0     2106 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/resync/config/market/benchmarking.py
+-rw-r--r--   0        0        0     2105 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/resync/config/market/dayahead.py
+-rw-r--r--   0        0        0      961 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/resync/config/market/market.py
+-rw-r--r--   0        0        0    10827 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/resync/config/market/rkom.py
+-rw-r--r--   0        0        0      417 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/resync/config/production/__init__.py
+-rw-r--r--   0        0        0      830 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/resync/config/production/connections.py
+-rw-r--r--   0        0        0     1414 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/resync/config/production/generator.py
+-rw-r--r--   0        0        0     5675 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/resync/config/production/plant.py
+-rw-r--r--   0        0        0     2868 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/resync/config/production/watercourse.py
+-rw-r--r--   0        0        0      244 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/resync/core/__init__.py
+-rw-r--r--   0        0        0     9185 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/resync/core/cdf.py
+-rw-r--r--   0        0        0      442 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/resync/core/echo.py
+-rw-r--r--   0        0        0    27692 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/resync/core/main.py
+-rw-r--r--   0        0        0     2563 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/resync/core/transform.py
+-rw-r--r--   0        0        0     2161 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/resync/core/validation.py
+-rw-r--r--   0        0        0      474 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/resync/diff/__init__.py
+-rw-r--r--   0        0        0     6774 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/resync/diff/core.py
+-rw-r--r--   0        0        0    12631 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/resync/diff/data_classes.py
+-rw-r--r--   0        0        0      424 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/resync/models/__init__.py
+-rw-r--r--   0        0        0       82 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/resync/models/_shared_v1_v2/__init__.py
+-rw-r--r--   0        0        0      811 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/resync/models/_shared_v1_v2/_to_instances.py
+-rw-r--r--   0        0        0    10495 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/resync/models/_shared_v1_v2/cogshop_model.py
+-rw-r--r--   0        0        0     1143 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/resync/models/_shared_v1_v2/market_model.py
+-rw-r--r--   0        0        0     8337 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/resync/models/_shared_v1_v2/production_model.py
+-rw-r--r--   0        0        0      788 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/resync/models/base/__init__.py
+-rw-r--r--   0        0        0    11195 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/resync/models/base/asset_model.py
+-rw-r--r--   0        0        0     9602 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/resync/models/base/asset_type.py
+-rw-r--r--   0        0        0     5251 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/resync/models/base/cdf_resources.py
+-rw-r--r--   0        0        0    11641 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/resync/models/base/data_model.py
+-rw-r--r--   0        0        0     1896 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/resync/models/base/dms_models.py
+-rw-r--r--   0        0        0      592 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/resync/models/base/graph_ql.py
+-rw-r--r--   0        0        0      148 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/resync/models/base/helpers.py
+-rw-r--r--   0        0        0     6668 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/resync/models/base/model.py
+-rw-r--r--   0        0        0     1705 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/resync/models/base/resource_type.py
+-rw-r--r--   0        0        0     1302 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/resync/models/v1/__init__.py
+-rw-r--r--   0        0        0     8141 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/resync/models/v1/cogshop.py
+-rw-r--r--   0        0        0      243 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/resync/models/v1/config_to_model/__init__.py
+-rw-r--r--   0        0        0     8940 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/resync/models/v1/config_to_model/to_cogshop_model.py
+-rw-r--r--   0        0        0    15631 2024-04-08 12:08:46.312464 cognite_power_ops-0.92.0/cognite/powerops/resync/models/v1/config_to_model/to_market_model.py
+-rw-r--r--   0        0        0     9726 2024-04-08 12:08:46.316464 cognite_power_ops-0.92.0/cognite/powerops/resync/models/v1/config_to_model/to_production_model.py
+-rw-r--r--   0        0        0      491 2024-04-08 12:08:46.316464 cognite_power_ops-0.92.0/cognite/powerops/resync/models/v1/graphql_schemas/__init__.py
+-rw-r--r--   0        0        0      832 2024-04-08 12:08:46.316464 cognite_power_ops-0.92.0/cognite/powerops/resync/models/v1/graphql_schemas/cogshop1.graphql
+-rw-r--r--   0        0        0     3499 2024-04-08 12:08:46.316464 cognite_power_ops-0.92.0/cognite/powerops/resync/models/v1/market/__init__.py
+-rw-r--r--   0        0        0     1291 2024-04-08 12:08:46.316464 cognite_power_ops-0.92.0/cognite/powerops/resync/models/v1/market/base.py
+-rw-r--r--   0        0        0     3184 2024-04-08 12:08:46.316464 cognite_power_ops-0.92.0/cognite/powerops/resync/models/v1/market/benchmark.py
+-rw-r--r--   0        0        0     2405 2024-04-08 12:08:46.316464 cognite_power_ops-0.92.0/cognite/powerops/resync/models/v1/market/dayahead.py
+-rw-r--r--   0        0        0     2797 2024-04-08 12:08:46.316464 cognite_power_ops-0.92.0/cognite/powerops/resync/models/v1/market/rkom.py
+-rw-r--r--   0        0        0     7801 2024-04-08 12:08:46.316464 cognite_power_ops-0.92.0/cognite/powerops/resync/models/v1/production.py
+-rw-r--r--   0        0        0      395 2024-04-08 12:08:46.316464 cognite_power_ops-0.92.0/cognite/powerops/resync/models/v2/__init__.py
+-rw-r--r--   0        0        0        0 2024-04-08 12:08:46.316464 cognite_power_ops-0.92.0/cognite/powerops/resync/models/v2/config_to_model/__init__.py
+-rw-r--r--   0        0        0     8873 2024-04-08 12:08:46.316464 cognite_power_ops-0.92.0/cognite/powerops/resync/models/v2/config_to_model/to_powerasset_model.py
+-rw-r--r--   0        0        0    27181 2024-04-08 12:08:46.316464 cognite_power_ops-0.92.0/cognite/powerops/resync/models/v2/powerops_models.py
+-rw-r--r--   0        0        0     7631 2024-04-08 12:08:46.316464 cognite_power_ops-0.92.0/cognite/powerops/resync/models/v2/production_dm.py
+-rw-r--r--   0        0        0      150 2024-04-08 12:08:46.316464 cognite_power_ops-0.92.0/cognite/powerops/resync/time_series_mapping/__init__.py
+-rw-r--r--   0        0        0     2149 2024-04-08 12:08:46.316464 cognite_power_ops-0.92.0/cognite/powerops/resync/time_series_mapping/mapping.py
+-rw-r--r--   0        0        0     2456 2024-04-08 12:08:46.316464 cognite_power_ops-0.92.0/cognite/powerops/resync/time_series_mapping/static_mapping.py
+-rw-r--r--   0        0        0        0 2024-04-08 12:08:46.316464 cognite_power_ops-0.92.0/cognite/powerops/resync/v2/__init__.py
+-rw-r--r--   0        0        0      238 2024-04-08 12:08:46.316464 cognite_power_ops-0.92.0/cognite/powerops/resync/v2/main.py
+-rw-r--r--   0        0        0     7548 2024-04-08 12:08:46.316464 cognite_power_ops-0.92.0/cognite/powerops/resync/v2/shop_to_assets.py
+-rw-r--r--   0        0        0     4524 2024-04-08 12:08:46.316464 cognite_power_ops-0.92.0/cognite/powerops/resync/validation.py
+-rw-r--r--   0        0        0        0 2024-04-08 12:08:46.316464 cognite_power_ops-0.92.0/cognite/powerops/utils/__init__.py
+-rw-r--r--   0        0        0      160 2024-04-08 12:08:46.316464 cognite_power_ops-0.92.0/cognite/powerops/utils/cdf/__init__.py
+-rw-r--r--   0        0        0     1389 2024-04-08 12:08:46.316464 cognite_power_ops-0.92.0/cognite/powerops/utils/cdf/_cdf_auth.py
+-rw-r--r--   0        0        0     3169 2024-04-08 12:08:46.316464 cognite_power_ops-0.92.0/cognite/powerops/utils/cdf/_settings.py
+-rw-r--r--   0        0        0     7517 2024-04-08 12:08:46.316464 cognite_power_ops-0.92.0/cognite/powerops/utils/cdf/calls.py
+-rw-r--r--   0        0        0    10874 2024-04-08 12:08:46.316464 cognite_power_ops-0.92.0/cognite/powerops/utils/cdf/extraction_pipelines.py
+-rw-r--r--   0        0        0      970 2024-04-08 12:08:46.316464 cognite_power_ops-0.92.0/cognite/powerops/utils/cdf/resource_creation.py
+-rw-r--r--   0        0        0     2540 2024-04-08 12:08:46.316464 cognite_power_ops-0.92.0/cognite/powerops/utils/lookup.py
+-rw-r--r--   0        0        0     1097 2024-04-08 12:08:46.316464 cognite_power_ops-0.92.0/cognite/powerops/utils/navigation.py
+-rw-r--r--   0        0        0      514 2024-04-08 12:08:46.316464 cognite_power_ops-0.92.0/cognite/powerops/utils/require.py
+-rw-r--r--   0        0        0      227 2024-04-08 12:08:46.316464 cognite_power_ops-0.92.0/cognite/powerops/utils/retry/__init__.py
+-rw-r--r--   0        0        0     5619 2024-04-08 12:08:46.316464 cognite_power_ops-0.92.0/cognite/powerops/utils/retry/api.py
+-rw-r--r--   0        0        0     7828 2024-04-08 12:08:46.316464 cognite_power_ops-0.92.0/cognite/powerops/utils/serialization.py
+-rw-r--r--   0        0        0     3667 2024-04-08 12:08:46.316464 cognite_power_ops-0.92.0/cognite/powerops/utils/time.py
+-rw-r--r--   0        0        0     3775 2024-04-08 12:08:46.320464 cognite_power_ops-0.92.0/pyproject.toml
+-rw-r--r--   0        0        0     5516 1970-01-01 00:00:00.000000 cognite_power_ops-0.92.0/PKG-INFO
```

### Comparing `cognite_power_ops-0.91.3/LICENSE` & `cognite_power_ops-0.92.0/LICENSE`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/README.md` & `cognite_power_ops-0.92.0/README.md`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/cdf_labels.py` & `cognite_power_ops-0.92.0/cognite/powerops/cdf_labels.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/cli.py` & `cognite_power_ops-0.92.0/cognite/powerops/cli.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/afrr_bid/_api/_core.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/afrr_bid/_api/_core.py`

 * *Files 1% similar despite different names*

```diff
@@ -478,14 +478,15 @@
 class QueryStep:
     # Setup Variables
     name: str
     expression: dm.query.ResultSetExpression
     max_retrieve_limit: int
     select: dm.query.Select
     result_cls: type[DomainModelCore] | None = None
+    is_single_direct_relation: bool = False
 
     # Query Variables
     cursor: str | None = None
     total_retrieved: int = 0
     results: list[Instance] = field(default_factory=list)
     last_batch_count: int = 0
 
@@ -495,14 +496,25 @@
         else:
             self.expression.limit = max(min(INSTANCE_QUERY_LIMIT, self.max_retrieve_limit - self.total_retrieved), 0)
 
     @property
     def is_unlimited(self) -> bool:
         return self.max_retrieve_limit in {None, -1, math.inf}
 
+    @property
+    def is_finished(self) -> bool:
+        return (
+            (not self.is_unlimited and self.total_retrieved >= self.max_retrieve_limit)
+            or self.cursor is None
+            or self.last_batch_count == 0
+            # Single direct relations are dependent on the parent node,
+            # so we assume that the parent node is the limiting factor.
+            or self.is_single_direct_relation
+        )
+
 
 class QueryBuilder(UserList, Generic[T_DomainModelList]):
     # The unique string is in case the data model has a field that ends with _\d+. This will make sure we don't
     # clean the name of the field.
     _unique_str = "a418"
     _name_pattern = re.compile(r"_a418\d+$")
 
@@ -565,20 +577,15 @@
             expression.last_batch_count = len(batch[expression.name])
             expression.total_retrieved += expression.last_batch_count
             expression.cursor = batch.cursors.get(expression.name)
             expression.results.extend(batch[expression.name].data)
 
     @property
     def is_finished(self):
-        return all(
-            (not expression.is_unlimited and expression.total_retrieved >= expression.max_retrieve_limit)
-            or expression.cursor is None
-            or expression.last_batch_count == 0
-            for expression in self
-        )
+        return all(expression.is_finished for expression in self)
 
     def unpack(self) -> T_DomainModelList:
         nodes_by_type: dict[str | None, dict[tuple[str, str], DomainModel]] = defaultdict(dict)
         edges_by_type_by_source_node: dict[tuple[str, str, str], dict[tuple[str, str], list[dm.Edge]]] = defaultdict(
             lambda: defaultdict(list)
         )
         relation_by_type_by_start_node: dict[tuple[str, str], dict[tuple[str, str], list[DomainRelation]]] = (
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/afrr_bid/_api/alert.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/afrr_bid/_api/alert.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/afrr_bid/_api/alert_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/afrr_bid/_api/alert_query.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/afrr_bid/_api/bid_document.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/afrr_bid/_api/bid_document.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/afrr_bid/_api/bid_document_alerts.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/afrr_bid/_api/bid_document_alerts.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/afrr_bid/_api/bid_document_bids.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/afrr_bid/_api/bid_document_bids.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/afrr_bid/_api/bid_document_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/water_partial_bid_calculation_output_query.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,75 +1,79 @@
 from __future__ import annotations
 
 import datetime
 from typing import TYPE_CHECKING
 
 from cognite.client import data_modeling as dm, CogniteClient
 
-from cognite.powerops.client._generated.afrr_bid.data_classes import (
+from cognite.powerops.client._generated.v1.data_classes import (
     DomainModelCore,
-    BidDocument,
-    PriceArea,
+    WaterPartialBidCalculationOutput,
+    BidMatrixRaw,
+    WaterPartialBidCalculationInput,
 )
 from ._core import DEFAULT_QUERY_LIMIT, QueryBuilder, QueryStep, QueryAPI, T_DomainModelList, _create_edge_filter
 
 if TYPE_CHECKING:
     from .alert_query import AlertQueryAPI
-    from .bid_row_query import BidRowQueryAPI
 
 
-class BidDocumentQueryAPI(QueryAPI[T_DomainModelList]):
+class WaterPartialBidCalculationOutputQueryAPI(QueryAPI[T_DomainModelList]):
     def __init__(
         self,
         client: CogniteClient,
         builder: QueryBuilder[T_DomainModelList],
         view_by_read_class: dict[type[DomainModelCore], dm.ViewId],
         filter_: dm.filters.Filter | None = None,
         limit: int = DEFAULT_QUERY_LIMIT,
     ):
         super().__init__(client, builder, view_by_read_class)
 
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("bid_document"),
+                name=self._builder.next_name("water_partial_bid_calculation_output"),
                 expression=dm.query.NodeResultSetExpression(
                     from_=self._builder[-1].name if self._builder else None,
                     filter=filter_,
                 ),
-                select=dm.query.Select([dm.query.SourceSelector(self._view_by_read_class[BidDocument], ["*"])]),
-                result_cls=BidDocument,
+                select=dm.query.Select(
+                    [dm.query.SourceSelector(self._view_by_read_class[WaterPartialBidCalculationOutput], ["*"])]
+                ),
+                result_cls=WaterPartialBidCalculationOutput,
                 max_retrieve_limit=limit,
             )
         )
 
     def alerts(
         self,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_QUERY_LIMIT,
-        retrieve_price_area: bool = False,
+        retrieve_raw_partial_matrix: bool = False,
+        retrieve_input_: bool = False,
     ) -> AlertQueryAPI[T_DomainModelList]:
-        """Query along the alert edges of the bid document.
+        """Query along the alert edges of the water partial bid calculation output.
 
         Args:
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of alert edges to return. Defaults to 25. Set to -1, float("inf") or None
                 to return all items.
-            retrieve_price_area: Whether to retrieve the price area for each bid document or not.
+            retrieve_raw_partial_matrix: Whether to retrieve the raw partial matrix for each water partial bid calculation output or not.
+            retrieve_input_: Whether to retrieve the input for each water partial bid calculation output or not.
 
         Returns:
             AlertQueryAPI: The query API for the alert.
         """
         from .alert_query import AlertQueryAPI
 
         from_ = self._builder[-1].name
 
         edge_filter = _create_edge_filter(
-            dm.DirectRelationReference("power-ops-types", "calculationIssue"),
+            dm.DirectRelationReference("sp_powerops_types", "calculationIssue"),
             external_id_prefix=external_id_prefix,
             space=space,
         )
         self._builder.append(
             QueryStep(
                 name=self._builder.next_name("alerts"),
                 expression=dm.query.EdgeResultSetExpression(
@@ -77,89 +81,70 @@
                     from_=from_,
                     direction="outwards",
                 ),
                 select=dm.query.Select(),
                 max_retrieve_limit=limit,
             )
         )
-        if retrieve_price_area:
-            self._query_append_price_area(from_)
+        if retrieve_raw_partial_matrix:
+            self._query_append_raw_partial_matrix(from_)
+        if retrieve_input_:
+            self._query_append_input_(from_)
         return AlertQueryAPI(self._client, self._builder, self._view_by_read_class, None, limit)
 
-    def bids(
+    def query(
         self,
-        external_id_prefix: str | None = None,
-        space: str | list[str] | None = None,
-        limit: int | None = DEFAULT_QUERY_LIMIT,
-        retrieve_price_area: bool = False,
-    ) -> BidRowQueryAPI[T_DomainModelList]:
-        """Query along the bid edges of the bid document.
+        retrieve_raw_partial_matrix: bool = False,
+        retrieve_input_: bool = False,
+    ) -> T_DomainModelList:
+        """Execute query and return the result.
 
         Args:
-            external_id_prefix: The prefix of the external ID to filter on.
-            space: The space to filter on.
-            limit: Maximum number of bid edges to return. Defaults to 25. Set to -1, float("inf") or None
-                to return all items.
-            retrieve_price_area: Whether to retrieve the price area for each bid document or not.
+            retrieve_raw_partial_matrix: Whether to retrieve the raw partial matrix for each water partial bid calculation output or not.
+            retrieve_input_: Whether to retrieve the input for each water partial bid calculation output or not.
 
         Returns:
-            BidRowQueryAPI: The query API for the bid row.
-        """
-        from .bid_row_query import BidRowQueryAPI
+            The list of the source nodes of the query.
 
+        """
         from_ = self._builder[-1].name
+        if retrieve_raw_partial_matrix:
+            self._query_append_raw_partial_matrix(from_)
+        if retrieve_input_:
+            self._query_append_input_(from_)
+        return self._query()
 
-        edge_filter = _create_edge_filter(
-            dm.DirectRelationReference("power-ops-types", "partialBid"),
-            external_id_prefix=external_id_prefix,
-            space=space,
-        )
+    def _query_append_raw_partial_matrix(self, from_: str) -> None:
+        view_id = self._view_by_read_class[BidMatrixRaw]
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("bids"),
-                expression=dm.query.EdgeResultSetExpression(
-                    filter=edge_filter,
+                name=self._builder.next_name("raw_partial_matrix"),
+                expression=dm.query.NodeResultSetExpression(
+                    filter=dm.filters.HasData(views=[view_id]),
                     from_=from_,
+                    through=self._view_by_read_class[WaterPartialBidCalculationOutput].as_property_ref(
+                        "rawPartialMatrix"
+                    ),
                     direction="outwards",
                 ),
-                select=dm.query.Select(),
-                max_retrieve_limit=limit,
-            )
+                select=dm.query.Select([dm.query.SourceSelector(view_id, ["*"])]),
+                max_retrieve_limit=-1,
+                result_cls=BidMatrixRaw,
+            ),
         )
-        if retrieve_price_area:
-            self._query_append_price_area(from_)
-        return BidRowQueryAPI(self._client, self._builder, self._view_by_read_class, None, limit)
-
-    def query(
-        self,
-        retrieve_price_area: bool = False,
-    ) -> T_DomainModelList:
-        """Execute query and return the result.
-
-        Args:
-            retrieve_price_area: Whether to retrieve the price area for each bid document or not.
-
-        Returns:
-            The list of the source nodes of the query.
-
-        """
-        from_ = self._builder[-1].name
-        if retrieve_price_area:
-            self._query_append_price_area(from_)
-        return self._query()
 
-    def _query_append_price_area(self, from_: str) -> None:
-        view_id = self._view_by_read_class[PriceArea]
+    def _query_append_input_(self, from_: str) -> None:
+        view_id = self._view_by_read_class[WaterPartialBidCalculationInput]
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("price_area"),
+                name=self._builder.next_name("input_"),
                 expression=dm.query.NodeResultSetExpression(
                     filter=dm.filters.HasData(views=[view_id]),
                     from_=from_,
-                    through=self._view_by_read_class[BidDocument].as_property_ref("priceArea"),
+                    through=self._view_by_read_class[WaterPartialBidCalculationOutput].as_property_ref("input"),
                     direction="outwards",
                 ),
                 select=dm.query.Select([dm.query.SourceSelector(view_id, ["*"])]),
                 max_retrieve_limit=-1,
-                result_cls=PriceArea,
+                result_cls=WaterPartialBidCalculationInput,
             ),
         )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/afrr_bid/_api/bid_method.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/afrr_bid/_api/bid_method.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/afrr_bid/_api/bid_method_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/afrr_bid/_api/bid_method_query.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/afrr_bid/_api/bid_row.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/afrr_bid/_api/bid_row.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/afrr_bid/_api/bid_row_alerts.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/afrr_bid/_api/bid_row_alerts.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/afrr_bid/_api/bid_row_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/shop_partial_bid_calculation_output_query.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,77 +1,79 @@
 from __future__ import annotations
 
 import datetime
 from typing import TYPE_CHECKING
 
 from cognite.client import data_modeling as dm, CogniteClient
 
-from cognite.powerops.client._generated.afrr_bid.data_classes import (
+from cognite.powerops.client._generated.v1.data_classes import (
     DomainModelCore,
-    BidRow,
-    BidRow,
-    BidMethod,
+    ShopPartialBidCalculationOutput,
+    MultiScenarioMatrixRaw,
+    ShopPartialBidCalculationInput,
 )
 from ._core import DEFAULT_QUERY_LIMIT, QueryBuilder, QueryStep, QueryAPI, T_DomainModelList, _create_edge_filter
 
 if TYPE_CHECKING:
     from .alert_query import AlertQueryAPI
 
 
-class BidRowQueryAPI(QueryAPI[T_DomainModelList]):
+class ShopPartialBidCalculationOutputQueryAPI(QueryAPI[T_DomainModelList]):
     def __init__(
         self,
         client: CogniteClient,
         builder: QueryBuilder[T_DomainModelList],
         view_by_read_class: dict[type[DomainModelCore], dm.ViewId],
         filter_: dm.filters.Filter | None = None,
         limit: int = DEFAULT_QUERY_LIMIT,
     ):
         super().__init__(client, builder, view_by_read_class)
 
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("bid_row"),
+                name=self._builder.next_name("shop_partial_bid_calculation_output"),
                 expression=dm.query.NodeResultSetExpression(
                     from_=self._builder[-1].name if self._builder else None,
                     filter=filter_,
                 ),
-                select=dm.query.Select([dm.query.SourceSelector(self._view_by_read_class[BidRow], ["*"])]),
-                result_cls=BidRow,
+                select=dm.query.Select(
+                    [dm.query.SourceSelector(self._view_by_read_class[ShopPartialBidCalculationOutput], ["*"])]
+                ),
+                result_cls=ShopPartialBidCalculationOutput,
                 max_retrieve_limit=limit,
             )
         )
 
     def alerts(
         self,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_QUERY_LIMIT,
-        retrieve_linked_bid: bool = False,
-        retrieve_method: bool = False,
+        retrieve_bid_matrix_raw: bool = False,
+        retrieve_input_: bool = False,
     ) -> AlertQueryAPI[T_DomainModelList]:
-        """Query along the alert edges of the bid row.
+        """Query along the alert edges of the shop partial bid calculation output.
 
         Args:
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of alert edges to return. Defaults to 25. Set to -1, float("inf") or None
                 to return all items.
-            retrieve_linked_bid: Whether to retrieve the linked bid for each bid row or not.
-            retrieve_method: Whether to retrieve the method for each bid row or not.
+            retrieve_bid_matrix_raw: Whether to retrieve the bid matrix raw for each shop partial bid calculation output or not.
+            retrieve_input_: Whether to retrieve the input for each shop partial bid calculation output or not.
 
         Returns:
             AlertQueryAPI: The query API for the alert.
         """
         from .alert_query import AlertQueryAPI
 
         from_ = self._builder[-1].name
 
         edge_filter = _create_edge_filter(
-            dm.DirectRelationReference("power-ops-types", "calculationIssue"),
+            dm.DirectRelationReference("sp_powerops_types", "calculationIssue"),
             external_id_prefix=external_id_prefix,
             space=space,
         )
         self._builder.append(
             QueryStep(
                 name=self._builder.next_name("alerts"),
                 expression=dm.query.EdgeResultSetExpression(
@@ -79,68 +81,68 @@
                     from_=from_,
                     direction="outwards",
                 ),
                 select=dm.query.Select(),
                 max_retrieve_limit=limit,
             )
         )
-        if retrieve_linked_bid:
-            self._query_append_linked_bid(from_)
-        if retrieve_method:
-            self._query_append_method(from_)
+        if retrieve_bid_matrix_raw:
+            self._query_append_bid_matrix_raw(from_)
+        if retrieve_input_:
+            self._query_append_input_(from_)
         return AlertQueryAPI(self._client, self._builder, self._view_by_read_class, None, limit)
 
     def query(
         self,
-        retrieve_linked_bid: bool = False,
-        retrieve_method: bool = False,
+        retrieve_bid_matrix_raw: bool = False,
+        retrieve_input_: bool = False,
     ) -> T_DomainModelList:
         """Execute query and return the result.
 
         Args:
-            retrieve_linked_bid: Whether to retrieve the linked bid for each bid row or not.
-            retrieve_method: Whether to retrieve the method for each bid row or not.
+            retrieve_bid_matrix_raw: Whether to retrieve the bid matrix raw for each shop partial bid calculation output or not.
+            retrieve_input_: Whether to retrieve the input for each shop partial bid calculation output or not.
 
         Returns:
             The list of the source nodes of the query.
 
         """
         from_ = self._builder[-1].name
-        if retrieve_linked_bid:
-            self._query_append_linked_bid(from_)
-        if retrieve_method:
-            self._query_append_method(from_)
+        if retrieve_bid_matrix_raw:
+            self._query_append_bid_matrix_raw(from_)
+        if retrieve_input_:
+            self._query_append_input_(from_)
         return self._query()
 
-    def _query_append_linked_bid(self, from_: str) -> None:
-        view_id = self._view_by_read_class[BidRow]
+    def _query_append_bid_matrix_raw(self, from_: str) -> None:
+        view_id = self._view_by_read_class[MultiScenarioMatrixRaw]
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("linked_bid"),
+                name=self._builder.next_name("bid_matrix_raw"),
                 expression=dm.query.NodeResultSetExpression(
                     filter=dm.filters.HasData(views=[view_id]),
                     from_=from_,
-                    through=self._view_by_read_class[BidRow].as_property_ref("linkedBid"),
+                    through=self._view_by_read_class[ShopPartialBidCalculationOutput].as_property_ref("bidMatrixRaw"),
                     direction="outwards",
                 ),
                 select=dm.query.Select([dm.query.SourceSelector(view_id, ["*"])]),
                 max_retrieve_limit=-1,
-                result_cls=BidRow,
+                result_cls=MultiScenarioMatrixRaw,
             ),
         )
 
-    def _query_append_method(self, from_: str) -> None:
-        view_id = self._view_by_read_class[BidMethod]
+    def _query_append_input_(self, from_: str) -> None:
+        view_id = self._view_by_read_class[ShopPartialBidCalculationInput]
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("method"),
+                name=self._builder.next_name("input_"),
                 expression=dm.query.NodeResultSetExpression(
                     filter=dm.filters.HasData(views=[view_id]),
                     from_=from_,
-                    through=self._view_by_read_class[BidRow].as_property_ref("method"),
+                    through=self._view_by_read_class[ShopPartialBidCalculationOutput].as_property_ref("input"),
                     direction="outwards",
                 ),
                 select=dm.query.Select([dm.query.SourceSelector(view_id, ["*"])]),
                 max_retrieve_limit=-1,
-                result_cls=BidMethod,
+                result_cls=ShopPartialBidCalculationInput,
             ),
         )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/afrr_bid/_api/price_area.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/afrr_bid/_api/price_area.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/afrr_bid/_api/price_area_activation_price_down.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/afrr_bid/_api/price_area_activation_price_down.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/afrr_bid/_api/price_area_activation_price_up.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/afrr_bid/_api/price_area_activation_price_up.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/afrr_bid/_api/price_area_capacity_price_down.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/afrr_bid/_api/price_area_capacity_price_down.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/afrr_bid/_api/price_area_capacity_price_up.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/afrr_bid/_api/price_area_capacity_price_up.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/afrr_bid/_api/price_area_own_capacity_allocation_down.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/afrr_bid/_api/price_area_own_capacity_allocation_down.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/afrr_bid/_api/price_area_own_capacity_allocation_up.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/afrr_bid/_api/price_area_own_capacity_allocation_up.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/afrr_bid/_api/price_area_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/afrr_bid/_api/price_area_query.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/afrr_bid/_api/price_area_relative_activation.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/afrr_bid/_api/price_area_relative_activation.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/afrr_bid/_api/price_area_total_capacity_allocation_down.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/afrr_bid/_api/price_area_total_capacity_allocation_down.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/afrr_bid/_api/price_area_total_capacity_allocation_up.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/afrr_bid/_api/price_area_total_capacity_allocation_up.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/afrr_bid/_api_client.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/afrr_bid/_api_client.py`

 * *Files 2% similar despite different names*

```diff
@@ -19,15 +19,15 @@
 
 
 class AFRRBidAPI:
     """
     AFRRBidAPI
 
     Generated with:
-        pygen = 0.99.14
+        pygen = 0.99.17
         cognite-sdk = 7.26.2
         pydantic = 2.6.4
 
     Data Model:
         space: power-ops-afrr-bid
         externalId: AFRRBid
         version: 1
@@ -37,15 +37,15 @@
         if isinstance(config_or_client, CogniteClient):
             client = config_or_client
         elif isinstance(config_or_client, ClientConfig):
             client = CogniteClient(config_or_client)
         else:
             raise ValueError(f"Expected CogniteClient or ClientConfig, got {type(config_or_client)}")
         # The client name is used for aggregated logging of Pygen Usage
-        client.config.client_name = "CognitePygen:0.99.14"
+        client.config.client_name = "CognitePygen:0.99.17"
 
         view_by_read_class = {
             data_classes.Alert: dm.ViewId("power-ops-shared", "Alert", "1"),
             data_classes.BidDocument: dm.ViewId("power-ops-afrr-bid", "BidDocument", "1"),
             data_classes.BidMethod: dm.ViewId("power-ops-afrr-bid", "BidMethod", "1"),
             data_classes.BidRow: dm.ViewId("power-ops-afrr-bid", "BidRow", "1"),
             data_classes.PriceArea: dm.ViewId("power-ops-afrr-bid", "PriceArea", "1"),
@@ -147,16 +147,16 @@
         Returns:
             The instance(s), i.e., nodes and edges which has been deleted. Empty list if nothing was deleted.
 
         Examples:
 
             Delete item by id:
 
-                >>> from omni import OmniClient
-                >>> client = OmniClient()
+                >>> from cognite.powerops.client._generated.afrr_bid import AFRRBidAPI
+                >>> client = AFRRBidAPI()
                 >>> client.delete("my_node_external_id")
         """
         if isinstance(external_id, str):
             return self._client.data_modeling.instances.delete(nodes=(space, external_id))
         else:
             return self._client.data_modeling.instances.delete(
                 nodes=[(space, id) for id in external_id],
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/afrr_bid/data_classes/__init__.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/afrr_bid/data_classes/__init__.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/afrr_bid/data_classes/_alert.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/afrr_bid/data_classes/_alert.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/afrr_bid/data_classes/_bid_document.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/afrr_bid/data_classes/_bid_document.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/afrr_bid/data_classes/_bid_method.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/afrr_bid/data_classes/_bid_method.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/afrr_bid/data_classes/_bid_row.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/afrr_bid/data_classes/_bid_row.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/afrr_bid/data_classes/_core.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_core.py`

 * *Files 2% similar despite different names*

```diff
@@ -40,15 +40,15 @@
         return_type=dict,
         when_used="unless-none",
     ),
     BeforeValidator(lambda v: CogniteTimeSeries.load(v) if isinstance(v, dict) else v),
 ]
 
 
-DEFAULT_INSTANCE_SPACE = "power-ops-instance"
+DEFAULT_INSTANCE_SPACE = "sp_powerops_instance_temp"
 
 
 @dataclass
 class ResourcesWrite:
     nodes: dm.NodeApplyList = field(default_factory=lambda: dm.NodeApplyList([]))
     edges: dm.EdgeApplyList = field(default_factory=lambda: dm.EdgeApplyList([]))
     time_series: TimeSeriesList = field(default_factory=lambda: TimeSeriesList([]))
@@ -462,32 +462,37 @@
         )
 
 
 T_DomainRelation = TypeVar("T_DomainRelation", bound=DomainRelation)
 
 
 def default_edge_external_id_factory(
-    start_node: DomainModelWrite | str, end_node: DomainModelWrite | str, edge_type: dm.DirectRelationReference
+    start_node: DomainModelWrite | str | dm.NodeId,
+    end_node: DomainModelWrite | str | dm.NodeId,
+    edge_type: dm.DirectRelationReference,
 ) -> str:
     start = start_node if isinstance(start_node, str) else start_node.external_id
     end = end_node if isinstance(end_node, str) else end_node.external_id
     return f"{start}:{end}"
 
 
 class DomainRelationWrite(BaseModel, extra=Extra.forbid, populate_by_name=True):
     external_id_factory: ClassVar[
-        Callable[[Union[DomainModelWrite, str], Union[DomainModelWrite, str], dm.DirectRelationReference], str]
+        Callable[
+            [
+                Union[DomainModelWrite, str, dm.NodeId],
+                Union[DomainModelWrite, str, dm.NodeId],
+                dm.DirectRelationReference,
+            ],
+            str,
+        ]
     ] = default_edge_external_id_factory
     data_record: DataRecordWrite = Field(default_factory=DataRecordWrite)
     external_id: Optional[str] = Field(None, min_length=1, max_length=255)
 
-    @property
-    def data_records(self) -> DataRecordWriteList:
-        return DataRecordWriteList([node.data_record for node in self])
-
     @abstractmethod
     def _to_instances_write(
         self,
         cache: set[tuple[str, str]],
         start_node: DomainModelWrite,
         edge_type: dm.DirectRelationReference,
         view_by_read_class: dict[type[DomainModelCore], dm.ViewId] | None,
@@ -569,24 +574,32 @@
                 write_none,
                 allow_version_increase,
             )
             resources.extend(other_resources)
 
         return resources
 
+    @classmethod
+    def reset_external_id_factory(cls) -> None:
+        cls.external_id_factory = default_edge_external_id_factory
+
 
 T_DomainRelationWrite = TypeVar("T_DomainRelationWrite", bound=DomainRelationWrite)
 
 
 class DomainRelationList(CoreList[T_DomainRelation]):
     _PARENT_CLASS = DomainRelation
 
     def as_edge_ids(self) -> list[dm.EdgeId]:
         return [edge.as_id() for edge in self]
 
+    @property
+    def data_records(self) -> DataRecordWriteList:
+        return DataRecordWriteList([connection.data_record for connection in self])
+
 
 T_DomainRelationList = TypeVar("T_DomainRelationList", bound=DomainRelationList)
 
 
 def unpack_properties(properties: Properties) -> Mapping[str, PropertyValue]:
     unpacked: dict[str, PropertyValue] = {}
     for view_properties in properties.values():
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/afrr_bid/data_classes/_price_area.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/afrr_bid/data_classes/_price_area.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/_core.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/_core.py`

 * *Files 2% similar despite different names*

```diff
@@ -478,14 +478,15 @@
 class QueryStep:
     # Setup Variables
     name: str
     expression: dm.query.ResultSetExpression
     max_retrieve_limit: int
     select: dm.query.Select
     result_cls: type[DomainModelCore] | None = None
+    is_single_direct_relation: bool = False
 
     # Query Variables
     cursor: str | None = None
     total_retrieved: int = 0
     results: list[Instance] = field(default_factory=list)
     last_batch_count: int = 0
 
@@ -495,14 +496,25 @@
         else:
             self.expression.limit = max(min(INSTANCE_QUERY_LIMIT, self.max_retrieve_limit - self.total_retrieved), 0)
 
     @property
     def is_unlimited(self) -> bool:
         return self.max_retrieve_limit in {None, -1, math.inf}
 
+    @property
+    def is_finished(self) -> bool:
+        return (
+            (not self.is_unlimited and self.total_retrieved >= self.max_retrieve_limit)
+            or self.cursor is None
+            or self.last_batch_count == 0
+            # Single direct relations are dependent on the parent node,
+            # so we assume that the parent node is the limiting factor.
+            or self.is_single_direct_relation
+        )
+
 
 class QueryBuilder(UserList, Generic[T_DomainModelList]):
     # The unique string is in case the data model has a field that ends with _\d+. This will make sure we don't
     # clean the name of the field.
     _unique_str = "a418"
     _name_pattern = re.compile(r"_a418\d+$")
 
@@ -565,20 +577,15 @@
             expression.last_batch_count = len(batch[expression.name])
             expression.total_retrieved += expression.last_batch_count
             expression.cursor = batch.cursors.get(expression.name)
             expression.results.extend(batch[expression.name].data)
 
     @property
     def is_finished(self):
-        return all(
-            (not expression.is_unlimited and expression.total_retrieved >= expression.max_retrieve_limit)
-            or expression.cursor is None
-            or expression.last_batch_count == 0
-            for expression in self
-        )
+        return all(expression.is_finished for expression in self)
 
     def unpack(self) -> T_DomainModelList:
         nodes_by_type: dict[str | None, dict[tuple[str, str], DomainModel]] = defaultdict(dict)
         edges_by_type_by_source_node: dict[tuple[str, str, str], dict[tuple[str, str], list[dm.Edge]]] = defaultdict(
             lambda: defaultdict(list)
         )
         relation_by_type_by_start_node: dict[tuple[str, str], dict[tuple[str, str], list[DomainRelation]]] = (
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/bid_method.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/bid_method.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/bid_method_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/bid_method_query.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/generator.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/generator.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/generator_efficiency_curve.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/generator_efficiency_curve.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/generator_efficiency_curve_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/generator_efficiency_curve_query.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/generator_is_available_time_series.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/generator_is_available_time_series.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/generator_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/shop_multi_scenario_method_query.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,120 +1,117 @@
 from __future__ import annotations
 
 import datetime
 from typing import TYPE_CHECKING
 
 from cognite.client import data_modeling as dm, CogniteClient
 
-from cognite.powerops.client._generated.assets.data_classes import (
+from cognite.powerops.client._generated.day_ahead_bid.data_classes import (
     DomainModelCore,
-    Generator,
-    GeneratorEfficiencyCurve,
+    SHOPMultiScenarioMethod,
+)
+from cognite.powerops.client._generated.day_ahead_bid.data_classes._shop_price_scenario import (
+    SHOPPriceScenario,
+    _create_shop_price_scenario_filter,
 )
 from ._core import DEFAULT_QUERY_LIMIT, QueryBuilder, QueryStep, QueryAPI, T_DomainModelList, _create_edge_filter
 
 if TYPE_CHECKING:
-    from .turbine_efficiency_curve_query import TurbineEfficiencyCurveQueryAPI
+    from .shop_price_scenario_query import SHOPPriceScenarioQueryAPI
 
 
-class GeneratorQueryAPI(QueryAPI[T_DomainModelList]):
+class SHOPMultiScenarioMethodQueryAPI(QueryAPI[T_DomainModelList]):
     def __init__(
         self,
         client: CogniteClient,
         builder: QueryBuilder[T_DomainModelList],
         view_by_read_class: dict[type[DomainModelCore], dm.ViewId],
         filter_: dm.filters.Filter | None = None,
         limit: int = DEFAULT_QUERY_LIMIT,
     ):
         super().__init__(client, builder, view_by_read_class)
 
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("generator"),
+                name=self._builder.next_name("shop_multi_scenario_method"),
                 expression=dm.query.NodeResultSetExpression(
                     from_=self._builder[-1].name if self._builder else None,
                     filter=filter_,
                 ),
-                select=dm.query.Select([dm.query.SourceSelector(self._view_by_read_class[Generator], ["*"])]),
-                result_cls=Generator,
+                select=dm.query.Select(
+                    [dm.query.SourceSelector(self._view_by_read_class[SHOPMultiScenarioMethod], ["*"])]
+                ),
+                result_cls=SHOPMultiScenarioMethod,
                 max_retrieve_limit=limit,
             )
         )
 
-    def turbine_curves(
+    def price_scenarios(
         self,
+        name: str | list[str] | None = None,
+        name_prefix: str | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
+        external_id_prefix_edge: str | None = None,
+        space_edge: str | list[str] | None = None,
+        filter: dm.Filter | None = None,
         limit: int | None = DEFAULT_QUERY_LIMIT,
-        retrieve_efficiency_curve: bool = False,
-    ) -> TurbineEfficiencyCurveQueryAPI[T_DomainModelList]:
-        """Query along the turbine curve edges of the generator.
+    ) -> SHOPPriceScenarioQueryAPI[T_DomainModelList]:
+        """Query along the price scenario edges of the shop multi scenario method.
 
         Args:
+            name: The name to filter on.
+            name_prefix: The prefix of the name to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of turbine curve edges to return. Defaults to 25. Set to -1, float("inf") or None
+            external_id_prefix_edge: The prefix of the external ID to filter on.
+            space_edge: The space to filter on.
+            filter: (Advanced) Filter applied to node. If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
+            limit: Maximum number of price scenario edges to return. Defaults to 3. Set to -1, float("inf") or None
                 to return all items.
-            retrieve_efficiency_curve: Whether to retrieve the efficiency curve for each generator or not.
 
         Returns:
-            TurbineEfficiencyCurveQueryAPI: The query API for the turbine efficiency curve.
+            SHOPPriceScenarioQueryAPI: The query API for the shop price scenario.
         """
-        from .turbine_efficiency_curve_query import TurbineEfficiencyCurveQueryAPI
+        from .shop_price_scenario_query import SHOPPriceScenarioQueryAPI
 
         from_ = self._builder[-1].name
-
         edge_filter = _create_edge_filter(
-            dm.DirectRelationReference("power-ops-types", "isSubAssetOf"),
-            external_id_prefix=external_id_prefix,
-            space=space,
+            dm.DirectRelationReference("power-ops-types", "PriceScenario"),
+            external_id_prefix=external_id_prefix_edge,
+            space=space_edge,
         )
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("turbine_curves"),
+                name=self._builder.next_name("price_scenarios"),
                 expression=dm.query.EdgeResultSetExpression(
                     filter=edge_filter,
                     from_=from_,
                     direction="outwards",
                 ),
                 select=dm.query.Select(),
                 max_retrieve_limit=limit,
             )
         )
-        if retrieve_efficiency_curve:
-            self._query_append_efficiency_curve(from_)
-        return TurbineEfficiencyCurveQueryAPI(self._client, self._builder, self._view_by_read_class, None, limit)
+
+        view_id = self._view_by_read_class[SHOPPriceScenario]
+        has_data = dm.filters.HasData(views=[view_id])
+        node_filer = _create_shop_price_scenario_filter(
+            view_id,
+            name,
+            name_prefix,
+            external_id_prefix,
+            space,
+            (filter and dm.filters.And(filter, has_data)) or has_data,
+        )
+        return SHOPPriceScenarioQueryAPI(self._client, self._builder, self._view_by_read_class, node_filer, limit)
 
     def query(
         self,
-        retrieve_efficiency_curve: bool = False,
     ) -> T_DomainModelList:
         """Execute query and return the result.
 
-        Args:
-            retrieve_efficiency_curve: Whether to retrieve the efficiency curve for each generator or not.
-
         Returns:
             The list of the source nodes of the query.
 
         """
-        from_ = self._builder[-1].name
-        if retrieve_efficiency_curve:
-            self._query_append_efficiency_curve(from_)
         return self._query()
-
-    def _query_append_efficiency_curve(self, from_: str) -> None:
-        view_id = self._view_by_read_class[GeneratorEfficiencyCurve]
-        self._builder.append(
-            QueryStep(
-                name=self._builder.next_name("efficiency_curve"),
-                expression=dm.query.NodeResultSetExpression(
-                    filter=dm.filters.HasData(views=[view_id]),
-                    from_=from_,
-                    through=self._view_by_read_class[Generator].as_property_ref("efficiencyCurve"),
-                    direction="outwards",
-                ),
-                select=dm.query.Select([dm.query.SourceSelector(view_id, ["*"])]),
-                max_retrieve_limit=-1,
-                result_cls=GeneratorEfficiencyCurve,
-            ),
-        )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/generator_start_stop_cost.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/generator_start_stop_cost.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/generator_turbine_curves.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/generator_turbine_curves.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/plant.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/plant.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/plant_feeding_fee_time_series.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/plant_feeding_fee_time_series.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/plant_generators.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/plant_generators.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/plant_head_direct_time_series.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/plant_head_direct_time_series.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/plant_inlet_level_time_series.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/plant_inlet_level_time_series.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/plant_outlet_level_time_series.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/plant_outlet_level_time_series.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/plant_p_max_time_series.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/plant_p_max_time_series.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/plant_p_min_time_series.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/plant_p_min_time_series.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/plant_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/shop_partial_bid_calculation_input_query.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,146 +1,150 @@
 from __future__ import annotations
 
 import datetime
 from typing import TYPE_CHECKING
 
 from cognite.client import data_modeling as dm, CogniteClient
 
-from cognite.powerops.client._generated.assets.data_classes import (
+from cognite.powerops.client._generated.v1.data_classes import (
     DomainModelCore,
-    Plant,
-    Watercourse,
-    Reservoir,
+    ShopPartialBidCalculationInput,
+    PlantShop,
+    MarketConfiguration,
 )
 from ._core import DEFAULT_QUERY_LIMIT, QueryBuilder, QueryStep, QueryAPI, T_DomainModelList, _create_edge_filter
 
 if TYPE_CHECKING:
-    from .generator_query import GeneratorQueryAPI
+    from .shop_result_price_prod_query import SHOPResultPriceProdQueryAPI
 
 
-class PlantQueryAPI(QueryAPI[T_DomainModelList]):
+class ShopPartialBidCalculationInputQueryAPI(QueryAPI[T_DomainModelList]):
     def __init__(
         self,
         client: CogniteClient,
         builder: QueryBuilder[T_DomainModelList],
         view_by_read_class: dict[type[DomainModelCore], dm.ViewId],
         filter_: dm.filters.Filter | None = None,
         limit: int = DEFAULT_QUERY_LIMIT,
     ):
         super().__init__(client, builder, view_by_read_class)
 
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("plant"),
+                name=self._builder.next_name("shop_partial_bid_calculation_input"),
                 expression=dm.query.NodeResultSetExpression(
                     from_=self._builder[-1].name if self._builder else None,
                     filter=filter_,
                 ),
-                select=dm.query.Select([dm.query.SourceSelector(self._view_by_read_class[Plant], ["*"])]),
-                result_cls=Plant,
+                select=dm.query.Select(
+                    [dm.query.SourceSelector(self._view_by_read_class[ShopPartialBidCalculationInput], ["*"])]
+                ),
+                result_cls=ShopPartialBidCalculationInput,
                 max_retrieve_limit=limit,
             )
         )
 
-    def generators(
+    def shop_result_price_prod(
         self,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_QUERY_LIMIT,
-        retrieve_watercourse: bool = False,
-        retrieve_inlet_reservoir: bool = False,
-    ) -> GeneratorQueryAPI[T_DomainModelList]:
-        """Query along the generator edges of the plant.
+        retrieve_plant: bool = False,
+        retrieve_market_configuration: bool = False,
+    ) -> SHOPResultPriceProdQueryAPI[T_DomainModelList]:
+        """Query along the shop result price prod edges of the shop partial bid calculation input.
 
         Args:
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of generator edges to return. Defaults to 25. Set to -1, float("inf") or None
+            limit: Maximum number of shop result price prod edges to return. Defaults to 25. Set to -1, float("inf") or None
                 to return all items.
-            retrieve_watercourse: Whether to retrieve the watercourse for each plant or not.
-            retrieve_inlet_reservoir: Whether to retrieve the inlet reservoir for each plant or not.
+            retrieve_plant: Whether to retrieve the plant for each shop partial bid calculation input or not.
+            retrieve_market_configuration: Whether to retrieve the market configuration for each shop partial bid calculation input or not.
 
         Returns:
-            GeneratorQueryAPI: The query API for the generator.
+            SHOPResultPriceProdQueryAPI: The query API for the shop result price prod.
         """
-        from .generator_query import GeneratorQueryAPI
+        from .shop_result_price_prod_query import SHOPResultPriceProdQueryAPI
 
         from_ = self._builder[-1].name
 
         edge_filter = _create_edge_filter(
-            dm.DirectRelationReference("power-ops-types", "isSubAssetOf"),
+            dm.DirectRelationReference("sp_powerops_types", "SHOPResultPriceProd"),
             external_id_prefix=external_id_prefix,
             space=space,
         )
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("generators"),
+                name=self._builder.next_name("shop_result_price_prod"),
                 expression=dm.query.EdgeResultSetExpression(
                     filter=edge_filter,
                     from_=from_,
                     direction="outwards",
                 ),
                 select=dm.query.Select(),
                 max_retrieve_limit=limit,
             )
         )
-        if retrieve_watercourse:
-            self._query_append_watercourse(from_)
-        if retrieve_inlet_reservoir:
-            self._query_append_inlet_reservoir(from_)
-        return GeneratorQueryAPI(self._client, self._builder, self._view_by_read_class, None, limit)
+        if retrieve_plant:
+            self._query_append_plant(from_)
+        if retrieve_market_configuration:
+            self._query_append_market_configuration(from_)
+        return SHOPResultPriceProdQueryAPI(self._client, self._builder, self._view_by_read_class, None, limit)
 
     def query(
         self,
-        retrieve_watercourse: bool = False,
-        retrieve_inlet_reservoir: bool = False,
+        retrieve_plant: bool = False,
+        retrieve_market_configuration: bool = False,
     ) -> T_DomainModelList:
         """Execute query and return the result.
 
         Args:
-            retrieve_watercourse: Whether to retrieve the watercourse for each plant or not.
-            retrieve_inlet_reservoir: Whether to retrieve the inlet reservoir for each plant or not.
+            retrieve_plant: Whether to retrieve the plant for each shop partial bid calculation input or not.
+            retrieve_market_configuration: Whether to retrieve the market configuration for each shop partial bid calculation input or not.
 
         Returns:
             The list of the source nodes of the query.
 
         """
         from_ = self._builder[-1].name
-        if retrieve_watercourse:
-            self._query_append_watercourse(from_)
-        if retrieve_inlet_reservoir:
-            self._query_append_inlet_reservoir(from_)
+        if retrieve_plant:
+            self._query_append_plant(from_)
+        if retrieve_market_configuration:
+            self._query_append_market_configuration(from_)
         return self._query()
 
-    def _query_append_watercourse(self, from_: str) -> None:
-        view_id = self._view_by_read_class[Watercourse]
+    def _query_append_plant(self, from_: str) -> None:
+        view_id = self._view_by_read_class[PlantShop]
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("watercourse"),
+                name=self._builder.next_name("plant"),
                 expression=dm.query.NodeResultSetExpression(
                     filter=dm.filters.HasData(views=[view_id]),
                     from_=from_,
-                    through=self._view_by_read_class[Plant].as_property_ref("watercourse"),
+                    through=self._view_by_read_class[ShopPartialBidCalculationInput].as_property_ref("plant"),
                     direction="outwards",
                 ),
                 select=dm.query.Select([dm.query.SourceSelector(view_id, ["*"])]),
                 max_retrieve_limit=-1,
-                result_cls=Watercourse,
+                result_cls=PlantShop,
             ),
         )
 
-    def _query_append_inlet_reservoir(self, from_: str) -> None:
-        view_id = self._view_by_read_class[Reservoir]
+    def _query_append_market_configuration(self, from_: str) -> None:
+        view_id = self._view_by_read_class[MarketConfiguration]
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("inlet_reservoir"),
+                name=self._builder.next_name("market_configuration"),
                 expression=dm.query.NodeResultSetExpression(
                     filter=dm.filters.HasData(views=[view_id]),
                     from_=from_,
-                    through=self._view_by_read_class[Plant].as_property_ref("inletReservoir"),
+                    through=self._view_by_read_class[ShopPartialBidCalculationInput].as_property_ref(
+                        "marketConfiguration"
+                    ),
                     direction="outwards",
                 ),
                 select=dm.query.Select([dm.query.SourceSelector(view_id, ["*"])]),
                 max_retrieve_limit=-1,
-                result_cls=Reservoir,
+                result_cls=MarketConfiguration,
             ),
         )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/plant_water_value_time_series.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/plant_water_value_time_series.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/price_area.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/price_area.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/price_area_activation_price_down.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/price_area_activation_price_down.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/price_area_activation_price_up.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/price_area_activation_price_up.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/price_area_capacity_price_down.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/price_area_capacity_price_down.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/price_area_capacity_price_up.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/price_area_capacity_price_up.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/price_area_day_ahead_price.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/price_area_day_ahead_price.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/price_area_main_scenario_day_ahead.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/price_area_main_scenario_day_ahead.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/price_area_own_capacity_allocation_down.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/price_area_own_capacity_allocation_down.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/price_area_own_capacity_allocation_up.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/price_area_own_capacity_allocation_up.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/price_area_plants.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/price_area_plants.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/price_area_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/task_dispatcher_water_output_query.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,165 +1,169 @@
 from __future__ import annotations
 
 import datetime
 from typing import TYPE_CHECKING
 
 from cognite.client import data_modeling as dm, CogniteClient
 
-from cognite.powerops.client._generated.assets.data_classes import (
+from cognite.powerops.client._generated.v1.data_classes import (
     DomainModelCore,
-    PriceArea,
-    BidMethod,
+    TaskDispatcherWaterOutput,
+    TaskDispatcherWaterInput,
 )
 from ._core import DEFAULT_QUERY_LIMIT, QueryBuilder, QueryStep, QueryAPI, T_DomainModelList, _create_edge_filter
 
 if TYPE_CHECKING:
-    from .plant_query import PlantQueryAPI
-    from .watercourse_query import WatercourseQueryAPI
+    from .alert_query import AlertQueryAPI
+    from .water_partial_bid_calculation_input_query import WaterPartialBidCalculationInputQueryAPI
 
 
-class PriceAreaQueryAPI(QueryAPI[T_DomainModelList]):
+class TaskDispatcherWaterOutputQueryAPI(QueryAPI[T_DomainModelList]):
     def __init__(
         self,
         client: CogniteClient,
         builder: QueryBuilder[T_DomainModelList],
         view_by_read_class: dict[type[DomainModelCore], dm.ViewId],
         filter_: dm.filters.Filter | None = None,
         limit: int = DEFAULT_QUERY_LIMIT,
     ):
         super().__init__(client, builder, view_by_read_class)
 
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("price_area"),
+                name=self._builder.next_name("task_dispatcher_water_output"),
                 expression=dm.query.NodeResultSetExpression(
                     from_=self._builder[-1].name if self._builder else None,
                     filter=filter_,
                 ),
-                select=dm.query.Select([dm.query.SourceSelector(self._view_by_read_class[PriceArea], ["*"])]),
-                result_cls=PriceArea,
+                select=dm.query.Select(
+                    [dm.query.SourceSelector(self._view_by_read_class[TaskDispatcherWaterOutput], ["*"])]
+                ),
+                result_cls=TaskDispatcherWaterOutput,
                 max_retrieve_limit=limit,
             )
         )
 
-    def plants(
+    def alerts(
         self,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_QUERY_LIMIT,
-        retrieve_default_method_day_ahead: bool = False,
-    ) -> PlantQueryAPI[T_DomainModelList]:
-        """Query along the plant edges of the price area.
+        retrieve_input_: bool = False,
+    ) -> AlertQueryAPI[T_DomainModelList]:
+        """Query along the alert edges of the task dispatcher water output.
 
         Args:
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of plant edges to return. Defaults to 25. Set to -1, float("inf") or None
+            limit: Maximum number of alert edges to return. Defaults to 25. Set to -1, float("inf") or None
                 to return all items.
-            retrieve_default_method_day_ahead: Whether to retrieve the default method day ahead for each price area or not.
+            retrieve_input_: Whether to retrieve the input for each task dispatcher water output or not.
 
         Returns:
-            PlantQueryAPI: The query API for the plant.
+            AlertQueryAPI: The query API for the alert.
         """
-        from .plant_query import PlantQueryAPI
+        from .alert_query import AlertQueryAPI
 
         from_ = self._builder[-1].name
 
         edge_filter = _create_edge_filter(
-            dm.DirectRelationReference("power-ops-types", "isSubAssetOf"),
+            dm.DirectRelationReference("sp_powerops_types", "calculationIssue"),
             external_id_prefix=external_id_prefix,
             space=space,
         )
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("plants"),
+                name=self._builder.next_name("alerts"),
                 expression=dm.query.EdgeResultSetExpression(
                     filter=edge_filter,
                     from_=from_,
                     direction="outwards",
                 ),
                 select=dm.query.Select(),
                 max_retrieve_limit=limit,
             )
         )
-        if retrieve_default_method_day_ahead:
-            self._query_append_default_method_day_ahead(from_)
-        return PlantQueryAPI(self._client, self._builder, self._view_by_read_class, None, limit)
+        if retrieve_input_:
+            self._query_append_input_(from_)
+        return AlertQueryAPI(self._client, self._builder, self._view_by_read_class, None, limit)
 
-    def watercourses(
+    def bid_calculation_tasks(
         self,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_QUERY_LIMIT,
-        retrieve_default_method_day_ahead: bool = False,
-    ) -> WatercourseQueryAPI[T_DomainModelList]:
-        """Query along the watercourse edges of the price area.
+        retrieve_input_: bool = False,
+    ) -> WaterPartialBidCalculationInputQueryAPI[T_DomainModelList]:
+        """Query along the bid calculation task edges of the task dispatcher water output.
 
         Args:
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of watercourse edges to return. Defaults to 25. Set to -1, float("inf") or None
+            limit: Maximum number of bid calculation task edges to return. Defaults to 25. Set to -1, float("inf") or None
                 to return all items.
-            retrieve_default_method_day_ahead: Whether to retrieve the default method day ahead for each price area or not.
+            retrieve_input_: Whether to retrieve the input for each task dispatcher water output or not.
 
         Returns:
-            WatercourseQueryAPI: The query API for the watercourse.
+            WaterPartialBidCalculationInputQueryAPI: The query API for the water partial bid calculation input.
         """
-        from .watercourse_query import WatercourseQueryAPI
+        from .water_partial_bid_calculation_input_query import WaterPartialBidCalculationInputQueryAPI
 
         from_ = self._builder[-1].name
 
         edge_filter = _create_edge_filter(
-            dm.DirectRelationReference("power-ops-types", "isSubAssetOf"),
+            dm.DirectRelationReference("sp_powerops_types", "Water.partialBidCalculations"),
             external_id_prefix=external_id_prefix,
             space=space,
         )
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("watercourses"),
+                name=self._builder.next_name("bid_calculation_tasks"),
                 expression=dm.query.EdgeResultSetExpression(
                     filter=edge_filter,
                     from_=from_,
                     direction="outwards",
                 ),
                 select=dm.query.Select(),
                 max_retrieve_limit=limit,
             )
         )
-        if retrieve_default_method_day_ahead:
-            self._query_append_default_method_day_ahead(from_)
-        return WatercourseQueryAPI(self._client, self._builder, self._view_by_read_class, None, limit)
+        if retrieve_input_:
+            self._query_append_input_(from_)
+        return WaterPartialBidCalculationInputQueryAPI(
+            self._client, self._builder, self._view_by_read_class, None, limit
+        )
 
     def query(
         self,
-        retrieve_default_method_day_ahead: bool = False,
+        retrieve_input_: bool = False,
     ) -> T_DomainModelList:
         """Execute query and return the result.
 
         Args:
-            retrieve_default_method_day_ahead: Whether to retrieve the default method day ahead for each price area or not.
+            retrieve_input_: Whether to retrieve the input for each task dispatcher water output or not.
 
         Returns:
             The list of the source nodes of the query.
 
         """
         from_ = self._builder[-1].name
-        if retrieve_default_method_day_ahead:
-            self._query_append_default_method_day_ahead(from_)
+        if retrieve_input_:
+            self._query_append_input_(from_)
         return self._query()
 
-    def _query_append_default_method_day_ahead(self, from_: str) -> None:
-        view_id = self._view_by_read_class[BidMethod]
+    def _query_append_input_(self, from_: str) -> None:
+        view_id = self._view_by_read_class[TaskDispatcherWaterInput]
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("default_method_day_ahead"),
+                name=self._builder.next_name("input_"),
                 expression=dm.query.NodeResultSetExpression(
                     filter=dm.filters.HasData(views=[view_id]),
                     from_=from_,
-                    through=self._view_by_read_class[PriceArea].as_property_ref("defaultMethodDayAhead"),
+                    through=self._view_by_read_class[TaskDispatcherWaterOutput].as_property_ref("input"),
                     direction="outwards",
                 ),
                 select=dm.query.Select([dm.query.SourceSelector(view_id, ["*"])]),
                 max_retrieve_limit=-1,
-                result_cls=BidMethod,
+                result_cls=TaskDispatcherWaterInput,
             ),
         )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/price_area_relative_activation.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/price_area_relative_activation.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/price_area_total_capacity_allocation_down.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/price_area_total_capacity_allocation_down.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/price_area_total_capacity_allocation_up.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/price_area_total_capacity_allocation_up.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/price_area_watercourses.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/price_area_watercourses.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/reservoir.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/reservoir.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/reservoir_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/reservoir_query.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/turbine_efficiency_curve.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/turbine_efficiency_curve.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/turbine_efficiency_curve_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/turbine_efficiency_curve_query.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/watercourse.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/watercourse.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/watercourse_plants.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/watercourse_plants.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/watercourse_production_obligation.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/watercourse_production_obligation.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api/watercourse_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/watercourse_query.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 from __future__ import annotations
 
 import datetime
 from typing import TYPE_CHECKING
 
 from cognite.client import data_modeling as dm, CogniteClient
 
-from cognite.powerops.client._generated.assets.data_classes import (
+from cognite.powerops.client._generated.v1.data_classes import (
     DomainModelCore,
     Watercourse,
 )
 from ._core import DEFAULT_QUERY_LIMIT, QueryBuilder, QueryStep, QueryAPI, T_DomainModelList, _create_edge_filter
 
 if TYPE_CHECKING:
     from .plant_query import PlantQueryAPI
@@ -57,15 +57,15 @@
             PlantQueryAPI: The query API for the plant.
         """
         from .plant_query import PlantQueryAPI
 
         from_ = self._builder[-1].name
 
         edge_filter = _create_edge_filter(
-            dm.DirectRelationReference("power-ops-types", "isSubAssetOf"),
+            dm.DirectRelationReference("sp_powerops_types", "isSubAssetOf"),
             external_id_prefix=external_id_prefix,
             space=space,
         )
         self._builder.append(
             QueryStep(
                 name=self._builder.next_name("plants"),
                 expression=dm.query.EdgeResultSetExpression(
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/_api_client.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api_client.py`

 * *Files 2% similar despite different names*

```diff
@@ -22,15 +22,15 @@
 
 
 class PowerAssetAPI:
     """
     PowerAssetAPI
 
     Generated with:
-        pygen = 0.99.14
+        pygen = 0.99.17
         cognite-sdk = 7.26.2
         pydantic = 2.6.4
 
     Data Model:
         space: power-ops-assets
         externalId: PowerAsset
         version: 1
@@ -40,15 +40,15 @@
         if isinstance(config_or_client, CogniteClient):
             client = config_or_client
         elif isinstance(config_or_client, ClientConfig):
             client = CogniteClient(config_or_client)
         else:
             raise ValueError(f"Expected CogniteClient or ClientConfig, got {type(config_or_client)}")
         # The client name is used for aggregated logging of Pygen Usage
-        client.config.client_name = "CognitePygen:0.99.14"
+        client.config.client_name = "CognitePygen:0.99.17"
 
         view_by_read_class = {
             data_classes.BidMethod: dm.ViewId("power-ops-shared", "BidMethod", "1"),
             data_classes.Generator: dm.ViewId("power-ops-assets", "Generator", "1"),
             data_classes.GeneratorEfficiencyCurve: dm.ViewId("power-ops-assets", "GeneratorEfficiencyCurve", "1"),
             data_classes.Plant: dm.ViewId("power-ops-assets", "Plant", "1"),
             data_classes.PriceArea: dm.ViewId("power-ops-assets", "PriceArea", "1"),
@@ -156,16 +156,16 @@
         Returns:
             The instance(s), i.e., nodes and edges which has been deleted. Empty list if nothing was deleted.
 
         Examples:
 
             Delete item by id:
 
-                >>> from omni import OmniClient
-                >>> client = OmniClient()
+                >>> from cognite.powerops.client._generated.assets import PowerAssetAPI
+                >>> client = PowerAssetAPI()
                 >>> client.delete("my_node_external_id")
         """
         if isinstance(external_id, str):
             return self._client.data_modeling.instances.delete(nodes=(space, external_id))
         else:
             return self._client.data_modeling.instances.delete(
                 nodes=[(space, id) for id in external_id],
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/data_classes/__init__.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/data_classes/__init__.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/data_classes/_bid_method.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/data_classes/_bid_method.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/data_classes/_core.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/afrr_bid/data_classes/_core.py`

 * *Files 2% similar despite different names*

```diff
@@ -462,32 +462,37 @@
         )
 
 
 T_DomainRelation = TypeVar("T_DomainRelation", bound=DomainRelation)
 
 
 def default_edge_external_id_factory(
-    start_node: DomainModelWrite | str, end_node: DomainModelWrite | str, edge_type: dm.DirectRelationReference
+    start_node: DomainModelWrite | str | dm.NodeId,
+    end_node: DomainModelWrite | str | dm.NodeId,
+    edge_type: dm.DirectRelationReference,
 ) -> str:
     start = start_node if isinstance(start_node, str) else start_node.external_id
     end = end_node if isinstance(end_node, str) else end_node.external_id
     return f"{start}:{end}"
 
 
 class DomainRelationWrite(BaseModel, extra=Extra.forbid, populate_by_name=True):
     external_id_factory: ClassVar[
-        Callable[[Union[DomainModelWrite, str], Union[DomainModelWrite, str], dm.DirectRelationReference], str]
+        Callable[
+            [
+                Union[DomainModelWrite, str, dm.NodeId],
+                Union[DomainModelWrite, str, dm.NodeId],
+                dm.DirectRelationReference,
+            ],
+            str,
+        ]
     ] = default_edge_external_id_factory
     data_record: DataRecordWrite = Field(default_factory=DataRecordWrite)
     external_id: Optional[str] = Field(None, min_length=1, max_length=255)
 
-    @property
-    def data_records(self) -> DataRecordWriteList:
-        return DataRecordWriteList([node.data_record for node in self])
-
     @abstractmethod
     def _to_instances_write(
         self,
         cache: set[tuple[str, str]],
         start_node: DomainModelWrite,
         edge_type: dm.DirectRelationReference,
         view_by_read_class: dict[type[DomainModelCore], dm.ViewId] | None,
@@ -569,24 +574,32 @@
                 write_none,
                 allow_version_increase,
             )
             resources.extend(other_resources)
 
         return resources
 
+    @classmethod
+    def reset_external_id_factory(cls) -> None:
+        cls.external_id_factory = default_edge_external_id_factory
+
 
 T_DomainRelationWrite = TypeVar("T_DomainRelationWrite", bound=DomainRelationWrite)
 
 
 class DomainRelationList(CoreList[T_DomainRelation]):
     _PARENT_CLASS = DomainRelation
 
     def as_edge_ids(self) -> list[dm.EdgeId]:
         return [edge.as_id() for edge in self]
 
+    @property
+    def data_records(self) -> DataRecordWriteList:
+        return DataRecordWriteList([connection.data_record for connection in self])
+
 
 T_DomainRelationList = TypeVar("T_DomainRelationList", bound=DomainRelationList)
 
 
 def unpack_properties(properties: Properties) -> Mapping[str, PropertyValue]:
     unpacked: dict[str, PropertyValue] = {}
     for view_properties in properties.values():
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/data_classes/_generator.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/data_classes/_generator.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/data_classes/_generator_efficiency_curve.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/data_classes/_generator_efficiency_curve.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/data_classes/_plant.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/data_classes/_plant.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/data_classes/_price_area.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/data_classes/_price_area.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/data_classes/_reservoir.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/data_classes/_reservoir.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/data_classes/_turbine_efficiency_curve.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/data_classes/_turbine_efficiency_curve.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/assets/data_classes/_watercourse.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/data_classes/_watercourse.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/cogshop1/_api/_core.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/cogshop1/_api/_core.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/cogshop1/_api/case.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/cogshop1/_api/case.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/cogshop1/_api/commands_config.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/cogshop1/_api/commands_config.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/cogshop1/_api/file_ref.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/cogshop1/_api/file_ref.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/cogshop1/_api/mapping.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/cogshop1/_api/mapping.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/cogshop1/_api/model_template.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/cogshop1/_api/model_template.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/cogshop1/_api/processing_log.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/cogshop1/_api/processing_log.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/cogshop1/_api/scenario.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/cogshop1/_api/scenario.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/cogshop1/_api/transformation.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/cogshop1/_api/transformation.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/cogshop1/_api_client.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/cogshop1/_api_client.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/cogshop1/data_classes/__init__.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/cogshop1/data_classes/__init__.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/cogshop1/data_classes/_case.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/cogshop1/data_classes/_case.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/cogshop1/data_classes/_commands_config.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/cogshop1/data_classes/_commands_config.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/cogshop1/data_classes/_core.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/cogshop1/data_classes/_core.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/cogshop1/data_classes/_file_ref.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/cogshop1/data_classes/_file_ref.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/cogshop1/data_classes/_mapping.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/cogshop1/data_classes/_mapping.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/cogshop1/data_classes/_model_template.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/cogshop1/data_classes/_model_template.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/cogshop1/data_classes/_processing_log.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/cogshop1/data_classes/_processing_log.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/cogshop1/data_classes/_scenario.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/cogshop1/data_classes/_scenario.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/cogshop1/data_classes/_transformation.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/cogshop1/data_classes/_transformation.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/_core.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/_core.py`

 * *Files 1% similar despite different names*

```diff
@@ -478,14 +478,15 @@
 class QueryStep:
     # Setup Variables
     name: str
     expression: dm.query.ResultSetExpression
     max_retrieve_limit: int
     select: dm.query.Select
     result_cls: type[DomainModelCore] | None = None
+    is_single_direct_relation: bool = False
 
     # Query Variables
     cursor: str | None = None
     total_retrieved: int = 0
     results: list[Instance] = field(default_factory=list)
     last_batch_count: int = 0
 
@@ -495,14 +496,25 @@
         else:
             self.expression.limit = max(min(INSTANCE_QUERY_LIMIT, self.max_retrieve_limit - self.total_retrieved), 0)
 
     @property
     def is_unlimited(self) -> bool:
         return self.max_retrieve_limit in {None, -1, math.inf}
 
+    @property
+    def is_finished(self) -> bool:
+        return (
+            (not self.is_unlimited and self.total_retrieved >= self.max_retrieve_limit)
+            or self.cursor is None
+            or self.last_batch_count == 0
+            # Single direct relations are dependent on the parent node,
+            # so we assume that the parent node is the limiting factor.
+            or self.is_single_direct_relation
+        )
+
 
 class QueryBuilder(UserList, Generic[T_DomainModelList]):
     # The unique string is in case the data model has a field that ends with _\d+. This will make sure we don't
     # clean the name of the field.
     _unique_str = "a418"
     _name_pattern = re.compile(r"_a418\d+$")
 
@@ -565,20 +577,15 @@
             expression.last_batch_count = len(batch[expression.name])
             expression.total_retrieved += expression.last_batch_count
             expression.cursor = batch.cursors.get(expression.name)
             expression.results.extend(batch[expression.name].data)
 
     @property
     def is_finished(self):
-        return all(
-            (not expression.is_unlimited and expression.total_retrieved >= expression.max_retrieve_limit)
-            or expression.cursor is None
-            or expression.last_batch_count == 0
-            for expression in self
-        )
+        return all(expression.is_finished for expression in self)
 
     def unpack(self) -> T_DomainModelList:
         nodes_by_type: dict[str | None, dict[tuple[str, str], DomainModel]] = defaultdict(dict)
         edges_by_type_by_source_node: dict[tuple[str, str, str], dict[tuple[str, str], list[dm.Edge]]] = defaultdict(
             lambda: defaultdict(list)
         )
         relation_by_type_by_start_node: dict[tuple[str, str], dict[tuple[str, str], list[DomainRelation]]] = (
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/alert.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/alert.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/alert_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/alert_query.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/basic_bid_matrix.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/basic_bid_matrix.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/basic_bid_matrix_alerts.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/basic_bid_matrix_alerts.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/basic_bid_matrix_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/basic_bid_matrix_query.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,18 +1,18 @@
 from __future__ import annotations
 
 import datetime
 from typing import TYPE_CHECKING
 
 from cognite.client import data_modeling as dm, CogniteClient
 
-from cognite.powerops.client._generated.day_ahead_bid.data_classes import (
+from cognite.powerops.client._generated.v1.data_classes import (
     DomainModelCore,
     BasicBidMatrix,
-    BidMethod,
+    BidMethodDayAhead,
 )
 from ._core import DEFAULT_QUERY_LIMIT, QueryBuilder, QueryStep, QueryAPI, T_DomainModelList, _create_edge_filter
 
 if TYPE_CHECKING:
     from .alert_query import AlertQueryAPI
 
 
@@ -60,15 +60,15 @@
             AlertQueryAPI: The query API for the alert.
         """
         from .alert_query import AlertQueryAPI
 
         from_ = self._builder[-1].name
 
         edge_filter = _create_edge_filter(
-            dm.DirectRelationReference("power-ops-types", "calculationIssue"),
+            dm.DirectRelationReference("sp_powerops_types", "calculationIssue"),
             external_id_prefix=external_id_prefix,
             space=space,
         )
         self._builder.append(
             QueryStep(
                 name=self._builder.next_name("alerts"),
                 expression=dm.query.EdgeResultSetExpression(
@@ -99,22 +99,22 @@
         """
         from_ = self._builder[-1].name
         if retrieve_method:
             self._query_append_method(from_)
         return self._query()
 
     def _query_append_method(self, from_: str) -> None:
-        view_id = self._view_by_read_class[BidMethod]
+        view_id = self._view_by_read_class[BidMethodDayAhead]
         self._builder.append(
             QueryStep(
                 name=self._builder.next_name("method"),
                 expression=dm.query.NodeResultSetExpression(
                     filter=dm.filters.HasData(views=[view_id]),
                     from_=from_,
                     through=self._view_by_read_class[BasicBidMatrix].as_property_ref("method"),
                     direction="outwards",
                 ),
                 select=dm.query.Select([dm.query.SourceSelector(view_id, ["*"])]),
                 max_retrieve_limit=-1,
-                result_cls=BidMethod,
+                result_cls=BidMethodDayAhead,
             ),
         )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/bid_document.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/bid_document.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/bid_document_alerts.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/bid_document_alerts.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/bid_document_partials.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/bid_document_partials.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/bid_document_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_configuration_water_query.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,225 +1,227 @@
 from __future__ import annotations
 
 import datetime
 from typing import TYPE_CHECKING
 
 from cognite.client import data_modeling as dm, CogniteClient
 
-from cognite.powerops.client._generated.day_ahead_bid.data_classes import (
+from cognite.powerops.client._generated.v1.data_classes import (
     DomainModelCore,
-    BidDocument,
+    BidConfigurationWater,
+    MarketConfiguration,
+    BidMethodWaterValue,
     PriceArea,
-    BidMethod,
-    BidMatrix,
 )
 from ._core import DEFAULT_QUERY_LIMIT, QueryBuilder, QueryStep, QueryAPI, T_DomainModelList, _create_edge_filter
 
 if TYPE_CHECKING:
-    from .alert_query import AlertQueryAPI
-    from .bid_matrix_query import BidMatrixQueryAPI
+    from .plant_query import PlantQueryAPI
+    from .watercourse_query import WatercourseQueryAPI
 
 
-class BidDocumentQueryAPI(QueryAPI[T_DomainModelList]):
+class BidConfigurationWaterQueryAPI(QueryAPI[T_DomainModelList]):
     def __init__(
         self,
         client: CogniteClient,
         builder: QueryBuilder[T_DomainModelList],
         view_by_read_class: dict[type[DomainModelCore], dm.ViewId],
         filter_: dm.filters.Filter | None = None,
         limit: int = DEFAULT_QUERY_LIMIT,
     ):
         super().__init__(client, builder, view_by_read_class)
 
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("bid_document"),
+                name=self._builder.next_name("bid_configuration_water"),
                 expression=dm.query.NodeResultSetExpression(
                     from_=self._builder[-1].name if self._builder else None,
                     filter=filter_,
                 ),
-                select=dm.query.Select([dm.query.SourceSelector(self._view_by_read_class[BidDocument], ["*"])]),
-                result_cls=BidDocument,
+                select=dm.query.Select(
+                    [dm.query.SourceSelector(self._view_by_read_class[BidConfigurationWater], ["*"])]
+                ),
+                result_cls=BidConfigurationWater,
                 max_retrieve_limit=limit,
             )
         )
 
-    def alerts(
+    def plants(
         self,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_QUERY_LIMIT,
-        retrieve_price_area: bool = False,
+        retrieve_market_configuration: bool = False,
         retrieve_method: bool = False,
-        retrieve_total: bool = False,
-    ) -> AlertQueryAPI[T_DomainModelList]:
-        """Query along the alert edges of the bid document.
+        retrieve_price_area: bool = False,
+    ) -> PlantQueryAPI[T_DomainModelList]:
+        """Query along the plant edges of the bid configuration water.
 
         Args:
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of alert edges to return. Defaults to 25. Set to -1, float("inf") or None
+            limit: Maximum number of plant edges to return. Defaults to 25. Set to -1, float("inf") or None
                 to return all items.
-            retrieve_price_area: Whether to retrieve the price area for each bid document or not.
-            retrieve_method: Whether to retrieve the method for each bid document or not.
-            retrieve_total: Whether to retrieve the total for each bid document or not.
+            retrieve_market_configuration: Whether to retrieve the market configuration for each bid configuration water or not.
+            retrieve_method: Whether to retrieve the method for each bid configuration water or not.
+            retrieve_price_area: Whether to retrieve the price area for each bid configuration water or not.
 
         Returns:
-            AlertQueryAPI: The query API for the alert.
+            PlantQueryAPI: The query API for the plant.
         """
-        from .alert_query import AlertQueryAPI
+        from .plant_query import PlantQueryAPI
 
         from_ = self._builder[-1].name
 
         edge_filter = _create_edge_filter(
-            dm.DirectRelationReference("power-ops-types", "calculationIssue"),
+            dm.DirectRelationReference("sp_powerops_types", "BidConfiguration.plants"),
             external_id_prefix=external_id_prefix,
             space=space,
         )
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("alerts"),
+                name=self._builder.next_name("plants"),
                 expression=dm.query.EdgeResultSetExpression(
                     filter=edge_filter,
                     from_=from_,
                     direction="outwards",
                 ),
                 select=dm.query.Select(),
                 max_retrieve_limit=limit,
             )
         )
-        if retrieve_price_area:
-            self._query_append_price_area(from_)
+        if retrieve_market_configuration:
+            self._query_append_market_configuration(from_)
         if retrieve_method:
             self._query_append_method(from_)
-        if retrieve_total:
-            self._query_append_total(from_)
-        return AlertQueryAPI(self._client, self._builder, self._view_by_read_class, None, limit)
+        if retrieve_price_area:
+            self._query_append_price_area(from_)
+        return PlantQueryAPI(self._client, self._builder, self._view_by_read_class, None, limit)
 
-    def partials(
+    def watercourses(
         self,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_QUERY_LIMIT,
-        retrieve_price_area: bool = False,
+        retrieve_market_configuration: bool = False,
         retrieve_method: bool = False,
-        retrieve_total: bool = False,
-    ) -> BidMatrixQueryAPI[T_DomainModelList]:
-        """Query along the partial edges of the bid document.
+        retrieve_price_area: bool = False,
+    ) -> WatercourseQueryAPI[T_DomainModelList]:
+        """Query along the watercourse edges of the bid configuration water.
 
         Args:
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of partial edges to return. Defaults to 25. Set to -1, float("inf") or None
+            limit: Maximum number of watercourse edges to return. Defaults to 25. Set to -1, float("inf") or None
                 to return all items.
-            retrieve_price_area: Whether to retrieve the price area for each bid document or not.
-            retrieve_method: Whether to retrieve the method for each bid document or not.
-            retrieve_total: Whether to retrieve the total for each bid document or not.
+            retrieve_market_configuration: Whether to retrieve the market configuration for each bid configuration water or not.
+            retrieve_method: Whether to retrieve the method for each bid configuration water or not.
+            retrieve_price_area: Whether to retrieve the price area for each bid configuration water or not.
 
         Returns:
-            BidMatrixQueryAPI: The query API for the bid matrix.
+            WatercourseQueryAPI: The query API for the watercourse.
         """
-        from .bid_matrix_query import BidMatrixQueryAPI
+        from .watercourse_query import WatercourseQueryAPI
 
         from_ = self._builder[-1].name
 
         edge_filter = _create_edge_filter(
-            dm.DirectRelationReference("power-ops-types", "partialBid"),
+            dm.DirectRelationReference("sp_powerops_types", "BidConfiguration.watercourses"),
             external_id_prefix=external_id_prefix,
             space=space,
         )
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("partials"),
+                name=self._builder.next_name("watercourses"),
                 expression=dm.query.EdgeResultSetExpression(
                     filter=edge_filter,
                     from_=from_,
                     direction="outwards",
                 ),
                 select=dm.query.Select(),
                 max_retrieve_limit=limit,
             )
         )
-        if retrieve_price_area:
-            self._query_append_price_area(from_)
+        if retrieve_market_configuration:
+            self._query_append_market_configuration(from_)
         if retrieve_method:
             self._query_append_method(from_)
-        if retrieve_total:
-            self._query_append_total(from_)
-        return BidMatrixQueryAPI(self._client, self._builder, self._view_by_read_class, None, limit)
+        if retrieve_price_area:
+            self._query_append_price_area(from_)
+        return WatercourseQueryAPI(self._client, self._builder, self._view_by_read_class, None, limit)
 
     def query(
         self,
-        retrieve_price_area: bool = False,
+        retrieve_market_configuration: bool = False,
         retrieve_method: bool = False,
-        retrieve_total: bool = False,
+        retrieve_price_area: bool = False,
     ) -> T_DomainModelList:
         """Execute query and return the result.
 
         Args:
-            retrieve_price_area: Whether to retrieve the price area for each bid document or not.
-            retrieve_method: Whether to retrieve the method for each bid document or not.
-            retrieve_total: Whether to retrieve the total for each bid document or not.
+            retrieve_market_configuration: Whether to retrieve the market configuration for each bid configuration water or not.
+            retrieve_method: Whether to retrieve the method for each bid configuration water or not.
+            retrieve_price_area: Whether to retrieve the price area for each bid configuration water or not.
 
         Returns:
             The list of the source nodes of the query.
 
         """
         from_ = self._builder[-1].name
-        if retrieve_price_area:
-            self._query_append_price_area(from_)
+        if retrieve_market_configuration:
+            self._query_append_market_configuration(from_)
         if retrieve_method:
             self._query_append_method(from_)
-        if retrieve_total:
-            self._query_append_total(from_)
+        if retrieve_price_area:
+            self._query_append_price_area(from_)
         return self._query()
 
-    def _query_append_price_area(self, from_: str) -> None:
-        view_id = self._view_by_read_class[PriceArea]
+    def _query_append_market_configuration(self, from_: str) -> None:
+        view_id = self._view_by_read_class[MarketConfiguration]
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("price_area"),
+                name=self._builder.next_name("market_configuration"),
                 expression=dm.query.NodeResultSetExpression(
                     filter=dm.filters.HasData(views=[view_id]),
                     from_=from_,
-                    through=self._view_by_read_class[BidDocument].as_property_ref("priceArea"),
+                    through=self._view_by_read_class[BidConfigurationWater].as_property_ref("marketConfiguration"),
                     direction="outwards",
                 ),
                 select=dm.query.Select([dm.query.SourceSelector(view_id, ["*"])]),
                 max_retrieve_limit=-1,
-                result_cls=PriceArea,
+                result_cls=MarketConfiguration,
             ),
         )
 
     def _query_append_method(self, from_: str) -> None:
-        view_id = self._view_by_read_class[BidMethod]
+        view_id = self._view_by_read_class[BidMethodWaterValue]
         self._builder.append(
             QueryStep(
                 name=self._builder.next_name("method"),
                 expression=dm.query.NodeResultSetExpression(
                     filter=dm.filters.HasData(views=[view_id]),
                     from_=from_,
-                    through=self._view_by_read_class[BidDocument].as_property_ref("method"),
+                    through=self._view_by_read_class[BidConfigurationWater].as_property_ref("method"),
                     direction="outwards",
                 ),
                 select=dm.query.Select([dm.query.SourceSelector(view_id, ["*"])]),
                 max_retrieve_limit=-1,
-                result_cls=BidMethod,
+                result_cls=BidMethodWaterValue,
             ),
         )
 
-    def _query_append_total(self, from_: str) -> None:
-        view_id = self._view_by_read_class[BidMatrix]
+    def _query_append_price_area(self, from_: str) -> None:
+        view_id = self._view_by_read_class[PriceArea]
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("total"),
+                name=self._builder.next_name("price_area"),
                 expression=dm.query.NodeResultSetExpression(
                     filter=dm.filters.HasData(views=[view_id]),
                     from_=from_,
-                    through=self._view_by_read_class[BidDocument].as_property_ref("total"),
+                    through=self._view_by_read_class[BidConfigurationWater].as_property_ref("priceArea"),
                     direction="outwards",
                 ),
                 select=dm.query.Select([dm.query.SourceSelector(view_id, ["*"])]),
                 max_retrieve_limit=-1,
-                result_cls=BidMatrix,
+                result_cls=PriceArea,
             ),
         )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/bid_matrix.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/bid_matrix.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/bid_matrix_alerts.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/bid_matrix_alerts.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/bid_matrix_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/custom_bid_matrix_query.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,74 +1,74 @@
 from __future__ import annotations
 
 import datetime
 from typing import TYPE_CHECKING
 
 from cognite.client import data_modeling as dm, CogniteClient
 
-from cognite.powerops.client._generated.day_ahead_bid.data_classes import (
+from cognite.powerops.client._generated.v1.data_classes import (
     DomainModelCore,
-    BidMatrix,
-    BidMethod,
+    CustomBidMatrix,
+    BidMethodCustom,
 )
 from ._core import DEFAULT_QUERY_LIMIT, QueryBuilder, QueryStep, QueryAPI, T_DomainModelList, _create_edge_filter
 
 if TYPE_CHECKING:
     from .alert_query import AlertQueryAPI
 
 
-class BidMatrixQueryAPI(QueryAPI[T_DomainModelList]):
+class CustomBidMatrixQueryAPI(QueryAPI[T_DomainModelList]):
     def __init__(
         self,
         client: CogniteClient,
         builder: QueryBuilder[T_DomainModelList],
         view_by_read_class: dict[type[DomainModelCore], dm.ViewId],
         filter_: dm.filters.Filter | None = None,
         limit: int = DEFAULT_QUERY_LIMIT,
     ):
         super().__init__(client, builder, view_by_read_class)
 
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("bid_matrix"),
+                name=self._builder.next_name("custom_bid_matrix"),
                 expression=dm.query.NodeResultSetExpression(
                     from_=self._builder[-1].name if self._builder else None,
                     filter=filter_,
                 ),
-                select=dm.query.Select([dm.query.SourceSelector(self._view_by_read_class[BidMatrix], ["*"])]),
-                result_cls=BidMatrix,
+                select=dm.query.Select([dm.query.SourceSelector(self._view_by_read_class[CustomBidMatrix], ["*"])]),
+                result_cls=CustomBidMatrix,
                 max_retrieve_limit=limit,
             )
         )
 
     def alerts(
         self,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_QUERY_LIMIT,
         retrieve_method: bool = False,
     ) -> AlertQueryAPI[T_DomainModelList]:
-        """Query along the alert edges of the bid matrix.
+        """Query along the alert edges of the custom bid matrix.
 
         Args:
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of alert edges to return. Defaults to 25. Set to -1, float("inf") or None
                 to return all items.
-            retrieve_method: Whether to retrieve the method for each bid matrix or not.
+            retrieve_method: Whether to retrieve the method for each custom bid matrix or not.
 
         Returns:
             AlertQueryAPI: The query API for the alert.
         """
         from .alert_query import AlertQueryAPI
 
         from_ = self._builder[-1].name
 
         edge_filter = _create_edge_filter(
-            dm.DirectRelationReference("power-ops-types", "calculationIssue"),
+            dm.DirectRelationReference("sp_powerops_types", "calculationIssue"),
             external_id_prefix=external_id_prefix,
             space=space,
         )
         self._builder.append(
             QueryStep(
                 name=self._builder.next_name("alerts"),
                 expression=dm.query.EdgeResultSetExpression(
@@ -87,34 +87,34 @@
     def query(
         self,
         retrieve_method: bool = False,
     ) -> T_DomainModelList:
         """Execute query and return the result.
 
         Args:
-            retrieve_method: Whether to retrieve the method for each bid matrix or not.
+            retrieve_method: Whether to retrieve the method for each custom bid matrix or not.
 
         Returns:
             The list of the source nodes of the query.
 
         """
         from_ = self._builder[-1].name
         if retrieve_method:
             self._query_append_method(from_)
         return self._query()
 
     def _query_append_method(self, from_: str) -> None:
-        view_id = self._view_by_read_class[BidMethod]
+        view_id = self._view_by_read_class[BidMethodCustom]
         self._builder.append(
             QueryStep(
                 name=self._builder.next_name("method"),
                 expression=dm.query.NodeResultSetExpression(
                     filter=dm.filters.HasData(views=[view_id]),
                     from_=from_,
-                    through=self._view_by_read_class[BidMatrix].as_property_ref("method"),
+                    through=self._view_by_read_class[CustomBidMatrix].as_property_ref("method"),
                     direction="outwards",
                 ),
                 select=dm.query.Select([dm.query.SourceSelector(view_id, ["*"])]),
                 max_retrieve_limit=-1,
-                result_cls=BidMethod,
+                result_cls=BidMethodCustom,
             ),
         )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/bid_method.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/bid_method.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/bid_method_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/bid_method_query.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/multi_scenario_matrix.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/multi_scenario_matrix.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/multi_scenario_matrix_alerts.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/multi_scenario_matrix_alerts.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/multi_scenario_matrix_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/multi_scenario_matrix_query.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,24 +1,24 @@
 from __future__ import annotations
 
 import datetime
 from typing import TYPE_CHECKING
 
 from cognite.client import data_modeling as dm, CogniteClient
 
-from cognite.powerops.client._generated.day_ahead_bid.data_classes import (
+from cognite.powerops.client._generated.v1.data_classes import (
     DomainModelCore,
     MultiScenarioMatrix,
-    BidMethod,
+    BidMethodSHOPMultiScenario,
 )
 from ._core import DEFAULT_QUERY_LIMIT, QueryBuilder, QueryStep, QueryAPI, T_DomainModelList, _create_edge_filter
 
 if TYPE_CHECKING:
     from .alert_query import AlertQueryAPI
-    from .shop_price_scenario_result_query import SHOPPriceScenarioResultQueryAPI
+    from .price_prod_case_query import PriceProdCaseQueryAPI
 
 
 class MultiScenarioMatrixQueryAPI(QueryAPI[T_DomainModelList]):
     def __init__(
         self,
         client: CogniteClient,
         builder: QueryBuilder[T_DomainModelList],
@@ -61,15 +61,15 @@
             AlertQueryAPI: The query API for the alert.
         """
         from .alert_query import AlertQueryAPI
 
         from_ = self._builder[-1].name
 
         edge_filter = _create_edge_filter(
-            dm.DirectRelationReference("power-ops-types", "calculationIssue"),
+            dm.DirectRelationReference("sp_powerops_types", "calculationIssue"),
             external_id_prefix=external_id_prefix,
             space=space,
         )
         self._builder.append(
             QueryStep(
                 name=self._builder.next_name("alerts"),
                 expression=dm.query.EdgeResultSetExpression(
@@ -87,33 +87,33 @@
 
     def scenario_results(
         self,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_QUERY_LIMIT,
         retrieve_method: bool = False,
-    ) -> SHOPPriceScenarioResultQueryAPI[T_DomainModelList]:
+    ) -> PriceProdCaseQueryAPI[T_DomainModelList]:
         """Query along the scenario result edges of the multi scenario matrix.
 
         Args:
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of scenario result edges to return. Defaults to 25. Set to -1, float("inf") or None
                 to return all items.
             retrieve_method: Whether to retrieve the method for each multi scenario matrix or not.
 
         Returns:
-            SHOPPriceScenarioResultQueryAPI: The query API for the shop price scenario result.
+            PriceProdCaseQueryAPI: The query API for the price prod case.
         """
-        from .shop_price_scenario_result_query import SHOPPriceScenarioResultQueryAPI
+        from .price_prod_case_query import PriceProdCaseQueryAPI
 
         from_ = self._builder[-1].name
 
         edge_filter = _create_edge_filter(
-            dm.DirectRelationReference("power-ops-types", "scenarioResult"),
+            dm.DirectRelationReference("sp_powerops_types", "MultiScenarioMatrix.scenarioResults"),
             external_id_prefix=external_id_prefix,
             space=space,
         )
         self._builder.append(
             QueryStep(
                 name=self._builder.next_name("scenario_results"),
                 expression=dm.query.EdgeResultSetExpression(
@@ -123,15 +123,15 @@
                 ),
                 select=dm.query.Select(),
                 max_retrieve_limit=limit,
             )
         )
         if retrieve_method:
             self._query_append_method(from_)
-        return SHOPPriceScenarioResultQueryAPI(self._client, self._builder, self._view_by_read_class, None, limit)
+        return PriceProdCaseQueryAPI(self._client, self._builder, self._view_by_read_class, None, limit)
 
     def query(
         self,
         retrieve_method: bool = False,
     ) -> T_DomainModelList:
         """Execute query and return the result.
 
@@ -144,22 +144,22 @@
         """
         from_ = self._builder[-1].name
         if retrieve_method:
             self._query_append_method(from_)
         return self._query()
 
     def _query_append_method(self, from_: str) -> None:
-        view_id = self._view_by_read_class[BidMethod]
+        view_id = self._view_by_read_class[BidMethodSHOPMultiScenario]
         self._builder.append(
             QueryStep(
                 name=self._builder.next_name("method"),
                 expression=dm.query.NodeResultSetExpression(
                     filter=dm.filters.HasData(views=[view_id]),
                     from_=from_,
                     through=self._view_by_read_class[MultiScenarioMatrix].as_property_ref("method"),
                     direction="outwards",
                 ),
                 select=dm.query.Select([dm.query.SourceSelector(view_id, ["*"])]),
                 max_retrieve_limit=-1,
-                result_cls=BidMethod,
+                result_cls=BidMethodSHOPMultiScenario,
             ),
         )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/multi_scenario_matrix_scenario_results.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/multi_scenario_matrix_scenario_results.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/price_area.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/price_area.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/price_area_main_scenario.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/price_area_main_scenario.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/price_area_price_scenarios.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/price_area_price_scenarios.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/price_area_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/price_area_query.py`

 * *Files 8% similar despite different names*

```diff
@@ -65,9 +65,10 @@
                     from_=from_,
                     through=self._view_by_read_class[PriceArea].as_property_ref("defaultMethod"),
                     direction="outwards",
                 ),
                 select=dm.query.Select([dm.query.SourceSelector(view_id, ["*"])]),
                 max_retrieve_limit=-1,
                 result_cls=BidMethod,
+                is_single_direct_relation=True,
             ),
         )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/shop_multi_scenario_method.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/shop_multi_scenario_method.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/shop_multi_scenario_method_price_scenarios.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/shop_multi_scenario_method_price_scenarios.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/shop_multi_scenario_method_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_method_shop_multi_scenario_query.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,89 +1,89 @@
 from __future__ import annotations
 
 import datetime
 from typing import TYPE_CHECKING
 
 from cognite.client import data_modeling as dm, CogniteClient
 
-from cognite.powerops.client._generated.day_ahead_bid.data_classes import (
+from cognite.powerops.client._generated.v1.data_classes import (
     DomainModelCore,
-    SHOPMultiScenarioMethod,
+    BidMethodSHOPMultiScenario,
 )
 from ._core import DEFAULT_QUERY_LIMIT, QueryBuilder, QueryStep, QueryAPI, T_DomainModelList, _create_edge_filter
 
 if TYPE_CHECKING:
-    from .shop_price_scenario_query import SHOPPriceScenarioQueryAPI
+    from .scenario_query import ScenarioQueryAPI
 
 
-class SHOPMultiScenarioMethodQueryAPI(QueryAPI[T_DomainModelList]):
+class BidMethodSHOPMultiScenarioQueryAPI(QueryAPI[T_DomainModelList]):
     def __init__(
         self,
         client: CogniteClient,
         builder: QueryBuilder[T_DomainModelList],
         view_by_read_class: dict[type[DomainModelCore], dm.ViewId],
         filter_: dm.filters.Filter | None = None,
         limit: int = DEFAULT_QUERY_LIMIT,
     ):
         super().__init__(client, builder, view_by_read_class)
 
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("shop_multi_scenario_method"),
+                name=self._builder.next_name("bid_method_shop_multi_scenario"),
                 expression=dm.query.NodeResultSetExpression(
                     from_=self._builder[-1].name if self._builder else None,
                     filter=filter_,
                 ),
                 select=dm.query.Select(
-                    [dm.query.SourceSelector(self._view_by_read_class[SHOPMultiScenarioMethod], ["*"])]
+                    [dm.query.SourceSelector(self._view_by_read_class[BidMethodSHOPMultiScenario], ["*"])]
                 ),
-                result_cls=SHOPMultiScenarioMethod,
+                result_cls=BidMethodSHOPMultiScenario,
                 max_retrieve_limit=limit,
             )
         )
 
-    def price_scenarios(
+    def scenarios(
         self,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_QUERY_LIMIT,
-    ) -> SHOPPriceScenarioQueryAPI[T_DomainModelList]:
-        """Query along the price scenario edges of the shop multi scenario method.
+    ) -> ScenarioQueryAPI[T_DomainModelList]:
+        """Query along the scenario edges of the bid method shop multi scenario.
 
         Args:
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of price scenario edges to return. Defaults to 25. Set to -1, float("inf") or None
+            limit: Maximum number of scenario edges to return. Defaults to 25. Set to -1, float("inf") or None
                 to return all items.
 
         Returns:
-            SHOPPriceScenarioQueryAPI: The query API for the shop price scenario.
+            ScenarioQueryAPI: The query API for the scenario.
         """
-        from .shop_price_scenario_query import SHOPPriceScenarioQueryAPI
+        from .scenario_query import ScenarioQueryAPI
 
         from_ = self._builder[-1].name
 
         edge_filter = _create_edge_filter(
-            dm.DirectRelationReference("power-ops-types", "PriceScenario"),
+            dm.DirectRelationReference("sp_powerops_types", "BidMethodDayahead.scenarios"),
             external_id_prefix=external_id_prefix,
             space=space,
         )
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("price_scenarios"),
+                name=self._builder.next_name("scenarios"),
                 expression=dm.query.EdgeResultSetExpression(
                     filter=edge_filter,
                     from_=from_,
                     direction="outwards",
                 ),
                 select=dm.query.Select(),
                 max_retrieve_limit=limit,
             )
         )
-        return SHOPPriceScenarioQueryAPI(self._client, self._builder, self._view_by_read_class, None, limit)
+        return ScenarioQueryAPI(self._client, self._builder, self._view_by_read_class, None, limit)
 
     def query(
         self,
     ) -> T_DomainModelList:
         """Execute query and return the result.
 
         Returns:
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/shop_price_scenario.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/shop_price_scenario.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/shop_price_scenario_price.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/shop_price_scenario_price.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/shop_price_scenario_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/shop_price_scenario_query.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/shop_price_scenario_result.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/shop_price_scenario_result.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/shop_price_scenario_result_price.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/shop_price_scenario_result_price.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/shop_price_scenario_result_production.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/shop_price_scenario_result_production.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/shop_price_scenario_result_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/shop_price_scenario_result_query.py`

 * *Files 2% similar despite different names*

```diff
@@ -67,9 +67,10 @@
                     from_=from_,
                     through=self._view_by_read_class[SHOPPriceScenarioResult].as_property_ref("priceScenario"),
                     direction="outwards",
                 ),
                 select=dm.query.Select([dm.query.SourceSelector(view_id, ["*"])]),
                 max_retrieve_limit=-1,
                 result_cls=SHOPPriceScenario,
+                is_single_direct_relation=True,
             ),
         )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/water_value_based_method.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/water_value_based_method.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api/water_value_based_method_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/water_value_based_method_query.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/_api_client.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api_client.py`

 * *Files 1% similar despite different names*

```diff
@@ -25,15 +25,15 @@
 
 
 class DayAheadBidAPI:
     """
     DayAheadBidAPI
 
     Generated with:
-        pygen = 0.99.14
+        pygen = 0.99.17
         cognite-sdk = 7.26.2
         pydantic = 2.6.4
 
     Data Model:
         space: power-ops-day-ahead-bid
         externalId: DayAheadBid
         version: 1
@@ -43,15 +43,15 @@
         if isinstance(config_or_client, CogniteClient):
             client = config_or_client
         elif isinstance(config_or_client, ClientConfig):
             client = CogniteClient(config_or_client)
         else:
             raise ValueError(f"Expected CogniteClient or ClientConfig, got {type(config_or_client)}")
         # The client name is used for aggregated logging of Pygen Usage
-        client.config.client_name = "CognitePygen:0.99.14"
+        client.config.client_name = "CognitePygen:0.99.17"
 
         view_by_read_class = {
             data_classes.Alert: dm.ViewId("power-ops-shared", "Alert", "1"),
             data_classes.BasicBidMatrix: dm.ViewId("power-ops-day-ahead-bid", "BasicBidMatrix", "1"),
             data_classes.BidDocument: dm.ViewId("power-ops-day-ahead-bid", "BidDocument", "1"),
             data_classes.BidMatrix: dm.ViewId("power-ops-day-ahead-bid", "BidMatrix", "1"),
             data_classes.BidMethod: dm.ViewId("power-ops-day-ahead-bid", "BidMethod", "1"),
@@ -165,16 +165,16 @@
         Returns:
             The instance(s), i.e., nodes and edges which has been deleted. Empty list if nothing was deleted.
 
         Examples:
 
             Delete item by id:
 
-                >>> from omni import OmniClient
-                >>> client = OmniClient()
+                >>> from cognite.powerops.client._generated.day_ahead_bid import DayAheadBidAPI
+                >>> client = DayAheadBidAPI()
                 >>> client.delete("my_node_external_id")
         """
         if isinstance(external_id, str):
             return self._client.data_modeling.instances.delete(nodes=(space, external_id))
         else:
             return self._client.data_modeling.instances.delete(
                 nodes=[(space, id) for id in external_id],
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/data_classes/__init__.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/data_classes/__init__.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/data_classes/_alert.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/data_classes/_alert.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/data_classes/_basic_bid_matrix.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/data_classes/_basic_bid_matrix.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/data_classes/_bid_document.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/data_classes/_bid_document.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/data_classes/_bid_matrix.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/data_classes/_bid_matrix.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/data_classes/_bid_method.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/data_classes/_bid_method.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/data_classes/_core.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/data_classes/_core.py`

 * *Files 2% similar despite different names*

```diff
@@ -462,32 +462,37 @@
         )
 
 
 T_DomainRelation = TypeVar("T_DomainRelation", bound=DomainRelation)
 
 
 def default_edge_external_id_factory(
-    start_node: DomainModelWrite | str, end_node: DomainModelWrite | str, edge_type: dm.DirectRelationReference
+    start_node: DomainModelWrite | str | dm.NodeId,
+    end_node: DomainModelWrite | str | dm.NodeId,
+    edge_type: dm.DirectRelationReference,
 ) -> str:
     start = start_node if isinstance(start_node, str) else start_node.external_id
     end = end_node if isinstance(end_node, str) else end_node.external_id
     return f"{start}:{end}"
 
 
 class DomainRelationWrite(BaseModel, extra=Extra.forbid, populate_by_name=True):
     external_id_factory: ClassVar[
-        Callable[[Union[DomainModelWrite, str], Union[DomainModelWrite, str], dm.DirectRelationReference], str]
+        Callable[
+            [
+                Union[DomainModelWrite, str, dm.NodeId],
+                Union[DomainModelWrite, str, dm.NodeId],
+                dm.DirectRelationReference,
+            ],
+            str,
+        ]
     ] = default_edge_external_id_factory
     data_record: DataRecordWrite = Field(default_factory=DataRecordWrite)
     external_id: Optional[str] = Field(None, min_length=1, max_length=255)
 
-    @property
-    def data_records(self) -> DataRecordWriteList:
-        return DataRecordWriteList([node.data_record for node in self])
-
     @abstractmethod
     def _to_instances_write(
         self,
         cache: set[tuple[str, str]],
         start_node: DomainModelWrite,
         edge_type: dm.DirectRelationReference,
         view_by_read_class: dict[type[DomainModelCore], dm.ViewId] | None,
@@ -569,24 +574,32 @@
                 write_none,
                 allow_version_increase,
             )
             resources.extend(other_resources)
 
         return resources
 
+    @classmethod
+    def reset_external_id_factory(cls) -> None:
+        cls.external_id_factory = default_edge_external_id_factory
+
 
 T_DomainRelationWrite = TypeVar("T_DomainRelationWrite", bound=DomainRelationWrite)
 
 
 class DomainRelationList(CoreList[T_DomainRelation]):
     _PARENT_CLASS = DomainRelation
 
     def as_edge_ids(self) -> list[dm.EdgeId]:
         return [edge.as_id() for edge in self]
 
+    @property
+    def data_records(self) -> DataRecordWriteList:
+        return DataRecordWriteList([connection.data_record for connection in self])
+
 
 T_DomainRelationList = TypeVar("T_DomainRelationList", bound=DomainRelationList)
 
 
 def unpack_properties(properties: Properties) -> Mapping[str, PropertyValue]:
     unpacked: dict[str, PropertyValue] = {}
     for view_properties in properties.values():
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/data_classes/_multi_scenario_matrix.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/data_classes/_multi_scenario_matrix.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/data_classes/_price_area.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/data_classes/_price_area.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/data_classes/_shop_multi_scenario_method.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/data_classes/_shop_multi_scenario_method.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/data_classes/_shop_price_scenario.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/data_classes/_shop_price_scenario.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/data_classes/_shop_price_scenario_result.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/data_classes/_shop_price_scenario_result.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/day_ahead_bid/data_classes/_water_value_based_method.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/data_classes/_water_value_based_method.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/_core.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/_core.py`

 * *Files 6% similar despite different names*

```diff
@@ -478,14 +478,15 @@
 class QueryStep:
     # Setup Variables
     name: str
     expression: dm.query.ResultSetExpression
     max_retrieve_limit: int
     select: dm.query.Select
     result_cls: type[DomainModelCore] | None = None
+    is_single_direct_relation: bool = False
 
     # Query Variables
     cursor: str | None = None
     total_retrieved: int = 0
     results: list[Instance] = field(default_factory=list)
     last_batch_count: int = 0
 
@@ -495,14 +496,25 @@
         else:
             self.expression.limit = max(min(INSTANCE_QUERY_LIMIT, self.max_retrieve_limit - self.total_retrieved), 0)
 
     @property
     def is_unlimited(self) -> bool:
         return self.max_retrieve_limit in {None, -1, math.inf}
 
+    @property
+    def is_finished(self) -> bool:
+        return (
+            (not self.is_unlimited and self.total_retrieved >= self.max_retrieve_limit)
+            or self.cursor is None
+            or self.last_batch_count == 0
+            # Single direct relations are dependent on the parent node,
+            # so we assume that the parent node is the limiting factor.
+            or self.is_single_direct_relation
+        )
+
 
 class QueryBuilder(UserList, Generic[T_DomainModelList]):
     # The unique string is in case the data model has a field that ends with _\d+. This will make sure we don't
     # clean the name of the field.
     _unique_str = "a418"
     _name_pattern = re.compile(r"_a418\d+$")
 
@@ -565,20 +577,15 @@
             expression.last_batch_count = len(batch[expression.name])
             expression.total_retrieved += expression.last_batch_count
             expression.cursor = batch.cursors.get(expression.name)
             expression.results.extend(batch[expression.name].data)
 
     @property
     def is_finished(self):
-        return all(
-            (not expression.is_unlimited and expression.total_retrieved >= expression.max_retrieve_limit)
-            or expression.cursor is None
-            or expression.last_batch_count == 0
-            for expression in self
-        )
+        return all(expression.is_finished for expression in self)
 
     def unpack(self) -> T_DomainModelList:
         nodes_by_type: dict[str | None, dict[tuple[str, str], DomainModel]] = defaultdict(dict)
         edges_by_type_by_source_node: dict[tuple[str, str, str], dict[tuple[str, str], list[dm.Edge]]] = defaultdict(
             lambda: defaultdict(list)
         )
         relation_by_type_by_start_node: dict[tuple[str, str], dict[tuple[str, str], list[DomainRelation]]] = (
@@ -780,142 +787,136 @@
             else:
                 self._output.append(item)
         else:
             raise RuntimeError("Missing '__typename' in GraphQL response. Cannot determine the type of the response.")
 
 
 _GRAPHQL_DATA_CLASS_BY_DATA_MODEL_BY_TYPE = {
-    dm.DataModelId("sp_powerops_models", "compute_SHOPBasedDayAhead", "1"): {
-        "TaskDispatcherShopInput": data_classes.TaskDispatcherShopInputGraphQL,
-        "TaskDispatcherShopOutput": data_classes.TaskDispatcherShopOutputGraphQL,
+    dm.DataModelId("sp_powerops_models_temp", "compute_SHOPBasedDayAhead", "1"): {
+        "TaskDispatcherInput": data_classes.TaskDispatcherInputGraphQL,
+        "TaskDispatcherOutput": data_classes.TaskDispatcherOutputGraphQL,
         "PreprocessorInput": data_classes.PreprocessorInputGraphQL,
         "PreprocessorOutput": data_classes.PreprocessorOutputGraphQL,
         "SHOPTriggerInput": data_classes.SHOPTriggerInputGraphQL,
         "SHOPTriggerOutput": data_classes.SHOPTriggerOutputGraphQL,
-        "ShopPartialBidCalculationInput": data_classes.ShopPartialBidCalculationInputGraphQL,
-        "ShopPartialBidCalculationOutput": data_classes.ShopPartialBidCalculationOutputGraphQL,
-        "BidMatrixRaw": data_classes.BidMatrixRawGraphQL,
-        "MultiScenarioMatrixRaw": data_classes.MultiScenarioMatrixRawGraphQL,
+        "PartialBidMatrixCalculationInput": data_classes.PartialBidMatrixCalculationInputGraphQL,
+        "ShopPartialBidMatrixCalculationInput": data_classes.ShopPartialBidMatrixCalculationInputGraphQL,
+        "PartialBidMatrixCalculationOutput": data_classes.PartialBidMatrixCalculationOutputGraphQL,
+        "BidMatrix": data_classes.BidMatrixGraphQL,
         "MarketConfiguration": data_classes.MarketConfigurationGraphQL,
-        "BidMethodSHOPMultiScenario": data_classes.BidMethodSHOPMultiScenarioGraphQL,
         "Scenario": data_classes.ScenarioGraphQL,
+        "ScenarioSet": data_classes.ScenarioSetGraphQL,
         "Mapping": data_classes.MappingGraphQL,
         "ModelTemplate": data_classes.ModelTemplateGraphQL,
         "SHOPResult": data_classes.SHOPResultGraphQL,
         "Case": data_classes.CaseGraphQL,
-        "SHOPResultPriceProd": data_classes.SHOPResultPriceProdGraphQL,
         "Alert": data_classes.AlertGraphQL,
-        "PlantShop": data_classes.PlantShopGraphQL,
-        "WatercourseShop": data_classes.WatercourseShopGraphQL,
-        "BidConfigurationShop": data_classes.BidConfigurationShopGraphQL,
+        "BidConfiguration": data_classes.BidConfigurationGraphQL,
         "PriceArea": data_classes.PriceAreaGraphQL,
-        "PriceProdCase": data_classes.PriceProdCaseGraphQL,
+        "PriceProduction": data_classes.PriceProductionGraphQL,
         "SHOPTimeSeries": data_classes.SHOPTimeSeriesGraphQL,
         "Commands": data_classes.CommandsGraphQL,
+        "FunctionInput": data_classes.FunctionInputGraphQL,
+        "FunctionOutput": data_classes.FunctionOutputGraphQL,
+        "PowerAsset": data_classes.PowerAssetGraphQL,
+        "PartialBidConfiguration": data_classes.PartialBidConfigurationGraphQL,
+        "ShopBasedPartialBidConfiguration": data_classes.ShopBasedPartialBidConfigurationGraphQL,
     },
-    dm.DataModelId("sp_powerops_models", "compute_TotalBidCalculation", "1"): {
-        "BidMatrixRaw": data_classes.BidMatrixRawGraphQL,
-        "MultiScenarioMatrixRaw": data_classes.MultiScenarioMatrixRawGraphQL,
+    dm.DataModelId("sp_powerops_models_temp", "compute_TotalBidMatrixCalculation", "1"): {
         "BidMatrix": data_classes.BidMatrixGraphQL,
-        "MultiScenarioMatrix": data_classes.MultiScenarioMatrixGraphQL,
-        "PartialPostProcessingInput": data_classes.PartialPostProcessingInputGraphQL,
-        "PartialPostProcessingOutput": data_classes.PartialPostProcessingOutputGraphQL,
         "TotalBidMatrixCalculationInput": data_classes.TotalBidMatrixCalculationInputGraphQL,
         "TotalBidMatrixCalculationOutput": data_classes.TotalBidMatrixCalculationOutputGraphQL,
         "BidDocumentDayAhead": data_classes.BidDocumentDayAheadGraphQL,
         "PriceArea": data_classes.PriceAreaGraphQL,
-        "BidMethodDayAhead": data_classes.BidMethodDayAheadGraphQL,
-        "BidMethodWaterValue": data_classes.BidMethodWaterValueGraphQL,
-        "BidMethodSHOPMultiScenario": data_classes.BidMethodSHOPMultiScenarioGraphQL,
         "Alert": data_classes.AlertGraphQL,
         "SHOPResult": data_classes.SHOPResultGraphQL,
-        "SHOPResultPriceProd": data_classes.SHOPResultPriceProdGraphQL,
         "MarketConfiguration": data_classes.MarketConfigurationGraphQL,
         "Scenario": data_classes.ScenarioGraphQL,
         "ModelTemplate": data_classes.ModelTemplateGraphQL,
         "Mapping": data_classes.MappingGraphQL,
-        "WatercourseShop": data_classes.WatercourseShopGraphQL,
-        "PriceProdCase": data_classes.PriceProdCaseGraphQL,
+        "PriceProduction": data_classes.PriceProductionGraphQL,
         "Case": data_classes.CaseGraphQL,
         "SHOPTimeSeries": data_classes.SHOPTimeSeriesGraphQL,
         "Commands": data_classes.CommandsGraphQL,
+        "FunctionInput": data_classes.FunctionInputGraphQL,
+        "FunctionOutput": data_classes.FunctionOutputGraphQL,
+        "BidDocument": data_classes.BidDocumentGraphQL,
+        "PowerAsset": data_classes.PowerAssetGraphQL,
+        "PartialBidConfiguration": data_classes.PartialBidConfigurationGraphQL,
+        "BidConfiguration": data_classes.BidConfigurationGraphQL,
     },
-    dm.DataModelId("sp_powerops_models", "compute_WaterValueBasedDayAheadBid", "1"): {
-        "TaskDispatcherWaterInput": data_classes.TaskDispatcherWaterInputGraphQL,
-        "TaskDispatcherWaterOutput": data_classes.TaskDispatcherWaterOutputGraphQL,
-        "BidCalculationTask": data_classes.BidCalculationTaskGraphQL,
-        "WaterPartialBidCalculationInput": data_classes.WaterPartialBidCalculationInputGraphQL,
-        "WaterPartialBidCalculationOutput": data_classes.WaterPartialBidCalculationOutputGraphQL,
-        "BidMatrixRaw": data_classes.BidMatrixRawGraphQL,
-        "BidMethodWaterValue": data_classes.BidMethodWaterValueGraphQL,
+    dm.DataModelId("sp_powerops_models_temp", "compute_WaterValueBasedDayAheadBid", "1"): {
+        "TaskDispatcherInput": data_classes.TaskDispatcherInputGraphQL,
+        "TaskDispatcherOutput": data_classes.TaskDispatcherOutputGraphQL,
+        "PartialBidMatrixCalculationInput": data_classes.PartialBidMatrixCalculationInputGraphQL,
+        "WaterValueBasedPartialBidMatrixCalculationInput": data_classes.WaterValueBasedPartialBidMatrixCalculationInputGraphQL,
+        "PartialBidMatrixCalculationOutput": data_classes.PartialBidMatrixCalculationOutputGraphQL,
         "Plant": data_classes.PlantGraphQL,
-        "Watercourse": data_classes.WatercourseGraphQL,
         "Alert": data_classes.AlertGraphQL,
-        "BidConfigurationWater": data_classes.BidConfigurationWaterGraphQL,
+        "BidConfiguration": data_classes.BidConfigurationGraphQL,
         "PriceArea": data_classes.PriceAreaGraphQL,
         "MarketConfiguration": data_classes.MarketConfigurationGraphQL,
-        "Reservoir": data_classes.ReservoirGraphQL,
         "Generator": data_classes.GeneratorGraphQL,
         "GeneratorEfficiencyCurve": data_classes.GeneratorEfficiencyCurveGraphQL,
         "TurbineEfficiencyCurve": data_classes.TurbineEfficiencyCurveGraphQL,
+        "FunctionInput": data_classes.FunctionInputGraphQL,
+        "FunctionOutput": data_classes.FunctionOutputGraphQL,
+        "BidMatrix": data_classes.BidMatrixGraphQL,
+        "PowerAsset": data_classes.PowerAssetGraphQL,
+        "PartialBidConfiguration": data_classes.PartialBidConfigurationGraphQL,
+        "WaterValueBasedPartialBidConfiguration": data_classes.WaterValueBasedPartialBidConfigurationGraphQL,
     },
-    dm.DataModelId("sp_powerops_models", "config_DayAheadConfiguration", "1"): {
+    dm.DataModelId("sp_powerops_models_temp", "config_DayAheadConfiguration", "1"): {
         "BidConfiguration": data_classes.BidConfigurationGraphQL,
-        "BidConfigurationShop": data_classes.BidConfigurationShopGraphQL,
-        "BidConfigurationWater": data_classes.BidConfigurationWaterGraphQL,
         "MarketConfiguration": data_classes.MarketConfigurationGraphQL,
+        "PriceArea": data_classes.PriceAreaGraphQL,
+        "PartialBidConfiguration": data_classes.PartialBidConfigurationGraphQL,
+        "ShopBasedPartialBidConfiguration": data_classes.ShopBasedPartialBidConfigurationGraphQL,
+        "WaterValueBasedPartialBidConfiguration": data_classes.WaterValueBasedPartialBidConfigurationGraphQL,
         "Scenario": data_classes.ScenarioGraphQL,
+        "ScenarioSet": data_classes.ScenarioSetGraphQL,
         "Mapping": data_classes.MappingGraphQL,
         "ModelTemplate": data_classes.ModelTemplateGraphQL,
-        "Watercourse": data_classes.WatercourseGraphQL,
-        "WatercourseShop": data_classes.WatercourseShopGraphQL,
-        "Plant": data_classes.PlantGraphQL,
-        "PlantShop": data_classes.PlantShopGraphQL,
         "Generator": data_classes.GeneratorGraphQL,
-        "Reservoir": data_classes.ReservoirGraphQL,
         "TurbineEfficiencyCurve": data_classes.TurbineEfficiencyCurveGraphQL,
         "GeneratorEfficiencyCurve": data_classes.GeneratorEfficiencyCurveGraphQL,
-        "BidMethodDayAhead": data_classes.BidMethodDayAheadGraphQL,
-        "BidMethodWaterValue": data_classes.BidMethodWaterValueGraphQL,
-        "BidMethodSHOPMultiScenario": data_classes.BidMethodSHOPMultiScenarioGraphQL,
-        "PriceArea": data_classes.PriceAreaGraphQL,
-        "BidMethod": data_classes.BidMethodGraphQL,
         "Commands": data_classes.CommandsGraphQL,
+        "PowerAsset": data_classes.PowerAssetGraphQL,
+        "Plant": data_classes.PlantGraphQL,
     },
-    dm.DataModelId("sp_powerops_models", "frontend_AFRRBid", "1"): {
+    dm.DataModelId("sp_powerops_models_temp", "frontend_AFRRBid", "1"): {
         "BidDocumentAFRR": data_classes.BidDocumentAFRRGraphQL,
+        "BidDocument": data_classes.BidDocumentGraphQL,
         "BidRow": data_classes.BidRowGraphQL,
         "PriceAreaAFRR": data_classes.PriceAreaAFRRGraphQL,
-        "BidMethodAFRR": data_classes.BidMethodAFRRGraphQL,
+        "PriceArea": data_classes.PriceAreaGraphQL,
         "Alert": data_classes.AlertGraphQL,
+        "PowerAsset": data_classes.PowerAssetGraphQL,
     },
-    dm.DataModelId("sp_powerops_models", "frontend_Asset", "1"): {
-        "PriceAreaAsset": data_classes.PriceAreaAssetGraphQL,
-        "Watercourse": data_classes.WatercourseGraphQL,
+    dm.DataModelId("sp_powerops_models_temp", "frontend_Asset", "1"): {
+        "PriceArea": data_classes.PriceAreaGraphQL,
+        "PowerAsset": data_classes.PowerAssetGraphQL,
         "Plant": data_classes.PlantGraphQL,
         "Generator": data_classes.GeneratorGraphQL,
-        "Reservoir": data_classes.ReservoirGraphQL,
         "TurbineEfficiencyCurve": data_classes.TurbineEfficiencyCurveGraphQL,
         "GeneratorEfficiencyCurve": data_classes.GeneratorEfficiencyCurveGraphQL,
-        "BidMethodDayAhead": data_classes.BidMethodDayAheadGraphQL,
     },
-    dm.DataModelId("sp_powerops_models", "frontend_DayAheadBid", "1"): {
+    dm.DataModelId("sp_powerops_models_temp", "frontend_DayAheadBid", "1"): {
         "BidDocumentDayAhead": data_classes.BidDocumentDayAheadGraphQL,
+        "BidDocument": data_classes.BidDocumentGraphQL,
         "BidMatrix": data_classes.BidMatrixGraphQL,
-        "MultiScenarioMatrix": data_classes.MultiScenarioMatrixGraphQL,
-        "BasicBidMatrix": data_classes.BasicBidMatrixGraphQL,
-        "CustomBidMatrix": data_classes.CustomBidMatrixGraphQL,
-        "BidMethodCustom": data_classes.BidMethodCustomGraphQL,
-        "BidMethodDayAhead": data_classes.BidMethodDayAheadGraphQL,
         "PriceArea": data_classes.PriceAreaGraphQL,
-        "BidMethodSHOPMultiScenario": data_classes.BidMethodSHOPMultiScenarioGraphQL,
-        "BidMethodWaterValue": data_classes.BidMethodWaterValueGraphQL,
         "Alert": data_classes.AlertGraphQL,
         "Scenario": data_classes.ScenarioGraphQL,
         "ModelTemplate": data_classes.ModelTemplateGraphQL,
         "Mapping": data_classes.MappingGraphQL,
-        "WatercourseShop": data_classes.WatercourseShopGraphQL,
-        "PriceProdCase": data_classes.PriceProdCaseGraphQL,
+        "PriceProduction": data_classes.PriceProductionGraphQL,
         "Commands": data_classes.CommandsGraphQL,
         "Case": data_classes.CaseGraphQL,
+        "PowerAsset": data_classes.PowerAssetGraphQL,
+        "BidConfiguration": data_classes.BidConfigurationGraphQL,
+        "PartialBidConfiguration": data_classes.PartialBidConfigurationGraphQL,
+        "SHOPResult": data_classes.SHOPResultGraphQL,
+        "MarketConfiguration": data_classes.MarketConfigurationGraphQL,
+        "SHOPTimeSeries": data_classes.SHOPTimeSeriesGraphQL,
     },
 }
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/alert.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/alert.py`

 * *Files 6% similar despite different names*

```diff
@@ -50,14 +50,16 @@
         )
         self._view_id = view_id
 
     def __call__(
         self,
         min_time: datetime.datetime | None = None,
         max_time: datetime.datetime | None = None,
+        process_id: str | list[str] | None = None,
+        process_id_prefix: str | None = None,
         title: str | list[str] | None = None,
         title_prefix: str | None = None,
         description: str | list[str] | None = None,
         description_prefix: str | None = None,
         severity: str | list[str] | None = None,
         severity_prefix: str | None = None,
         alert_type: str | list[str] | None = None,
@@ -72,14 +74,16 @@
         filter: dm.Filter | None = None,
     ) -> AlertQueryAPI[AlertList]:
         """Query starting at alerts.
 
         Args:
             min_time: The minimum value of the time to filter on.
             max_time: The maximum value of the time to filter on.
+            process_id: The process id to filter on.
+            process_id_prefix: The prefix of the process id to filter on.
             title: The title to filter on.
             title_prefix: The prefix of the title to filter on.
             description: The description to filter on.
             description_prefix: The prefix of the description to filter on.
             severity: The severity to filter on.
             severity_prefix: The prefix of the severity to filter on.
             alert_type: The alert type to filter on.
@@ -98,14 +102,16 @@
 
         """
         has_data = dm.filters.HasData(views=[self._view_id])
         filter_ = _create_alert_filter(
             self._view_id,
             min_time,
             max_time,
+            process_id,
+            process_id_prefix,
             title,
             title_prefix,
             description,
             description_prefix,
             severity,
             severity_prefix,
             alert_type,
@@ -223,14 +229,16 @@
 
     def search(
         self,
         query: str,
         properties: AlertTextFields | Sequence[AlertTextFields] | None = None,
         min_time: datetime.datetime | None = None,
         max_time: datetime.datetime | None = None,
+        process_id: str | list[str] | None = None,
+        process_id_prefix: str | None = None,
         title: str | list[str] | None = None,
         title_prefix: str | None = None,
         description: str | list[str] | None = None,
         description_prefix: str | None = None,
         severity: str | list[str] | None = None,
         severity_prefix: str | None = None,
         alert_type: str | list[str] | None = None,
@@ -247,14 +255,16 @@
         """Search alerts
 
         Args:
             query: The search query,
             properties: The property to search, if nothing is passed all text fields will be searched.
             min_time: The minimum value of the time to filter on.
             max_time: The maximum value of the time to filter on.
+            process_id: The process id to filter on.
+            process_id_prefix: The prefix of the process id to filter on.
             title: The title to filter on.
             title_prefix: The prefix of the title to filter on.
             description: The description to filter on.
             description_prefix: The prefix of the description to filter on.
             severity: The severity to filter on.
             severity_prefix: The prefix of the severity to filter on.
             alert_type: The alert type to filter on.
@@ -280,14 +290,16 @@
                 >>> alerts = client.alert.search('my_alert')
 
         """
         filter_ = _create_alert_filter(
             self._view_id,
             min_time,
             max_time,
+            process_id,
+            process_id_prefix,
             title,
             title_prefix,
             description,
             description_prefix,
             severity,
             severity_prefix,
             alert_type,
@@ -313,14 +325,16 @@
         ),
         property: AlertFields | Sequence[AlertFields] | None = None,
         group_by: None = None,
         query: str | None = None,
         search_properties: AlertTextFields | Sequence[AlertTextFields] | None = None,
         min_time: datetime.datetime | None = None,
         max_time: datetime.datetime | None = None,
+        process_id: str | list[str] | None = None,
+        process_id_prefix: str | None = None,
         title: str | list[str] | None = None,
         title_prefix: str | None = None,
         description: str | list[str] | None = None,
         description_prefix: str | None = None,
         severity: str | list[str] | None = None,
         severity_prefix: str | None = None,
         alert_type: str | list[str] | None = None,
@@ -346,14 +360,16 @@
         ),
         property: AlertFields | Sequence[AlertFields] | None = None,
         group_by: AlertFields | Sequence[AlertFields] = None,
         query: str | None = None,
         search_properties: AlertTextFields | Sequence[AlertTextFields] | None = None,
         min_time: datetime.datetime | None = None,
         max_time: datetime.datetime | None = None,
+        process_id: str | list[str] | None = None,
+        process_id_prefix: str | None = None,
         title: str | list[str] | None = None,
         title_prefix: str | None = None,
         description: str | list[str] | None = None,
         description_prefix: str | None = None,
         severity: str | list[str] | None = None,
         severity_prefix: str | None = None,
         alert_type: str | list[str] | None = None,
@@ -378,14 +394,16 @@
         ),
         property: AlertFields | Sequence[AlertFields] | None = None,
         group_by: AlertFields | Sequence[AlertFields] | None = None,
         query: str | None = None,
         search_property: AlertTextFields | Sequence[AlertTextFields] | None = None,
         min_time: datetime.datetime | None = None,
         max_time: datetime.datetime | None = None,
+        process_id: str | list[str] | None = None,
+        process_id_prefix: str | None = None,
         title: str | list[str] | None = None,
         title_prefix: str | None = None,
         description: str | list[str] | None = None,
         description_prefix: str | None = None,
         severity: str | list[str] | None = None,
         severity_prefix: str | None = None,
         alert_type: str | list[str] | None = None,
@@ -405,14 +423,16 @@
             aggregate: The aggregation to perform.
             property: The property to perform aggregation on.
             group_by: The property to group by when doing the aggregation.
             query: The query to search for in the text field.
             search_property: The text field to search in.
             min_time: The minimum value of the time to filter on.
             max_time: The maximum value of the time to filter on.
+            process_id: The process id to filter on.
+            process_id_prefix: The prefix of the process id to filter on.
             title: The title to filter on.
             title_prefix: The prefix of the title to filter on.
             description: The description to filter on.
             description_prefix: The prefix of the description to filter on.
             severity: The severity to filter on.
             severity_prefix: The prefix of the severity to filter on.
             alert_type: The alert type to filter on.
@@ -439,14 +459,16 @@
 
         """
 
         filter_ = _create_alert_filter(
             self._view_id,
             min_time,
             max_time,
+            process_id,
+            process_id_prefix,
             title,
             title_prefix,
             description,
             description_prefix,
             severity,
             severity_prefix,
             alert_type,
@@ -475,14 +497,16 @@
         self,
         property: AlertFields,
         interval: float,
         query: str | None = None,
         search_property: AlertTextFields | Sequence[AlertTextFields] | None = None,
         min_time: datetime.datetime | None = None,
         max_time: datetime.datetime | None = None,
+        process_id: str | list[str] | None = None,
+        process_id_prefix: str | None = None,
         title: str | list[str] | None = None,
         title_prefix: str | None = None,
         description: str | list[str] | None = None,
         description_prefix: str | None = None,
         severity: str | list[str] | None = None,
         severity_prefix: str | None = None,
         alert_type: str | list[str] | None = None,
@@ -501,14 +525,16 @@
         Args:
             property: The property to use as the value in the histogram.
             interval: The interval to use for the histogram bins.
             query: The query to search for in the text field.
             search_property: The text field to search in.
             min_time: The minimum value of the time to filter on.
             max_time: The maximum value of the time to filter on.
+            process_id: The process id to filter on.
+            process_id_prefix: The prefix of the process id to filter on.
             title: The title to filter on.
             title_prefix: The prefix of the title to filter on.
             description: The description to filter on.
             description_prefix: The prefix of the description to filter on.
             severity: The severity to filter on.
             severity_prefix: The prefix of the severity to filter on.
             alert_type: The alert type to filter on.
@@ -526,14 +552,16 @@
             Bucketed histogram results.
 
         """
         filter_ = _create_alert_filter(
             self._view_id,
             min_time,
             max_time,
+            process_id,
+            process_id_prefix,
             title,
             title_prefix,
             description,
             description_prefix,
             severity,
             severity_prefix,
             alert_type,
@@ -557,14 +585,16 @@
             filter_,
         )
 
     def list(
         self,
         min_time: datetime.datetime | None = None,
         max_time: datetime.datetime | None = None,
+        process_id: str | list[str] | None = None,
+        process_id_prefix: str | None = None,
         title: str | list[str] | None = None,
         title_prefix: str | None = None,
         description: str | list[str] | None = None,
         description_prefix: str | None = None,
         severity: str | list[str] | None = None,
         severity_prefix: str | None = None,
         alert_type: str | list[str] | None = None,
@@ -579,14 +609,16 @@
         filter: dm.Filter | None = None,
     ) -> AlertList:
         """List/filter alerts
 
         Args:
             min_time: The minimum value of the time to filter on.
             max_time: The maximum value of the time to filter on.
+            process_id: The process id to filter on.
+            process_id_prefix: The prefix of the process id to filter on.
             title: The title to filter on.
             title_prefix: The prefix of the title to filter on.
             description: The description to filter on.
             description_prefix: The prefix of the description to filter on.
             severity: The severity to filter on.
             severity_prefix: The prefix of the severity to filter on.
             alert_type: The alert type to filter on.
@@ -612,14 +644,16 @@
                 >>> alerts = client.alert.list(limit=5)
 
         """
         filter_ = _create_alert_filter(
             self._view_id,
             min_time,
             max_time,
+            process_id,
+            process_id_prefix,
             title,
             title_prefix,
             description,
             description_prefix,
             severity,
             severity_prefix,
             alert_type,
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/alert_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/alert_query.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/basic_bid_matrix.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/basic_bid_matrix.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/basic_bid_matrix_alerts.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/basic_bid_matrix_alerts.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/basic_bid_matrix_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/partial_post_processing_input_query.py`

 * *Files 15% similar despite different names*

```diff
@@ -3,118 +3,120 @@
 import datetime
 from typing import TYPE_CHECKING
 
 from cognite.client import data_modeling as dm, CogniteClient
 
 from cognite.powerops.client._generated.v1.data_classes import (
     DomainModelCore,
-    BasicBidMatrix,
-    BidMethodDayAhead,
+    PartialPostProcessingInput,
+    MarketConfiguration,
 )
 from ._core import DEFAULT_QUERY_LIMIT, QueryBuilder, QueryStep, QueryAPI, T_DomainModelList, _create_edge_filter
 
 if TYPE_CHECKING:
-    from .alert_query import AlertQueryAPI
+    from .bid_matrix_raw_query import BidMatrixRawQueryAPI
 
 
-class BasicBidMatrixQueryAPI(QueryAPI[T_DomainModelList]):
+class PartialPostProcessingInputQueryAPI(QueryAPI[T_DomainModelList]):
     def __init__(
         self,
         client: CogniteClient,
         builder: QueryBuilder[T_DomainModelList],
         view_by_read_class: dict[type[DomainModelCore], dm.ViewId],
         filter_: dm.filters.Filter | None = None,
         limit: int = DEFAULT_QUERY_LIMIT,
     ):
         super().__init__(client, builder, view_by_read_class)
 
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("basic_bid_matrix"),
+                name=self._builder.next_name("partial_post_processing_input"),
                 expression=dm.query.NodeResultSetExpression(
                     from_=self._builder[-1].name if self._builder else None,
                     filter=filter_,
                 ),
-                select=dm.query.Select([dm.query.SourceSelector(self._view_by_read_class[BasicBidMatrix], ["*"])]),
-                result_cls=BasicBidMatrix,
+                select=dm.query.Select(
+                    [dm.query.SourceSelector(self._view_by_read_class[PartialPostProcessingInput], ["*"])]
+                ),
+                result_cls=PartialPostProcessingInput,
                 max_retrieve_limit=limit,
             )
         )
 
-    def alerts(
+    def partial_bid_matrices_raw(
         self,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_QUERY_LIMIT,
-        retrieve_method: bool = False,
-    ) -> AlertQueryAPI[T_DomainModelList]:
-        """Query along the alert edges of the basic bid matrix.
+        retrieve_market_config: bool = False,
+    ) -> BidMatrixRawQueryAPI[T_DomainModelList]:
+        """Query along the partial bid matrices raw edges of the partial post processing input.
 
         Args:
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of alert edges to return. Defaults to 25. Set to -1, float("inf") or None
+            limit: Maximum number of partial bid matrices raw edges to return. Defaults to 25. Set to -1, float("inf") or None
                 to return all items.
-            retrieve_method: Whether to retrieve the method for each basic bid matrix or not.
+            retrieve_market_config: Whether to retrieve the market config for each partial post processing input or not.
 
         Returns:
-            AlertQueryAPI: The query API for the alert.
+            BidMatrixRawQueryAPI: The query API for the bid matrix raw.
         """
-        from .alert_query import AlertQueryAPI
+        from .bid_matrix_raw_query import BidMatrixRawQueryAPI
 
         from_ = self._builder[-1].name
 
         edge_filter = _create_edge_filter(
-            dm.DirectRelationReference("sp_powerops_types", "calculationIssue"),
+            dm.DirectRelationReference("sp_powerops_types", "partialBidMatricesRaw"),
             external_id_prefix=external_id_prefix,
             space=space,
         )
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("alerts"),
+                name=self._builder.next_name("partial_bid_matrices_raw"),
                 expression=dm.query.EdgeResultSetExpression(
                     filter=edge_filter,
                     from_=from_,
                     direction="outwards",
                 ),
                 select=dm.query.Select(),
                 max_retrieve_limit=limit,
             )
         )
-        if retrieve_method:
-            self._query_append_method(from_)
-        return AlertQueryAPI(self._client, self._builder, self._view_by_read_class, None, limit)
+        if retrieve_market_config:
+            self._query_append_market_config(from_)
+        return BidMatrixRawQueryAPI(self._client, self._builder, self._view_by_read_class, None, limit)
 
     def query(
         self,
-        retrieve_method: bool = False,
+        retrieve_market_config: bool = False,
     ) -> T_DomainModelList:
         """Execute query and return the result.
 
         Args:
-            retrieve_method: Whether to retrieve the method for each basic bid matrix or not.
+            retrieve_market_config: Whether to retrieve the market config for each partial post processing input or not.
 
         Returns:
             The list of the source nodes of the query.
 
         """
         from_ = self._builder[-1].name
-        if retrieve_method:
-            self._query_append_method(from_)
+        if retrieve_market_config:
+            self._query_append_market_config(from_)
         return self._query()
 
-    def _query_append_method(self, from_: str) -> None:
-        view_id = self._view_by_read_class[BidMethodDayAhead]
+    def _query_append_market_config(self, from_: str) -> None:
+        view_id = self._view_by_read_class[MarketConfiguration]
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("method"),
+                name=self._builder.next_name("market_config"),
                 expression=dm.query.NodeResultSetExpression(
                     filter=dm.filters.HasData(views=[view_id]),
                     from_=from_,
-                    through=self._view_by_read_class[BasicBidMatrix].as_property_ref("method"),
+                    through=self._view_by_read_class[PartialPostProcessingInput].as_property_ref("marketConfig"),
                     direction="outwards",
                 ),
                 select=dm.query.Select([dm.query.SourceSelector(view_id, ["*"])]),
                 max_retrieve_limit=-1,
-                result_cls=BidMethodDayAhead,
+                result_cls=MarketConfiguration,
             ),
         )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_calculation_task.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_calculation_task.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_calculation_task_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_calculation_task_query.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_configuration.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_partial_bid_configuration.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,218 +1,288 @@
 from __future__ import annotations
 
-from collections.abc import Sequence
-from typing import overload
 import warnings
+from typing import Any, Literal, Optional, Union
 
-from cognite.client import CogniteClient
 from cognite.client import data_modeling as dm
+from pydantic import Field
+from pydantic import field_validator, model_validator
 
-from cognite.powerops.client._generated.v1.data_classes._core import DEFAULT_INSTANCE_SPACE
-from cognite.powerops.client._generated.v1.data_classes import (
+from ._core import (
+    DEFAULT_INSTANCE_SPACE,
+    DataRecord,
+    DataRecordGraphQL,
+    DataRecordWrite,
+    DomainModel,
     DomainModelCore,
     DomainModelWrite,
-    ResourcesWriteResult,
-    BidConfiguration,
-    BidConfigurationWrite,
-    BidConfigurationList,
-    BidConfigurationWriteList,
-)
-from cognite.powerops.client._generated.v1.data_classes._bid_configuration import (
-    _create_bid_configuration_filter,
-)
-from ._core import (
-    DEFAULT_LIMIT_READ,
-    DEFAULT_QUERY_LIMIT,
-    Aggregations,
-    NodeAPI,
-    SequenceNotStr,
-    QueryStep,
-    QueryBuilder,
+    DomainModelWriteList,
+    DomainModelList,
+    DomainRelationWrite,
+    GraphQLCore,
+    ResourcesWrite,
 )
-from .bid_configuration_query import BidConfigurationQueryAPI
 
 
-class BidConfigurationAPI(NodeAPI[BidConfiguration, BidConfigurationWrite, BidConfigurationList]):
-    def __init__(self, client: CogniteClient, view_by_read_class: dict[type[DomainModelCore], dm.ViewId]):
-        view_id = view_by_read_class[BidConfiguration]
-        super().__init__(
-            client=client,
-            sources=view_id,
-            class_type=BidConfiguration,
-            class_list=BidConfigurationList,
-            class_write_list=BidConfigurationWriteList,
-            view_by_read_class=view_by_read_class,
+__all__ = [
+    "PartialBidConfiguration",
+    "PartialBidConfigurationWrite",
+    "PartialBidConfigurationApply",
+    "PartialBidConfigurationList",
+    "PartialBidConfigurationWriteList",
+    "PartialBidConfigurationApplyList",
+    "PartialBidConfigurationFields",
+    "PartialBidConfigurationTextFields",
+]
+
+
+PartialBidConfigurationTextFields = Literal["name", "method"]
+PartialBidConfigurationFields = Literal["name", "method", "add_steps"]
+
+_PARTIALBIDCONFIGURATION_PROPERTIES_BY_FIELD = {
+    "name": "name",
+    "method": "method",
+    "add_steps": "addSteps",
+}
+
+
+class PartialBidConfigurationGraphQL(GraphQLCore):
+    """This represents the reading version of partial bid configuration, used
+    when data is retrieved from CDF using GraphQL.
+
+    It is used when retrieving data from CDF using GraphQL.
+
+    Args:
+        space: The space where the node is located.
+        external_id: The external id of the partial bid configuration.
+        data_record: The data record of the partial bid configuration node.
+        name: Name for the PartialBidConfiguration
+        method: Name of the method used for the bid calculation
+        add_steps: TODO definition
+    """
+
+    view_id = dm.ViewId("sp_powerops_models_temp", "PartialBidConfiguration", "1")
+    name: Optional[str] = None
+    method: Optional[str] = None
+    add_steps: Optional[bool] = Field(None, alias="addSteps")
+
+    @model_validator(mode="before")
+    def parse_data_record(cls, values: Any) -> Any:
+        if not isinstance(values, dict):
+            return values
+        if "lastUpdatedTime" in values or "createdTime" in values:
+            values["dataRecord"] = DataRecordGraphQL(
+                created_time=values.pop("createdTime", None),
+                last_updated_time=values.pop("lastUpdatedTime", None),
+            )
+        return values
+
+    def as_read(self) -> PartialBidConfiguration:
+        """Convert this GraphQL format of partial bid configuration to the reading format."""
+        if self.data_record is None:
+            raise ValueError("This object cannot be converted to a read format because it lacks a data record.")
+        return PartialBidConfiguration(
+            space=self.space,
+            external_id=self.external_id,
+            data_record=DataRecord(
+                version=0,
+                last_updated_time=self.data_record.last_updated_time,
+                created_time=self.data_record.created_time,
+            ),
+            name=self.name,
+            method=self.method,
+            add_steps=self.add_steps,
         )
-        self._view_id = view_id
 
-    def __call__(
-        self,
-        market_configuration: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
-        external_id_prefix: str | None = None,
-        space: str | list[str] | None = None,
-        limit: int | None = DEFAULT_QUERY_LIMIT,
-        filter: dm.Filter | None = None,
-    ) -> BidConfigurationQueryAPI[BidConfigurationList]:
-        """Query starting at bid configurations.
-
-        Args:
-            market_configuration: The market configuration to filter on.
-            external_id_prefix: The prefix of the external ID to filter on.
-            space: The space to filter on.
-            limit: Maximum number of bid configurations to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
-            filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
-
-        Returns:
-            A query API for bid configurations.
-
-        """
-        has_data = dm.filters.HasData(views=[self._view_id])
-        filter_ = _create_bid_configuration_filter(
-            self._view_id,
-            market_configuration,
-            external_id_prefix,
-            space,
-            (filter and dm.filters.And(filter, has_data)) or has_data,
+    def as_write(self) -> PartialBidConfigurationWrite:
+        """Convert this GraphQL format of partial bid configuration to the writing format."""
+        return PartialBidConfigurationWrite(
+            space=self.space,
+            external_id=self.external_id,
+            data_record=DataRecordWrite(existing_version=0),
+            name=self.name,
+            method=self.method,
+            add_steps=self.add_steps,
         )
-        builder = QueryBuilder(BidConfigurationList)
-        return BidConfigurationQueryAPI(self._client, builder, self._view_by_read_class, filter_, limit)
 
-    def apply(
-        self,
-        bid_configuration: BidConfigurationWrite | Sequence[BidConfigurationWrite],
-        replace: bool = False,
-        write_none: bool = False,
-    ) -> ResourcesWriteResult:
-        """Add or update (upsert) bid configurations.
 
-        Args:
-            bid_configuration: Bid configuration or sequence of bid configurations to upsert.
-            replace (bool): How do we behave when a property value exists? Do we replace all matching and existing values with the supplied values (true)?
-                Or should we merge in new values for properties together with the existing values (false)? Note: This setting applies for all nodes or edges specified in the ingestion call.
-            write_none (bool): This method, will by default, skip properties that are set to None. However, if you want to set properties to None,
-                you can set this parameter to True. Note this only applies to properties that are nullable.
-        Returns:
-            Created instance(s), i.e., nodes, edges, and time series.
-
-        Examples:
-
-            Create a new bid_configuration:
-
-                >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
-                >>> from cognite.powerops.client._generated.v1.data_classes import BidConfigurationWrite
-                >>> client = PowerOpsModelsV1Client()
-                >>> bid_configuration = BidConfigurationWrite(external_id="my_bid_configuration", ...)
-                >>> result = client.bid_configuration.apply(bid_configuration)
+class PartialBidConfiguration(DomainModel):
+    """This represents the reading version of partial bid configuration.
 
-        """
+    It is used to when data is retrieved from CDF.
+
+    Args:
+        space: The space where the node is located.
+        external_id: The external id of the partial bid configuration.
+        data_record: The data record of the partial bid configuration node.
+        name: Name for the PartialBidConfiguration
+        method: Name of the method used for the bid calculation
+        add_steps: TODO definition
+    """
+
+    space: str = DEFAULT_INSTANCE_SPACE
+    node_type: Union[dm.DirectRelationReference, None] = None
+    name: str
+    method: Optional[str] = None
+    add_steps: bool = Field(alias="addSteps")
+
+    def as_write(self) -> PartialBidConfigurationWrite:
+        """Convert this read version of partial bid configuration to the writing version."""
+        return PartialBidConfigurationWrite(
+            space=self.space,
+            external_id=self.external_id,
+            data_record=DataRecordWrite(existing_version=self.data_record.version),
+            name=self.name,
+            method=self.method,
+            add_steps=self.add_steps,
+        )
+
+    def as_apply(self) -> PartialBidConfigurationWrite:
+        """Convert this read version of partial bid configuration to the writing version."""
         warnings.warn(
-            "The .apply method is deprecated and will be removed in v1.0. "
-            "Please use the .upsert method on the client instead. This means instead of "
-            "`my_client.bid_configuration.apply(my_items)` please use `my_client.upsert(my_items)`."
-            "The motivation is that all apply methods are the same, and having one apply method per API "
-            " class encourages users to create items in small batches, which is inefficient."
-            "In addition, .upsert method is more descriptive of what the method does.",
+            "as_apply is deprecated and will be removed in v1.0. Use as_write instead.",
             UserWarning,
             stacklevel=2,
         )
-        return self._apply(bid_configuration, replace, write_none)
+        return self.as_write()
+
+
+class PartialBidConfigurationWrite(DomainModelWrite):
+    """This represents the writing version of partial bid configuration.
+
+    It is used to when data is sent to CDF.
+
+    Args:
+        space: The space where the node is located.
+        external_id: The external id of the partial bid configuration.
+        data_record: The data record of the partial bid configuration node.
+        name: Name for the PartialBidConfiguration
+        method: Name of the method used for the bid calculation
+        add_steps: TODO definition
+    """
+
+    space: str = DEFAULT_INSTANCE_SPACE
+    node_type: Union[dm.DirectRelationReference, None] = None
+    name: str
+    method: Optional[str] = None
+    add_steps: bool = Field(alias="addSteps")
+
+    def _to_instances_write(
+        self,
+        cache: set[tuple[str, str]],
+        view_by_read_class: dict[type[DomainModelCore], dm.ViewId] | None,
+        write_none: bool = False,
+        allow_version_increase: bool = False,
+    ) -> ResourcesWrite:
+        resources = ResourcesWrite()
+        if self.as_tuple_id() in cache:
+            return resources
+
+        write_view = (view_by_read_class or {}).get(
+            PartialBidConfiguration, dm.ViewId("sp_powerops_models_temp", "PartialBidConfiguration", "1")
+        )
+
+        properties: dict[str, Any] = {}
 
-    def delete(
-        self, external_id: str | SequenceNotStr[str], space: str = DEFAULT_INSTANCE_SPACE
-    ) -> dm.InstancesDeleteResult:
-        """Delete one or more bid configuration.
+        if self.name is not None:
+            properties["name"] = self.name
 
-        Args:
-            external_id: External id of the bid configuration to delete.
-            space: The space where all the bid configuration are located.
+        if self.method is not None or write_none:
+            properties["method"] = self.method
 
-        Returns:
-            The instance(s), i.e., nodes and edges which has been deleted. Empty list if nothing was deleted.
+        if self.add_steps is not None:
+            properties["addSteps"] = self.add_steps
+
+        if properties:
+            this_node = dm.NodeApply(
+                space=self.space,
+                external_id=self.external_id,
+                existing_version=None if allow_version_increase else self.data_record.existing_version,
+                type=self.node_type,
+                sources=[
+                    dm.NodeOrEdgeData(
+                        source=write_view,
+                        properties=properties,
+                    )
+                ],
+            )
+            resources.nodes.append(this_node)
+            cache.add(self.as_tuple_id())
 
-        Examples:
+        return resources
 
-            Delete bid_configuration by id:
 
-                >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
-                >>> client = PowerOpsModelsV1Client()
-                >>> client.bid_configuration.delete("my_bid_configuration")
-        """
+class PartialBidConfigurationApply(PartialBidConfigurationWrite):
+    def __new__(cls, *args, **kwargs) -> PartialBidConfigurationApply:
         warnings.warn(
-            "The .delete method is deprecated and will be removed in v1.0. "
-            "Please use the .delete method on the client instead. This means instead of "
-            "`my_client.bid_configuration.delete(my_ids)` please use `my_client.delete(my_ids)`."
-            "The motivation is that all delete methods are the same, and having one delete method per API "
-            " class encourages users to delete items in small batches, which is inefficient.",
+            "PartialBidConfigurationApply is deprecated and will be removed in v1.0. Use PartialBidConfigurationWrite instead."
+            "The motivation for this change is that Write is a more descriptive name for the writing version of the"
+            "PartialBidConfiguration.",
             UserWarning,
             stacklevel=2,
         )
-        return self._delete(external_id, space)
+        return super().__new__(cls)
 
-    @overload
-    def retrieve(self, external_id: str, space: str = DEFAULT_INSTANCE_SPACE) -> BidConfiguration | None: ...
 
-    @overload
-    def retrieve(
-        self, external_id: SequenceNotStr[str], space: str = DEFAULT_INSTANCE_SPACE
-    ) -> BidConfigurationList: ...
+class PartialBidConfigurationList(DomainModelList[PartialBidConfiguration]):
+    """List of partial bid configurations in the read version."""
 
-    def retrieve(
-        self, external_id: str | SequenceNotStr[str], space: str = DEFAULT_INSTANCE_SPACE
-    ) -> BidConfiguration | BidConfigurationList | None:
-        """Retrieve one or more bid configurations by id(s).
+    _INSTANCE = PartialBidConfiguration
 
-        Args:
-            external_id: External id or list of external ids of the bid configurations.
-            space: The space where all the bid configurations are located.
+    def as_write(self) -> PartialBidConfigurationWriteList:
+        """Convert these read versions of partial bid configuration to the writing versions."""
+        return PartialBidConfigurationWriteList([node.as_write() for node in self.data])
 
-        Returns:
-            The requested bid configurations.
+    def as_apply(self) -> PartialBidConfigurationWriteList:
+        """Convert these read versions of primitive nullable to the writing versions."""
+        warnings.warn(
+            "as_apply is deprecated and will be removed in v1.0. Use as_write instead.",
+            UserWarning,
+            stacklevel=2,
+        )
+        return self.as_write()
 
-        Examples:
 
-            Retrieve bid_configuration by id:
+class PartialBidConfigurationWriteList(DomainModelWriteList[PartialBidConfigurationWrite]):
+    """List of partial bid configurations in the writing version."""
 
-                >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
-                >>> client = PowerOpsModelsV1Client()
-                >>> bid_configuration = client.bid_configuration.retrieve("my_bid_configuration")
+    _INSTANCE = PartialBidConfigurationWrite
 
-        """
-        return self._retrieve(external_id, space)
 
-    def list(
-        self,
-        market_configuration: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
-        external_id_prefix: str | None = None,
-        space: str | list[str] | None = None,
-        limit: int | None = DEFAULT_LIMIT_READ,
-        filter: dm.Filter | None = None,
-    ) -> BidConfigurationList:
-        """List/filter bid configurations
-
-        Args:
-            market_configuration: The market configuration to filter on.
-            external_id_prefix: The prefix of the external ID to filter on.
-            space: The space to filter on.
-            limit: Maximum number of bid configurations to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
-            filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
-
-        Returns:
-            List of requested bid configurations
-
-        Examples:
-
-            List bid configurations and limit to 5:
-
-                >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
-                >>> client = PowerOpsModelsV1Client()
-                >>> bid_configurations = client.bid_configuration.list(limit=5)
-
-        """
-        filter_ = _create_bid_configuration_filter(
-            self._view_id,
-            market_configuration,
-            external_id_prefix,
-            space,
-            filter,
-        )
-        return self._list(limit=limit, filter=filter_)
+class PartialBidConfigurationApplyList(PartialBidConfigurationWriteList): ...
+
+
+def _create_partial_bid_configuration_filter(
+    view_id: dm.ViewId,
+    name: str | list[str] | None = None,
+    name_prefix: str | None = None,
+    method: str | list[str] | None = None,
+    method_prefix: str | None = None,
+    add_steps: bool | None = None,
+    external_id_prefix: str | None = None,
+    space: str | list[str] | None = None,
+    filter: dm.Filter | None = None,
+) -> dm.Filter | None:
+    filters = []
+    if isinstance(name, str):
+        filters.append(dm.filters.Equals(view_id.as_property_ref("name"), value=name))
+    if name and isinstance(name, list):
+        filters.append(dm.filters.In(view_id.as_property_ref("name"), values=name))
+    if name_prefix is not None:
+        filters.append(dm.filters.Prefix(view_id.as_property_ref("name"), value=name_prefix))
+    if isinstance(method, str):
+        filters.append(dm.filters.Equals(view_id.as_property_ref("method"), value=method))
+    if method and isinstance(method, list):
+        filters.append(dm.filters.In(view_id.as_property_ref("method"), values=method))
+    if method_prefix is not None:
+        filters.append(dm.filters.Prefix(view_id.as_property_ref("method"), value=method_prefix))
+    if isinstance(add_steps, bool):
+        filters.append(dm.filters.Equals(view_id.as_property_ref("addSteps"), value=add_steps))
+    if external_id_prefix is not None:
+        filters.append(dm.filters.Prefix(["node", "externalId"], value=external_id_prefix))
+    if isinstance(space, str):
+        filters.append(dm.filters.Equals(["node", "space"], value=space))
+    if space and isinstance(space, list):
+        filters.append(dm.filters.In(["node", "space"], values=space))
+    if filter:
+        filters.append(filter)
+    return dm.filters.And(*filters) if filters else None
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_configuration_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/task_dispatcher_shop_input_query.py`

 * *Files 14% similar despite different names*

```diff
@@ -3,71 +3,73 @@
 import datetime
 from typing import TYPE_CHECKING
 
 from cognite.client import data_modeling as dm, CogniteClient
 
 from cognite.powerops.client._generated.v1.data_classes import (
     DomainModelCore,
-    BidConfiguration,
-    MarketConfiguration,
+    TaskDispatcherShopInput,
+    BidConfigurationShop,
 )
 from ._core import DEFAULT_QUERY_LIMIT, QueryBuilder, QueryStep, QueryAPI, T_DomainModelList, _create_edge_filter
 
 
-class BidConfigurationQueryAPI(QueryAPI[T_DomainModelList]):
+class TaskDispatcherShopInputQueryAPI(QueryAPI[T_DomainModelList]):
     def __init__(
         self,
         client: CogniteClient,
         builder: QueryBuilder[T_DomainModelList],
         view_by_read_class: dict[type[DomainModelCore], dm.ViewId],
         filter_: dm.filters.Filter | None = None,
         limit: int = DEFAULT_QUERY_LIMIT,
     ):
         super().__init__(client, builder, view_by_read_class)
 
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("bid_configuration"),
+                name=self._builder.next_name("task_dispatcher_shop_input"),
                 expression=dm.query.NodeResultSetExpression(
                     from_=self._builder[-1].name if self._builder else None,
                     filter=filter_,
                 ),
-                select=dm.query.Select([dm.query.SourceSelector(self._view_by_read_class[BidConfiguration], ["*"])]),
-                result_cls=BidConfiguration,
+                select=dm.query.Select(
+                    [dm.query.SourceSelector(self._view_by_read_class[TaskDispatcherShopInput], ["*"])]
+                ),
+                result_cls=TaskDispatcherShopInput,
                 max_retrieve_limit=limit,
             )
         )
 
     def query(
         self,
-        retrieve_market_configuration: bool = False,
+        retrieve_bid_configuration: bool = False,
     ) -> T_DomainModelList:
         """Execute query and return the result.
 
         Args:
-            retrieve_market_configuration: Whether to retrieve the market configuration for each bid configuration or not.
+            retrieve_bid_configuration: Whether to retrieve the bid configuration for each task dispatcher shop input or not.
 
         Returns:
             The list of the source nodes of the query.
 
         """
         from_ = self._builder[-1].name
-        if retrieve_market_configuration:
-            self._query_append_market_configuration(from_)
+        if retrieve_bid_configuration:
+            self._query_append_bid_configuration(from_)
         return self._query()
 
-    def _query_append_market_configuration(self, from_: str) -> None:
-        view_id = self._view_by_read_class[MarketConfiguration]
+    def _query_append_bid_configuration(self, from_: str) -> None:
+        view_id = self._view_by_read_class[BidConfigurationShop]
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("market_configuration"),
+                name=self._builder.next_name("bid_configuration"),
                 expression=dm.query.NodeResultSetExpression(
                     filter=dm.filters.HasData(views=[view_id]),
                     from_=from_,
-                    through=self._view_by_read_class[BidConfiguration].as_property_ref("marketConfiguration"),
+                    through=self._view_by_read_class[TaskDispatcherShopInput].as_property_ref("bidConfiguration"),
                     direction="outwards",
                 ),
                 select=dm.query.Select([dm.query.SourceSelector(view_id, ["*"])]),
                 max_retrieve_limit=-1,
-                result_cls=MarketConfiguration,
+                result_cls=BidConfigurationShop,
             ),
         )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_configuration_shop.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_configuration_shop.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_configuration_shop_plants_shop.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_configuration_shop_plants_shop.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_configuration_shop_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_configuration_shop_query.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_configuration_shop_watercourses_shop.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_configuration_shop_watercourses_shop.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_configuration_water.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_configuration_water.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_configuration_water_plants.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_configuration_water_plants.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_configuration_water_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_configuration_query.py`

 * *Files 16% similar despite different names*

```diff
@@ -3,225 +3,179 @@
 import datetime
 from typing import TYPE_CHECKING
 
 from cognite.client import data_modeling as dm, CogniteClient
 
 from cognite.powerops.client._generated.v1.data_classes import (
     DomainModelCore,
-    BidConfigurationWater,
+    BidConfiguration,
     MarketConfiguration,
-    BidMethodWaterValue,
     PriceArea,
 )
+from cognite.powerops.client._generated.v1.data_classes._partial_bid_configuration import (
+    PartialBidConfiguration,
+    _create_partial_bid_configuration_filter,
+)
 from ._core import DEFAULT_QUERY_LIMIT, QueryBuilder, QueryStep, QueryAPI, T_DomainModelList, _create_edge_filter
 
 if TYPE_CHECKING:
-    from .plant_query import PlantQueryAPI
-    from .watercourse_query import WatercourseQueryAPI
+    from .partial_bid_configuration_query import PartialBidConfigurationQueryAPI
 
 
-class BidConfigurationWaterQueryAPI(QueryAPI[T_DomainModelList]):
+class BidConfigurationQueryAPI(QueryAPI[T_DomainModelList]):
     def __init__(
         self,
         client: CogniteClient,
         builder: QueryBuilder[T_DomainModelList],
         view_by_read_class: dict[type[DomainModelCore], dm.ViewId],
         filter_: dm.filters.Filter | None = None,
         limit: int = DEFAULT_QUERY_LIMIT,
     ):
         super().__init__(client, builder, view_by_read_class)
 
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("bid_configuration_water"),
+                name=self._builder.next_name("bid_configuration"),
                 expression=dm.query.NodeResultSetExpression(
                     from_=self._builder[-1].name if self._builder else None,
                     filter=filter_,
                 ),
-                select=dm.query.Select(
-                    [dm.query.SourceSelector(self._view_by_read_class[BidConfigurationWater], ["*"])]
-                ),
-                result_cls=BidConfigurationWater,
+                select=dm.query.Select([dm.query.SourceSelector(self._view_by_read_class[BidConfiguration], ["*"])]),
+                result_cls=BidConfiguration,
                 max_retrieve_limit=limit,
             )
         )
 
-    def plants(
+    def partials(
         self,
+        name: str | list[str] | None = None,
+        name_prefix: str | None = None,
+        method: str | list[str] | None = None,
+        method_prefix: str | None = None,
+        add_steps: bool | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
+        external_id_prefix_edge: str | None = None,
+        space_edge: str | list[str] | None = None,
+        filter: dm.Filter | None = None,
         limit: int | None = DEFAULT_QUERY_LIMIT,
         retrieve_market_configuration: bool = False,
-        retrieve_method: bool = False,
         retrieve_price_area: bool = False,
-    ) -> PlantQueryAPI[T_DomainModelList]:
-        """Query along the plant edges of the bid configuration water.
+    ) -> PartialBidConfigurationQueryAPI[T_DomainModelList]:
+        """Query along the partial edges of the bid configuration.
 
         Args:
+            name: The name to filter on.
+            name_prefix: The prefix of the name to filter on.
+            method: The method to filter on.
+            method_prefix: The prefix of the method to filter on.
+            add_steps: The add step to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of plant edges to return. Defaults to 25. Set to -1, float("inf") or None
+            external_id_prefix_edge: The prefix of the external ID to filter on.
+            space_edge: The space to filter on.
+            filter: (Advanced) Filter applied to node. If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
+            limit: Maximum number of partial edges to return. Defaults to 3. Set to -1, float("inf") or None
                 to return all items.
-            retrieve_market_configuration: Whether to retrieve the market configuration for each bid configuration water or not.
-            retrieve_method: Whether to retrieve the method for each bid configuration water or not.
-            retrieve_price_area: Whether to retrieve the price area for each bid configuration water or not.
+            retrieve_market_configuration: Whether to retrieve the market configuration for each bid configuration or not.
+            retrieve_price_area: Whether to retrieve the price area for each bid configuration or not.
 
         Returns:
-            PlantQueryAPI: The query API for the plant.
+            PartialBidConfigurationQueryAPI: The query API for the partial bid configuration.
         """
-        from .plant_query import PlantQueryAPI
+        from .partial_bid_configuration_query import PartialBidConfigurationQueryAPI
 
         from_ = self._builder[-1].name
-
         edge_filter = _create_edge_filter(
-            dm.DirectRelationReference("sp_powerops_types", "BidConfiguration.plants"),
-            external_id_prefix=external_id_prefix,
-            space=space,
+            dm.DirectRelationReference("sp_powerops_types_temp", "BidConfiguration.partials"),
+            external_id_prefix=external_id_prefix_edge,
+            space=space_edge,
         )
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("plants"),
+                name=self._builder.next_name("partials"),
                 expression=dm.query.EdgeResultSetExpression(
                     filter=edge_filter,
                     from_=from_,
                     direction="outwards",
                 ),
                 select=dm.query.Select(),
                 max_retrieve_limit=limit,
             )
         )
-        if retrieve_market_configuration:
-            self._query_append_market_configuration(from_)
-        if retrieve_method:
-            self._query_append_method(from_)
-        if retrieve_price_area:
-            self._query_append_price_area(from_)
-        return PlantQueryAPI(self._client, self._builder, self._view_by_read_class, None, limit)
-
-    def watercourses(
-        self,
-        external_id_prefix: str | None = None,
-        space: str | list[str] | None = None,
-        limit: int | None = DEFAULT_QUERY_LIMIT,
-        retrieve_market_configuration: bool = False,
-        retrieve_method: bool = False,
-        retrieve_price_area: bool = False,
-    ) -> WatercourseQueryAPI[T_DomainModelList]:
-        """Query along the watercourse edges of the bid configuration water.
-
-        Args:
-            external_id_prefix: The prefix of the external ID to filter on.
-            space: The space to filter on.
-            limit: Maximum number of watercourse edges to return. Defaults to 25. Set to -1, float("inf") or None
-                to return all items.
-            retrieve_market_configuration: Whether to retrieve the market configuration for each bid configuration water or not.
-            retrieve_method: Whether to retrieve the method for each bid configuration water or not.
-            retrieve_price_area: Whether to retrieve the price area for each bid configuration water or not.
-
-        Returns:
-            WatercourseQueryAPI: The query API for the watercourse.
-        """
-        from .watercourse_query import WatercourseQueryAPI
-
-        from_ = self._builder[-1].name
 
-        edge_filter = _create_edge_filter(
-            dm.DirectRelationReference("sp_powerops_types", "BidConfiguration.watercourses"),
-            external_id_prefix=external_id_prefix,
-            space=space,
-        )
-        self._builder.append(
-            QueryStep(
-                name=self._builder.next_name("watercourses"),
-                expression=dm.query.EdgeResultSetExpression(
-                    filter=edge_filter,
-                    from_=from_,
-                    direction="outwards",
-                ),
-                select=dm.query.Select(),
-                max_retrieve_limit=limit,
-            )
+        view_id = self._view_by_read_class[PartialBidConfiguration]
+        has_data = dm.filters.HasData(views=[view_id])
+        node_filer = _create_partial_bid_configuration_filter(
+            view_id,
+            name,
+            name_prefix,
+            method,
+            method_prefix,
+            add_steps,
+            external_id_prefix,
+            space,
+            (filter and dm.filters.And(filter, has_data)) or has_data,
         )
         if retrieve_market_configuration:
             self._query_append_market_configuration(from_)
-        if retrieve_method:
-            self._query_append_method(from_)
         if retrieve_price_area:
             self._query_append_price_area(from_)
-        return WatercourseQueryAPI(self._client, self._builder, self._view_by_read_class, None, limit)
+        return PartialBidConfigurationQueryAPI(self._client, self._builder, self._view_by_read_class, node_filer, limit)
 
     def query(
         self,
         retrieve_market_configuration: bool = False,
-        retrieve_method: bool = False,
         retrieve_price_area: bool = False,
     ) -> T_DomainModelList:
         """Execute query and return the result.
 
         Args:
-            retrieve_market_configuration: Whether to retrieve the market configuration for each bid configuration water or not.
-            retrieve_method: Whether to retrieve the method for each bid configuration water or not.
-            retrieve_price_area: Whether to retrieve the price area for each bid configuration water or not.
+            retrieve_market_configuration: Whether to retrieve the market configuration for each bid configuration or not.
+            retrieve_price_area: Whether to retrieve the price area for each bid configuration or not.
 
         Returns:
             The list of the source nodes of the query.
 
         """
         from_ = self._builder[-1].name
         if retrieve_market_configuration:
             self._query_append_market_configuration(from_)
-        if retrieve_method:
-            self._query_append_method(from_)
         if retrieve_price_area:
             self._query_append_price_area(from_)
         return self._query()
 
     def _query_append_market_configuration(self, from_: str) -> None:
         view_id = self._view_by_read_class[MarketConfiguration]
         self._builder.append(
             QueryStep(
                 name=self._builder.next_name("market_configuration"),
                 expression=dm.query.NodeResultSetExpression(
                     filter=dm.filters.HasData(views=[view_id]),
                     from_=from_,
-                    through=self._view_by_read_class[BidConfigurationWater].as_property_ref("marketConfiguration"),
+                    through=self._view_by_read_class[BidConfiguration].as_property_ref("marketConfiguration"),
                     direction="outwards",
                 ),
                 select=dm.query.Select([dm.query.SourceSelector(view_id, ["*"])]),
                 max_retrieve_limit=-1,
                 result_cls=MarketConfiguration,
-            ),
-        )
-
-    def _query_append_method(self, from_: str) -> None:
-        view_id = self._view_by_read_class[BidMethodWaterValue]
-        self._builder.append(
-            QueryStep(
-                name=self._builder.next_name("method"),
-                expression=dm.query.NodeResultSetExpression(
-                    filter=dm.filters.HasData(views=[view_id]),
-                    from_=from_,
-                    through=self._view_by_read_class[BidConfigurationWater].as_property_ref("method"),
-                    direction="outwards",
-                ),
-                select=dm.query.Select([dm.query.SourceSelector(view_id, ["*"])]),
-                max_retrieve_limit=-1,
-                result_cls=BidMethodWaterValue,
+                is_single_direct_relation=True,
             ),
         )
 
     def _query_append_price_area(self, from_: str) -> None:
         view_id = self._view_by_read_class[PriceArea]
         self._builder.append(
             QueryStep(
                 name=self._builder.next_name("price_area"),
                 expression=dm.query.NodeResultSetExpression(
                     filter=dm.filters.HasData(views=[view_id]),
                     from_=from_,
-                    through=self._view_by_read_class[BidConfigurationWater].as_property_ref("priceArea"),
+                    through=self._view_by_read_class[BidConfiguration].as_property_ref("priceArea"),
                     direction="outwards",
                 ),
                 select=dm.query.Select([dm.query.SourceSelector(view_id, ["*"])]),
                 max_retrieve_limit=-1,
                 result_cls=PriceArea,
+                is_single_direct_relation=True,
             ),
         )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_configuration_water_watercourses.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_configuration_water_watercourses.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_document_afrr.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_document_afrr.py`

 * *Files 6% similar despite different names*

```diff
@@ -54,14 +54,16 @@
         self.alerts_edge = BidDocumentAFRRAlertsAPI(client)
         self.bids_edge = BidDocumentAFRRBidsAPI(client)
 
     def __call__(
         self,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
+        process_id: str | list[str] | None = None,
+        process_id_prefix: str | None = None,
         min_delivery_date: datetime.date | None = None,
         max_delivery_date: datetime.date | None = None,
         min_start_calculation: datetime.datetime | None = None,
         max_start_calculation: datetime.datetime | None = None,
         min_end_calculation: datetime.datetime | None = None,
         max_end_calculation: datetime.datetime | None = None,
         is_complete: bool | None = None,
@@ -72,14 +74,16 @@
         filter: dm.Filter | None = None,
     ) -> BidDocumentAFRRQueryAPI[BidDocumentAFRRList]:
         """Query starting at bid document afrrs.
 
         Args:
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
+            process_id: The process id to filter on.
+            process_id_prefix: The prefix of the process id to filter on.
             min_delivery_date: The minimum value of the delivery date to filter on.
             max_delivery_date: The maximum value of the delivery date to filter on.
             min_start_calculation: The minimum value of the start calculation to filter on.
             max_start_calculation: The maximum value of the start calculation to filter on.
             min_end_calculation: The minimum value of the end calculation to filter on.
             max_end_calculation: The maximum value of the end calculation to filter on.
             is_complete: The is complete to filter on.
@@ -94,14 +98,16 @@
 
         """
         has_data = dm.filters.HasData(views=[self._view_id])
         filter_ = _create_bid_document_afrr_filter(
             self._view_id,
             name,
             name_prefix,
+            process_id,
+            process_id_prefix,
             min_delivery_date,
             max_delivery_date,
             min_start_calculation,
             max_start_calculation,
             min_end_calculation,
             max_end_calculation,
             is_complete,
@@ -221,34 +227,36 @@
             external_id,
             space,
             retrieve_edges=True,
             edge_api_name_type_direction_view_id_penta=[
                 (
                     self.alerts_edge,
                     "alerts",
-                    dm.DirectRelationReference("sp_powerops_types", "calculationIssue"),
+                    dm.DirectRelationReference("sp_powerops_types_temp", "calculationIssue"),
                     "outwards",
-                    dm.ViewId("sp_powerops_models", "Alert", "1"),
+                    dm.ViewId("sp_powerops_models_temp", "Alert", "1"),
                 ),
                 (
                     self.bids_edge,
                     "bids",
-                    dm.DirectRelationReference("sp_powerops_types", "partialBid"),
+                    dm.DirectRelationReference("sp_powerops_types_temp", "partialBid"),
                     "outwards",
-                    dm.ViewId("sp_powerops_models", "BidRow", "1"),
+                    dm.ViewId("sp_powerops_models_temp", "BidRow", "1"),
                 ),
             ],
         )
 
     def search(
         self,
         query: str,
         properties: BidDocumentAFRRTextFields | Sequence[BidDocumentAFRRTextFields] | None = None,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
+        process_id: str | list[str] | None = None,
+        process_id_prefix: str | None = None,
         min_delivery_date: datetime.date | None = None,
         max_delivery_date: datetime.date | None = None,
         min_start_calculation: datetime.datetime | None = None,
         max_start_calculation: datetime.datetime | None = None,
         min_end_calculation: datetime.datetime | None = None,
         max_end_calculation: datetime.datetime | None = None,
         is_complete: bool | None = None,
@@ -261,14 +269,16 @@
         """Search bid document afrrs
 
         Args:
             query: The search query,
             properties: The property to search, if nothing is passed all text fields will be searched.
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
+            process_id: The process id to filter on.
+            process_id_prefix: The prefix of the process id to filter on.
             min_delivery_date: The minimum value of the delivery date to filter on.
             max_delivery_date: The maximum value of the delivery date to filter on.
             min_start_calculation: The minimum value of the start calculation to filter on.
             max_start_calculation: The maximum value of the start calculation to filter on.
             min_end_calculation: The minimum value of the end calculation to filter on.
             max_end_calculation: The maximum value of the end calculation to filter on.
             is_complete: The is complete to filter on.
@@ -290,14 +300,16 @@
                 >>> bid_document_afrrs = client.bid_document_afrr.search('my_bid_document_afrr')
 
         """
         filter_ = _create_bid_document_afrr_filter(
             self._view_id,
             name,
             name_prefix,
+            process_id,
+            process_id_prefix,
             min_delivery_date,
             max_delivery_date,
             min_start_calculation,
             max_start_calculation,
             min_end_calculation,
             max_end_calculation,
             is_complete,
@@ -319,14 +331,16 @@
         ),
         property: BidDocumentAFRRFields | Sequence[BidDocumentAFRRFields] | None = None,
         group_by: None = None,
         query: str | None = None,
         search_properties: BidDocumentAFRRTextFields | Sequence[BidDocumentAFRRTextFields] | None = None,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
+        process_id: str | list[str] | None = None,
+        process_id_prefix: str | None = None,
         min_delivery_date: datetime.date | None = None,
         max_delivery_date: datetime.date | None = None,
         min_start_calculation: datetime.datetime | None = None,
         max_start_calculation: datetime.datetime | None = None,
         min_end_calculation: datetime.datetime | None = None,
         max_end_calculation: datetime.datetime | None = None,
         is_complete: bool | None = None,
@@ -348,14 +362,16 @@
         ),
         property: BidDocumentAFRRFields | Sequence[BidDocumentAFRRFields] | None = None,
         group_by: BidDocumentAFRRFields | Sequence[BidDocumentAFRRFields] = None,
         query: str | None = None,
         search_properties: BidDocumentAFRRTextFields | Sequence[BidDocumentAFRRTextFields] | None = None,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
+        process_id: str | list[str] | None = None,
+        process_id_prefix: str | None = None,
         min_delivery_date: datetime.date | None = None,
         max_delivery_date: datetime.date | None = None,
         min_start_calculation: datetime.datetime | None = None,
         max_start_calculation: datetime.datetime | None = None,
         min_end_calculation: datetime.datetime | None = None,
         max_end_calculation: datetime.datetime | None = None,
         is_complete: bool | None = None,
@@ -376,14 +392,16 @@
         ),
         property: BidDocumentAFRRFields | Sequence[BidDocumentAFRRFields] | None = None,
         group_by: BidDocumentAFRRFields | Sequence[BidDocumentAFRRFields] | None = None,
         query: str | None = None,
         search_property: BidDocumentAFRRTextFields | Sequence[BidDocumentAFRRTextFields] | None = None,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
+        process_id: str | list[str] | None = None,
+        process_id_prefix: str | None = None,
         min_delivery_date: datetime.date | None = None,
         max_delivery_date: datetime.date | None = None,
         min_start_calculation: datetime.datetime | None = None,
         max_start_calculation: datetime.datetime | None = None,
         min_end_calculation: datetime.datetime | None = None,
         max_end_calculation: datetime.datetime | None = None,
         is_complete: bool | None = None,
@@ -399,14 +417,16 @@
             aggregate: The aggregation to perform.
             property: The property to perform aggregation on.
             group_by: The property to group by when doing the aggregation.
             query: The query to search for in the text field.
             search_property: The text field to search in.
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
+            process_id: The process id to filter on.
+            process_id_prefix: The prefix of the process id to filter on.
             min_delivery_date: The minimum value of the delivery date to filter on.
             max_delivery_date: The maximum value of the delivery date to filter on.
             min_start_calculation: The minimum value of the start calculation to filter on.
             max_start_calculation: The maximum value of the start calculation to filter on.
             min_end_calculation: The minimum value of the end calculation to filter on.
             max_end_calculation: The maximum value of the end calculation to filter on.
             is_complete: The is complete to filter on.
@@ -429,14 +449,16 @@
 
         """
 
         filter_ = _create_bid_document_afrr_filter(
             self._view_id,
             name,
             name_prefix,
+            process_id,
+            process_id_prefix,
             min_delivery_date,
             max_delivery_date,
             min_start_calculation,
             max_start_calculation,
             min_end_calculation,
             max_end_calculation,
             is_complete,
@@ -461,14 +483,16 @@
         self,
         property: BidDocumentAFRRFields,
         interval: float,
         query: str | None = None,
         search_property: BidDocumentAFRRTextFields | Sequence[BidDocumentAFRRTextFields] | None = None,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
+        process_id: str | list[str] | None = None,
+        process_id_prefix: str | None = None,
         min_delivery_date: datetime.date | None = None,
         max_delivery_date: datetime.date | None = None,
         min_start_calculation: datetime.datetime | None = None,
         max_start_calculation: datetime.datetime | None = None,
         min_end_calculation: datetime.datetime | None = None,
         max_end_calculation: datetime.datetime | None = None,
         is_complete: bool | None = None,
@@ -483,14 +507,16 @@
         Args:
             property: The property to use as the value in the histogram.
             interval: The interval to use for the histogram bins.
             query: The query to search for in the text field.
             search_property: The text field to search in.
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
+            process_id: The process id to filter on.
+            process_id_prefix: The prefix of the process id to filter on.
             min_delivery_date: The minimum value of the delivery date to filter on.
             max_delivery_date: The maximum value of the delivery date to filter on.
             min_start_calculation: The minimum value of the start calculation to filter on.
             max_start_calculation: The maximum value of the start calculation to filter on.
             min_end_calculation: The minimum value of the end calculation to filter on.
             max_end_calculation: The maximum value of the end calculation to filter on.
             is_complete: The is complete to filter on.
@@ -504,14 +530,16 @@
             Bucketed histogram results.
 
         """
         filter_ = _create_bid_document_afrr_filter(
             self._view_id,
             name,
             name_prefix,
+            process_id,
+            process_id_prefix,
             min_delivery_date,
             max_delivery_date,
             min_start_calculation,
             max_start_calculation,
             min_end_calculation,
             max_end_calculation,
             is_complete,
@@ -531,14 +559,16 @@
             filter_,
         )
 
     def list(
         self,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
+        process_id: str | list[str] | None = None,
+        process_id_prefix: str | None = None,
         min_delivery_date: datetime.date | None = None,
         max_delivery_date: datetime.date | None = None,
         min_start_calculation: datetime.datetime | None = None,
         max_start_calculation: datetime.datetime | None = None,
         min_end_calculation: datetime.datetime | None = None,
         max_end_calculation: datetime.datetime | None = None,
         is_complete: bool | None = None,
@@ -550,14 +580,16 @@
         retrieve_edges: bool = True,
     ) -> BidDocumentAFRRList:
         """List/filter bid document afrrs
 
         Args:
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
+            process_id: The process id to filter on.
+            process_id_prefix: The prefix of the process id to filter on.
             min_delivery_date: The minimum value of the delivery date to filter on.
             max_delivery_date: The maximum value of the delivery date to filter on.
             min_start_calculation: The minimum value of the start calculation to filter on.
             max_start_calculation: The maximum value of the start calculation to filter on.
             min_end_calculation: The minimum value of the end calculation to filter on.
             max_end_calculation: The maximum value of the end calculation to filter on.
             is_complete: The is complete to filter on.
@@ -580,14 +612,16 @@
                 >>> bid_document_afrrs = client.bid_document_afrr.list(limit=5)
 
         """
         filter_ = _create_bid_document_afrr_filter(
             self._view_id,
             name,
             name_prefix,
+            process_id,
+            process_id_prefix,
             min_delivery_date,
             max_delivery_date,
             min_start_calculation,
             max_start_calculation,
             min_end_calculation,
             max_end_calculation,
             is_complete,
@@ -601,20 +635,20 @@
             limit=limit,
             filter=filter_,
             retrieve_edges=retrieve_edges,
             edge_api_name_type_direction_view_id_penta=[
                 (
                     self.alerts_edge,
                     "alerts",
-                    dm.DirectRelationReference("sp_powerops_types", "calculationIssue"),
+                    dm.DirectRelationReference("sp_powerops_types_temp", "calculationIssue"),
                     "outwards",
-                    dm.ViewId("sp_powerops_models", "Alert", "1"),
+                    dm.ViewId("sp_powerops_models_temp", "Alert", "1"),
                 ),
                 (
                     self.bids_edge,
                     "bids",
-                    dm.DirectRelationReference("sp_powerops_types", "partialBid"),
+                    dm.DirectRelationReference("sp_powerops_types_temp", "partialBid"),
                     "outwards",
-                    dm.ViewId("sp_powerops_models", "BidRow", "1"),
+                    dm.ViewId("sp_powerops_models_temp", "BidRow", "1"),
                 ),
             ],
         )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_document_afrr_alerts.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_document_afrr_alerts.py`

 * *Files 8% similar despite different names*

```diff
@@ -39,15 +39,15 @@
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
                 >>> bid_document_afrr = client.bid_document_afrr.alerts_edge.list("my_bid_document_afrr", limit=5)
 
         """
         filter_ = _create_edge_filter(
-            dm.DirectRelationReference("sp_powerops_types", "calculationIssue"),
+            dm.DirectRelationReference("sp_powerops_types_temp", "calculationIssue"),
             from_bid_document_afrr,
             from_bid_document_afrr_space,
             to_alert,
             to_alert_space,
             external_id_prefix,
             space,
         )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_document_afrr_bids.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_document_afrr_bids.py`

 * *Files 0% similar despite different names*

```diff
@@ -39,15 +39,15 @@
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
                 >>> bid_document_afrr = client.bid_document_afrr.bids_edge.list("my_bid_document_afrr", limit=5)
 
         """
         filter_ = _create_edge_filter(
-            dm.DirectRelationReference("sp_powerops_types", "partialBid"),
+            dm.DirectRelationReference("sp_powerops_types_temp", "partialBid"),
             from_bid_document_afrr,
             from_bid_document_afrr_space,
             to_bid_row,
             to_bid_row_space,
             external_id_prefix,
             space,
         )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_document_afrr_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/multi_scenario_matrix_raw_query.py`

 * *Files 20% similar despite different names*

```diff
@@ -3,63 +3,65 @@
 import datetime
 from typing import TYPE_CHECKING
 
 from cognite.client import data_modeling as dm, CogniteClient
 
 from cognite.powerops.client._generated.v1.data_classes import (
     DomainModelCore,
-    BidDocumentAFRR,
-    PriceAreaAFRR,
+    MultiScenarioMatrixRaw,
+    BidMethodSHOPMultiScenario,
 )
 from ._core import DEFAULT_QUERY_LIMIT, QueryBuilder, QueryStep, QueryAPI, T_DomainModelList, _create_edge_filter
 
 if TYPE_CHECKING:
     from .alert_query import AlertQueryAPI
-    from .bid_row_query import BidRowQueryAPI
+    from .shop_result_price_prod_query import SHOPResultPriceProdQueryAPI
 
 
-class BidDocumentAFRRQueryAPI(QueryAPI[T_DomainModelList]):
+class MultiScenarioMatrixRawQueryAPI(QueryAPI[T_DomainModelList]):
     def __init__(
         self,
         client: CogniteClient,
         builder: QueryBuilder[T_DomainModelList],
         view_by_read_class: dict[type[DomainModelCore], dm.ViewId],
         filter_: dm.filters.Filter | None = None,
         limit: int = DEFAULT_QUERY_LIMIT,
     ):
         super().__init__(client, builder, view_by_read_class)
 
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("bid_document_afrr"),
+                name=self._builder.next_name("multi_scenario_matrix_raw"),
                 expression=dm.query.NodeResultSetExpression(
                     from_=self._builder[-1].name if self._builder else None,
                     filter=filter_,
                 ),
-                select=dm.query.Select([dm.query.SourceSelector(self._view_by_read_class[BidDocumentAFRR], ["*"])]),
-                result_cls=BidDocumentAFRR,
+                select=dm.query.Select(
+                    [dm.query.SourceSelector(self._view_by_read_class[MultiScenarioMatrixRaw], ["*"])]
+                ),
+                result_cls=MultiScenarioMatrixRaw,
                 max_retrieve_limit=limit,
             )
         )
 
     def alerts(
         self,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_QUERY_LIMIT,
-        retrieve_price_area: bool = False,
+        retrieve_method: bool = False,
     ) -> AlertQueryAPI[T_DomainModelList]:
-        """Query along the alert edges of the bid document afrr.
+        """Query along the alert edges of the multi scenario matrix raw.
 
         Args:
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of alert edges to return. Defaults to 25. Set to -1, float("inf") or None
                 to return all items.
-            retrieve_price_area: Whether to retrieve the price area for each bid document afrr or not.
+            retrieve_method: Whether to retrieve the method for each multi scenario matrix raw or not.
 
         Returns:
             AlertQueryAPI: The query API for the alert.
         """
         from .alert_query import AlertQueryAPI
 
         from_ = self._builder[-1].name
@@ -77,89 +79,89 @@
                     from_=from_,
                     direction="outwards",
                 ),
                 select=dm.query.Select(),
                 max_retrieve_limit=limit,
             )
         )
-        if retrieve_price_area:
-            self._query_append_price_area(from_)
+        if retrieve_method:
+            self._query_append_method(from_)
         return AlertQueryAPI(self._client, self._builder, self._view_by_read_class, None, limit)
 
-    def bids(
+    def shop_results(
         self,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_QUERY_LIMIT,
-        retrieve_price_area: bool = False,
-    ) -> BidRowQueryAPI[T_DomainModelList]:
-        """Query along the bid edges of the bid document afrr.
+        retrieve_method: bool = False,
+    ) -> SHOPResultPriceProdQueryAPI[T_DomainModelList]:
+        """Query along the shop result edges of the multi scenario matrix raw.
 
         Args:
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of bid edges to return. Defaults to 25. Set to -1, float("inf") or None
+            limit: Maximum number of shop result edges to return. Defaults to 25. Set to -1, float("inf") or None
                 to return all items.
-            retrieve_price_area: Whether to retrieve the price area for each bid document afrr or not.
+            retrieve_method: Whether to retrieve the method for each multi scenario matrix raw or not.
 
         Returns:
-            BidRowQueryAPI: The query API for the bid row.
+            SHOPResultPriceProdQueryAPI: The query API for the shop result price prod.
         """
-        from .bid_row_query import BidRowQueryAPI
+        from .shop_result_price_prod_query import SHOPResultPriceProdQueryAPI
 
         from_ = self._builder[-1].name
 
         edge_filter = _create_edge_filter(
-            dm.DirectRelationReference("sp_powerops_types", "partialBid"),
+            dm.DirectRelationReference("sp_powerops_types", "MultiScenarioMatrix.shopResults"),
             external_id_prefix=external_id_prefix,
             space=space,
         )
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("bids"),
+                name=self._builder.next_name("shop_results"),
                 expression=dm.query.EdgeResultSetExpression(
                     filter=edge_filter,
                     from_=from_,
                     direction="outwards",
                 ),
                 select=dm.query.Select(),
                 max_retrieve_limit=limit,
             )
         )
-        if retrieve_price_area:
-            self._query_append_price_area(from_)
-        return BidRowQueryAPI(self._client, self._builder, self._view_by_read_class, None, limit)
+        if retrieve_method:
+            self._query_append_method(from_)
+        return SHOPResultPriceProdQueryAPI(self._client, self._builder, self._view_by_read_class, None, limit)
 
     def query(
         self,
-        retrieve_price_area: bool = False,
+        retrieve_method: bool = False,
     ) -> T_DomainModelList:
         """Execute query and return the result.
 
         Args:
-            retrieve_price_area: Whether to retrieve the price area for each bid document afrr or not.
+            retrieve_method: Whether to retrieve the method for each multi scenario matrix raw or not.
 
         Returns:
             The list of the source nodes of the query.
 
         """
         from_ = self._builder[-1].name
-        if retrieve_price_area:
-            self._query_append_price_area(from_)
+        if retrieve_method:
+            self._query_append_method(from_)
         return self._query()
 
-    def _query_append_price_area(self, from_: str) -> None:
-        view_id = self._view_by_read_class[PriceAreaAFRR]
+    def _query_append_method(self, from_: str) -> None:
+        view_id = self._view_by_read_class[BidMethodSHOPMultiScenario]
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("price_area"),
+                name=self._builder.next_name("method"),
                 expression=dm.query.NodeResultSetExpression(
                     filter=dm.filters.HasData(views=[view_id]),
                     from_=from_,
-                    through=self._view_by_read_class[BidDocumentAFRR].as_property_ref("priceArea"),
+                    through=self._view_by_read_class[MultiScenarioMatrixRaw].as_property_ref("method"),
                     direction="outwards",
                 ),
                 select=dm.query.Select([dm.query.SourceSelector(view_id, ["*"])]),
                 max_retrieve_limit=-1,
-                result_cls=PriceAreaAFRR,
+                result_cls=BidMethodSHOPMultiScenario,
             ),
         )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_document_day_ahead.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_document_day_ahead.py`

 * *Files 4% similar despite different names*

```diff
@@ -54,43 +54,45 @@
         self.alerts_edge = BidDocumentDayAheadAlertsAPI(client)
         self.partials_edge = BidDocumentDayAheadPartialsAPI(client)
 
     def __call__(
         self,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
+        process_id: str | list[str] | None = None,
+        process_id_prefix: str | None = None,
         min_delivery_date: datetime.date | None = None,
         max_delivery_date: datetime.date | None = None,
         min_start_calculation: datetime.datetime | None = None,
         max_start_calculation: datetime.datetime | None = None,
         min_end_calculation: datetime.datetime | None = None,
         max_end_calculation: datetime.datetime | None = None,
         is_complete: bool | None = None,
-        price_area: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
-        method: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        bid_configuration: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         total: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_QUERY_LIMIT,
         filter: dm.Filter | None = None,
     ) -> BidDocumentDayAheadQueryAPI[BidDocumentDayAheadList]:
         """Query starting at bid document day aheads.
 
         Args:
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
+            process_id: The process id to filter on.
+            process_id_prefix: The prefix of the process id to filter on.
             min_delivery_date: The minimum value of the delivery date to filter on.
             max_delivery_date: The maximum value of the delivery date to filter on.
             min_start_calculation: The minimum value of the start calculation to filter on.
             max_start_calculation: The maximum value of the start calculation to filter on.
             min_end_calculation: The minimum value of the end calculation to filter on.
             max_end_calculation: The maximum value of the end calculation to filter on.
             is_complete: The is complete to filter on.
-            price_area: The price area to filter on.
-            method: The method to filter on.
+            bid_configuration: The bid configuration to filter on.
             total: The total to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of bid document day aheads to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
@@ -98,23 +100,24 @@
 
         """
         has_data = dm.filters.HasData(views=[self._view_id])
         filter_ = _create_bid_document_day_ahead_filter(
             self._view_id,
             name,
             name_prefix,
+            process_id,
+            process_id_prefix,
             min_delivery_date,
             max_delivery_date,
             min_start_calculation,
             max_start_calculation,
             min_end_calculation,
             max_end_calculation,
             is_complete,
-            price_area,
-            method,
+            bid_configuration,
             total,
             external_id_prefix,
             space,
             (filter and dm.filters.And(filter, has_data)) or has_data,
         )
         builder = QueryBuilder(BidDocumentDayAheadList)
         return BidDocumentDayAheadQueryAPI(self._client, builder, self._view_by_read_class, filter_, limit)
@@ -227,65 +230,67 @@
             external_id,
             space,
             retrieve_edges=True,
             edge_api_name_type_direction_view_id_penta=[
                 (
                     self.alerts_edge,
                     "alerts",
-                    dm.DirectRelationReference("sp_powerops_types", "calculationIssue"),
+                    dm.DirectRelationReference("sp_powerops_types_temp", "calculationIssue"),
                     "outwards",
-                    dm.ViewId("sp_powerops_models", "Alert", "1"),
+                    dm.ViewId("sp_powerops_models_temp", "Alert", "1"),
                 ),
                 (
                     self.partials_edge,
                     "partials",
-                    dm.DirectRelationReference("sp_powerops_types", "partialBid"),
+                    dm.DirectRelationReference("sp_powerops_types_temp", "partialBid"),
                     "outwards",
-                    dm.ViewId("sp_powerops_models", "BidMatrix", "1"),
+                    dm.ViewId("sp_powerops_models_temp", "BidMatrix", "1"),
                 ),
             ],
         )
 
     def search(
         self,
         query: str,
         properties: BidDocumentDayAheadTextFields | Sequence[BidDocumentDayAheadTextFields] | None = None,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
+        process_id: str | list[str] | None = None,
+        process_id_prefix: str | None = None,
         min_delivery_date: datetime.date | None = None,
         max_delivery_date: datetime.date | None = None,
         min_start_calculation: datetime.datetime | None = None,
         max_start_calculation: datetime.datetime | None = None,
         min_end_calculation: datetime.datetime | None = None,
         max_end_calculation: datetime.datetime | None = None,
         is_complete: bool | None = None,
-        price_area: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
-        method: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        bid_configuration: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         total: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> BidDocumentDayAheadList:
         """Search bid document day aheads
 
         Args:
             query: The search query,
             properties: The property to search, if nothing is passed all text fields will be searched.
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
+            process_id: The process id to filter on.
+            process_id_prefix: The prefix of the process id to filter on.
             min_delivery_date: The minimum value of the delivery date to filter on.
             max_delivery_date: The maximum value of the delivery date to filter on.
             min_start_calculation: The minimum value of the start calculation to filter on.
             max_start_calculation: The maximum value of the start calculation to filter on.
             min_end_calculation: The minimum value of the end calculation to filter on.
             max_end_calculation: The maximum value of the end calculation to filter on.
             is_complete: The is complete to filter on.
-            price_area: The price area to filter on.
-            method: The method to filter on.
+            bid_configuration: The bid configuration to filter on.
             total: The total to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of bid document day aheads to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
@@ -300,23 +305,24 @@
                 >>> bid_document_day_aheads = client.bid_document_day_ahead.search('my_bid_document_day_ahead')
 
         """
         filter_ = _create_bid_document_day_ahead_filter(
             self._view_id,
             name,
             name_prefix,
+            process_id,
+            process_id_prefix,
             min_delivery_date,
             max_delivery_date,
             min_start_calculation,
             max_start_calculation,
             min_end_calculation,
             max_end_calculation,
             is_complete,
-            price_area,
-            method,
+            bid_configuration,
             total,
             external_id_prefix,
             space,
             filter,
         )
         return self._search(self._view_id, query, _BIDDOCUMENTDAYAHEAD_PROPERTIES_BY_FIELD, properties, filter_, limit)
 
@@ -331,23 +337,24 @@
         ),
         property: BidDocumentDayAheadFields | Sequence[BidDocumentDayAheadFields] | None = None,
         group_by: None = None,
         query: str | None = None,
         search_properties: BidDocumentDayAheadTextFields | Sequence[BidDocumentDayAheadTextFields] | None = None,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
+        process_id: str | list[str] | None = None,
+        process_id_prefix: str | None = None,
         min_delivery_date: datetime.date | None = None,
         max_delivery_date: datetime.date | None = None,
         min_start_calculation: datetime.datetime | None = None,
         max_start_calculation: datetime.datetime | None = None,
         min_end_calculation: datetime.datetime | None = None,
         max_end_calculation: datetime.datetime | None = None,
         is_complete: bool | None = None,
-        price_area: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
-        method: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        bid_configuration: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         total: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> list[dm.aggregations.AggregatedNumberedValue]: ...
 
@@ -362,23 +369,24 @@
         ),
         property: BidDocumentDayAheadFields | Sequence[BidDocumentDayAheadFields] | None = None,
         group_by: BidDocumentDayAheadFields | Sequence[BidDocumentDayAheadFields] = None,
         query: str | None = None,
         search_properties: BidDocumentDayAheadTextFields | Sequence[BidDocumentDayAheadTextFields] | None = None,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
+        process_id: str | list[str] | None = None,
+        process_id_prefix: str | None = None,
         min_delivery_date: datetime.date | None = None,
         max_delivery_date: datetime.date | None = None,
         min_start_calculation: datetime.datetime | None = None,
         max_start_calculation: datetime.datetime | None = None,
         min_end_calculation: datetime.datetime | None = None,
         max_end_calculation: datetime.datetime | None = None,
         is_complete: bool | None = None,
-        price_area: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
-        method: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        bid_configuration: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         total: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> InstanceAggregationResultList: ...
 
@@ -392,23 +400,24 @@
         ),
         property: BidDocumentDayAheadFields | Sequence[BidDocumentDayAheadFields] | None = None,
         group_by: BidDocumentDayAheadFields | Sequence[BidDocumentDayAheadFields] | None = None,
         query: str | None = None,
         search_property: BidDocumentDayAheadTextFields | Sequence[BidDocumentDayAheadTextFields] | None = None,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
+        process_id: str | list[str] | None = None,
+        process_id_prefix: str | None = None,
         min_delivery_date: datetime.date | None = None,
         max_delivery_date: datetime.date | None = None,
         min_start_calculation: datetime.datetime | None = None,
         max_start_calculation: datetime.datetime | None = None,
         min_end_calculation: datetime.datetime | None = None,
         max_end_calculation: datetime.datetime | None = None,
         is_complete: bool | None = None,
-        price_area: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
-        method: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        bid_configuration: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         total: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> list[dm.aggregations.AggregatedNumberedValue] | InstanceAggregationResultList:
         """Aggregate data across bid document day aheads
@@ -417,23 +426,24 @@
             aggregate: The aggregation to perform.
             property: The property to perform aggregation on.
             group_by: The property to group by when doing the aggregation.
             query: The query to search for in the text field.
             search_property: The text field to search in.
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
+            process_id: The process id to filter on.
+            process_id_prefix: The prefix of the process id to filter on.
             min_delivery_date: The minimum value of the delivery date to filter on.
             max_delivery_date: The maximum value of the delivery date to filter on.
             min_start_calculation: The minimum value of the start calculation to filter on.
             max_start_calculation: The maximum value of the start calculation to filter on.
             min_end_calculation: The minimum value of the end calculation to filter on.
             max_end_calculation: The maximum value of the end calculation to filter on.
             is_complete: The is complete to filter on.
-            price_area: The price area to filter on.
-            method: The method to filter on.
+            bid_configuration: The bid configuration to filter on.
             total: The total to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of bid document day aheads to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
@@ -449,23 +459,24 @@
 
         """
 
         filter_ = _create_bid_document_day_ahead_filter(
             self._view_id,
             name,
             name_prefix,
+            process_id,
+            process_id_prefix,
             min_delivery_date,
             max_delivery_date,
             min_start_calculation,
             max_start_calculation,
             min_end_calculation,
             max_end_calculation,
             is_complete,
-            price_area,
-            method,
+            bid_configuration,
             total,
             external_id_prefix,
             space,
             filter,
         )
         return self._aggregate(
             self._view_id,
@@ -483,23 +494,24 @@
         self,
         property: BidDocumentDayAheadFields,
         interval: float,
         query: str | None = None,
         search_property: BidDocumentDayAheadTextFields | Sequence[BidDocumentDayAheadTextFields] | None = None,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
+        process_id: str | list[str] | None = None,
+        process_id_prefix: str | None = None,
         min_delivery_date: datetime.date | None = None,
         max_delivery_date: datetime.date | None = None,
         min_start_calculation: datetime.datetime | None = None,
         max_start_calculation: datetime.datetime | None = None,
         min_end_calculation: datetime.datetime | None = None,
         max_end_calculation: datetime.datetime | None = None,
         is_complete: bool | None = None,
-        price_area: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
-        method: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        bid_configuration: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         total: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> dm.aggregations.HistogramValue:
         """Produces histograms for bid document day aheads
@@ -507,46 +519,48 @@
         Args:
             property: The property to use as the value in the histogram.
             interval: The interval to use for the histogram bins.
             query: The query to search for in the text field.
             search_property: The text field to search in.
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
+            process_id: The process id to filter on.
+            process_id_prefix: The prefix of the process id to filter on.
             min_delivery_date: The minimum value of the delivery date to filter on.
             max_delivery_date: The maximum value of the delivery date to filter on.
             min_start_calculation: The minimum value of the start calculation to filter on.
             max_start_calculation: The maximum value of the start calculation to filter on.
             min_end_calculation: The minimum value of the end calculation to filter on.
             max_end_calculation: The maximum value of the end calculation to filter on.
             is_complete: The is complete to filter on.
-            price_area: The price area to filter on.
-            method: The method to filter on.
+            bid_configuration: The bid configuration to filter on.
             total: The total to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of bid document day aheads to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
             Bucketed histogram results.
 
         """
         filter_ = _create_bid_document_day_ahead_filter(
             self._view_id,
             name,
             name_prefix,
+            process_id,
+            process_id_prefix,
             min_delivery_date,
             max_delivery_date,
             min_start_calculation,
             max_start_calculation,
             min_end_calculation,
             max_end_calculation,
             is_complete,
-            price_area,
-            method,
+            bid_configuration,
             total,
             external_id_prefix,
             space,
             filter,
         )
         return self._histogram(
             self._view_id,
@@ -559,44 +573,46 @@
             filter_,
         )
 
     def list(
         self,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
+        process_id: str | list[str] | None = None,
+        process_id_prefix: str | None = None,
         min_delivery_date: datetime.date | None = None,
         max_delivery_date: datetime.date | None = None,
         min_start_calculation: datetime.datetime | None = None,
         max_start_calculation: datetime.datetime | None = None,
         min_end_calculation: datetime.datetime | None = None,
         max_end_calculation: datetime.datetime | None = None,
         is_complete: bool | None = None,
-        price_area: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
-        method: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        bid_configuration: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         total: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
         retrieve_edges: bool = True,
     ) -> BidDocumentDayAheadList:
         """List/filter bid document day aheads
 
         Args:
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
+            process_id: The process id to filter on.
+            process_id_prefix: The prefix of the process id to filter on.
             min_delivery_date: The minimum value of the delivery date to filter on.
             max_delivery_date: The maximum value of the delivery date to filter on.
             min_start_calculation: The minimum value of the start calculation to filter on.
             max_start_calculation: The maximum value of the start calculation to filter on.
             min_end_calculation: The minimum value of the end calculation to filter on.
             max_end_calculation: The maximum value of the end calculation to filter on.
             is_complete: The is complete to filter on.
-            price_area: The price area to filter on.
-            method: The method to filter on.
+            bid_configuration: The bid configuration to filter on.
             total: The total to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of bid document day aheads to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
             retrieve_edges: Whether to retrieve `alerts` or `partials` external ids for the bid document day aheads. Defaults to True.
 
@@ -612,43 +628,44 @@
                 >>> bid_document_day_aheads = client.bid_document_day_ahead.list(limit=5)
 
         """
         filter_ = _create_bid_document_day_ahead_filter(
             self._view_id,
             name,
             name_prefix,
+            process_id,
+            process_id_prefix,
             min_delivery_date,
             max_delivery_date,
             min_start_calculation,
             max_start_calculation,
             min_end_calculation,
             max_end_calculation,
             is_complete,
-            price_area,
-            method,
+            bid_configuration,
             total,
             external_id_prefix,
             space,
             filter,
         )
 
         return self._list(
             limit=limit,
             filter=filter_,
             retrieve_edges=retrieve_edges,
             edge_api_name_type_direction_view_id_penta=[
                 (
                     self.alerts_edge,
                     "alerts",
-                    dm.DirectRelationReference("sp_powerops_types", "calculationIssue"),
+                    dm.DirectRelationReference("sp_powerops_types_temp", "calculationIssue"),
                     "outwards",
-                    dm.ViewId("sp_powerops_models", "Alert", "1"),
+                    dm.ViewId("sp_powerops_models_temp", "Alert", "1"),
                 ),
                 (
                     self.partials_edge,
                     "partials",
-                    dm.DirectRelationReference("sp_powerops_types", "partialBid"),
+                    dm.DirectRelationReference("sp_powerops_types_temp", "partialBid"),
                     "outwards",
-                    dm.ViewId("sp_powerops_models", "BidMatrix", "1"),
+                    dm.ViewId("sp_powerops_models_temp", "BidMatrix", "1"),
                 ),
             ],
         )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_document_day_ahead_alerts.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_document_day_ahead_alerts.py`

 * *Files 1% similar despite different names*

```diff
@@ -39,15 +39,15 @@
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
                 >>> bid_document_day_ahead = client.bid_document_day_ahead.alerts_edge.list("my_bid_document_day_ahead", limit=5)
 
         """
         filter_ = _create_edge_filter(
-            dm.DirectRelationReference("sp_powerops_types", "calculationIssue"),
+            dm.DirectRelationReference("sp_powerops_types_temp", "calculationIssue"),
             from_bid_document_day_ahead,
             from_bid_document_day_ahead_space,
             to_alert,
             to_alert_space,
             external_id_prefix,
             space,
         )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_document_day_ahead_partials.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_document_day_ahead_partials.py`

 * *Files 1% similar despite different names*

```diff
@@ -39,15 +39,15 @@
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
                 >>> bid_document_day_ahead = client.bid_document_day_ahead.partials_edge.list("my_bid_document_day_ahead", limit=5)
 
         """
         filter_ = _create_edge_filter(
-            dm.DirectRelationReference("sp_powerops_types", "partialBid"),
+            dm.DirectRelationReference("sp_powerops_types_temp", "partialBid"),
             from_bid_document_day_ahead,
             from_bid_document_day_ahead_space,
             to_bid_matrix,
             to_bid_matrix_space,
             external_id_prefix,
             space,
         )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_document_day_ahead_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/task_dispatcher_shop_output_query.py`

 * *Files 14% similar despite different names*

```diff
@@ -3,69 +3,66 @@
 import datetime
 from typing import TYPE_CHECKING
 
 from cognite.client import data_modeling as dm, CogniteClient
 
 from cognite.powerops.client._generated.v1.data_classes import (
     DomainModelCore,
-    BidDocumentDayAhead,
-    PriceArea,
-    BidMethodDayAhead,
-    BidMatrix,
+    TaskDispatcherShopOutput,
+    TaskDispatcherShopInput,
 )
 from ._core import DEFAULT_QUERY_LIMIT, QueryBuilder, QueryStep, QueryAPI, T_DomainModelList, _create_edge_filter
 
 if TYPE_CHECKING:
     from .alert_query import AlertQueryAPI
-    from .bid_matrix_query import BidMatrixQueryAPI
+    from .shop_partial_bid_calculation_input_query import ShopPartialBidCalculationInputQueryAPI
+    from .preprocessor_input_query import PreprocessorInputQueryAPI
 
 
-class BidDocumentDayAheadQueryAPI(QueryAPI[T_DomainModelList]):
+class TaskDispatcherShopOutputQueryAPI(QueryAPI[T_DomainModelList]):
     def __init__(
         self,
         client: CogniteClient,
         builder: QueryBuilder[T_DomainModelList],
         view_by_read_class: dict[type[DomainModelCore], dm.ViewId],
         filter_: dm.filters.Filter | None = None,
         limit: int = DEFAULT_QUERY_LIMIT,
     ):
         super().__init__(client, builder, view_by_read_class)
 
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("bid_document_day_ahead"),
+                name=self._builder.next_name("task_dispatcher_shop_output"),
                 expression=dm.query.NodeResultSetExpression(
                     from_=self._builder[-1].name if self._builder else None,
                     filter=filter_,
                 ),
-                select=dm.query.Select([dm.query.SourceSelector(self._view_by_read_class[BidDocumentDayAhead], ["*"])]),
-                result_cls=BidDocumentDayAhead,
+                select=dm.query.Select(
+                    [dm.query.SourceSelector(self._view_by_read_class[TaskDispatcherShopOutput], ["*"])]
+                ),
+                result_cls=TaskDispatcherShopOutput,
                 max_retrieve_limit=limit,
             )
         )
 
     def alerts(
         self,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_QUERY_LIMIT,
-        retrieve_price_area: bool = False,
-        retrieve_method: bool = False,
-        retrieve_total: bool = False,
+        retrieve_input_: bool = False,
     ) -> AlertQueryAPI[T_DomainModelList]:
-        """Query along the alert edges of the bid document day ahead.
+        """Query along the alert edges of the task dispatcher shop output.
 
         Args:
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of alert edges to return. Defaults to 25. Set to -1, float("inf") or None
                 to return all items.
-            retrieve_price_area: Whether to retrieve the price area for each bid document day ahead or not.
-            retrieve_method: Whether to retrieve the method for each bid document day ahead or not.
-            retrieve_total: Whether to retrieve the total for each bid document day ahead or not.
+            retrieve_input_: Whether to retrieve the input for each task dispatcher shop output or not.
 
         Returns:
             AlertQueryAPI: The query API for the alert.
         """
         from .alert_query import AlertQueryAPI
 
         from_ = self._builder[-1].name
@@ -83,143 +80,135 @@
                     from_=from_,
                     direction="outwards",
                 ),
                 select=dm.query.Select(),
                 max_retrieve_limit=limit,
             )
         )
-        if retrieve_price_area:
-            self._query_append_price_area(from_)
-        if retrieve_method:
-            self._query_append_method(from_)
-        if retrieve_total:
-            self._query_append_total(from_)
+        if retrieve_input_:
+            self._query_append_input_(from_)
         return AlertQueryAPI(self._client, self._builder, self._view_by_read_class, None, limit)
 
-    def partials(
+    def partial_bid_calculations(
         self,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_QUERY_LIMIT,
-        retrieve_price_area: bool = False,
-        retrieve_method: bool = False,
-        retrieve_total: bool = False,
-    ) -> BidMatrixQueryAPI[T_DomainModelList]:
-        """Query along the partial edges of the bid document day ahead.
+        retrieve_input_: bool = False,
+    ) -> ShopPartialBidCalculationInputQueryAPI[T_DomainModelList]:
+        """Query along the partial bid calculation edges of the task dispatcher shop output.
 
         Args:
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of partial edges to return. Defaults to 25. Set to -1, float("inf") or None
+            limit: Maximum number of partial bid calculation edges to return. Defaults to 25. Set to -1, float("inf") or None
                 to return all items.
-            retrieve_price_area: Whether to retrieve the price area for each bid document day ahead or not.
-            retrieve_method: Whether to retrieve the method for each bid document day ahead or not.
-            retrieve_total: Whether to retrieve the total for each bid document day ahead or not.
+            retrieve_input_: Whether to retrieve the input for each task dispatcher shop output or not.
 
         Returns:
-            BidMatrixQueryAPI: The query API for the bid matrix.
+            ShopPartialBidCalculationInputQueryAPI: The query API for the shop partial bid calculation input.
         """
-        from .bid_matrix_query import BidMatrixQueryAPI
+        from .shop_partial_bid_calculation_input_query import ShopPartialBidCalculationInputQueryAPI
 
         from_ = self._builder[-1].name
 
         edge_filter = _create_edge_filter(
-            dm.DirectRelationReference("sp_powerops_types", "partialBid"),
+            dm.DirectRelationReference("sp_powerops_types", "ShopPartialBidCalculationInput"),
             external_id_prefix=external_id_prefix,
             space=space,
         )
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("partials"),
+                name=self._builder.next_name("partial_bid_calculations"),
                 expression=dm.query.EdgeResultSetExpression(
                     filter=edge_filter,
                     from_=from_,
                     direction="outwards",
                 ),
                 select=dm.query.Select(),
                 max_retrieve_limit=limit,
             )
         )
-        if retrieve_price_area:
-            self._query_append_price_area(from_)
-        if retrieve_method:
-            self._query_append_method(from_)
-        if retrieve_total:
-            self._query_append_total(from_)
-        return BidMatrixQueryAPI(self._client, self._builder, self._view_by_read_class, None, limit)
+        if retrieve_input_:
+            self._query_append_input_(from_)
+        return ShopPartialBidCalculationInputQueryAPI(
+            self._client, self._builder, self._view_by_read_class, None, limit
+        )
 
-    def query(
+    def preprocessor_calculations(
         self,
-        retrieve_price_area: bool = False,
-        retrieve_method: bool = False,
-        retrieve_total: bool = False,
-    ) -> T_DomainModelList:
-        """Execute query and return the result.
+        external_id_prefix: str | None = None,
+        space: str | list[str] | None = None,
+        limit: int | None = DEFAULT_QUERY_LIMIT,
+        retrieve_input_: bool = False,
+    ) -> PreprocessorInputQueryAPI[T_DomainModelList]:
+        """Query along the preprocessor calculation edges of the task dispatcher shop output.
 
         Args:
-            retrieve_price_area: Whether to retrieve the price area for each bid document day ahead or not.
-            retrieve_method: Whether to retrieve the method for each bid document day ahead or not.
-            retrieve_total: Whether to retrieve the total for each bid document day ahead or not.
+            external_id_prefix: The prefix of the external ID to filter on.
+            space: The space to filter on.
+            limit: Maximum number of preprocessor calculation edges to return. Defaults to 25. Set to -1, float("inf") or None
+                to return all items.
+            retrieve_input_: Whether to retrieve the input for each task dispatcher shop output or not.
 
         Returns:
-            The list of the source nodes of the query.
-
+            PreprocessorInputQueryAPI: The query API for the preprocessor input.
         """
+        from .preprocessor_input_query import PreprocessorInputQueryAPI
+
         from_ = self._builder[-1].name
-        if retrieve_price_area:
-            self._query_append_price_area(from_)
-        if retrieve_method:
-            self._query_append_method(from_)
-        if retrieve_total:
-            self._query_append_total(from_)
-        return self._query()
 
-    def _query_append_price_area(self, from_: str) -> None:
-        view_id = self._view_by_read_class[PriceArea]
-        self._builder.append(
-            QueryStep(
-                name=self._builder.next_name("price_area"),
-                expression=dm.query.NodeResultSetExpression(
-                    filter=dm.filters.HasData(views=[view_id]),
-                    from_=from_,
-                    through=self._view_by_read_class[BidDocumentDayAhead].as_property_ref("priceArea"),
-                    direction="outwards",
-                ),
-                select=dm.query.Select([dm.query.SourceSelector(view_id, ["*"])]),
-                max_retrieve_limit=-1,
-                result_cls=PriceArea,
-            ),
+        edge_filter = _create_edge_filter(
+            dm.DirectRelationReference("sp_powerops_types", "PreprocessorInput"),
+            external_id_prefix=external_id_prefix,
+            space=space,
         )
-
-    def _query_append_method(self, from_: str) -> None:
-        view_id = self._view_by_read_class[BidMethodDayAhead]
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("method"),
-                expression=dm.query.NodeResultSetExpression(
-                    filter=dm.filters.HasData(views=[view_id]),
+                name=self._builder.next_name("preprocessor_calculations"),
+                expression=dm.query.EdgeResultSetExpression(
+                    filter=edge_filter,
                     from_=from_,
-                    through=self._view_by_read_class[BidDocumentDayAhead].as_property_ref("method"),
                     direction="outwards",
                 ),
-                select=dm.query.Select([dm.query.SourceSelector(view_id, ["*"])]),
-                max_retrieve_limit=-1,
-                result_cls=BidMethodDayAhead,
-            ),
+                select=dm.query.Select(),
+                max_retrieve_limit=limit,
+            )
         )
+        if retrieve_input_:
+            self._query_append_input_(from_)
+        return PreprocessorInputQueryAPI(self._client, self._builder, self._view_by_read_class, None, limit)
+
+    def query(
+        self,
+        retrieve_input_: bool = False,
+    ) -> T_DomainModelList:
+        """Execute query and return the result.
+
+        Args:
+            retrieve_input_: Whether to retrieve the input for each task dispatcher shop output or not.
+
+        Returns:
+            The list of the source nodes of the query.
+
+        """
+        from_ = self._builder[-1].name
+        if retrieve_input_:
+            self._query_append_input_(from_)
+        return self._query()
 
-    def _query_append_total(self, from_: str) -> None:
-        view_id = self._view_by_read_class[BidMatrix]
+    def _query_append_input_(self, from_: str) -> None:
+        view_id = self._view_by_read_class[TaskDispatcherShopInput]
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("total"),
+                name=self._builder.next_name("input_"),
                 expression=dm.query.NodeResultSetExpression(
                     filter=dm.filters.HasData(views=[view_id]),
                     from_=from_,
-                    through=self._view_by_read_class[BidDocumentDayAhead].as_property_ref("total"),
+                    through=self._view_by_read_class[TaskDispatcherShopOutput].as_property_ref("input"),
                     direction="outwards",
                 ),
                 select=dm.query.Select([dm.query.SourceSelector(view_id, ["*"])]),
                 max_retrieve_limit=-1,
-                result_cls=BidMatrix,
+                result_cls=TaskDispatcherShopInput,
             ),
         )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_matrix.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_matrix_raw.py`

 * *Files 8% similar despite different names*

```diff
@@ -9,202 +9,202 @@
 from cognite.client.data_classes.data_modeling.instances import InstanceAggregationResultList
 
 from cognite.powerops.client._generated.v1.data_classes._core import DEFAULT_INSTANCE_SPACE
 from cognite.powerops.client._generated.v1.data_classes import (
     DomainModelCore,
     DomainModelWrite,
     ResourcesWriteResult,
-    BidMatrix,
-    BidMatrixWrite,
-    BidMatrixFields,
-    BidMatrixList,
-    BidMatrixWriteList,
-    BidMatrixTextFields,
+    BidMatrixRaw,
+    BidMatrixRawWrite,
+    BidMatrixRawFields,
+    BidMatrixRawList,
+    BidMatrixRawWriteList,
+    BidMatrixRawTextFields,
 )
-from cognite.powerops.client._generated.v1.data_classes._bid_matrix import (
-    _BIDMATRIX_PROPERTIES_BY_FIELD,
-    _create_bid_matrix_filter,
+from cognite.powerops.client._generated.v1.data_classes._bid_matrix_raw import (
+    _BIDMATRIXRAW_PROPERTIES_BY_FIELD,
+    _create_bid_matrix_raw_filter,
 )
 from ._core import (
     DEFAULT_LIMIT_READ,
     DEFAULT_QUERY_LIMIT,
     Aggregations,
     NodeAPI,
     SequenceNotStr,
     QueryStep,
     QueryBuilder,
 )
-from .bid_matrix_alerts import BidMatrixAlertsAPI
-from .bid_matrix_query import BidMatrixQueryAPI
+from .bid_matrix_raw_alerts import BidMatrixRawAlertsAPI
+from .bid_matrix_raw_query import BidMatrixRawQueryAPI
 
 
-class BidMatrixAPI(NodeAPI[BidMatrix, BidMatrixWrite, BidMatrixList]):
+class BidMatrixRawAPI(NodeAPI[BidMatrixRaw, BidMatrixRawWrite, BidMatrixRawList]):
     def __init__(self, client: CogniteClient, view_by_read_class: dict[type[DomainModelCore], dm.ViewId]):
-        view_id = view_by_read_class[BidMatrix]
+        view_id = view_by_read_class[BidMatrixRaw]
         super().__init__(
             client=client,
             sources=view_id,
-            class_type=BidMatrix,
-            class_list=BidMatrixList,
-            class_write_list=BidMatrixWriteList,
+            class_type=BidMatrixRaw,
+            class_list=BidMatrixRawList,
+            class_write_list=BidMatrixRawWriteList,
             view_by_read_class=view_by_read_class,
         )
         self._view_id = view_id
-        self.alerts_edge = BidMatrixAlertsAPI(client)
+        self.alerts_edge = BidMatrixRawAlertsAPI(client)
 
     def __call__(
         self,
         resource_cost: str | list[str] | None = None,
         resource_cost_prefix: str | None = None,
         asset_type: str | list[str] | None = None,
         asset_type_prefix: str | None = None,
         asset_id: str | list[str] | None = None,
         asset_id_prefix: str | None = None,
         is_processed: bool | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_QUERY_LIMIT,
         filter: dm.Filter | None = None,
-    ) -> BidMatrixQueryAPI[BidMatrixList]:
-        """Query starting at bid matrixes.
+    ) -> BidMatrixRawQueryAPI[BidMatrixRawList]:
+        """Query starting at bid matrix raws.
 
         Args:
             resource_cost: The resource cost to filter on.
             resource_cost_prefix: The prefix of the resource cost to filter on.
             asset_type: The asset type to filter on.
             asset_type_prefix: The prefix of the asset type to filter on.
             asset_id: The asset id to filter on.
             asset_id_prefix: The prefix of the asset id to filter on.
             is_processed: The is processed to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of bid matrixes to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of bid matrix raws to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
-            A query API for bid matrixes.
+            A query API for bid matrix raws.
 
         """
         has_data = dm.filters.HasData(views=[self._view_id])
-        filter_ = _create_bid_matrix_filter(
+        filter_ = _create_bid_matrix_raw_filter(
             self._view_id,
             resource_cost,
             resource_cost_prefix,
             asset_type,
             asset_type_prefix,
             asset_id,
             asset_id_prefix,
             is_processed,
             external_id_prefix,
             space,
             (filter and dm.filters.And(filter, has_data)) or has_data,
         )
-        builder = QueryBuilder(BidMatrixList)
-        return BidMatrixQueryAPI(self._client, builder, self._view_by_read_class, filter_, limit)
+        builder = QueryBuilder(BidMatrixRawList)
+        return BidMatrixRawQueryAPI(self._client, builder, self._view_by_read_class, filter_, limit)
 
     def apply(
         self,
-        bid_matrix: BidMatrixWrite | Sequence[BidMatrixWrite],
+        bid_matrix_raw: BidMatrixRawWrite | Sequence[BidMatrixRawWrite],
         replace: bool = False,
         write_none: bool = False,
     ) -> ResourcesWriteResult:
-        """Add or update (upsert) bid matrixes.
+        """Add or update (upsert) bid matrix raws.
 
-        Note: This method iterates through all nodes and timeseries linked to bid_matrix and creates them including the edges
+        Note: This method iterates through all nodes and timeseries linked to bid_matrix_raw and creates them including the edges
         between the nodes. For example, if any of `alerts` are set, then these
         nodes as well as any nodes linked to them, and all the edges linking these nodes will be created.
 
         Args:
-            bid_matrix: Bid matrix or sequence of bid matrixes to upsert.
+            bid_matrix_raw: Bid matrix raw or sequence of bid matrix raws to upsert.
             replace (bool): How do we behave when a property value exists? Do we replace all matching and existing values with the supplied values (true)?
                 Or should we merge in new values for properties together with the existing values (false)? Note: This setting applies for all nodes or edges specified in the ingestion call.
             write_none (bool): This method, will by default, skip properties that are set to None. However, if you want to set properties to None,
                 you can set this parameter to True. Note this only applies to properties that are nullable.
         Returns:
             Created instance(s), i.e., nodes, edges, and time series.
 
         Examples:
 
-            Create a new bid_matrix:
+            Create a new bid_matrix_raw:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
-                >>> from cognite.powerops.client._generated.v1.data_classes import BidMatrixWrite
+                >>> from cognite.powerops.client._generated.v1.data_classes import BidMatrixRawWrite
                 >>> client = PowerOpsModelsV1Client()
-                >>> bid_matrix = BidMatrixWrite(external_id="my_bid_matrix", ...)
-                >>> result = client.bid_matrix.apply(bid_matrix)
+                >>> bid_matrix_raw = BidMatrixRawWrite(external_id="my_bid_matrix_raw", ...)
+                >>> result = client.bid_matrix_raw.apply(bid_matrix_raw)
 
         """
         warnings.warn(
             "The .apply method is deprecated and will be removed in v1.0. "
             "Please use the .upsert method on the client instead. This means instead of "
-            "`my_client.bid_matrix.apply(my_items)` please use `my_client.upsert(my_items)`."
+            "`my_client.bid_matrix_raw.apply(my_items)` please use `my_client.upsert(my_items)`."
             "The motivation is that all apply methods are the same, and having one apply method per API "
             " class encourages users to create items in small batches, which is inefficient."
             "In addition, .upsert method is more descriptive of what the method does.",
             UserWarning,
             stacklevel=2,
         )
-        return self._apply(bid_matrix, replace, write_none)
+        return self._apply(bid_matrix_raw, replace, write_none)
 
     def delete(
         self, external_id: str | SequenceNotStr[str], space: str = DEFAULT_INSTANCE_SPACE
     ) -> dm.InstancesDeleteResult:
-        """Delete one or more bid matrix.
+        """Delete one or more bid matrix raw.
 
         Args:
-            external_id: External id of the bid matrix to delete.
-            space: The space where all the bid matrix are located.
+            external_id: External id of the bid matrix raw to delete.
+            space: The space where all the bid matrix raw are located.
 
         Returns:
             The instance(s), i.e., nodes and edges which has been deleted. Empty list if nothing was deleted.
 
         Examples:
 
-            Delete bid_matrix by id:
+            Delete bid_matrix_raw by id:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> client.bid_matrix.delete("my_bid_matrix")
+                >>> client.bid_matrix_raw.delete("my_bid_matrix_raw")
         """
         warnings.warn(
             "The .delete method is deprecated and will be removed in v1.0. "
             "Please use the .delete method on the client instead. This means instead of "
-            "`my_client.bid_matrix.delete(my_ids)` please use `my_client.delete(my_ids)`."
+            "`my_client.bid_matrix_raw.delete(my_ids)` please use `my_client.delete(my_ids)`."
             "The motivation is that all delete methods are the same, and having one delete method per API "
             " class encourages users to delete items in small batches, which is inefficient.",
             UserWarning,
             stacklevel=2,
         )
         return self._delete(external_id, space)
 
     @overload
-    def retrieve(self, external_id: str, space: str = DEFAULT_INSTANCE_SPACE) -> BidMatrix | None: ...
+    def retrieve(self, external_id: str, space: str = DEFAULT_INSTANCE_SPACE) -> BidMatrixRaw | None: ...
 
     @overload
-    def retrieve(self, external_id: SequenceNotStr[str], space: str = DEFAULT_INSTANCE_SPACE) -> BidMatrixList: ...
+    def retrieve(self, external_id: SequenceNotStr[str], space: str = DEFAULT_INSTANCE_SPACE) -> BidMatrixRawList: ...
 
     def retrieve(
         self, external_id: str | SequenceNotStr[str], space: str = DEFAULT_INSTANCE_SPACE
-    ) -> BidMatrix | BidMatrixList | None:
-        """Retrieve one or more bid matrixes by id(s).
+    ) -> BidMatrixRaw | BidMatrixRawList | None:
+        """Retrieve one or more bid matrix raws by id(s).
 
         Args:
-            external_id: External id or list of external ids of the bid matrixes.
-            space: The space where all the bid matrixes are located.
+            external_id: External id or list of external ids of the bid matrix raws.
+            space: The space where all the bid matrix raws are located.
 
         Returns:
-            The requested bid matrixes.
+            The requested bid matrix raws.
 
         Examples:
 
-            Retrieve bid_matrix by id:
+            Retrieve bid_matrix_raw by id:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> bid_matrix = client.bid_matrix.retrieve("my_bid_matrix")
+                >>> bid_matrix_raw = client.bid_matrix_raw.retrieve("my_bid_matrix_raw")
 
         """
         return self._retrieve(
             external_id,
             space,
             retrieve_edges=True,
             edge_api_name_type_direction_view_id_penta=[
@@ -217,84 +217,84 @@
                 ),
             ],
         )
 
     def search(
         self,
         query: str,
-        properties: BidMatrixTextFields | Sequence[BidMatrixTextFields] | None = None,
+        properties: BidMatrixRawTextFields | Sequence[BidMatrixRawTextFields] | None = None,
         resource_cost: str | list[str] | None = None,
         resource_cost_prefix: str | None = None,
         asset_type: str | list[str] | None = None,
         asset_type_prefix: str | None = None,
         asset_id: str | list[str] | None = None,
         asset_id_prefix: str | None = None,
         is_processed: bool | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
-    ) -> BidMatrixList:
-        """Search bid matrixes
+    ) -> BidMatrixRawList:
+        """Search bid matrix raws
 
         Args:
             query: The search query,
             properties: The property to search, if nothing is passed all text fields will be searched.
             resource_cost: The resource cost to filter on.
             resource_cost_prefix: The prefix of the resource cost to filter on.
             asset_type: The asset type to filter on.
             asset_type_prefix: The prefix of the asset type to filter on.
             asset_id: The asset id to filter on.
             asset_id_prefix: The prefix of the asset id to filter on.
             is_processed: The is processed to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of bid matrixes to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of bid matrix raws to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
-            Search results bid matrixes matching the query.
+            Search results bid matrix raws matching the query.
 
         Examples:
 
-           Search for 'my_bid_matrix' in all text properties:
+           Search for 'my_bid_matrix_raw' in all text properties:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> bid_matrixes = client.bid_matrix.search('my_bid_matrix')
+                >>> bid_matrix_raws = client.bid_matrix_raw.search('my_bid_matrix_raw')
 
         """
-        filter_ = _create_bid_matrix_filter(
+        filter_ = _create_bid_matrix_raw_filter(
             self._view_id,
             resource_cost,
             resource_cost_prefix,
             asset_type,
             asset_type_prefix,
             asset_id,
             asset_id_prefix,
             is_processed,
             external_id_prefix,
             space,
             filter,
         )
-        return self._search(self._view_id, query, _BIDMATRIX_PROPERTIES_BY_FIELD, properties, filter_, limit)
+        return self._search(self._view_id, query, _BIDMATRIXRAW_PROPERTIES_BY_FIELD, properties, filter_, limit)
 
     @overload
     def aggregate(
         self,
         aggregations: (
             Aggregations
             | dm.aggregations.MetricAggregation
             | Sequence[Aggregations]
             | Sequence[dm.aggregations.MetricAggregation]
         ),
-        property: BidMatrixFields | Sequence[BidMatrixFields] | None = None,
+        property: BidMatrixRawFields | Sequence[BidMatrixRawFields] | None = None,
         group_by: None = None,
         query: str | None = None,
-        search_properties: BidMatrixTextFields | Sequence[BidMatrixTextFields] | None = None,
+        search_properties: BidMatrixRawTextFields | Sequence[BidMatrixRawTextFields] | None = None,
         resource_cost: str | list[str] | None = None,
         resource_cost_prefix: str | None = None,
         asset_type: str | list[str] | None = None,
         asset_type_prefix: str | None = None,
         asset_id: str | list[str] | None = None,
         asset_id_prefix: str | None = None,
         is_processed: bool | None = None,
@@ -309,18 +309,18 @@
         self,
         aggregations: (
             Aggregations
             | dm.aggregations.MetricAggregation
             | Sequence[Aggregations]
             | Sequence[dm.aggregations.MetricAggregation]
         ),
-        property: BidMatrixFields | Sequence[BidMatrixFields] | None = None,
-        group_by: BidMatrixFields | Sequence[BidMatrixFields] = None,
+        property: BidMatrixRawFields | Sequence[BidMatrixRawFields] | None = None,
+        group_by: BidMatrixRawFields | Sequence[BidMatrixRawFields] = None,
         query: str | None = None,
-        search_properties: BidMatrixTextFields | Sequence[BidMatrixTextFields] | None = None,
+        search_properties: BidMatrixRawTextFields | Sequence[BidMatrixRawTextFields] | None = None,
         resource_cost: str | list[str] | None = None,
         resource_cost_prefix: str | None = None,
         asset_type: str | list[str] | None = None,
         asset_type_prefix: str | None = None,
         asset_id: str | list[str] | None = None,
         asset_id_prefix: str | None = None,
         is_processed: bool | None = None,
@@ -334,31 +334,31 @@
         self,
         aggregate: (
             Aggregations
             | dm.aggregations.MetricAggregation
             | Sequence[Aggregations]
             | Sequence[dm.aggregations.MetricAggregation]
         ),
-        property: BidMatrixFields | Sequence[BidMatrixFields] | None = None,
-        group_by: BidMatrixFields | Sequence[BidMatrixFields] | None = None,
+        property: BidMatrixRawFields | Sequence[BidMatrixRawFields] | None = None,
+        group_by: BidMatrixRawFields | Sequence[BidMatrixRawFields] | None = None,
         query: str | None = None,
-        search_property: BidMatrixTextFields | Sequence[BidMatrixTextFields] | None = None,
+        search_property: BidMatrixRawTextFields | Sequence[BidMatrixRawTextFields] | None = None,
         resource_cost: str | list[str] | None = None,
         resource_cost_prefix: str | None = None,
         asset_type: str | list[str] | None = None,
         asset_type_prefix: str | None = None,
         asset_id: str | list[str] | None = None,
         asset_id_prefix: str | None = None,
         is_processed: bool | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> list[dm.aggregations.AggregatedNumberedValue] | InstanceAggregationResultList:
-        """Aggregate data across bid matrixes
+        """Aggregate data across bid matrix raws
 
         Args:
             aggregate: The aggregation to perform.
             property: The property to perform aggregation on.
             group_by: The property to group by when doing the aggregation.
             query: The query to search for in the text field.
             search_property: The text field to search in.
@@ -367,31 +367,31 @@
             asset_type: The asset type to filter on.
             asset_type_prefix: The prefix of the asset type to filter on.
             asset_id: The asset id to filter on.
             asset_id_prefix: The prefix of the asset id to filter on.
             is_processed: The is processed to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of bid matrixes to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of bid matrix raws to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
             Aggregation results.
 
         Examples:
 
-            Count bid matrixes in space `my_space`:
+            Count bid matrix raws in space `my_space`:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> result = client.bid_matrix.aggregate("count", space="my_space")
+                >>> result = client.bid_matrix_raw.aggregate("count", space="my_space")
 
         """
 
-        filter_ = _create_bid_matrix_filter(
+        filter_ = _create_bid_matrix_raw_filter(
             self._view_id,
             resource_cost,
             resource_cost_prefix,
             asset_type,
             asset_type_prefix,
             asset_id,
             asset_id_prefix,
@@ -399,42 +399,42 @@
             external_id_prefix,
             space,
             filter,
         )
         return self._aggregate(
             self._view_id,
             aggregate,
-            _BIDMATRIX_PROPERTIES_BY_FIELD,
+            _BIDMATRIXRAW_PROPERTIES_BY_FIELD,
             property,
             group_by,
             query,
             search_property,
             limit,
             filter_,
         )
 
     def histogram(
         self,
-        property: BidMatrixFields,
+        property: BidMatrixRawFields,
         interval: float,
         query: str | None = None,
-        search_property: BidMatrixTextFields | Sequence[BidMatrixTextFields] | None = None,
+        search_property: BidMatrixRawTextFields | Sequence[BidMatrixRawTextFields] | None = None,
         resource_cost: str | list[str] | None = None,
         resource_cost_prefix: str | None = None,
         asset_type: str | list[str] | None = None,
         asset_type_prefix: str | None = None,
         asset_id: str | list[str] | None = None,
         asset_id_prefix: str | None = None,
         is_processed: bool | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> dm.aggregations.HistogramValue:
-        """Produces histograms for bid matrixes
+        """Produces histograms for bid matrix raws
 
         Args:
             property: The property to use as the value in the histogram.
             interval: The interval to use for the histogram bins.
             query: The query to search for in the text field.
             search_property: The text field to search in.
             resource_cost: The resource cost to filter on.
@@ -442,22 +442,22 @@
             asset_type: The asset type to filter on.
             asset_type_prefix: The prefix of the asset type to filter on.
             asset_id: The asset id to filter on.
             asset_id_prefix: The prefix of the asset id to filter on.
             is_processed: The is processed to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of bid matrixes to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of bid matrix raws to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
             Bucketed histogram results.
 
         """
-        filter_ = _create_bid_matrix_filter(
+        filter_ = _create_bid_matrix_raw_filter(
             self._view_id,
             resource_cost,
             resource_cost_prefix,
             asset_type,
             asset_type_prefix,
             asset_id,
             asset_id_prefix,
@@ -466,15 +466,15 @@
             space,
             filter,
         )
         return self._histogram(
             self._view_id,
             property,
             interval,
-            _BIDMATRIX_PROPERTIES_BY_FIELD,
+            _BIDMATRIXRAW_PROPERTIES_BY_FIELD,
             query,
             search_property,
             limit,
             filter_,
         )
 
     def list(
@@ -487,44 +487,44 @@
         asset_id_prefix: str | None = None,
         is_processed: bool | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
         retrieve_edges: bool = True,
-    ) -> BidMatrixList:
-        """List/filter bid matrixes
+    ) -> BidMatrixRawList:
+        """List/filter bid matrix raws
 
         Args:
             resource_cost: The resource cost to filter on.
             resource_cost_prefix: The prefix of the resource cost to filter on.
             asset_type: The asset type to filter on.
             asset_type_prefix: The prefix of the asset type to filter on.
             asset_id: The asset id to filter on.
             asset_id_prefix: The prefix of the asset id to filter on.
             is_processed: The is processed to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of bid matrixes to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of bid matrix raws to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
-            retrieve_edges: Whether to retrieve `alerts` external ids for the bid matrixes. Defaults to True.
+            retrieve_edges: Whether to retrieve `alerts` external ids for the bid matrix raws. Defaults to True.
 
         Returns:
-            List of requested bid matrixes
+            List of requested bid matrix raws
 
         Examples:
 
-            List bid matrixes and limit to 5:
+            List bid matrix raws and limit to 5:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> bid_matrixes = client.bid_matrix.list(limit=5)
+                >>> bid_matrix_raws = client.bid_matrix_raw.list(limit=5)
 
         """
-        filter_ = _create_bid_matrix_filter(
+        filter_ = _create_bid_matrix_raw_filter(
             self._view_id,
             resource_cost,
             resource_cost_prefix,
             asset_type,
             asset_type_prefix,
             asset_id,
             asset_id_prefix,
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_matrix_alerts.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_matrix_alerts.py`

 * *Files 5% similar despite different names*

```diff
@@ -39,15 +39,15 @@
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
                 >>> bid_matrix = client.bid_matrix.alerts_edge.list("my_bid_matrix", limit=5)
 
         """
         filter_ = _create_edge_filter(
-            dm.DirectRelationReference("sp_powerops_types", "calculationIssue"),
+            dm.DirectRelationReference("sp_powerops_types_temp", "calculationIssue"),
             from_bid_matrix,
             from_bid_matrix_space,
             to_alert,
             to_alert_space,
             external_id_prefix,
             space,
         )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_matrix_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_matrix_raw_query.py`

 * *Files 2% similar despite different names*

```diff
@@ -3,53 +3,53 @@
 import datetime
 from typing import TYPE_CHECKING
 
 from cognite.client import data_modeling as dm, CogniteClient
 
 from cognite.powerops.client._generated.v1.data_classes import (
     DomainModelCore,
-    BidMatrix,
+    BidMatrixRaw,
 )
 from ._core import DEFAULT_QUERY_LIMIT, QueryBuilder, QueryStep, QueryAPI, T_DomainModelList, _create_edge_filter
 
 if TYPE_CHECKING:
     from .alert_query import AlertQueryAPI
 
 
-class BidMatrixQueryAPI(QueryAPI[T_DomainModelList]):
+class BidMatrixRawQueryAPI(QueryAPI[T_DomainModelList]):
     def __init__(
         self,
         client: CogniteClient,
         builder: QueryBuilder[T_DomainModelList],
         view_by_read_class: dict[type[DomainModelCore], dm.ViewId],
         filter_: dm.filters.Filter | None = None,
         limit: int = DEFAULT_QUERY_LIMIT,
     ):
         super().__init__(client, builder, view_by_read_class)
 
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("bid_matrix"),
+                name=self._builder.next_name("bid_matrix_raw"),
                 expression=dm.query.NodeResultSetExpression(
                     from_=self._builder[-1].name if self._builder else None,
                     filter=filter_,
                 ),
-                select=dm.query.Select([dm.query.SourceSelector(self._view_by_read_class[BidMatrix], ["*"])]),
-                result_cls=BidMatrix,
+                select=dm.query.Select([dm.query.SourceSelector(self._view_by_read_class[BidMatrixRaw], ["*"])]),
+                result_cls=BidMatrixRaw,
                 max_retrieve_limit=limit,
             )
         )
 
     def alerts(
         self,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_QUERY_LIMIT,
     ) -> AlertQueryAPI[T_DomainModelList]:
-        """Query along the alert edges of the bid matrix.
+        """Query along the alert edges of the bid matrix raw.
 
         Args:
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of alert edges to return. Defaults to 25. Set to -1, float("inf") or None
                 to return all items.
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_matrix_raw.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/multi_scenario_matrix.py`

 * *Files 14% similar despite different names*

```diff
@@ -9,299 +9,317 @@
 from cognite.client.data_classes.data_modeling.instances import InstanceAggregationResultList
 
 from cognite.powerops.client._generated.v1.data_classes._core import DEFAULT_INSTANCE_SPACE
 from cognite.powerops.client._generated.v1.data_classes import (
     DomainModelCore,
     DomainModelWrite,
     ResourcesWriteResult,
-    BidMatrixRaw,
-    BidMatrixRawWrite,
-    BidMatrixRawFields,
-    BidMatrixRawList,
-    BidMatrixRawWriteList,
-    BidMatrixRawTextFields,
+    MultiScenarioMatrix,
+    MultiScenarioMatrixWrite,
+    MultiScenarioMatrixFields,
+    MultiScenarioMatrixList,
+    MultiScenarioMatrixWriteList,
+    MultiScenarioMatrixTextFields,
 )
-from cognite.powerops.client._generated.v1.data_classes._bid_matrix_raw import (
-    _BIDMATRIXRAW_PROPERTIES_BY_FIELD,
-    _create_bid_matrix_raw_filter,
+from cognite.powerops.client._generated.v1.data_classes._multi_scenario_matrix import (
+    _MULTISCENARIOMATRIX_PROPERTIES_BY_FIELD,
+    _create_multi_scenario_matrix_filter,
 )
 from ._core import (
     DEFAULT_LIMIT_READ,
     DEFAULT_QUERY_LIMIT,
     Aggregations,
     NodeAPI,
     SequenceNotStr,
     QueryStep,
     QueryBuilder,
 )
-from .bid_matrix_raw_alerts import BidMatrixRawAlertsAPI
-from .bid_matrix_raw_query import BidMatrixRawQueryAPI
+from .multi_scenario_matrix_alerts import MultiScenarioMatrixAlertsAPI
+from .multi_scenario_matrix_scenario_results import MultiScenarioMatrixScenarioResultsAPI
+from .multi_scenario_matrix_query import MultiScenarioMatrixQueryAPI
 
 
-class BidMatrixRawAPI(NodeAPI[BidMatrixRaw, BidMatrixRawWrite, BidMatrixRawList]):
+class MultiScenarioMatrixAPI(NodeAPI[MultiScenarioMatrix, MultiScenarioMatrixWrite, MultiScenarioMatrixList]):
     def __init__(self, client: CogniteClient, view_by_read_class: dict[type[DomainModelCore], dm.ViewId]):
-        view_id = view_by_read_class[BidMatrixRaw]
+        view_id = view_by_read_class[MultiScenarioMatrix]
         super().__init__(
             client=client,
             sources=view_id,
-            class_type=BidMatrixRaw,
-            class_list=BidMatrixRawList,
-            class_write_list=BidMatrixRawWriteList,
+            class_type=MultiScenarioMatrix,
+            class_list=MultiScenarioMatrixList,
+            class_write_list=MultiScenarioMatrixWriteList,
             view_by_read_class=view_by_read_class,
         )
         self._view_id = view_id
-        self.alerts_edge = BidMatrixRawAlertsAPI(client)
+        self.alerts_edge = MultiScenarioMatrixAlertsAPI(client)
+        self.scenario_results_edge = MultiScenarioMatrixScenarioResultsAPI(client)
 
     def __call__(
         self,
         resource_cost: str | list[str] | None = None,
         resource_cost_prefix: str | None = None,
         asset_type: str | list[str] | None = None,
         asset_type_prefix: str | None = None,
         asset_id: str | list[str] | None = None,
         asset_id_prefix: str | None = None,
         is_processed: bool | None = None,
+        method: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_QUERY_LIMIT,
         filter: dm.Filter | None = None,
-    ) -> BidMatrixRawQueryAPI[BidMatrixRawList]:
-        """Query starting at bid matrix raws.
+    ) -> MultiScenarioMatrixQueryAPI[MultiScenarioMatrixList]:
+        """Query starting at multi scenario matrixes.
 
         Args:
             resource_cost: The resource cost to filter on.
             resource_cost_prefix: The prefix of the resource cost to filter on.
             asset_type: The asset type to filter on.
             asset_type_prefix: The prefix of the asset type to filter on.
             asset_id: The asset id to filter on.
             asset_id_prefix: The prefix of the asset id to filter on.
             is_processed: The is processed to filter on.
+            method: The method to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of bid matrix raws to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of multi scenario matrixes to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
-            A query API for bid matrix raws.
+            A query API for multi scenario matrixes.
 
         """
         has_data = dm.filters.HasData(views=[self._view_id])
-        filter_ = _create_bid_matrix_raw_filter(
+        filter_ = _create_multi_scenario_matrix_filter(
             self._view_id,
             resource_cost,
             resource_cost_prefix,
             asset_type,
             asset_type_prefix,
             asset_id,
             asset_id_prefix,
             is_processed,
+            method,
             external_id_prefix,
             space,
             (filter and dm.filters.And(filter, has_data)) or has_data,
         )
-        builder = QueryBuilder(BidMatrixRawList)
-        return BidMatrixRawQueryAPI(self._client, builder, self._view_by_read_class, filter_, limit)
+        builder = QueryBuilder(MultiScenarioMatrixList)
+        return MultiScenarioMatrixQueryAPI(self._client, builder, self._view_by_read_class, filter_, limit)
 
     def apply(
         self,
-        bid_matrix_raw: BidMatrixRawWrite | Sequence[BidMatrixRawWrite],
+        multi_scenario_matrix: MultiScenarioMatrixWrite | Sequence[MultiScenarioMatrixWrite],
         replace: bool = False,
         write_none: bool = False,
     ) -> ResourcesWriteResult:
-        """Add or update (upsert) bid matrix raws.
+        """Add or update (upsert) multi scenario matrixes.
 
-        Note: This method iterates through all nodes and timeseries linked to bid_matrix_raw and creates them including the edges
-        between the nodes. For example, if any of `alerts` are set, then these
+        Note: This method iterates through all nodes and timeseries linked to multi_scenario_matrix and creates them including the edges
+        between the nodes. For example, if any of `alerts` or `scenario_results` are set, then these
         nodes as well as any nodes linked to them, and all the edges linking these nodes will be created.
 
         Args:
-            bid_matrix_raw: Bid matrix raw or sequence of bid matrix raws to upsert.
+            multi_scenario_matrix: Multi scenario matrix or sequence of multi scenario matrixes to upsert.
             replace (bool): How do we behave when a property value exists? Do we replace all matching and existing values with the supplied values (true)?
                 Or should we merge in new values for properties together with the existing values (false)? Note: This setting applies for all nodes or edges specified in the ingestion call.
             write_none (bool): This method, will by default, skip properties that are set to None. However, if you want to set properties to None,
                 you can set this parameter to True. Note this only applies to properties that are nullable.
         Returns:
             Created instance(s), i.e., nodes, edges, and time series.
 
         Examples:
 
-            Create a new bid_matrix_raw:
+            Create a new multi_scenario_matrix:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
-                >>> from cognite.powerops.client._generated.v1.data_classes import BidMatrixRawWrite
+                >>> from cognite.powerops.client._generated.v1.data_classes import MultiScenarioMatrixWrite
                 >>> client = PowerOpsModelsV1Client()
-                >>> bid_matrix_raw = BidMatrixRawWrite(external_id="my_bid_matrix_raw", ...)
-                >>> result = client.bid_matrix_raw.apply(bid_matrix_raw)
+                >>> multi_scenario_matrix = MultiScenarioMatrixWrite(external_id="my_multi_scenario_matrix", ...)
+                >>> result = client.multi_scenario_matrix.apply(multi_scenario_matrix)
 
         """
         warnings.warn(
             "The .apply method is deprecated and will be removed in v1.0. "
             "Please use the .upsert method on the client instead. This means instead of "
-            "`my_client.bid_matrix_raw.apply(my_items)` please use `my_client.upsert(my_items)`."
+            "`my_client.multi_scenario_matrix.apply(my_items)` please use `my_client.upsert(my_items)`."
             "The motivation is that all apply methods are the same, and having one apply method per API "
             " class encourages users to create items in small batches, which is inefficient."
             "In addition, .upsert method is more descriptive of what the method does.",
             UserWarning,
             stacklevel=2,
         )
-        return self._apply(bid_matrix_raw, replace, write_none)
+        return self._apply(multi_scenario_matrix, replace, write_none)
 
     def delete(
         self, external_id: str | SequenceNotStr[str], space: str = DEFAULT_INSTANCE_SPACE
     ) -> dm.InstancesDeleteResult:
-        """Delete one or more bid matrix raw.
+        """Delete one or more multi scenario matrix.
 
         Args:
-            external_id: External id of the bid matrix raw to delete.
-            space: The space where all the bid matrix raw are located.
+            external_id: External id of the multi scenario matrix to delete.
+            space: The space where all the multi scenario matrix are located.
 
         Returns:
             The instance(s), i.e., nodes and edges which has been deleted. Empty list if nothing was deleted.
 
         Examples:
 
-            Delete bid_matrix_raw by id:
+            Delete multi_scenario_matrix by id:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> client.bid_matrix_raw.delete("my_bid_matrix_raw")
+                >>> client.multi_scenario_matrix.delete("my_multi_scenario_matrix")
         """
         warnings.warn(
             "The .delete method is deprecated and will be removed in v1.0. "
             "Please use the .delete method on the client instead. This means instead of "
-            "`my_client.bid_matrix_raw.delete(my_ids)` please use `my_client.delete(my_ids)`."
+            "`my_client.multi_scenario_matrix.delete(my_ids)` please use `my_client.delete(my_ids)`."
             "The motivation is that all delete methods are the same, and having one delete method per API "
             " class encourages users to delete items in small batches, which is inefficient.",
             UserWarning,
             stacklevel=2,
         )
         return self._delete(external_id, space)
 
     @overload
-    def retrieve(self, external_id: str, space: str = DEFAULT_INSTANCE_SPACE) -> BidMatrixRaw | None: ...
+    def retrieve(self, external_id: str, space: str = DEFAULT_INSTANCE_SPACE) -> MultiScenarioMatrix | None: ...
 
     @overload
-    def retrieve(self, external_id: SequenceNotStr[str], space: str = DEFAULT_INSTANCE_SPACE) -> BidMatrixRawList: ...
+    def retrieve(
+        self, external_id: SequenceNotStr[str], space: str = DEFAULT_INSTANCE_SPACE
+    ) -> MultiScenarioMatrixList: ...
 
     def retrieve(
         self, external_id: str | SequenceNotStr[str], space: str = DEFAULT_INSTANCE_SPACE
-    ) -> BidMatrixRaw | BidMatrixRawList | None:
-        """Retrieve one or more bid matrix raws by id(s).
+    ) -> MultiScenarioMatrix | MultiScenarioMatrixList | None:
+        """Retrieve one or more multi scenario matrixes by id(s).
 
         Args:
-            external_id: External id or list of external ids of the bid matrix raws.
-            space: The space where all the bid matrix raws are located.
+            external_id: External id or list of external ids of the multi scenario matrixes.
+            space: The space where all the multi scenario matrixes are located.
 
         Returns:
-            The requested bid matrix raws.
+            The requested multi scenario matrixes.
 
         Examples:
 
-            Retrieve bid_matrix_raw by id:
+            Retrieve multi_scenario_matrix by id:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> bid_matrix_raw = client.bid_matrix_raw.retrieve("my_bid_matrix_raw")
+                >>> multi_scenario_matrix = client.multi_scenario_matrix.retrieve("my_multi_scenario_matrix")
 
         """
         return self._retrieve(
             external_id,
             space,
             retrieve_edges=True,
             edge_api_name_type_direction_view_id_penta=[
                 (
                     self.alerts_edge,
                     "alerts",
                     dm.DirectRelationReference("sp_powerops_types", "calculationIssue"),
                     "outwards",
                     dm.ViewId("sp_powerops_models", "Alert", "1"),
                 ),
+                (
+                    self.scenario_results_edge,
+                    "scenario_results",
+                    dm.DirectRelationReference("sp_powerops_types", "MultiScenarioMatrix.scenarioResults"),
+                    "outwards",
+                    dm.ViewId("sp_powerops_models", "PriceProdCase", "1"),
+                ),
             ],
         )
 
     def search(
         self,
         query: str,
-        properties: BidMatrixRawTextFields | Sequence[BidMatrixRawTextFields] | None = None,
+        properties: MultiScenarioMatrixTextFields | Sequence[MultiScenarioMatrixTextFields] | None = None,
         resource_cost: str | list[str] | None = None,
         resource_cost_prefix: str | None = None,
         asset_type: str | list[str] | None = None,
         asset_type_prefix: str | None = None,
         asset_id: str | list[str] | None = None,
         asset_id_prefix: str | None = None,
         is_processed: bool | None = None,
+        method: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
-    ) -> BidMatrixRawList:
-        """Search bid matrix raws
+    ) -> MultiScenarioMatrixList:
+        """Search multi scenario matrixes
 
         Args:
             query: The search query,
             properties: The property to search, if nothing is passed all text fields will be searched.
             resource_cost: The resource cost to filter on.
             resource_cost_prefix: The prefix of the resource cost to filter on.
             asset_type: The asset type to filter on.
             asset_type_prefix: The prefix of the asset type to filter on.
             asset_id: The asset id to filter on.
             asset_id_prefix: The prefix of the asset id to filter on.
             is_processed: The is processed to filter on.
+            method: The method to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of bid matrix raws to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of multi scenario matrixes to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
-            Search results bid matrix raws matching the query.
+            Search results multi scenario matrixes matching the query.
 
         Examples:
 
-           Search for 'my_bid_matrix_raw' in all text properties:
+           Search for 'my_multi_scenario_matrix' in all text properties:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> bid_matrix_raws = client.bid_matrix_raw.search('my_bid_matrix_raw')
+                >>> multi_scenario_matrixes = client.multi_scenario_matrix.search('my_multi_scenario_matrix')
 
         """
-        filter_ = _create_bid_matrix_raw_filter(
+        filter_ = _create_multi_scenario_matrix_filter(
             self._view_id,
             resource_cost,
             resource_cost_prefix,
             asset_type,
             asset_type_prefix,
             asset_id,
             asset_id_prefix,
             is_processed,
+            method,
             external_id_prefix,
             space,
             filter,
         )
-        return self._search(self._view_id, query, _BIDMATRIXRAW_PROPERTIES_BY_FIELD, properties, filter_, limit)
+        return self._search(self._view_id, query, _MULTISCENARIOMATRIX_PROPERTIES_BY_FIELD, properties, filter_, limit)
 
     @overload
     def aggregate(
         self,
         aggregations: (
             Aggregations
             | dm.aggregations.MetricAggregation
             | Sequence[Aggregations]
             | Sequence[dm.aggregations.MetricAggregation]
         ),
-        property: BidMatrixRawFields | Sequence[BidMatrixRawFields] | None = None,
+        property: MultiScenarioMatrixFields | Sequence[MultiScenarioMatrixFields] | None = None,
         group_by: None = None,
         query: str | None = None,
-        search_properties: BidMatrixRawTextFields | Sequence[BidMatrixRawTextFields] | None = None,
+        search_properties: MultiScenarioMatrixTextFields | Sequence[MultiScenarioMatrixTextFields] | None = None,
         resource_cost: str | list[str] | None = None,
         resource_cost_prefix: str | None = None,
         asset_type: str | list[str] | None = None,
         asset_type_prefix: str | None = None,
         asset_id: str | list[str] | None = None,
         asset_id_prefix: str | None = None,
         is_processed: bool | None = None,
+        method: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> list[dm.aggregations.AggregatedNumberedValue]: ...
 
     @overload
@@ -309,172 +327,179 @@
         self,
         aggregations: (
             Aggregations
             | dm.aggregations.MetricAggregation
             | Sequence[Aggregations]
             | Sequence[dm.aggregations.MetricAggregation]
         ),
-        property: BidMatrixRawFields | Sequence[BidMatrixRawFields] | None = None,
-        group_by: BidMatrixRawFields | Sequence[BidMatrixRawFields] = None,
+        property: MultiScenarioMatrixFields | Sequence[MultiScenarioMatrixFields] | None = None,
+        group_by: MultiScenarioMatrixFields | Sequence[MultiScenarioMatrixFields] = None,
         query: str | None = None,
-        search_properties: BidMatrixRawTextFields | Sequence[BidMatrixRawTextFields] | None = None,
+        search_properties: MultiScenarioMatrixTextFields | Sequence[MultiScenarioMatrixTextFields] | None = None,
         resource_cost: str | list[str] | None = None,
         resource_cost_prefix: str | None = None,
         asset_type: str | list[str] | None = None,
         asset_type_prefix: str | None = None,
         asset_id: str | list[str] | None = None,
         asset_id_prefix: str | None = None,
         is_processed: bool | None = None,
+        method: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> InstanceAggregationResultList: ...
 
     def aggregate(
         self,
         aggregate: (
             Aggregations
             | dm.aggregations.MetricAggregation
             | Sequence[Aggregations]
             | Sequence[dm.aggregations.MetricAggregation]
         ),
-        property: BidMatrixRawFields | Sequence[BidMatrixRawFields] | None = None,
-        group_by: BidMatrixRawFields | Sequence[BidMatrixRawFields] | None = None,
+        property: MultiScenarioMatrixFields | Sequence[MultiScenarioMatrixFields] | None = None,
+        group_by: MultiScenarioMatrixFields | Sequence[MultiScenarioMatrixFields] | None = None,
         query: str | None = None,
-        search_property: BidMatrixRawTextFields | Sequence[BidMatrixRawTextFields] | None = None,
+        search_property: MultiScenarioMatrixTextFields | Sequence[MultiScenarioMatrixTextFields] | None = None,
         resource_cost: str | list[str] | None = None,
         resource_cost_prefix: str | None = None,
         asset_type: str | list[str] | None = None,
         asset_type_prefix: str | None = None,
         asset_id: str | list[str] | None = None,
         asset_id_prefix: str | None = None,
         is_processed: bool | None = None,
+        method: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> list[dm.aggregations.AggregatedNumberedValue] | InstanceAggregationResultList:
-        """Aggregate data across bid matrix raws
+        """Aggregate data across multi scenario matrixes
 
         Args:
             aggregate: The aggregation to perform.
             property: The property to perform aggregation on.
             group_by: The property to group by when doing the aggregation.
             query: The query to search for in the text field.
             search_property: The text field to search in.
             resource_cost: The resource cost to filter on.
             resource_cost_prefix: The prefix of the resource cost to filter on.
             asset_type: The asset type to filter on.
             asset_type_prefix: The prefix of the asset type to filter on.
             asset_id: The asset id to filter on.
             asset_id_prefix: The prefix of the asset id to filter on.
             is_processed: The is processed to filter on.
+            method: The method to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of bid matrix raws to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of multi scenario matrixes to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
             Aggregation results.
 
         Examples:
 
-            Count bid matrix raws in space `my_space`:
+            Count multi scenario matrixes in space `my_space`:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> result = client.bid_matrix_raw.aggregate("count", space="my_space")
+                >>> result = client.multi_scenario_matrix.aggregate("count", space="my_space")
 
         """
 
-        filter_ = _create_bid_matrix_raw_filter(
+        filter_ = _create_multi_scenario_matrix_filter(
             self._view_id,
             resource_cost,
             resource_cost_prefix,
             asset_type,
             asset_type_prefix,
             asset_id,
             asset_id_prefix,
             is_processed,
+            method,
             external_id_prefix,
             space,
             filter,
         )
         return self._aggregate(
             self._view_id,
             aggregate,
-            _BIDMATRIXRAW_PROPERTIES_BY_FIELD,
+            _MULTISCENARIOMATRIX_PROPERTIES_BY_FIELD,
             property,
             group_by,
             query,
             search_property,
             limit,
             filter_,
         )
 
     def histogram(
         self,
-        property: BidMatrixRawFields,
+        property: MultiScenarioMatrixFields,
         interval: float,
         query: str | None = None,
-        search_property: BidMatrixRawTextFields | Sequence[BidMatrixRawTextFields] | None = None,
+        search_property: MultiScenarioMatrixTextFields | Sequence[MultiScenarioMatrixTextFields] | None = None,
         resource_cost: str | list[str] | None = None,
         resource_cost_prefix: str | None = None,
         asset_type: str | list[str] | None = None,
         asset_type_prefix: str | None = None,
         asset_id: str | list[str] | None = None,
         asset_id_prefix: str | None = None,
         is_processed: bool | None = None,
+        method: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> dm.aggregations.HistogramValue:
-        """Produces histograms for bid matrix raws
+        """Produces histograms for multi scenario matrixes
 
         Args:
             property: The property to use as the value in the histogram.
             interval: The interval to use for the histogram bins.
             query: The query to search for in the text field.
             search_property: The text field to search in.
             resource_cost: The resource cost to filter on.
             resource_cost_prefix: The prefix of the resource cost to filter on.
             asset_type: The asset type to filter on.
             asset_type_prefix: The prefix of the asset type to filter on.
             asset_id: The asset id to filter on.
             asset_id_prefix: The prefix of the asset id to filter on.
             is_processed: The is processed to filter on.
+            method: The method to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of bid matrix raws to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of multi scenario matrixes to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
             Bucketed histogram results.
 
         """
-        filter_ = _create_bid_matrix_raw_filter(
+        filter_ = _create_multi_scenario_matrix_filter(
             self._view_id,
             resource_cost,
             resource_cost_prefix,
             asset_type,
             asset_type_prefix,
             asset_id,
             asset_id_prefix,
             is_processed,
+            method,
             external_id_prefix,
             space,
             filter,
         )
         return self._histogram(
             self._view_id,
             property,
             interval,
-            _BIDMATRIXRAW_PROPERTIES_BY_FIELD,
+            _MULTISCENARIOMATRIX_PROPERTIES_BY_FIELD,
             query,
             search_property,
             limit,
             filter_,
         )
 
     def list(
@@ -482,57 +507,60 @@
         resource_cost: str | list[str] | None = None,
         resource_cost_prefix: str | None = None,
         asset_type: str | list[str] | None = None,
         asset_type_prefix: str | None = None,
         asset_id: str | list[str] | None = None,
         asset_id_prefix: str | None = None,
         is_processed: bool | None = None,
+        method: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
         retrieve_edges: bool = True,
-    ) -> BidMatrixRawList:
-        """List/filter bid matrix raws
+    ) -> MultiScenarioMatrixList:
+        """List/filter multi scenario matrixes
 
         Args:
             resource_cost: The resource cost to filter on.
             resource_cost_prefix: The prefix of the resource cost to filter on.
             asset_type: The asset type to filter on.
             asset_type_prefix: The prefix of the asset type to filter on.
             asset_id: The asset id to filter on.
             asset_id_prefix: The prefix of the asset id to filter on.
             is_processed: The is processed to filter on.
+            method: The method to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of bid matrix raws to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of multi scenario matrixes to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
-            retrieve_edges: Whether to retrieve `alerts` external ids for the bid matrix raws. Defaults to True.
+            retrieve_edges: Whether to retrieve `alerts` or `scenario_results` external ids for the multi scenario matrixes. Defaults to True.
 
         Returns:
-            List of requested bid matrix raws
+            List of requested multi scenario matrixes
 
         Examples:
 
-            List bid matrix raws and limit to 5:
+            List multi scenario matrixes and limit to 5:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> bid_matrix_raws = client.bid_matrix_raw.list(limit=5)
+                >>> multi_scenario_matrixes = client.multi_scenario_matrix.list(limit=5)
 
         """
-        filter_ = _create_bid_matrix_raw_filter(
+        filter_ = _create_multi_scenario_matrix_filter(
             self._view_id,
             resource_cost,
             resource_cost_prefix,
             asset_type,
             asset_type_prefix,
             asset_id,
             asset_id_prefix,
             is_processed,
+            method,
             external_id_prefix,
             space,
             filter,
         )
 
         return self._list(
             limit=limit,
@@ -542,9 +570,16 @@
                 (
                     self.alerts_edge,
                     "alerts",
                     dm.DirectRelationReference("sp_powerops_types", "calculationIssue"),
                     "outwards",
                     dm.ViewId("sp_powerops_models", "Alert", "1"),
                 ),
+                (
+                    self.scenario_results_edge,
+                    "scenario_results",
+                    dm.DirectRelationReference("sp_powerops_types", "MultiScenarioMatrix.scenarioResults"),
+                    "outwards",
+                    dm.ViewId("sp_powerops_models", "PriceProdCase", "1"),
+                ),
             ],
         )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_matrix_raw_alerts.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_matrix_raw_alerts.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_matrix_raw_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/preprocessor_input_query.py`

 * *Files 18% similar despite different names*

```diff
@@ -3,89 +3,72 @@
 import datetime
 from typing import TYPE_CHECKING
 
 from cognite.client import data_modeling as dm, CogniteClient
 
 from cognite.powerops.client._generated.v1.data_classes import (
     DomainModelCore,
-    BidMatrixRaw,
+    PreprocessorInput,
+    Scenario,
 )
 from ._core import DEFAULT_QUERY_LIMIT, QueryBuilder, QueryStep, QueryAPI, T_DomainModelList, _create_edge_filter
 
-if TYPE_CHECKING:
-    from .alert_query import AlertQueryAPI
 
-
-class BidMatrixRawQueryAPI(QueryAPI[T_DomainModelList]):
+class PreprocessorInputQueryAPI(QueryAPI[T_DomainModelList]):
     def __init__(
         self,
         client: CogniteClient,
         builder: QueryBuilder[T_DomainModelList],
         view_by_read_class: dict[type[DomainModelCore], dm.ViewId],
         filter_: dm.filters.Filter | None = None,
         limit: int = DEFAULT_QUERY_LIMIT,
     ):
         super().__init__(client, builder, view_by_read_class)
 
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("bid_matrix_raw"),
+                name=self._builder.next_name("preprocessor_input"),
                 expression=dm.query.NodeResultSetExpression(
                     from_=self._builder[-1].name if self._builder else None,
                     filter=filter_,
                 ),
-                select=dm.query.Select([dm.query.SourceSelector(self._view_by_read_class[BidMatrixRaw], ["*"])]),
-                result_cls=BidMatrixRaw,
+                select=dm.query.Select([dm.query.SourceSelector(self._view_by_read_class[PreprocessorInput], ["*"])]),
+                result_cls=PreprocessorInput,
                 max_retrieve_limit=limit,
             )
         )
 
-    def alerts(
+    def query(
         self,
-        external_id_prefix: str | None = None,
-        space: str | list[str] | None = None,
-        limit: int | None = DEFAULT_QUERY_LIMIT,
-    ) -> AlertQueryAPI[T_DomainModelList]:
-        """Query along the alert edges of the bid matrix raw.
+        retrieve_scenario: bool = False,
+    ) -> T_DomainModelList:
+        """Execute query and return the result.
 
         Args:
-            external_id_prefix: The prefix of the external ID to filter on.
-            space: The space to filter on.
-            limit: Maximum number of alert edges to return. Defaults to 25. Set to -1, float("inf") or None
-                to return all items.
+            retrieve_scenario: Whether to retrieve the scenario for each preprocessor input or not.
 
         Returns:
-            AlertQueryAPI: The query API for the alert.
-        """
-        from .alert_query import AlertQueryAPI
+            The list of the source nodes of the query.
 
+        """
         from_ = self._builder[-1].name
+        if retrieve_scenario:
+            self._query_append_scenario(from_)
+        return self._query()
 
-        edge_filter = _create_edge_filter(
-            dm.DirectRelationReference("sp_powerops_types", "calculationIssue"),
-            external_id_prefix=external_id_prefix,
-            space=space,
-        )
+    def _query_append_scenario(self, from_: str) -> None:
+        view_id = self._view_by_read_class[Scenario]
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("alerts"),
-                expression=dm.query.EdgeResultSetExpression(
-                    filter=edge_filter,
+                name=self._builder.next_name("scenario"),
+                expression=dm.query.NodeResultSetExpression(
+                    filter=dm.filters.HasData(views=[view_id]),
                     from_=from_,
+                    through=self._view_by_read_class[PreprocessorInput].as_property_ref("scenario"),
                     direction="outwards",
                 ),
-                select=dm.query.Select(),
-                max_retrieve_limit=limit,
-            )
+                select=dm.query.Select([dm.query.SourceSelector(view_id, ["*"])]),
+                max_retrieve_limit=-1,
+                result_cls=Scenario,
+                is_single_direct_relation=True,
+            ),
         )
-        return AlertQueryAPI(self._client, self._builder, self._view_by_read_class, None, limit)
-
-    def query(
-        self,
-    ) -> T_DomainModelList:
-        """Execute query and return the result.
-
-        Returns:
-            The list of the source nodes of the query.
-
-        """
-        return self._query()
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_method.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_method.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_method_afrr.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_method_afrr.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_method_afrr_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_method_afrr_query.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_method_custom.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_method_custom.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_method_custom_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_method_custom_query.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_method_day_ahead.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_method_day_ahead.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_method_day_ahead_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_method_day_ahead_query.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_method_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_method_query.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_method_shop_multi_scenario.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_method_shop_multi_scenario.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_method_shop_multi_scenario_price_scenarios.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_method_shop_multi_scenario_price_scenarios.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_method_shop_multi_scenario_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/price_prod_case_query.py`

 * *Files 19% similar despite different names*

```diff
@@ -3,91 +3,71 @@
 import datetime
 from typing import TYPE_CHECKING
 
 from cognite.client import data_modeling as dm, CogniteClient
 
 from cognite.powerops.client._generated.v1.data_classes import (
     DomainModelCore,
-    BidMethodSHOPMultiScenario,
+    PriceProdCase,
+    Case,
 )
 from ._core import DEFAULT_QUERY_LIMIT, QueryBuilder, QueryStep, QueryAPI, T_DomainModelList, _create_edge_filter
 
-if TYPE_CHECKING:
-    from .scenario_query import ScenarioQueryAPI
 
-
-class BidMethodSHOPMultiScenarioQueryAPI(QueryAPI[T_DomainModelList]):
+class PriceProdCaseQueryAPI(QueryAPI[T_DomainModelList]):
     def __init__(
         self,
         client: CogniteClient,
         builder: QueryBuilder[T_DomainModelList],
         view_by_read_class: dict[type[DomainModelCore], dm.ViewId],
         filter_: dm.filters.Filter | None = None,
         limit: int = DEFAULT_QUERY_LIMIT,
     ):
         super().__init__(client, builder, view_by_read_class)
 
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("bid_method_shop_multi_scenario"),
+                name=self._builder.next_name("price_prod_case"),
                 expression=dm.query.NodeResultSetExpression(
                     from_=self._builder[-1].name if self._builder else None,
                     filter=filter_,
                 ),
-                select=dm.query.Select(
-                    [dm.query.SourceSelector(self._view_by_read_class[BidMethodSHOPMultiScenario], ["*"])]
-                ),
-                result_cls=BidMethodSHOPMultiScenario,
+                select=dm.query.Select([dm.query.SourceSelector(self._view_by_read_class[PriceProdCase], ["*"])]),
+                result_cls=PriceProdCase,
                 max_retrieve_limit=limit,
             )
         )
 
-    def scenarios(
+    def query(
         self,
-        external_id_prefix: str | None = None,
-        space: str | list[str] | None = None,
-        limit: int | None = DEFAULT_QUERY_LIMIT,
-    ) -> ScenarioQueryAPI[T_DomainModelList]:
-        """Query along the scenario edges of the bid method shop multi scenario.
+        retrieve_case: bool = False,
+    ) -> T_DomainModelList:
+        """Execute query and return the result.
 
         Args:
-            external_id_prefix: The prefix of the external ID to filter on.
-            space: The space to filter on.
-            limit: Maximum number of scenario edges to return. Defaults to 25. Set to -1, float("inf") or None
-                to return all items.
+            retrieve_case: Whether to retrieve the case for each price prod case or not.
 
         Returns:
-            ScenarioQueryAPI: The query API for the scenario.
-        """
-        from .scenario_query import ScenarioQueryAPI
+            The list of the source nodes of the query.
 
+        """
         from_ = self._builder[-1].name
+        if retrieve_case:
+            self._query_append_case(from_)
+        return self._query()
 
-        edge_filter = _create_edge_filter(
-            dm.DirectRelationReference("sp_powerops_types", "BidMethodDayahead.scenarios"),
-            external_id_prefix=external_id_prefix,
-            space=space,
-        )
+    def _query_append_case(self, from_: str) -> None:
+        view_id = self._view_by_read_class[Case]
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("scenarios"),
-                expression=dm.query.EdgeResultSetExpression(
-                    filter=edge_filter,
+                name=self._builder.next_name("case"),
+                expression=dm.query.NodeResultSetExpression(
+                    filter=dm.filters.HasData(views=[view_id]),
                     from_=from_,
+                    through=self._view_by_read_class[PriceProdCase].as_property_ref("case"),
                     direction="outwards",
                 ),
-                select=dm.query.Select(),
-                max_retrieve_limit=limit,
-            )
+                select=dm.query.Select([dm.query.SourceSelector(view_id, ["*"])]),
+                max_retrieve_limit=-1,
+                result_cls=Case,
+            ),
         )
-        return ScenarioQueryAPI(self._client, self._builder, self._view_by_read_class, None, limit)
-
-    def query(
-        self,
-    ) -> T_DomainModelList:
-        """Execute query and return the result.
-
-        Returns:
-            The list of the source nodes of the query.
-
-        """
-        return self._query()
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_method_shop_multi_scenario_scenarios.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_method_shop_multi_scenario_scenarios.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_method_water_value.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_method_water_value.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_method_water_value_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_method_water_value_query.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_row.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/price_area_afrr.py`

 * *Files 14% similar despite different names*

```diff
@@ -9,348 +9,319 @@
 from cognite.client.data_classes.data_modeling.instances import InstanceAggregationResultList
 
 from cognite.powerops.client._generated.v1.data_classes._core import DEFAULT_INSTANCE_SPACE
 from cognite.powerops.client._generated.v1.data_classes import (
     DomainModelCore,
     DomainModelWrite,
     ResourcesWriteResult,
-    BidRow,
-    BidRowWrite,
-    BidRowFields,
-    BidRowList,
-    BidRowWriteList,
-    BidRowTextFields,
+    PriceAreaAFRR,
+    PriceAreaAFRRWrite,
+    PriceAreaAFRRFields,
+    PriceAreaAFRRList,
+    PriceAreaAFRRWriteList,
+    PriceAreaAFRRTextFields,
 )
-from cognite.powerops.client._generated.v1.data_classes._bid_row import (
-    _BIDROW_PROPERTIES_BY_FIELD,
-    _create_bid_row_filter,
+from cognite.powerops.client._generated.v1.data_classes._price_area_afrr import (
+    _PRICEAREAAFRR_PROPERTIES_BY_FIELD,
+    _create_price_area_afrr_filter,
 )
 from ._core import (
     DEFAULT_LIMIT_READ,
     DEFAULT_QUERY_LIMIT,
     Aggregations,
     NodeAPI,
     SequenceNotStr,
     QueryStep,
     QueryBuilder,
 )
-from .bid_row_alerts import BidRowAlertsAPI
-from .bid_row_query import BidRowQueryAPI
+from .price_area_afrr_capacity_price_up import PriceAreaAFRRCapacityPriceUpAPI
+from .price_area_afrr_capacity_price_down import PriceAreaAFRRCapacityPriceDownAPI
+from .price_area_afrr_activation_price_up import PriceAreaAFRRActivationPriceUpAPI
+from .price_area_afrr_activation_price_down import PriceAreaAFRRActivationPriceDownAPI
+from .price_area_afrr_relative_activation import PriceAreaAFRRRelativeActivationAPI
+from .price_area_afrr_total_capacity_allocation_up import PriceAreaAFRRTotalCapacityAllocationUpAPI
+from .price_area_afrr_total_capacity_allocation_down import PriceAreaAFRRTotalCapacityAllocationDownAPI
+from .price_area_afrr_own_capacity_allocation_up import PriceAreaAFRROwnCapacityAllocationUpAPI
+from .price_area_afrr_own_capacity_allocation_down import PriceAreaAFRROwnCapacityAllocationDownAPI
+from .price_area_afrr_query import PriceAreaAFRRQueryAPI
 
 
-class BidRowAPI(NodeAPI[BidRow, BidRowWrite, BidRowList]):
+class PriceAreaAFRRAPI(NodeAPI[PriceAreaAFRR, PriceAreaAFRRWrite, PriceAreaAFRRList]):
     def __init__(self, client: CogniteClient, view_by_read_class: dict[type[DomainModelCore], dm.ViewId]):
-        view_id = view_by_read_class[BidRow]
+        view_id = view_by_read_class[PriceAreaAFRR]
         super().__init__(
             client=client,
             sources=view_id,
-            class_type=BidRow,
-            class_list=BidRowList,
-            class_write_list=BidRowWriteList,
+            class_type=PriceAreaAFRR,
+            class_list=PriceAreaAFRRList,
+            class_write_list=PriceAreaAFRRWriteList,
             view_by_read_class=view_by_read_class,
         )
         self._view_id = view_id
-        self.alerts_edge = BidRowAlertsAPI(client)
+        self.capacity_price_up = PriceAreaAFRRCapacityPriceUpAPI(client, view_id)
+        self.capacity_price_down = PriceAreaAFRRCapacityPriceDownAPI(client, view_id)
+        self.activation_price_up = PriceAreaAFRRActivationPriceUpAPI(client, view_id)
+        self.activation_price_down = PriceAreaAFRRActivationPriceDownAPI(client, view_id)
+        self.relative_activation = PriceAreaAFRRRelativeActivationAPI(client, view_id)
+        self.total_capacity_allocation_up = PriceAreaAFRRTotalCapacityAllocationUpAPI(client, view_id)
+        self.total_capacity_allocation_down = PriceAreaAFRRTotalCapacityAllocationDownAPI(client, view_id)
+        self.own_capacity_allocation_up = PriceAreaAFRROwnCapacityAllocationUpAPI(client, view_id)
+        self.own_capacity_allocation_down = PriceAreaAFRROwnCapacityAllocationDownAPI(client, view_id)
 
     def __call__(
         self,
-        min_price: float | None = None,
-        max_price: float | None = None,
-        product: str | list[str] | None = None,
-        product_prefix: str | None = None,
-        is_divisible: bool | None = None,
-        is_block: bool | None = None,
-        exclusive_group_id: str | list[str] | None = None,
-        exclusive_group_id_prefix: str | None = None,
-        linked_bid: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        name: str | list[str] | None = None,
+        name_prefix: str | None = None,
+        display_name: str | list[str] | None = None,
+        display_name_prefix: str | None = None,
+        min_ordering: int | None = None,
+        max_ordering: int | None = None,
         asset_type: str | list[str] | None = None,
         asset_type_prefix: str | None = None,
-        asset_id: str | list[str] | None = None,
-        asset_id_prefix: str | None = None,
-        method: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        timezone: str | list[str] | None = None,
+        timezone_prefix: str | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_QUERY_LIMIT,
         filter: dm.Filter | None = None,
-    ) -> BidRowQueryAPI[BidRowList]:
-        """Query starting at bid rows.
+    ) -> PriceAreaAFRRQueryAPI[PriceAreaAFRRList]:
+        """Query starting at price area afrrs.
 
         Args:
-            min_price: The minimum value of the price to filter on.
-            max_price: The maximum value of the price to filter on.
-            product: The product to filter on.
-            product_prefix: The prefix of the product to filter on.
-            is_divisible: The is divisible to filter on.
-            is_block: The is block to filter on.
-            exclusive_group_id: The exclusive group id to filter on.
-            exclusive_group_id_prefix: The prefix of the exclusive group id to filter on.
-            linked_bid: The linked bid to filter on.
+            name: The name to filter on.
+            name_prefix: The prefix of the name to filter on.
+            display_name: The display name to filter on.
+            display_name_prefix: The prefix of the display name to filter on.
+            min_ordering: The minimum value of the ordering to filter on.
+            max_ordering: The maximum value of the ordering to filter on.
             asset_type: The asset type to filter on.
             asset_type_prefix: The prefix of the asset type to filter on.
-            asset_id: The asset id to filter on.
-            asset_id_prefix: The prefix of the asset id to filter on.
-            method: The method to filter on.
+            timezone: The timezone to filter on.
+            timezone_prefix: The prefix of the timezone to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of bid rows to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of price area afrrs to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
-            A query API for bid rows.
+            A query API for price area afrrs.
 
         """
         has_data = dm.filters.HasData(views=[self._view_id])
-        filter_ = _create_bid_row_filter(
+        filter_ = _create_price_area_afrr_filter(
             self._view_id,
-            min_price,
-            max_price,
-            product,
-            product_prefix,
-            is_divisible,
-            is_block,
-            exclusive_group_id,
-            exclusive_group_id_prefix,
-            linked_bid,
+            name,
+            name_prefix,
+            display_name,
+            display_name_prefix,
+            min_ordering,
+            max_ordering,
             asset_type,
             asset_type_prefix,
-            asset_id,
-            asset_id_prefix,
-            method,
+            timezone,
+            timezone_prefix,
             external_id_prefix,
             space,
             (filter and dm.filters.And(filter, has_data)) or has_data,
         )
-        builder = QueryBuilder(BidRowList)
-        return BidRowQueryAPI(self._client, builder, self._view_by_read_class, filter_, limit)
+        builder = QueryBuilder(PriceAreaAFRRList)
+        return PriceAreaAFRRQueryAPI(self._client, builder, self._view_by_read_class, filter_, limit)
 
     def apply(
         self,
-        bid_row: BidRowWrite | Sequence[BidRowWrite],
+        price_area_afrr: PriceAreaAFRRWrite | Sequence[PriceAreaAFRRWrite],
         replace: bool = False,
         write_none: bool = False,
     ) -> ResourcesWriteResult:
-        """Add or update (upsert) bid rows.
-
-        Note: This method iterates through all nodes and timeseries linked to bid_row and creates them including the edges
-        between the nodes. For example, if any of `alerts` are set, then these
-        nodes as well as any nodes linked to them, and all the edges linking these nodes will be created.
+        """Add or update (upsert) price area afrrs.
 
         Args:
-            bid_row: Bid row or sequence of bid rows to upsert.
+            price_area_afrr: Price area afrr or sequence of price area afrrs to upsert.
             replace (bool): How do we behave when a property value exists? Do we replace all matching and existing values with the supplied values (true)?
                 Or should we merge in new values for properties together with the existing values (false)? Note: This setting applies for all nodes or edges specified in the ingestion call.
             write_none (bool): This method, will by default, skip properties that are set to None. However, if you want to set properties to None,
                 you can set this parameter to True. Note this only applies to properties that are nullable.
         Returns:
             Created instance(s), i.e., nodes, edges, and time series.
 
         Examples:
 
-            Create a new bid_row:
+            Create a new price_area_afrr:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
-                >>> from cognite.powerops.client._generated.v1.data_classes import BidRowWrite
+                >>> from cognite.powerops.client._generated.v1.data_classes import PriceAreaAFRRWrite
                 >>> client = PowerOpsModelsV1Client()
-                >>> bid_row = BidRowWrite(external_id="my_bid_row", ...)
-                >>> result = client.bid_row.apply(bid_row)
+                >>> price_area_afrr = PriceAreaAFRRWrite(external_id="my_price_area_afrr", ...)
+                >>> result = client.price_area_afrr.apply(price_area_afrr)
 
         """
         warnings.warn(
             "The .apply method is deprecated and will be removed in v1.0. "
             "Please use the .upsert method on the client instead. This means instead of "
-            "`my_client.bid_row.apply(my_items)` please use `my_client.upsert(my_items)`."
+            "`my_client.price_area_afrr.apply(my_items)` please use `my_client.upsert(my_items)`."
             "The motivation is that all apply methods are the same, and having one apply method per API "
             " class encourages users to create items in small batches, which is inefficient."
             "In addition, .upsert method is more descriptive of what the method does.",
             UserWarning,
             stacklevel=2,
         )
-        return self._apply(bid_row, replace, write_none)
+        return self._apply(price_area_afrr, replace, write_none)
 
     def delete(
         self, external_id: str | SequenceNotStr[str], space: str = DEFAULT_INSTANCE_SPACE
     ) -> dm.InstancesDeleteResult:
-        """Delete one or more bid row.
+        """Delete one or more price area afrr.
 
         Args:
-            external_id: External id of the bid row to delete.
-            space: The space where all the bid row are located.
+            external_id: External id of the price area afrr to delete.
+            space: The space where all the price area afrr are located.
 
         Returns:
             The instance(s), i.e., nodes and edges which has been deleted. Empty list if nothing was deleted.
 
         Examples:
 
-            Delete bid_row by id:
+            Delete price_area_afrr by id:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> client.bid_row.delete("my_bid_row")
+                >>> client.price_area_afrr.delete("my_price_area_afrr")
         """
         warnings.warn(
             "The .delete method is deprecated and will be removed in v1.0. "
             "Please use the .delete method on the client instead. This means instead of "
-            "`my_client.bid_row.delete(my_ids)` please use `my_client.delete(my_ids)`."
+            "`my_client.price_area_afrr.delete(my_ids)` please use `my_client.delete(my_ids)`."
             "The motivation is that all delete methods are the same, and having one delete method per API "
             " class encourages users to delete items in small batches, which is inefficient.",
             UserWarning,
             stacklevel=2,
         )
         return self._delete(external_id, space)
 
     @overload
-    def retrieve(self, external_id: str, space: str = DEFAULT_INSTANCE_SPACE) -> BidRow | None: ...
+    def retrieve(self, external_id: str, space: str = DEFAULT_INSTANCE_SPACE) -> PriceAreaAFRR | None: ...
 
     @overload
-    def retrieve(self, external_id: SequenceNotStr[str], space: str = DEFAULT_INSTANCE_SPACE) -> BidRowList: ...
+    def retrieve(self, external_id: SequenceNotStr[str], space: str = DEFAULT_INSTANCE_SPACE) -> PriceAreaAFRRList: ...
 
     def retrieve(
         self, external_id: str | SequenceNotStr[str], space: str = DEFAULT_INSTANCE_SPACE
-    ) -> BidRow | BidRowList | None:
-        """Retrieve one or more bid rows by id(s).
+    ) -> PriceAreaAFRR | PriceAreaAFRRList | None:
+        """Retrieve one or more price area afrrs by id(s).
 
         Args:
-            external_id: External id or list of external ids of the bid rows.
-            space: The space where all the bid rows are located.
+            external_id: External id or list of external ids of the price area afrrs.
+            space: The space where all the price area afrrs are located.
 
         Returns:
-            The requested bid rows.
+            The requested price area afrrs.
 
         Examples:
 
-            Retrieve bid_row by id:
+            Retrieve price_area_afrr by id:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> bid_row = client.bid_row.retrieve("my_bid_row")
+                >>> price_area_afrr = client.price_area_afrr.retrieve("my_price_area_afrr")
 
         """
-        return self._retrieve(
-            external_id,
-            space,
-            retrieve_edges=True,
-            edge_api_name_type_direction_view_id_penta=[
-                (
-                    self.alerts_edge,
-                    "alerts",
-                    dm.DirectRelationReference("sp_powerops_types", "calculationIssue"),
-                    "outwards",
-                    dm.ViewId("sp_powerops_models", "Alert", "1"),
-                ),
-            ],
-        )
+        return self._retrieve(external_id, space)
 
     def search(
         self,
         query: str,
-        properties: BidRowTextFields | Sequence[BidRowTextFields] | None = None,
-        min_price: float | None = None,
-        max_price: float | None = None,
-        product: str | list[str] | None = None,
-        product_prefix: str | None = None,
-        is_divisible: bool | None = None,
-        is_block: bool | None = None,
-        exclusive_group_id: str | list[str] | None = None,
-        exclusive_group_id_prefix: str | None = None,
-        linked_bid: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        properties: PriceAreaAFRRTextFields | Sequence[PriceAreaAFRRTextFields] | None = None,
+        name: str | list[str] | None = None,
+        name_prefix: str | None = None,
+        display_name: str | list[str] | None = None,
+        display_name_prefix: str | None = None,
+        min_ordering: int | None = None,
+        max_ordering: int | None = None,
         asset_type: str | list[str] | None = None,
         asset_type_prefix: str | None = None,
-        asset_id: str | list[str] | None = None,
-        asset_id_prefix: str | None = None,
-        method: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        timezone: str | list[str] | None = None,
+        timezone_prefix: str | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
-    ) -> BidRowList:
-        """Search bid rows
+    ) -> PriceAreaAFRRList:
+        """Search price area afrrs
 
         Args:
             query: The search query,
             properties: The property to search, if nothing is passed all text fields will be searched.
-            min_price: The minimum value of the price to filter on.
-            max_price: The maximum value of the price to filter on.
-            product: The product to filter on.
-            product_prefix: The prefix of the product to filter on.
-            is_divisible: The is divisible to filter on.
-            is_block: The is block to filter on.
-            exclusive_group_id: The exclusive group id to filter on.
-            exclusive_group_id_prefix: The prefix of the exclusive group id to filter on.
-            linked_bid: The linked bid to filter on.
+            name: The name to filter on.
+            name_prefix: The prefix of the name to filter on.
+            display_name: The display name to filter on.
+            display_name_prefix: The prefix of the display name to filter on.
+            min_ordering: The minimum value of the ordering to filter on.
+            max_ordering: The maximum value of the ordering to filter on.
             asset_type: The asset type to filter on.
             asset_type_prefix: The prefix of the asset type to filter on.
-            asset_id: The asset id to filter on.
-            asset_id_prefix: The prefix of the asset id to filter on.
-            method: The method to filter on.
+            timezone: The timezone to filter on.
+            timezone_prefix: The prefix of the timezone to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of bid rows to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of price area afrrs to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
-            Search results bid rows matching the query.
+            Search results price area afrrs matching the query.
 
         Examples:
 
-           Search for 'my_bid_row' in all text properties:
+           Search for 'my_price_area_afrr' in all text properties:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> bid_rows = client.bid_row.search('my_bid_row')
+                >>> price_area_afrrs = client.price_area_afrr.search('my_price_area_afrr')
 
         """
-        filter_ = _create_bid_row_filter(
+        filter_ = _create_price_area_afrr_filter(
             self._view_id,
-            min_price,
-            max_price,
-            product,
-            product_prefix,
-            is_divisible,
-            is_block,
-            exclusive_group_id,
-            exclusive_group_id_prefix,
-            linked_bid,
+            name,
+            name_prefix,
+            display_name,
+            display_name_prefix,
+            min_ordering,
+            max_ordering,
             asset_type,
             asset_type_prefix,
-            asset_id,
-            asset_id_prefix,
-            method,
+            timezone,
+            timezone_prefix,
             external_id_prefix,
             space,
             filter,
         )
-        return self._search(self._view_id, query, _BIDROW_PROPERTIES_BY_FIELD, properties, filter_, limit)
+        return self._search(self._view_id, query, _PRICEAREAAFRR_PROPERTIES_BY_FIELD, properties, filter_, limit)
 
     @overload
     def aggregate(
         self,
         aggregations: (
             Aggregations
             | dm.aggregations.MetricAggregation
             | Sequence[Aggregations]
             | Sequence[dm.aggregations.MetricAggregation]
         ),
-        property: BidRowFields | Sequence[BidRowFields] | None = None,
+        property: PriceAreaAFRRFields | Sequence[PriceAreaAFRRFields] | None = None,
         group_by: None = None,
         query: str | None = None,
-        search_properties: BidRowTextFields | Sequence[BidRowTextFields] | None = None,
-        min_price: float | None = None,
-        max_price: float | None = None,
-        product: str | list[str] | None = None,
-        product_prefix: str | None = None,
-        is_divisible: bool | None = None,
-        is_block: bool | None = None,
-        exclusive_group_id: str | list[str] | None = None,
-        exclusive_group_id_prefix: str | None = None,
-        linked_bid: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        search_properties: PriceAreaAFRRTextFields | Sequence[PriceAreaAFRRTextFields] | None = None,
+        name: str | list[str] | None = None,
+        name_prefix: str | None = None,
+        display_name: str | list[str] | None = None,
+        display_name_prefix: str | None = None,
+        min_ordering: int | None = None,
+        max_ordering: int | None = None,
         asset_type: str | list[str] | None = None,
         asset_type_prefix: str | None = None,
-        asset_id: str | list[str] | None = None,
-        asset_id_prefix: str | None = None,
-        method: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        timezone: str | list[str] | None = None,
+        timezone_prefix: str | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> list[dm.aggregations.AggregatedNumberedValue]: ...
 
     @overload
@@ -358,312 +329,256 @@
         self,
         aggregations: (
             Aggregations
             | dm.aggregations.MetricAggregation
             | Sequence[Aggregations]
             | Sequence[dm.aggregations.MetricAggregation]
         ),
-        property: BidRowFields | Sequence[BidRowFields] | None = None,
-        group_by: BidRowFields | Sequence[BidRowFields] = None,
+        property: PriceAreaAFRRFields | Sequence[PriceAreaAFRRFields] | None = None,
+        group_by: PriceAreaAFRRFields | Sequence[PriceAreaAFRRFields] = None,
         query: str | None = None,
-        search_properties: BidRowTextFields | Sequence[BidRowTextFields] | None = None,
-        min_price: float | None = None,
-        max_price: float | None = None,
-        product: str | list[str] | None = None,
-        product_prefix: str | None = None,
-        is_divisible: bool | None = None,
-        is_block: bool | None = None,
-        exclusive_group_id: str | list[str] | None = None,
-        exclusive_group_id_prefix: str | None = None,
-        linked_bid: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        search_properties: PriceAreaAFRRTextFields | Sequence[PriceAreaAFRRTextFields] | None = None,
+        name: str | list[str] | None = None,
+        name_prefix: str | None = None,
+        display_name: str | list[str] | None = None,
+        display_name_prefix: str | None = None,
+        min_ordering: int | None = None,
+        max_ordering: int | None = None,
         asset_type: str | list[str] | None = None,
         asset_type_prefix: str | None = None,
-        asset_id: str | list[str] | None = None,
-        asset_id_prefix: str | None = None,
-        method: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        timezone: str | list[str] | None = None,
+        timezone_prefix: str | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> InstanceAggregationResultList: ...
 
     def aggregate(
         self,
         aggregate: (
             Aggregations
             | dm.aggregations.MetricAggregation
             | Sequence[Aggregations]
             | Sequence[dm.aggregations.MetricAggregation]
         ),
-        property: BidRowFields | Sequence[BidRowFields] | None = None,
-        group_by: BidRowFields | Sequence[BidRowFields] | None = None,
+        property: PriceAreaAFRRFields | Sequence[PriceAreaAFRRFields] | None = None,
+        group_by: PriceAreaAFRRFields | Sequence[PriceAreaAFRRFields] | None = None,
         query: str | None = None,
-        search_property: BidRowTextFields | Sequence[BidRowTextFields] | None = None,
-        min_price: float | None = None,
-        max_price: float | None = None,
-        product: str | list[str] | None = None,
-        product_prefix: str | None = None,
-        is_divisible: bool | None = None,
-        is_block: bool | None = None,
-        exclusive_group_id: str | list[str] | None = None,
-        exclusive_group_id_prefix: str | None = None,
-        linked_bid: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        search_property: PriceAreaAFRRTextFields | Sequence[PriceAreaAFRRTextFields] | None = None,
+        name: str | list[str] | None = None,
+        name_prefix: str | None = None,
+        display_name: str | list[str] | None = None,
+        display_name_prefix: str | None = None,
+        min_ordering: int | None = None,
+        max_ordering: int | None = None,
         asset_type: str | list[str] | None = None,
         asset_type_prefix: str | None = None,
-        asset_id: str | list[str] | None = None,
-        asset_id_prefix: str | None = None,
-        method: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        timezone: str | list[str] | None = None,
+        timezone_prefix: str | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> list[dm.aggregations.AggregatedNumberedValue] | InstanceAggregationResultList:
-        """Aggregate data across bid rows
+        """Aggregate data across price area afrrs
 
         Args:
             aggregate: The aggregation to perform.
             property: The property to perform aggregation on.
             group_by: The property to group by when doing the aggregation.
             query: The query to search for in the text field.
             search_property: The text field to search in.
-            min_price: The minimum value of the price to filter on.
-            max_price: The maximum value of the price to filter on.
-            product: The product to filter on.
-            product_prefix: The prefix of the product to filter on.
-            is_divisible: The is divisible to filter on.
-            is_block: The is block to filter on.
-            exclusive_group_id: The exclusive group id to filter on.
-            exclusive_group_id_prefix: The prefix of the exclusive group id to filter on.
-            linked_bid: The linked bid to filter on.
+            name: The name to filter on.
+            name_prefix: The prefix of the name to filter on.
+            display_name: The display name to filter on.
+            display_name_prefix: The prefix of the display name to filter on.
+            min_ordering: The minimum value of the ordering to filter on.
+            max_ordering: The maximum value of the ordering to filter on.
             asset_type: The asset type to filter on.
             asset_type_prefix: The prefix of the asset type to filter on.
-            asset_id: The asset id to filter on.
-            asset_id_prefix: The prefix of the asset id to filter on.
-            method: The method to filter on.
+            timezone: The timezone to filter on.
+            timezone_prefix: The prefix of the timezone to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of bid rows to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of price area afrrs to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
             Aggregation results.
 
         Examples:
 
-            Count bid rows in space `my_space`:
+            Count price area afrrs in space `my_space`:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> result = client.bid_row.aggregate("count", space="my_space")
+                >>> result = client.price_area_afrr.aggregate("count", space="my_space")
 
         """
 
-        filter_ = _create_bid_row_filter(
+        filter_ = _create_price_area_afrr_filter(
             self._view_id,
-            min_price,
-            max_price,
-            product,
-            product_prefix,
-            is_divisible,
-            is_block,
-            exclusive_group_id,
-            exclusive_group_id_prefix,
-            linked_bid,
+            name,
+            name_prefix,
+            display_name,
+            display_name_prefix,
+            min_ordering,
+            max_ordering,
             asset_type,
             asset_type_prefix,
-            asset_id,
-            asset_id_prefix,
-            method,
+            timezone,
+            timezone_prefix,
             external_id_prefix,
             space,
             filter,
         )
         return self._aggregate(
             self._view_id,
             aggregate,
-            _BIDROW_PROPERTIES_BY_FIELD,
+            _PRICEAREAAFRR_PROPERTIES_BY_FIELD,
             property,
             group_by,
             query,
             search_property,
             limit,
             filter_,
         )
 
     def histogram(
         self,
-        property: BidRowFields,
+        property: PriceAreaAFRRFields,
         interval: float,
         query: str | None = None,
-        search_property: BidRowTextFields | Sequence[BidRowTextFields] | None = None,
-        min_price: float | None = None,
-        max_price: float | None = None,
-        product: str | list[str] | None = None,
-        product_prefix: str | None = None,
-        is_divisible: bool | None = None,
-        is_block: bool | None = None,
-        exclusive_group_id: str | list[str] | None = None,
-        exclusive_group_id_prefix: str | None = None,
-        linked_bid: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        search_property: PriceAreaAFRRTextFields | Sequence[PriceAreaAFRRTextFields] | None = None,
+        name: str | list[str] | None = None,
+        name_prefix: str | None = None,
+        display_name: str | list[str] | None = None,
+        display_name_prefix: str | None = None,
+        min_ordering: int | None = None,
+        max_ordering: int | None = None,
         asset_type: str | list[str] | None = None,
         asset_type_prefix: str | None = None,
-        asset_id: str | list[str] | None = None,
-        asset_id_prefix: str | None = None,
-        method: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        timezone: str | list[str] | None = None,
+        timezone_prefix: str | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> dm.aggregations.HistogramValue:
-        """Produces histograms for bid rows
+        """Produces histograms for price area afrrs
 
         Args:
             property: The property to use as the value in the histogram.
             interval: The interval to use for the histogram bins.
             query: The query to search for in the text field.
             search_property: The text field to search in.
-            min_price: The minimum value of the price to filter on.
-            max_price: The maximum value of the price to filter on.
-            product: The product to filter on.
-            product_prefix: The prefix of the product to filter on.
-            is_divisible: The is divisible to filter on.
-            is_block: The is block to filter on.
-            exclusive_group_id: The exclusive group id to filter on.
-            exclusive_group_id_prefix: The prefix of the exclusive group id to filter on.
-            linked_bid: The linked bid to filter on.
+            name: The name to filter on.
+            name_prefix: The prefix of the name to filter on.
+            display_name: The display name to filter on.
+            display_name_prefix: The prefix of the display name to filter on.
+            min_ordering: The minimum value of the ordering to filter on.
+            max_ordering: The maximum value of the ordering to filter on.
             asset_type: The asset type to filter on.
             asset_type_prefix: The prefix of the asset type to filter on.
-            asset_id: The asset id to filter on.
-            asset_id_prefix: The prefix of the asset id to filter on.
-            method: The method to filter on.
+            timezone: The timezone to filter on.
+            timezone_prefix: The prefix of the timezone to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of bid rows to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of price area afrrs to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
             Bucketed histogram results.
 
         """
-        filter_ = _create_bid_row_filter(
+        filter_ = _create_price_area_afrr_filter(
             self._view_id,
-            min_price,
-            max_price,
-            product,
-            product_prefix,
-            is_divisible,
-            is_block,
-            exclusive_group_id,
-            exclusive_group_id_prefix,
-            linked_bid,
+            name,
+            name_prefix,
+            display_name,
+            display_name_prefix,
+            min_ordering,
+            max_ordering,
             asset_type,
             asset_type_prefix,
-            asset_id,
-            asset_id_prefix,
-            method,
+            timezone,
+            timezone_prefix,
             external_id_prefix,
             space,
             filter,
         )
         return self._histogram(
             self._view_id,
             property,
             interval,
-            _BIDROW_PROPERTIES_BY_FIELD,
+            _PRICEAREAAFRR_PROPERTIES_BY_FIELD,
             query,
             search_property,
             limit,
             filter_,
         )
 
     def list(
         self,
-        min_price: float | None = None,
-        max_price: float | None = None,
-        product: str | list[str] | None = None,
-        product_prefix: str | None = None,
-        is_divisible: bool | None = None,
-        is_block: bool | None = None,
-        exclusive_group_id: str | list[str] | None = None,
-        exclusive_group_id_prefix: str | None = None,
-        linked_bid: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        name: str | list[str] | None = None,
+        name_prefix: str | None = None,
+        display_name: str | list[str] | None = None,
+        display_name_prefix: str | None = None,
+        min_ordering: int | None = None,
+        max_ordering: int | None = None,
         asset_type: str | list[str] | None = None,
         asset_type_prefix: str | None = None,
-        asset_id: str | list[str] | None = None,
-        asset_id_prefix: str | None = None,
-        method: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        timezone: str | list[str] | None = None,
+        timezone_prefix: str | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
-        retrieve_edges: bool = True,
-    ) -> BidRowList:
-        """List/filter bid rows
+    ) -> PriceAreaAFRRList:
+        """List/filter price area afrrs
 
         Args:
-            min_price: The minimum value of the price to filter on.
-            max_price: The maximum value of the price to filter on.
-            product: The product to filter on.
-            product_prefix: The prefix of the product to filter on.
-            is_divisible: The is divisible to filter on.
-            is_block: The is block to filter on.
-            exclusive_group_id: The exclusive group id to filter on.
-            exclusive_group_id_prefix: The prefix of the exclusive group id to filter on.
-            linked_bid: The linked bid to filter on.
+            name: The name to filter on.
+            name_prefix: The prefix of the name to filter on.
+            display_name: The display name to filter on.
+            display_name_prefix: The prefix of the display name to filter on.
+            min_ordering: The minimum value of the ordering to filter on.
+            max_ordering: The maximum value of the ordering to filter on.
             asset_type: The asset type to filter on.
             asset_type_prefix: The prefix of the asset type to filter on.
-            asset_id: The asset id to filter on.
-            asset_id_prefix: The prefix of the asset id to filter on.
-            method: The method to filter on.
+            timezone: The timezone to filter on.
+            timezone_prefix: The prefix of the timezone to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of bid rows to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of price area afrrs to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
-            retrieve_edges: Whether to retrieve `alerts` external ids for the bid rows. Defaults to True.
 
         Returns:
-            List of requested bid rows
+            List of requested price area afrrs
 
         Examples:
 
-            List bid rows and limit to 5:
+            List price area afrrs and limit to 5:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> bid_rows = client.bid_row.list(limit=5)
+                >>> price_area_afrrs = client.price_area_afrr.list(limit=5)
 
         """
-        filter_ = _create_bid_row_filter(
+        filter_ = _create_price_area_afrr_filter(
             self._view_id,
-            min_price,
-            max_price,
-            product,
-            product_prefix,
-            is_divisible,
-            is_block,
-            exclusive_group_id,
-            exclusive_group_id_prefix,
-            linked_bid,
+            name,
+            name_prefix,
+            display_name,
+            display_name_prefix,
+            min_ordering,
+            max_ordering,
             asset_type,
             asset_type_prefix,
-            asset_id,
-            asset_id_prefix,
-            method,
+            timezone,
+            timezone_prefix,
             external_id_prefix,
             space,
             filter,
         )
-
-        return self._list(
-            limit=limit,
-            filter=filter_,
-            retrieve_edges=retrieve_edges,
-            edge_api_name_type_direction_view_id_penta=[
-                (
-                    self.alerts_edge,
-                    "alerts",
-                    dm.DirectRelationReference("sp_powerops_types", "calculationIssue"),
-                    "outwards",
-                    dm.ViewId("sp_powerops_models", "Alert", "1"),
-                ),
-            ],
-        )
+        return self._list(limit=limit, filter=filter_)
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_row_alerts.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_row_alerts.py`

 * *Files 0% similar despite different names*

```diff
@@ -39,15 +39,15 @@
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
                 >>> bid_row = client.bid_row.alerts_edge.list("my_bid_row", limit=5)
 
         """
         filter_ = _create_edge_filter(
-            dm.DirectRelationReference("sp_powerops_types", "calculationIssue"),
+            dm.DirectRelationReference("sp_powerops_types_temp", "calculationIssue"),
             from_bid_row,
             from_bid_row_space,
             to_alert,
             to_alert_space,
             external_id_prefix,
             space,
         )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/bid_row_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/model_template_query.py`

 * *Files 21% similar despite different names*

```diff
@@ -3,144 +3,125 @@
 import datetime
 from typing import TYPE_CHECKING
 
 from cognite.client import data_modeling as dm, CogniteClient
 
 from cognite.powerops.client._generated.v1.data_classes import (
     DomainModelCore,
-    BidRow,
-    BidRow,
-    BidMethodAFRR,
+    ModelTemplate,
+)
+from cognite.powerops.client._generated.v1.data_classes._mapping import (
+    Mapping,
+    _create_mapping_filter,
 )
 from ._core import DEFAULT_QUERY_LIMIT, QueryBuilder, QueryStep, QueryAPI, T_DomainModelList, _create_edge_filter
 
 if TYPE_CHECKING:
-    from .alert_query import AlertQueryAPI
+    from .mapping_query import MappingQueryAPI
 
 
-class BidRowQueryAPI(QueryAPI[T_DomainModelList]):
+class ModelTemplateQueryAPI(QueryAPI[T_DomainModelList]):
     def __init__(
         self,
         client: CogniteClient,
         builder: QueryBuilder[T_DomainModelList],
         view_by_read_class: dict[type[DomainModelCore], dm.ViewId],
         filter_: dm.filters.Filter | None = None,
         limit: int = DEFAULT_QUERY_LIMIT,
     ):
         super().__init__(client, builder, view_by_read_class)
 
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("bid_row"),
+                name=self._builder.next_name("model_template"),
                 expression=dm.query.NodeResultSetExpression(
                     from_=self._builder[-1].name if self._builder else None,
                     filter=filter_,
                 ),
-                select=dm.query.Select([dm.query.SourceSelector(self._view_by_read_class[BidRow], ["*"])]),
-                result_cls=BidRow,
+                select=dm.query.Select([dm.query.SourceSelector(self._view_by_read_class[ModelTemplate], ["*"])]),
+                result_cls=ModelTemplate,
                 max_retrieve_limit=limit,
             )
         )
 
-    def alerts(
+    def base_mappings(
         self,
+        shop_path: str | list[str] | None = None,
+        shop_path_prefix: str | None = None,
+        retrieve: str | list[str] | None = None,
+        retrieve_prefix: str | None = None,
+        aggregation: str | list[str] | None = None,
+        aggregation_prefix: str | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
+        external_id_prefix_edge: str | None = None,
+        space_edge: str | list[str] | None = None,
+        filter: dm.Filter | None = None,
         limit: int | None = DEFAULT_QUERY_LIMIT,
-        retrieve_linked_bid: bool = False,
-        retrieve_method: bool = False,
-    ) -> AlertQueryAPI[T_DomainModelList]:
-        """Query along the alert edges of the bid row.
+    ) -> MappingQueryAPI[T_DomainModelList]:
+        """Query along the base mapping edges of the model template.
 
         Args:
+            shop_path: The shop path to filter on.
+            shop_path_prefix: The prefix of the shop path to filter on.
+            retrieve: The retrieve to filter on.
+            retrieve_prefix: The prefix of the retrieve to filter on.
+            aggregation: The aggregation to filter on.
+            aggregation_prefix: The prefix of the aggregation to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of alert edges to return. Defaults to 25. Set to -1, float("inf") or None
+            external_id_prefix_edge: The prefix of the external ID to filter on.
+            space_edge: The space to filter on.
+            filter: (Advanced) Filter applied to node. If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
+            limit: Maximum number of base mapping edges to return. Defaults to 3. Set to -1, float("inf") or None
                 to return all items.
-            retrieve_linked_bid: Whether to retrieve the linked bid for each bid row or not.
-            retrieve_method: Whether to retrieve the method for each bid row or not.
 
         Returns:
-            AlertQueryAPI: The query API for the alert.
+            MappingQueryAPI: The query API for the mapping.
         """
-        from .alert_query import AlertQueryAPI
+        from .mapping_query import MappingQueryAPI
 
         from_ = self._builder[-1].name
-
         edge_filter = _create_edge_filter(
-            dm.DirectRelationReference("sp_powerops_types", "calculationIssue"),
-            external_id_prefix=external_id_prefix,
-            space=space,
+            dm.DirectRelationReference("sp_powerops_types_temp", "ModelTemplate.baseMappings"),
+            external_id_prefix=external_id_prefix_edge,
+            space=space_edge,
         )
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("alerts"),
+                name=self._builder.next_name("base_mappings"),
                 expression=dm.query.EdgeResultSetExpression(
                     filter=edge_filter,
                     from_=from_,
                     direction="outwards",
                 ),
                 select=dm.query.Select(),
                 max_retrieve_limit=limit,
             )
         )
-        if retrieve_linked_bid:
-            self._query_append_linked_bid(from_)
-        if retrieve_method:
-            self._query_append_method(from_)
-        return AlertQueryAPI(self._client, self._builder, self._view_by_read_class, None, limit)
+
+        view_id = self._view_by_read_class[Mapping]
+        has_data = dm.filters.HasData(views=[view_id])
+        node_filer = _create_mapping_filter(
+            view_id,
+            shop_path,
+            shop_path_prefix,
+            retrieve,
+            retrieve_prefix,
+            aggregation,
+            aggregation_prefix,
+            external_id_prefix,
+            space,
+            (filter and dm.filters.And(filter, has_data)) or has_data,
+        )
+        return MappingQueryAPI(self._client, self._builder, self._view_by_read_class, node_filer, limit)
 
     def query(
         self,
-        retrieve_linked_bid: bool = False,
-        retrieve_method: bool = False,
     ) -> T_DomainModelList:
         """Execute query and return the result.
 
-        Args:
-            retrieve_linked_bid: Whether to retrieve the linked bid for each bid row or not.
-            retrieve_method: Whether to retrieve the method for each bid row or not.
-
         Returns:
             The list of the source nodes of the query.
 
         """
-        from_ = self._builder[-1].name
-        if retrieve_linked_bid:
-            self._query_append_linked_bid(from_)
-        if retrieve_method:
-            self._query_append_method(from_)
         return self._query()
-
-    def _query_append_linked_bid(self, from_: str) -> None:
-        view_id = self._view_by_read_class[BidRow]
-        self._builder.append(
-            QueryStep(
-                name=self._builder.next_name("linked_bid"),
-                expression=dm.query.NodeResultSetExpression(
-                    filter=dm.filters.HasData(views=[view_id]),
-                    from_=from_,
-                    through=self._view_by_read_class[BidRow].as_property_ref("linkedBid"),
-                    direction="outwards",
-                ),
-                select=dm.query.Select([dm.query.SourceSelector(view_id, ["*"])]),
-                max_retrieve_limit=-1,
-                result_cls=BidRow,
-            ),
-        )
-
-    def _query_append_method(self, from_: str) -> None:
-        view_id = self._view_by_read_class[BidMethodAFRR]
-        self._builder.append(
-            QueryStep(
-                name=self._builder.next_name("method"),
-                expression=dm.query.NodeResultSetExpression(
-                    filter=dm.filters.HasData(views=[view_id]),
-                    from_=from_,
-                    through=self._view_by_read_class[BidRow].as_property_ref("method"),
-                    direction="outwards",
-                ),
-                select=dm.query.Select([dm.query.SourceSelector(view_id, ["*"])]),
-                max_retrieve_limit=-1,
-                result_cls=BidMethodAFRR,
-            ),
-        )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/case.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/case.py`

 * *Files 2% similar despite different names*

```diff
@@ -48,18 +48,18 @@
             view_by_read_class=view_by_read_class,
         )
         self._view_id = view_id
 
     def __call__(
         self,
         scenario: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
-        min_start_time: datetime.date | None = None,
-        max_start_time: datetime.date | None = None,
-        min_end_time: datetime.date | None = None,
-        max_end_time: datetime.date | None = None,
+        min_start_time: datetime.datetime | None = None,
+        max_start_time: datetime.datetime | None = None,
+        min_end_time: datetime.datetime | None = None,
+        max_end_time: datetime.datetime | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_QUERY_LIMIT,
         filter: dm.Filter | None = None,
     ) -> CaseQueryAPI[CaseList]:
         """Query starting at cases.
 
@@ -201,18 +201,18 @@
             | dm.aggregations.MetricAggregation
             | Sequence[Aggregations]
             | Sequence[dm.aggregations.MetricAggregation]
         ),
         property: CaseFields | Sequence[CaseFields] | None = None,
         group_by: None = None,
         scenario: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
-        min_start_time: datetime.date | None = None,
-        max_start_time: datetime.date | None = None,
-        min_end_time: datetime.date | None = None,
-        max_end_time: datetime.date | None = None,
+        min_start_time: datetime.datetime | None = None,
+        max_start_time: datetime.datetime | None = None,
+        min_end_time: datetime.datetime | None = None,
+        max_end_time: datetime.datetime | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> list[dm.aggregations.AggregatedNumberedValue]: ...
 
     @overload
@@ -223,18 +223,18 @@
             | dm.aggregations.MetricAggregation
             | Sequence[Aggregations]
             | Sequence[dm.aggregations.MetricAggregation]
         ),
         property: CaseFields | Sequence[CaseFields] | None = None,
         group_by: CaseFields | Sequence[CaseFields] = None,
         scenario: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
-        min_start_time: datetime.date | None = None,
-        max_start_time: datetime.date | None = None,
-        min_end_time: datetime.date | None = None,
-        max_end_time: datetime.date | None = None,
+        min_start_time: datetime.datetime | None = None,
+        max_start_time: datetime.datetime | None = None,
+        min_end_time: datetime.datetime | None = None,
+        max_end_time: datetime.datetime | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> InstanceAggregationResultList: ...
 
     def aggregate(
@@ -244,18 +244,18 @@
             | dm.aggregations.MetricAggregation
             | Sequence[Aggregations]
             | Sequence[dm.aggregations.MetricAggregation]
         ),
         property: CaseFields | Sequence[CaseFields] | None = None,
         group_by: CaseFields | Sequence[CaseFields] | None = None,
         scenario: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
-        min_start_time: datetime.date | None = None,
-        max_start_time: datetime.date | None = None,
-        min_end_time: datetime.date | None = None,
-        max_end_time: datetime.date | None = None,
+        min_start_time: datetime.datetime | None = None,
+        max_start_time: datetime.datetime | None = None,
+        min_end_time: datetime.datetime | None = None,
+        max_end_time: datetime.datetime | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> list[dm.aggregations.AggregatedNumberedValue] | InstanceAggregationResultList:
         """Aggregate data across cases
 
@@ -310,18 +310,18 @@
         )
 
     def histogram(
         self,
         property: CaseFields,
         interval: float,
         scenario: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
-        min_start_time: datetime.date | None = None,
-        max_start_time: datetime.date | None = None,
-        min_end_time: datetime.date | None = None,
-        max_end_time: datetime.date | None = None,
+        min_start_time: datetime.datetime | None = None,
+        max_start_time: datetime.datetime | None = None,
+        min_end_time: datetime.datetime | None = None,
+        max_end_time: datetime.datetime | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> dm.aggregations.HistogramValue:
         """Produces histograms for cases
 
@@ -363,18 +363,18 @@
             limit,
             filter_,
         )
 
     def list(
         self,
         scenario: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
-        min_start_time: datetime.date | None = None,
-        max_start_time: datetime.date | None = None,
-        min_end_time: datetime.date | None = None,
-        max_end_time: datetime.date | None = None,
+        min_start_time: datetime.datetime | None = None,
+        max_start_time: datetime.datetime | None = None,
+        min_end_time: datetime.datetime | None = None,
+        max_end_time: datetime.datetime | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> CaseList:
         """List/filter cases
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/case_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/case_query.py`

 * *Files 8% similar despite different names*

```diff
@@ -65,9 +65,10 @@
                     from_=from_,
                     through=self._view_by_read_class[Case].as_property_ref("scenario"),
                     direction="outwards",
                 ),
                 select=dm.query.Select([dm.query.SourceSelector(view_id, ["*"])]),
                 max_retrieve_limit=-1,
                 result_cls=Scenario,
+                is_single_direct_relation=True,
             ),
         )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/commands.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/commands.py`

 * *Files 14% similar despite different names*

```diff
@@ -47,34 +47,40 @@
             class_write_list=CommandsWriteList,
             view_by_read_class=view_by_read_class,
         )
         self._view_id = view_id
 
     def __call__(
         self,
+        name: str | list[str] | None = None,
+        name_prefix: str | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_QUERY_LIMIT,
         filter: dm.Filter | None = None,
     ) -> CommandsQueryAPI[CommandsList]:
         """Query starting at commands.
 
         Args:
+            name: The name to filter on.
+            name_prefix: The prefix of the name to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of commands to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
             A query API for commands.
 
         """
         has_data = dm.filters.HasData(views=[self._view_id])
         filter_ = _create_command_filter(
             self._view_id,
+            name,
+            name_prefix,
             external_id_prefix,
             space,
             (filter and dm.filters.And(filter, has_data)) or has_data,
         )
         builder = QueryBuilder(CommandsList)
         return CommandsQueryAPI(self._client, builder, self._view_by_read_class, filter_, limit)
 
@@ -178,24 +184,28 @@
         """
         return self._retrieve(external_id, space)
 
     def search(
         self,
         query: str,
         properties: CommandsTextFields | Sequence[CommandsTextFields] | None = None,
+        name: str | list[str] | None = None,
+        name_prefix: str | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> CommandsList:
         """Search commands
 
         Args:
             query: The search query,
             properties: The property to search, if nothing is passed all text fields will be searched.
+            name: The name to filter on.
+            name_prefix: The prefix of the name to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of commands to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
             Search results commands matching the query.
@@ -207,14 +217,16 @@
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
                 >>> commands = client.commands.search('my_command')
 
         """
         filter_ = _create_command_filter(
             self._view_id,
+            name,
+            name_prefix,
             external_id_prefix,
             space,
             filter,
         )
         return self._search(self._view_id, query, _COMMANDS_PROPERTIES_BY_FIELD, properties, filter_, limit)
 
     @overload
@@ -226,14 +238,16 @@
             | Sequence[Aggregations]
             | Sequence[dm.aggregations.MetricAggregation]
         ),
         property: CommandsFields | Sequence[CommandsFields] | None = None,
         group_by: None = None,
         query: str | None = None,
         search_properties: CommandsTextFields | Sequence[CommandsTextFields] | None = None,
+        name: str | list[str] | None = None,
+        name_prefix: str | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> list[dm.aggregations.AggregatedNumberedValue]: ...
 
     @overload
@@ -245,14 +259,16 @@
             | Sequence[Aggregations]
             | Sequence[dm.aggregations.MetricAggregation]
         ),
         property: CommandsFields | Sequence[CommandsFields] | None = None,
         group_by: CommandsFields | Sequence[CommandsFields] = None,
         query: str | None = None,
         search_properties: CommandsTextFields | Sequence[CommandsTextFields] | None = None,
+        name: str | list[str] | None = None,
+        name_prefix: str | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> InstanceAggregationResultList: ...
 
     def aggregate(
@@ -263,27 +279,31 @@
             | Sequence[Aggregations]
             | Sequence[dm.aggregations.MetricAggregation]
         ),
         property: CommandsFields | Sequence[CommandsFields] | None = None,
         group_by: CommandsFields | Sequence[CommandsFields] | None = None,
         query: str | None = None,
         search_property: CommandsTextFields | Sequence[CommandsTextFields] | None = None,
+        name: str | list[str] | None = None,
+        name_prefix: str | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> list[dm.aggregations.AggregatedNumberedValue] | InstanceAggregationResultList:
         """Aggregate data across commands
 
         Args:
             aggregate: The aggregation to perform.
             property: The property to perform aggregation on.
             group_by: The property to group by when doing the aggregation.
             query: The query to search for in the text field.
             search_property: The text field to search in.
+            name: The name to filter on.
+            name_prefix: The prefix of the name to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of commands to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
             Aggregation results.
@@ -296,14 +316,16 @@
                 >>> client = PowerOpsModelsV1Client()
                 >>> result = client.commands.aggregate("count", space="my_space")
 
         """
 
         filter_ = _create_command_filter(
             self._view_id,
+            name,
+            name_prefix,
             external_id_prefix,
             space,
             filter,
         )
         return self._aggregate(
             self._view_id,
             aggregate,
@@ -318,37 +340,43 @@
 
     def histogram(
         self,
         property: CommandsFields,
         interval: float,
         query: str | None = None,
         search_property: CommandsTextFields | Sequence[CommandsTextFields] | None = None,
+        name: str | list[str] | None = None,
+        name_prefix: str | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> dm.aggregations.HistogramValue:
         """Produces histograms for commands
 
         Args:
             property: The property to use as the value in the histogram.
             interval: The interval to use for the histogram bins.
             query: The query to search for in the text field.
             search_property: The text field to search in.
+            name: The name to filter on.
+            name_prefix: The prefix of the name to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of commands to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
             Bucketed histogram results.
 
         """
         filter_ = _create_command_filter(
             self._view_id,
+            name,
+            name_prefix,
             external_id_prefix,
             space,
             filter,
         )
         return self._histogram(
             self._view_id,
             property,
@@ -358,22 +386,26 @@
             search_property,
             limit,
             filter_,
         )
 
     def list(
         self,
+        name: str | list[str] | None = None,
+        name_prefix: str | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> CommandsList:
         """List/filter commands
 
         Args:
+            name: The name to filter on.
+            name_prefix: The prefix of the name to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of commands to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
             List of requested commands
@@ -385,12 +417,14 @@
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
                 >>> commands = client.commands.list(limit=5)
 
         """
         filter_ = _create_command_filter(
             self._view_id,
+            name,
+            name_prefix,
             external_id_prefix,
             space,
             filter,
         )
         return self._list(limit=limit, filter=filter_)
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/commands_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/commands_query.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/custom_bid_matrix.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/custom_bid_matrix.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/custom_bid_matrix_alerts.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/custom_bid_matrix_alerts.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/custom_bid_matrix_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/scenario_raw_query.py`

 * *Files 15% similar despite different names*

```diff
@@ -3,118 +3,118 @@
 import datetime
 from typing import TYPE_CHECKING
 
 from cognite.client import data_modeling as dm, CogniteClient
 
 from cognite.powerops.client._generated.v1.data_classes import (
     DomainModelCore,
-    CustomBidMatrix,
-    BidMethodCustom,
+    ScenarioRaw,
+    ModelTemplate,
 )
 from ._core import DEFAULT_QUERY_LIMIT, QueryBuilder, QueryStep, QueryAPI, T_DomainModelList, _create_edge_filter
 
 if TYPE_CHECKING:
-    from .alert_query import AlertQueryAPI
+    from .mapping_query import MappingQueryAPI
 
 
-class CustomBidMatrixQueryAPI(QueryAPI[T_DomainModelList]):
+class ScenarioRawQueryAPI(QueryAPI[T_DomainModelList]):
     def __init__(
         self,
         client: CogniteClient,
         builder: QueryBuilder[T_DomainModelList],
         view_by_read_class: dict[type[DomainModelCore], dm.ViewId],
         filter_: dm.filters.Filter | None = None,
         limit: int = DEFAULT_QUERY_LIMIT,
     ):
         super().__init__(client, builder, view_by_read_class)
 
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("custom_bid_matrix"),
+                name=self._builder.next_name("scenario_raw"),
                 expression=dm.query.NodeResultSetExpression(
                     from_=self._builder[-1].name if self._builder else None,
                     filter=filter_,
                 ),
-                select=dm.query.Select([dm.query.SourceSelector(self._view_by_read_class[CustomBidMatrix], ["*"])]),
-                result_cls=CustomBidMatrix,
+                select=dm.query.Select([dm.query.SourceSelector(self._view_by_read_class[ScenarioRaw], ["*"])]),
+                result_cls=ScenarioRaw,
                 max_retrieve_limit=limit,
             )
         )
 
-    def alerts(
+    def mappings_override(
         self,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_QUERY_LIMIT,
-        retrieve_method: bool = False,
-    ) -> AlertQueryAPI[T_DomainModelList]:
-        """Query along the alert edges of the custom bid matrix.
+        retrieve_model_template: bool = False,
+    ) -> MappingQueryAPI[T_DomainModelList]:
+        """Query along the mappings override edges of the scenario raw.
 
         Args:
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of alert edges to return. Defaults to 25. Set to -1, float("inf") or None
+            limit: Maximum number of mappings override edges to return. Defaults to 25. Set to -1, float("inf") or None
                 to return all items.
-            retrieve_method: Whether to retrieve the method for each custom bid matrix or not.
+            retrieve_model_template: Whether to retrieve the model template for each scenario raw or not.
 
         Returns:
-            AlertQueryAPI: The query API for the alert.
+            MappingQueryAPI: The query API for the mapping.
         """
-        from .alert_query import AlertQueryAPI
+        from .mapping_query import MappingQueryAPI
 
         from_ = self._builder[-1].name
 
         edge_filter = _create_edge_filter(
-            dm.DirectRelationReference("sp_powerops_types", "calculationIssue"),
+            dm.DirectRelationReference("sp_powerops_types", "Mapping"),
             external_id_prefix=external_id_prefix,
             space=space,
         )
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("alerts"),
+                name=self._builder.next_name("mappings_override"),
                 expression=dm.query.EdgeResultSetExpression(
                     filter=edge_filter,
                     from_=from_,
                     direction="outwards",
                 ),
                 select=dm.query.Select(),
                 max_retrieve_limit=limit,
             )
         )
-        if retrieve_method:
-            self._query_append_method(from_)
-        return AlertQueryAPI(self._client, self._builder, self._view_by_read_class, None, limit)
+        if retrieve_model_template:
+            self._query_append_model_template(from_)
+        return MappingQueryAPI(self._client, self._builder, self._view_by_read_class, None, limit)
 
     def query(
         self,
-        retrieve_method: bool = False,
+        retrieve_model_template: bool = False,
     ) -> T_DomainModelList:
         """Execute query and return the result.
 
         Args:
-            retrieve_method: Whether to retrieve the method for each custom bid matrix or not.
+            retrieve_model_template: Whether to retrieve the model template for each scenario raw or not.
 
         Returns:
             The list of the source nodes of the query.
 
         """
         from_ = self._builder[-1].name
-        if retrieve_method:
-            self._query_append_method(from_)
+        if retrieve_model_template:
+            self._query_append_model_template(from_)
         return self._query()
 
-    def _query_append_method(self, from_: str) -> None:
-        view_id = self._view_by_read_class[BidMethodCustom]
+    def _query_append_model_template(self, from_: str) -> None:
+        view_id = self._view_by_read_class[ModelTemplate]
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("method"),
+                name=self._builder.next_name("model_template"),
                 expression=dm.query.NodeResultSetExpression(
                     filter=dm.filters.HasData(views=[view_id]),
                     from_=from_,
-                    through=self._view_by_read_class[CustomBidMatrix].as_property_ref("method"),
+                    through=self._view_by_read_class[ScenarioRaw].as_property_ref("modelTemplate"),
                     direction="outwards",
                 ),
                 select=dm.query.Select([dm.query.SourceSelector(view_id, ["*"])]),
                 max_retrieve_limit=-1,
-                result_cls=BidMethodCustom,
+                result_cls=ModelTemplate,
             ),
         )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/generator.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/generator.py`

 * *Files 15% similar despite different names*

```diff
@@ -29,17 +29,17 @@
     DEFAULT_QUERY_LIMIT,
     Aggregations,
     NodeAPI,
     SequenceNotStr,
     QueryStep,
     QueryBuilder,
 )
-from .generator_turbine_curves import GeneratorTurbineCurvesAPI
+from .generator_turbine_efficiency_curves import GeneratorTurbineEfficiencyCurvesAPI
 from .generator_start_stop_cost import GeneratorStartStopCostAPI
-from .generator_is_available_time_series import GeneratorIsAvailableTimeSeriesAPI
+from .generator_availability_time_series import GeneratorAvailabilityTimeSeriesAPI
 from .generator_query import GeneratorQueryAPI
 
 
 class GeneratorAPI(NodeAPI[Generator, GeneratorWrite, GeneratorList]):
     def __init__(self, client: CogniteClient, view_by_read_class: dict[type[DomainModelCore], dm.ViewId]):
         view_id = view_by_read_class[Generator]
         super().__init__(
@@ -47,54 +47,58 @@
             sources=view_id,
             class_type=Generator,
             class_list=GeneratorList,
             class_write_list=GeneratorWriteList,
             view_by_read_class=view_by_read_class,
         )
         self._view_id = view_id
-        self.turbine_curves_edge = GeneratorTurbineCurvesAPI(client)
+        self.turbine_efficiency_curves_edge = GeneratorTurbineEfficiencyCurvesAPI(client)
         self.start_stop_cost = GeneratorStartStopCostAPI(client, view_id)
-        self.is_available_time_series = GeneratorIsAvailableTimeSeriesAPI(client, view_id)
+        self.availability_time_series = GeneratorAvailabilityTimeSeriesAPI(client, view_id)
 
     def __call__(
         self,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
         display_name: str | list[str] | None = None,
         display_name_prefix: str | None = None,
         min_ordering: int | None = None,
         max_ordering: int | None = None,
-        min_p_min: float | None = None,
-        max_p_min: float | None = None,
-        min_penstock: int | None = None,
-        max_penstock: int | None = None,
+        asset_type: str | list[str] | None = None,
+        asset_type_prefix: str | None = None,
+        min_production_min: float | None = None,
+        max_production_min: float | None = None,
+        min_penstock_number: int | None = None,
+        max_penstock_number: int | None = None,
         min_start_cost: float | None = None,
         max_start_cost: float | None = None,
-        efficiency_curve: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        generator_efficiency_curve: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_QUERY_LIMIT,
         filter: dm.Filter | None = None,
     ) -> GeneratorQueryAPI[GeneratorList]:
         """Query starting at generators.
 
         Args:
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
             display_name: The display name to filter on.
             display_name_prefix: The prefix of the display name to filter on.
             min_ordering: The minimum value of the ordering to filter on.
             max_ordering: The maximum value of the ordering to filter on.
-            min_p_min: The minimum value of the p min to filter on.
-            max_p_min: The maximum value of the p min to filter on.
-            min_penstock: The minimum value of the penstock to filter on.
-            max_penstock: The maximum value of the penstock to filter on.
+            asset_type: The asset type to filter on.
+            asset_type_prefix: The prefix of the asset type to filter on.
+            min_production_min: The minimum value of the production min to filter on.
+            max_production_min: The maximum value of the production min to filter on.
+            min_penstock_number: The minimum value of the penstock number to filter on.
+            max_penstock_number: The maximum value of the penstock number to filter on.
             min_start_cost: The minimum value of the start cost to filter on.
             max_start_cost: The maximum value of the start cost to filter on.
-            efficiency_curve: The efficiency curve to filter on.
+            generator_efficiency_curve: The generator efficiency curve to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of generators to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
             A query API for generators.
@@ -105,21 +109,23 @@
             self._view_id,
             name,
             name_prefix,
             display_name,
             display_name_prefix,
             min_ordering,
             max_ordering,
-            min_p_min,
-            max_p_min,
-            min_penstock,
-            max_penstock,
+            asset_type,
+            asset_type_prefix,
+            min_production_min,
+            max_production_min,
+            min_penstock_number,
+            max_penstock_number,
             min_start_cost,
             max_start_cost,
-            efficiency_curve,
+            generator_efficiency_curve,
             external_id_prefix,
             space,
             (filter and dm.filters.And(filter, has_data)) or has_data,
         )
         builder = QueryBuilder(GeneratorList)
         return GeneratorQueryAPI(self._client, builder, self._view_by_read_class, filter_, limit)
 
@@ -128,15 +134,15 @@
         generator: GeneratorWrite | Sequence[GeneratorWrite],
         replace: bool = False,
         write_none: bool = False,
     ) -> ResourcesWriteResult:
         """Add or update (upsert) generators.
 
         Note: This method iterates through all nodes and timeseries linked to generator and creates them including the edges
-        between the nodes. For example, if any of `turbine_curves` are set, then these
+        between the nodes. For example, if any of `turbine_efficiency_curves` are set, then these
         nodes as well as any nodes linked to them, and all the edges linking these nodes will be created.
 
         Args:
             generator: Generator or sequence of generators to upsert.
             replace (bool): How do we behave when a property value exists? Do we replace all matching and existing values with the supplied values (true)?
                 Or should we merge in new values for properties together with the existing values (false)? Note: This setting applies for all nodes or edges specified in the ingestion call.
             write_none (bool): This method, will by default, skip properties that are set to None. However, if you want to set properties to None,
@@ -227,40 +233,42 @@
         """
         return self._retrieve(
             external_id,
             space,
             retrieve_edges=True,
             edge_api_name_type_direction_view_id_penta=[
                 (
-                    self.turbine_curves_edge,
-                    "turbine_curves",
-                    dm.DirectRelationReference("sp_powerops_types", "isSubAssetOf"),
+                    self.turbine_efficiency_curves_edge,
+                    "turbine_efficiency_curves",
+                    dm.DirectRelationReference("sp_powerops_types_temp", "isSubAssetOf"),
                     "outwards",
-                    dm.ViewId("sp_powerops_models", "TurbineEfficiencyCurve", "1"),
+                    dm.ViewId("sp_powerops_models_temp", "TurbineEfficiencyCurve", "1"),
                 ),
             ],
         )
 
     def search(
         self,
         query: str,
         properties: GeneratorTextFields | Sequence[GeneratorTextFields] | None = None,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
         display_name: str | list[str] | None = None,
         display_name_prefix: str | None = None,
         min_ordering: int | None = None,
         max_ordering: int | None = None,
-        min_p_min: float | None = None,
-        max_p_min: float | None = None,
-        min_penstock: int | None = None,
-        max_penstock: int | None = None,
+        asset_type: str | list[str] | None = None,
+        asset_type_prefix: str | None = None,
+        min_production_min: float | None = None,
+        max_production_min: float | None = None,
+        min_penstock_number: int | None = None,
+        max_penstock_number: int | None = None,
         min_start_cost: float | None = None,
         max_start_cost: float | None = None,
-        efficiency_curve: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        generator_efficiency_curve: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> GeneratorList:
         """Search generators
 
@@ -269,21 +277,23 @@
             properties: The property to search, if nothing is passed all text fields will be searched.
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
             display_name: The display name to filter on.
             display_name_prefix: The prefix of the display name to filter on.
             min_ordering: The minimum value of the ordering to filter on.
             max_ordering: The maximum value of the ordering to filter on.
-            min_p_min: The minimum value of the p min to filter on.
-            max_p_min: The maximum value of the p min to filter on.
-            min_penstock: The minimum value of the penstock to filter on.
-            max_penstock: The maximum value of the penstock to filter on.
+            asset_type: The asset type to filter on.
+            asset_type_prefix: The prefix of the asset type to filter on.
+            min_production_min: The minimum value of the production min to filter on.
+            max_production_min: The maximum value of the production min to filter on.
+            min_penstock_number: The minimum value of the penstock number to filter on.
+            max_penstock_number: The maximum value of the penstock number to filter on.
             min_start_cost: The minimum value of the start cost to filter on.
             max_start_cost: The maximum value of the start cost to filter on.
-            efficiency_curve: The efficiency curve to filter on.
+            generator_efficiency_curve: The generator efficiency curve to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of generators to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
             Search results generators matching the query.
@@ -301,21 +311,23 @@
             self._view_id,
             name,
             name_prefix,
             display_name,
             display_name_prefix,
             min_ordering,
             max_ordering,
-            min_p_min,
-            max_p_min,
-            min_penstock,
-            max_penstock,
+            asset_type,
+            asset_type_prefix,
+            min_production_min,
+            max_production_min,
+            min_penstock_number,
+            max_penstock_number,
             min_start_cost,
             max_start_cost,
-            efficiency_curve,
+            generator_efficiency_curve,
             external_id_prefix,
             space,
             filter,
         )
         return self._search(self._view_id, query, _GENERATOR_PROPERTIES_BY_FIELD, properties, filter_, limit)
 
     @overload
@@ -333,21 +345,23 @@
         search_properties: GeneratorTextFields | Sequence[GeneratorTextFields] | None = None,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
         display_name: str | list[str] | None = None,
         display_name_prefix: str | None = None,
         min_ordering: int | None = None,
         max_ordering: int | None = None,
-        min_p_min: float | None = None,
-        max_p_min: float | None = None,
-        min_penstock: int | None = None,
-        max_penstock: int | None = None,
+        asset_type: str | list[str] | None = None,
+        asset_type_prefix: str | None = None,
+        min_production_min: float | None = None,
+        max_production_min: float | None = None,
+        min_penstock_number: int | None = None,
+        max_penstock_number: int | None = None,
         min_start_cost: float | None = None,
         max_start_cost: float | None = None,
-        efficiency_curve: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        generator_efficiency_curve: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> list[dm.aggregations.AggregatedNumberedValue]: ...
 
     @overload
@@ -365,21 +379,23 @@
         search_properties: GeneratorTextFields | Sequence[GeneratorTextFields] | None = None,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
         display_name: str | list[str] | None = None,
         display_name_prefix: str | None = None,
         min_ordering: int | None = None,
         max_ordering: int | None = None,
-        min_p_min: float | None = None,
-        max_p_min: float | None = None,
-        min_penstock: int | None = None,
-        max_penstock: int | None = None,
+        asset_type: str | list[str] | None = None,
+        asset_type_prefix: str | None = None,
+        min_production_min: float | None = None,
+        max_production_min: float | None = None,
+        min_penstock_number: int | None = None,
+        max_penstock_number: int | None = None,
         min_start_cost: float | None = None,
         max_start_cost: float | None = None,
-        efficiency_curve: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        generator_efficiency_curve: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> InstanceAggregationResultList: ...
 
     def aggregate(
@@ -396,21 +412,23 @@
         search_property: GeneratorTextFields | Sequence[GeneratorTextFields] | None = None,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
         display_name: str | list[str] | None = None,
         display_name_prefix: str | None = None,
         min_ordering: int | None = None,
         max_ordering: int | None = None,
-        min_p_min: float | None = None,
-        max_p_min: float | None = None,
-        min_penstock: int | None = None,
-        max_penstock: int | None = None,
+        asset_type: str | list[str] | None = None,
+        asset_type_prefix: str | None = None,
+        min_production_min: float | None = None,
+        max_production_min: float | None = None,
+        min_penstock_number: int | None = None,
+        max_penstock_number: int | None = None,
         min_start_cost: float | None = None,
         max_start_cost: float | None = None,
-        efficiency_curve: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        generator_efficiency_curve: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> list[dm.aggregations.AggregatedNumberedValue] | InstanceAggregationResultList:
         """Aggregate data across generators
 
@@ -422,21 +440,23 @@
             search_property: The text field to search in.
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
             display_name: The display name to filter on.
             display_name_prefix: The prefix of the display name to filter on.
             min_ordering: The minimum value of the ordering to filter on.
             max_ordering: The maximum value of the ordering to filter on.
-            min_p_min: The minimum value of the p min to filter on.
-            max_p_min: The maximum value of the p min to filter on.
-            min_penstock: The minimum value of the penstock to filter on.
-            max_penstock: The maximum value of the penstock to filter on.
+            asset_type: The asset type to filter on.
+            asset_type_prefix: The prefix of the asset type to filter on.
+            min_production_min: The minimum value of the production min to filter on.
+            max_production_min: The maximum value of the production min to filter on.
+            min_penstock_number: The minimum value of the penstock number to filter on.
+            max_penstock_number: The maximum value of the penstock number to filter on.
             min_start_cost: The minimum value of the start cost to filter on.
             max_start_cost: The maximum value of the start cost to filter on.
-            efficiency_curve: The efficiency curve to filter on.
+            generator_efficiency_curve: The generator efficiency curve to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of generators to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
             Aggregation results.
@@ -455,21 +475,23 @@
             self._view_id,
             name,
             name_prefix,
             display_name,
             display_name_prefix,
             min_ordering,
             max_ordering,
-            min_p_min,
-            max_p_min,
-            min_penstock,
-            max_penstock,
+            asset_type,
+            asset_type_prefix,
+            min_production_min,
+            max_production_min,
+            min_penstock_number,
+            max_penstock_number,
             min_start_cost,
             max_start_cost,
-            efficiency_curve,
+            generator_efficiency_curve,
             external_id_prefix,
             space,
             filter,
         )
         return self._aggregate(
             self._view_id,
             aggregate,
@@ -490,21 +512,23 @@
         search_property: GeneratorTextFields | Sequence[GeneratorTextFields] | None = None,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
         display_name: str | list[str] | None = None,
         display_name_prefix: str | None = None,
         min_ordering: int | None = None,
         max_ordering: int | None = None,
-        min_p_min: float | None = None,
-        max_p_min: float | None = None,
-        min_penstock: int | None = None,
-        max_penstock: int | None = None,
+        asset_type: str | list[str] | None = None,
+        asset_type_prefix: str | None = None,
+        min_production_min: float | None = None,
+        max_production_min: float | None = None,
+        min_penstock_number: int | None = None,
+        max_penstock_number: int | None = None,
         min_start_cost: float | None = None,
         max_start_cost: float | None = None,
-        efficiency_curve: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        generator_efficiency_curve: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> dm.aggregations.HistogramValue:
         """Produces histograms for generators
 
@@ -515,21 +539,23 @@
             search_property: The text field to search in.
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
             display_name: The display name to filter on.
             display_name_prefix: The prefix of the display name to filter on.
             min_ordering: The minimum value of the ordering to filter on.
             max_ordering: The maximum value of the ordering to filter on.
-            min_p_min: The minimum value of the p min to filter on.
-            max_p_min: The maximum value of the p min to filter on.
-            min_penstock: The minimum value of the penstock to filter on.
-            max_penstock: The maximum value of the penstock to filter on.
+            asset_type: The asset type to filter on.
+            asset_type_prefix: The prefix of the asset type to filter on.
+            min_production_min: The minimum value of the production min to filter on.
+            max_production_min: The maximum value of the production min to filter on.
+            min_penstock_number: The minimum value of the penstock number to filter on.
+            max_penstock_number: The maximum value of the penstock number to filter on.
             min_start_cost: The minimum value of the start cost to filter on.
             max_start_cost: The maximum value of the start cost to filter on.
-            efficiency_curve: The efficiency curve to filter on.
+            generator_efficiency_curve: The generator efficiency curve to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of generators to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
             Bucketed histogram results.
@@ -539,21 +565,23 @@
             self._view_id,
             name,
             name_prefix,
             display_name,
             display_name_prefix,
             min_ordering,
             max_ordering,
-            min_p_min,
-            max_p_min,
-            min_penstock,
-            max_penstock,
+            asset_type,
+            asset_type_prefix,
+            min_production_min,
+            max_production_min,
+            min_penstock_number,
+            max_penstock_number,
             min_start_cost,
             max_start_cost,
-            efficiency_curve,
+            generator_efficiency_curve,
             external_id_prefix,
             space,
             filter,
         )
         return self._histogram(
             self._view_id,
             property,
@@ -569,21 +597,23 @@
         self,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
         display_name: str | list[str] | None = None,
         display_name_prefix: str | None = None,
         min_ordering: int | None = None,
         max_ordering: int | None = None,
-        min_p_min: float | None = None,
-        max_p_min: float | None = None,
-        min_penstock: int | None = None,
-        max_penstock: int | None = None,
+        asset_type: str | list[str] | None = None,
+        asset_type_prefix: str | None = None,
+        min_production_min: float | None = None,
+        max_production_min: float | None = None,
+        min_penstock_number: int | None = None,
+        max_penstock_number: int | None = None,
         min_start_cost: float | None = None,
         max_start_cost: float | None = None,
-        efficiency_curve: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        generator_efficiency_curve: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
         retrieve_edges: bool = True,
     ) -> GeneratorList:
         """List/filter generators
@@ -591,26 +621,28 @@
         Args:
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
             display_name: The display name to filter on.
             display_name_prefix: The prefix of the display name to filter on.
             min_ordering: The minimum value of the ordering to filter on.
             max_ordering: The maximum value of the ordering to filter on.
-            min_p_min: The minimum value of the p min to filter on.
-            max_p_min: The maximum value of the p min to filter on.
-            min_penstock: The minimum value of the penstock to filter on.
-            max_penstock: The maximum value of the penstock to filter on.
+            asset_type: The asset type to filter on.
+            asset_type_prefix: The prefix of the asset type to filter on.
+            min_production_min: The minimum value of the production min to filter on.
+            max_production_min: The maximum value of the production min to filter on.
+            min_penstock_number: The minimum value of the penstock number to filter on.
+            max_penstock_number: The maximum value of the penstock number to filter on.
             min_start_cost: The minimum value of the start cost to filter on.
             max_start_cost: The maximum value of the start cost to filter on.
-            efficiency_curve: The efficiency curve to filter on.
+            generator_efficiency_curve: The generator efficiency curve to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of generators to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
-            retrieve_edges: Whether to retrieve `turbine_curves` external ids for the generators. Defaults to True.
+            retrieve_edges: Whether to retrieve `turbine_efficiency_curves` external ids for the generators. Defaults to True.
 
         Returns:
             List of requested generators
 
         Examples:
 
             List generators and limit to 5:
@@ -624,33 +656,35 @@
             self._view_id,
             name,
             name_prefix,
             display_name,
             display_name_prefix,
             min_ordering,
             max_ordering,
-            min_p_min,
-            max_p_min,
-            min_penstock,
-            max_penstock,
+            asset_type,
+            asset_type_prefix,
+            min_production_min,
+            max_production_min,
+            min_penstock_number,
+            max_penstock_number,
             min_start_cost,
             max_start_cost,
-            efficiency_curve,
+            generator_efficiency_curve,
             external_id_prefix,
             space,
             filter,
         )
 
         return self._list(
             limit=limit,
             filter=filter_,
             retrieve_edges=retrieve_edges,
             edge_api_name_type_direction_view_id_penta=[
                 (
-                    self.turbine_curves_edge,
-                    "turbine_curves",
-                    dm.DirectRelationReference("sp_powerops_types", "isSubAssetOf"),
+                    self.turbine_efficiency_curves_edge,
+                    "turbine_efficiency_curves",
+                    dm.DirectRelationReference("sp_powerops_types_temp", "isSubAssetOf"),
                     "outwards",
-                    dm.ViewId("sp_powerops_models", "TurbineEfficiencyCurve", "1"),
+                    dm.ViewId("sp_powerops_models_temp", "TurbineEfficiencyCurve", "1"),
                 ),
             ],
         )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/generator_efficiency_curve.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/generator_efficiency_curve.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/generator_efficiency_curve_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/generator_efficiency_curve_query.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/generator_is_available_time_series.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/generator_is_available_time_series.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/generator_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/price_area_asset_query.py`

 * *Files 21% similar despite different names*

```diff
@@ -3,118 +3,130 @@
 import datetime
 from typing import TYPE_CHECKING
 
 from cognite.client import data_modeling as dm, CogniteClient
 
 from cognite.powerops.client._generated.v1.data_classes import (
     DomainModelCore,
-    Generator,
-    GeneratorEfficiencyCurve,
+    PriceAreaAsset,
 )
 from ._core import DEFAULT_QUERY_LIMIT, QueryBuilder, QueryStep, QueryAPI, T_DomainModelList, _create_edge_filter
 
 if TYPE_CHECKING:
-    from .turbine_efficiency_curve_query import TurbineEfficiencyCurveQueryAPI
+    from .plant_query import PlantQueryAPI
+    from .watercourse_query import WatercourseQueryAPI
 
 
-class GeneratorQueryAPI(QueryAPI[T_DomainModelList]):
+class PriceAreaAssetQueryAPI(QueryAPI[T_DomainModelList]):
     def __init__(
         self,
         client: CogniteClient,
         builder: QueryBuilder[T_DomainModelList],
         view_by_read_class: dict[type[DomainModelCore], dm.ViewId],
         filter_: dm.filters.Filter | None = None,
         limit: int = DEFAULT_QUERY_LIMIT,
     ):
         super().__init__(client, builder, view_by_read_class)
 
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("generator"),
+                name=self._builder.next_name("price_area_asset"),
                 expression=dm.query.NodeResultSetExpression(
                     from_=self._builder[-1].name if self._builder else None,
                     filter=filter_,
                 ),
-                select=dm.query.Select([dm.query.SourceSelector(self._view_by_read_class[Generator], ["*"])]),
-                result_cls=Generator,
+                select=dm.query.Select([dm.query.SourceSelector(self._view_by_read_class[PriceAreaAsset], ["*"])]),
+                result_cls=PriceAreaAsset,
                 max_retrieve_limit=limit,
             )
         )
 
-    def turbine_curves(
+    def plants(
         self,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_QUERY_LIMIT,
-        retrieve_efficiency_curve: bool = False,
-    ) -> TurbineEfficiencyCurveQueryAPI[T_DomainModelList]:
-        """Query along the turbine curve edges of the generator.
+    ) -> PlantQueryAPI[T_DomainModelList]:
+        """Query along the plant edges of the price area asset.
 
         Args:
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of turbine curve edges to return. Defaults to 25. Set to -1, float("inf") or None
+            limit: Maximum number of plant edges to return. Defaults to 25. Set to -1, float("inf") or None
                 to return all items.
-            retrieve_efficiency_curve: Whether to retrieve the efficiency curve for each generator or not.
 
         Returns:
-            TurbineEfficiencyCurveQueryAPI: The query API for the turbine efficiency curve.
+            PlantQueryAPI: The query API for the plant.
         """
-        from .turbine_efficiency_curve_query import TurbineEfficiencyCurveQueryAPI
+        from .plant_query import PlantQueryAPI
 
         from_ = self._builder[-1].name
 
         edge_filter = _create_edge_filter(
-            dm.DirectRelationReference("sp_powerops_types", "isSubAssetOf"),
+            dm.DirectRelationReference("sp_powerops_types", "isPlantOf"),
             external_id_prefix=external_id_prefix,
             space=space,
         )
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("turbine_curves"),
+                name=self._builder.next_name("plants"),
                 expression=dm.query.EdgeResultSetExpression(
                     filter=edge_filter,
                     from_=from_,
                     direction="outwards",
                 ),
                 select=dm.query.Select(),
                 max_retrieve_limit=limit,
             )
         )
-        if retrieve_efficiency_curve:
-            self._query_append_efficiency_curve(from_)
-        return TurbineEfficiencyCurveQueryAPI(self._client, self._builder, self._view_by_read_class, None, limit)
+        return PlantQueryAPI(self._client, self._builder, self._view_by_read_class, None, limit)
 
-    def query(
+    def watercourses(
         self,
-        retrieve_efficiency_curve: bool = False,
-    ) -> T_DomainModelList:
-        """Execute query and return the result.
+        external_id_prefix: str | None = None,
+        space: str | list[str] | None = None,
+        limit: int | None = DEFAULT_QUERY_LIMIT,
+    ) -> WatercourseQueryAPI[T_DomainModelList]:
+        """Query along the watercourse edges of the price area asset.
 
         Args:
-            retrieve_efficiency_curve: Whether to retrieve the efficiency curve for each generator or not.
+            external_id_prefix: The prefix of the external ID to filter on.
+            space: The space to filter on.
+            limit: Maximum number of watercourse edges to return. Defaults to 25. Set to -1, float("inf") or None
+                to return all items.
 
         Returns:
-            The list of the source nodes of the query.
-
+            WatercourseQueryAPI: The query API for the watercourse.
         """
+        from .watercourse_query import WatercourseQueryAPI
+
         from_ = self._builder[-1].name
-        if retrieve_efficiency_curve:
-            self._query_append_efficiency_curve(from_)
-        return self._query()
 
-    def _query_append_efficiency_curve(self, from_: str) -> None:
-        view_id = self._view_by_read_class[GeneratorEfficiencyCurve]
+        edge_filter = _create_edge_filter(
+            dm.DirectRelationReference("sp_powerops_types", "isWatercourseOf"),
+            external_id_prefix=external_id_prefix,
+            space=space,
+        )
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("efficiency_curve"),
-                expression=dm.query.NodeResultSetExpression(
-                    filter=dm.filters.HasData(views=[view_id]),
+                name=self._builder.next_name("watercourses"),
+                expression=dm.query.EdgeResultSetExpression(
+                    filter=edge_filter,
                     from_=from_,
-                    through=self._view_by_read_class[Generator].as_property_ref("efficiencyCurve"),
                     direction="outwards",
                 ),
-                select=dm.query.Select([dm.query.SourceSelector(view_id, ["*"])]),
-                max_retrieve_limit=-1,
-                result_cls=GeneratorEfficiencyCurve,
-            ),
+                select=dm.query.Select(),
+                max_retrieve_limit=limit,
+            )
         )
+        return WatercourseQueryAPI(self._client, self._builder, self._view_by_read_class, None, limit)
+
+    def query(
+        self,
+    ) -> T_DomainModelList:
+        """Execute query and return the result.
+
+        Returns:
+            The list of the source nodes of the query.
+
+        """
+        return self._query()
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/generator_start_stop_cost.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/mapping_timeseries.py`

 * *Files 7% similar despite different names*

```diff
@@ -5,23 +5,21 @@
 from typing import Literal
 
 import pandas as pd
 from cognite.client import CogniteClient
 from cognite.client import data_modeling as dm
 from cognite.client.data_classes import Datapoints, DatapointsArrayList, DatapointsList, TimeSeriesList
 from cognite.client.data_classes.datapoints import Aggregate
-from cognite.powerops.client._generated.v1.data_classes._generator import _create_generator_filter
+from cognite.powerops.client._generated.v1.data_classes._mapping import _create_mapping_filter
 from ._core import DEFAULT_LIMIT_READ, INSTANCE_QUERY_LIMIT
 
-ColumnNames = Literal[
-    "name", "displayName", "ordering", "pMin", "penstock", "startCost", "startStopCost", "isAvailableTimeSeries"
-]
+ColumnNames = Literal["shopPath", "timeseries", "retrieve", "aggregation"]
 
 
-class GeneratorStartStopCostQuery:
+class MappingTimeseriesQuery:
     def __init__(
         self,
         client: CogniteClient,
         view_id: dm.ViewId,
         timeseries_limit: int = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ):
@@ -38,15 +36,15 @@
         aggregates: Aggregate | list[Aggregate] | None = None,
         granularity: str | None = None,
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         limit: int | None = None,
         include_outside_points: bool = False,
     ) -> DatapointsList:
-        """`Retrieve datapoints for the `generator.start_stop_cost` timeseries.
+        """`Retrieve datapoints for the `mapping.timeseries` timeseries.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -63,19 +61,19 @@
 
         Returns:
             A ``DatapointsList`` with the requested datapoints.
 
         Examples:
 
             In this example,
-            we are using the time-ago format to get raw data for the 'my_start_stop_cost' from 2 weeks ago up until now::
+            we are using the time-ago format to get raw data for the 'my_timeseries' from 2 weeks ago up until now::
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> generator_datapoints = client.generator.start_stop_cost(external_id="my_start_stop_cost").retrieve(start="2w-ago")
+                >>> mapping_datapoints = client.mapping.timeseries(external_id="my_timeseries").retrieve(start="2w-ago")
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra()
         if external_ids:
             return self._client.time_series.data.retrieve(
                 external_id=list(external_ids),
                 start=start,
                 end=end,
@@ -97,15 +95,15 @@
         aggregates: Aggregate | list[Aggregate] | None = None,
         granularity: str | None = None,
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         limit: int | None = None,
         include_outside_points: bool = False,
     ) -> DatapointsArrayList:
-        """`Retrieve numpy arrays for the `generator.start_stop_cost` timeseries.
+        """`Retrieve numpy arrays for the `mapping.timeseries` timeseries.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -122,19 +120,19 @@
 
         Returns:
             A ``DatapointsArrayList`` with the requested datapoints.
 
         Examples:
 
             In this example,
-            we are using the time-ago format to get raw data for the 'my_start_stop_cost' from 2 weeks ago up until now::
+            we are using the time-ago format to get raw data for the 'my_timeseries' from 2 weeks ago up until now::
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> generator_datapoints = client.generator.start_stop_cost(external_id="my_start_stop_cost").retrieve_array(start="2w-ago")
+                >>> mapping_datapoints = client.mapping.timeseries(external_id="my_timeseries").retrieve_array(start="2w-ago")
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra()
         if external_ids:
             return self._client.time_series.data.retrieve_arrays(
                 external_id=list(external_ids),
                 start=start,
                 end=end,
@@ -158,17 +156,17 @@
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         limit: int | None = None,
         include_outside_points: bool = False,
         uniform_index: bool = False,
         include_aggregate_name: bool = True,
         include_granularity_name: bool = False,
-        column_names: ColumnNames | list[ColumnNames] = "startStopCost",
+        column_names: ColumnNames | list[ColumnNames] = "timeseries",
     ) -> pd.DataFrame:
-        """`Retrieve DataFrames for the `generator.start_stop_cost` timeseries.
+        """`Retrieve DataFrames for the `mapping.timeseries` timeseries.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -181,28 +179,28 @@
             target_unit: The unit_external_id of the data points returned. If the time series does not have an unit_external_id that can be converted to the target_unit, an error will be returned. Cannot be used with target_unit_system.
             target_unit_system: The unit system of the data points returned. Cannot be used with target_unit.
             limit: Maximum number of datapoints to return for each time series. Default: None (no limit)
             include_outside_points: Whether to include outside points. Not allowed when fetching aggregates. Default: False
             uniform_index: If only querying aggregates AND a single granularity is used, AND no limit is used, specifying `uniform_index=True` will return a dataframe with an equidistant datetime index from the earliest `start` to the latest `end` (missing values will be NaNs). If these requirements are not met, a ValueError is raised. Default: False
             include_aggregate_name: Include 'aggregate' in the column name, e.g. `my-ts|average`. Ignored for raw time series. Default: True
             include_granularity_name: Include 'granularity' in the column name, e.g. `my-ts|12h`. Added after 'aggregate' when present. Ignored for raw time series. Default: False
-            column_names: Which property to use for column names. Defauts to startStopCost
+            column_names: Which property to use for column names. Defauts to timeseries
 
 
         Returns:
             A ``DataFrame`` with the requested datapoints.
 
         Examples:
 
             In this example,
-            we are using the time-ago format to get raw data for the 'my_start_stop_cost' from 2 weeks ago up until now::
+            we are using the time-ago format to get raw data for the 'my_timeseries' from 2 weeks ago up until now::
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> generator_datapoints = client.generator.start_stop_cost(external_id="my_start_stop_cost").retrieve_dataframe(start="2w-ago")
+                >>> mapping_datapoints = client.mapping.timeseries(external_id="my_timeseries").retrieve_dataframe(start="2w-ago")
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra(column_names)
         if external_ids:
             df = self._client.time_series.data.retrieve_dataframe(
                 external_id=list(external_ids),
                 start=start,
                 end=end,
@@ -235,17 +233,17 @@
         aggregates: Aggregate | Sequence[Aggregate] | None = None,
         granularity: str | None = None,
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         uniform_index: bool = False,
         include_aggregate_name: bool = True,
         include_granularity_name: bool = False,
-        column_names: ColumnNames | list[ColumnNames] = "startStopCost",
+        column_names: ColumnNames | list[ColumnNames] = "timeseries",
     ) -> pd.DataFrame:
-        """Retrieve DataFrames for the `generator.start_stop_cost` timeseries in Timezone.
+        """Retrieve DataFrames for the `mapping.timeseries` timeseries in Timezone.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -258,30 +256,30 @@
             target_unit: The unit_external_id of the data points returned. If the time series does not have an unit_external_id that can be converted to the target_unit, an error will be returned. Cannot be used with target_unit_system.
             target_unit_system: The unit system of the data points returned. Cannot be used with target_unit.
             limit: Maximum number of datapoints to return for each time series. Default: None (no limit)
             include_outside_points: Whether to include outside points. Not allowed when fetching aggregates. Default: False
             uniform_index: If only querying aggregates AND a single granularity is used, AND no limit is used, specifying `uniform_index=True` will return a dataframe with an equidistant datetime index from the earliest `start` to the latest `end` (missing values will be NaNs). If these requirements are not met, a ValueError is raised. Default: False
             include_aggregate_name: Include 'aggregate' in the column name, e.g. `my-ts|average`. Ignored for raw time series. Default: True
             include_granularity_name: Include 'granularity' in the column name, e.g. `my-ts|12h`. Added after 'aggregate' when present. Ignored for raw time series. Default: False
-            column_names: Which property to use for column names. Defauts to startStopCost
+            column_names: Which property to use for column names. Defauts to timeseries
 
 
         Returns:
             A ``DataFrame`` with the requested datapoints.
 
         Examples:
 
             In this example,
-            get weekly aggregates for the 'my_start_stop_cost' for the first month of 2023 in Oslo time:
+            get weekly aggregates for the 'my_timeseries' for the first month of 2023 in Oslo time:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> from datetime import datetime, timezone
                 >>> client = PowerOpsModelsV1Client()
-                >>> generator_datapoints = client.generator.start_stop_cost(
-                ...     external_id="my_start_stop_cost").retrieve_dataframe_in_timezone(
+                >>> mapping_datapoints = client.mapping.timeseries(
+                ...     external_id="my_timeseries").retrieve_dataframe_in_timezone(
                 ...         datetime(2023, 1, 1, tzinfo=ZoneInfo("Europe/Oslo")),
                 ...         datetime(2023, 1, 2, tzinfo=ZoneInfo("Europe/Oslo")),
                 ...         aggregates="average",
                 ...         granularity="1week",
                 ...     )
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra(column_names)
@@ -319,17 +317,17 @@
                 external_id=list(external_ids),
                 before=before,
             )
         else:
             return None
 
     def _retrieve_timeseries_external_ids_with_extra(
-        self, extra_properties: ColumnNames | list[ColumnNames] = "startStopCost"
+        self, extra_properties: ColumnNames | list[ColumnNames] = "timeseries"
     ) -> dict[str, list[str]]:
-        return _retrieve_timeseries_external_ids_with_extra_start_stop_cost(
+        return _retrieve_timeseries_external_ids_with_extra_timesery(
             self._client,
             self._view_id,
             self._filter,
             self._timeseries_limit,
             extra_properties,
         )
 
@@ -337,218 +335,176 @@
     def _rename_columns(
         external_ids: dict[str, list[str]],
         df: pd.DataFrame,
         column_names: ColumnNames | list[ColumnNames],
         include_aggregate_name: bool,
         include_granularity_name: bool,
     ) -> pd.DataFrame:
-        if isinstance(column_names, str) and column_names == "startStopCost":
+        if isinstance(column_names, str) and column_names == "timeseries":
             return df
         splits = sum(included for included in [include_aggregate_name, include_granularity_name])
         if splits == 0:
             df.columns = ["-".join(external_ids[external_id]) for external_id in df.columns]
         else:
             column_parts = (col.rsplit("|", maxsplit=splits) for col in df.columns)
             df.columns = [
                 "-".join(external_ids[external_id]) + "|" + "|".join(parts) for external_id, *parts in column_parts
             ]
         return df
 
 
-class GeneratorStartStopCostAPI:
+class MappingTimeseriesAPI:
     def __init__(self, client: CogniteClient, view_id: dm.ViewId):
         self._client = client
         self._view_id = view_id
 
     def __call__(
         self,
-        name: str | list[str] | None = None,
-        name_prefix: str | None = None,
-        display_name: str | list[str] | None = None,
-        display_name_prefix: str | None = None,
-        min_ordering: int | None = None,
-        max_ordering: int | None = None,
-        min_p_min: float | None = None,
-        max_p_min: float | None = None,
-        min_penstock: int | None = None,
-        max_penstock: int | None = None,
-        min_start_cost: float | None = None,
-        max_start_cost: float | None = None,
-        efficiency_curve: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        shop_path: str | list[str] | None = None,
+        shop_path_prefix: str | None = None,
+        retrieve: str | list[str] | None = None,
+        retrieve_prefix: str | None = None,
+        aggregation: str | list[str] | None = None,
+        aggregation_prefix: str | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
-    ) -> GeneratorStartStopCostQuery:
-        """Query timeseries `generator.start_stop_cost`
+    ) -> MappingTimeseriesQuery:
+        """Query timeseries `mapping.timeseries`
 
         Args:
-            name: The name to filter on.
-            name_prefix: The prefix of the name to filter on.
-            display_name: The display name to filter on.
-            display_name_prefix: The prefix of the display name to filter on.
-            min_ordering: The minimum value of the ordering to filter on.
-            max_ordering: The maximum value of the ordering to filter on.
-            min_p_min: The minimum value of the p min to filter on.
-            max_p_min: The maximum value of the p min to filter on.
-            min_penstock: The minimum value of the penstock to filter on.
-            max_penstock: The maximum value of the penstock to filter on.
-            min_start_cost: The minimum value of the start cost to filter on.
-            max_start_cost: The maximum value of the start cost to filter on.
-            efficiency_curve: The efficiency curve to filter on.
+            shop_path: The shop path to filter on.
+            shop_path_prefix: The prefix of the shop path to filter on.
+            retrieve: The retrieve to filter on.
+            retrieve_prefix: The prefix of the retrieve to filter on.
+            aggregation: The aggregation to filter on.
+            aggregation_prefix: The prefix of the aggregation to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of generators to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of mappings to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
-            A query object that can be used to retrieve datapoins for the generator.start_stop_cost timeseries
+            A query object that can be used to retrieve datapoins for the mapping.timeseries timeseries
             selected in this method.
 
         Examples:
 
-            Retrieve all data for 5 generator.start_stop_cost timeseries:
+            Retrieve all data for 5 mapping.timeseries timeseries:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> generators = client.generator.start_stop_cost(limit=5).retrieve()
+                >>> mappings = client.mapping.timeseries(limit=5).retrieve()
 
         """
-        filter_ = _create_generator_filter(
+        filter_ = _create_mapping_filter(
             self._view_id,
-            name,
-            name_prefix,
-            display_name,
-            display_name_prefix,
-            min_ordering,
-            max_ordering,
-            min_p_min,
-            max_p_min,
-            min_penstock,
-            max_penstock,
-            min_start_cost,
-            max_start_cost,
-            efficiency_curve,
+            shop_path,
+            shop_path_prefix,
+            retrieve,
+            retrieve_prefix,
+            aggregation,
+            aggregation_prefix,
             external_id_prefix,
             space,
             filter,
         )
 
-        return GeneratorStartStopCostQuery(
+        return MappingTimeseriesQuery(
             client=self._client,
             view_id=self._view_id,
             timeseries_limit=limit,
             filter=filter_,
         )
 
     def list(
         self,
-        name: str | list[str] | None = None,
-        name_prefix: str | None = None,
-        display_name: str | list[str] | None = None,
-        display_name_prefix: str | None = None,
-        min_ordering: int | None = None,
-        max_ordering: int | None = None,
-        min_p_min: float | None = None,
-        max_p_min: float | None = None,
-        min_penstock: int | None = None,
-        max_penstock: int | None = None,
-        min_start_cost: float | None = None,
-        max_start_cost: float | None = None,
-        efficiency_curve: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        shop_path: str | list[str] | None = None,
+        shop_path_prefix: str | None = None,
+        retrieve: str | list[str] | None = None,
+        retrieve_prefix: str | None = None,
+        aggregation: str | list[str] | None = None,
+        aggregation_prefix: str | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> TimeSeriesList:
-        """List timeseries `generator.start_stop_cost`
+        """List timeseries `mapping.timeseries`
 
         Args:
-            name: The name to filter on.
-            name_prefix: The prefix of the name to filter on.
-            display_name: The display name to filter on.
-            display_name_prefix: The prefix of the display name to filter on.
-            min_ordering: The minimum value of the ordering to filter on.
-            max_ordering: The maximum value of the ordering to filter on.
-            min_p_min: The minimum value of the p min to filter on.
-            max_p_min: The maximum value of the p min to filter on.
-            min_penstock: The minimum value of the penstock to filter on.
-            max_penstock: The maximum value of the penstock to filter on.
-            min_start_cost: The minimum value of the start cost to filter on.
-            max_start_cost: The maximum value of the start cost to filter on.
-            efficiency_curve: The efficiency curve to filter on.
+            shop_path: The shop path to filter on.
+            shop_path_prefix: The prefix of the shop path to filter on.
+            retrieve: The retrieve to filter on.
+            retrieve_prefix: The prefix of the retrieve to filter on.
+            aggregation: The aggregation to filter on.
+            aggregation_prefix: The prefix of the aggregation to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of generators to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of mappings to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
-            List of Timeseries generator.start_stop_cost.
+            List of Timeseries mapping.timeseries.
 
         Examples:
 
-            List generator.start_stop_cost and limit to 5:
+            List mapping.timeseries and limit to 5:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> generators = client.generator.start_stop_cost.list(limit=5)
+                >>> mappings = client.mapping.timeseries.list(limit=5)
 
         """
-        filter_ = _create_generator_filter(
+        filter_ = _create_mapping_filter(
             self._view_id,
-            name,
-            name_prefix,
-            display_name,
-            display_name_prefix,
-            min_ordering,
-            max_ordering,
-            min_p_min,
-            max_p_min,
-            min_penstock,
-            max_penstock,
-            min_start_cost,
-            max_start_cost,
-            efficiency_curve,
+            shop_path,
+            shop_path_prefix,
+            retrieve,
+            retrieve_prefix,
+            aggregation,
+            aggregation_prefix,
             external_id_prefix,
             space,
             filter,
         )
-        external_ids = _retrieve_timeseries_external_ids_with_extra_start_stop_cost(
+        external_ids = _retrieve_timeseries_external_ids_with_extra_timesery(
             self._client, self._view_id, filter_, limit
         )
         if external_ids:
             return self._client.time_series.retrieve_multiple(external_ids=list(external_ids))
         else:
             return TimeSeriesList([])
 
 
-def _retrieve_timeseries_external_ids_with_extra_start_stop_cost(
+def _retrieve_timeseries_external_ids_with_extra_timesery(
     client: CogniteClient,
     view_id: dm.ViewId,
     filter_: dm.Filter | None,
     limit: int,
-    extra_properties: ColumnNames | list[ColumnNames] = "startStopCost",
+    extra_properties: ColumnNames | list[ColumnNames] = "timeseries",
 ) -> dict[str, list[str]]:
     limit = float("inf") if limit is None or limit == -1 else limit
-    properties = ["startStopCost"]
-    if extra_properties == "startStopCost":
+    properties = ["timeseries"]
+    if extra_properties == "timeseries":
         ...
-    elif isinstance(extra_properties, str) and extra_properties != "startStopCost":
+    elif isinstance(extra_properties, str) and extra_properties != "timeseries":
         properties.append(extra_properties)
     elif isinstance(extra_properties, list):
-        properties.extend([prop for prop in extra_properties if prop != "startStopCost"])
+        properties.extend([prop for prop in extra_properties if prop != "timeseries"])
     else:
         raise ValueError(f"Invalid value for extra_properties: {extra_properties}")
 
     if isinstance(extra_properties, str):
         extra_list = [extra_properties]
     else:
         extra_list = extra_properties
     has_data = dm.filters.HasData(views=[view_id])
-    has_property = dm.filters.Exists(property=view_id.as_property_ref("startStopCost"))
+    has_property = dm.filters.Exists(property=view_id.as_property_ref("timeseries"))
     filter_ = dm.filters.And(filter_, has_data, has_property) if filter_ else dm.filters.And(has_data, has_property)
 
     cursor = None
     external_ids: dict[str, list[str]] = {}
     total_retrieved = 0
     while True:
         query_limit = max(min(INSTANCE_QUERY_LIMIT, limit - total_retrieved), 0)
@@ -562,15 +518,15 @@
                     [dm.query.SourceSelector(view_id, properties)],
                 )
             },
             cursors={"nodes": cursor},
         )
         result = client.data_modeling.instances.query(query)
         batch_external_ids = {
-            node.properties[view_id]["startStopCost"]: [node.properties[view_id].get(prop, "") for prop in extra_list]
+            node.properties[view_id]["timeseries"]: [node.properties[view_id].get(prop, "") for prop in extra_list]
             for node in result.data["nodes"].data
         }
         total_retrieved += len(batch_external_ids)
         external_ids.update(batch_external_ids)
         cursor = result.cursors["nodes"]
         if total_retrieved >= limit or cursor is None:
             break
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/generator_turbine_curves.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/generator_turbine_curves.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/mapping.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/mapping.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/mapping_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/mapping_query.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/mapping_timeseries.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/shop_time_series_timeseries.py`

 * *Files 6% similar despite different names*

```diff
@@ -5,21 +5,21 @@
 from typing import Literal
 
 import pandas as pd
 from cognite.client import CogniteClient
 from cognite.client import data_modeling as dm
 from cognite.client.data_classes import Datapoints, DatapointsArrayList, DatapointsList, TimeSeriesList
 from cognite.client.data_classes.datapoints import Aggregate
-from cognite.powerops.client._generated.v1.data_classes._mapping import _create_mapping_filter
+from cognite.powerops.client._generated.v1.data_classes._shop_time_series import _create_shop_time_series_filter
 from ._core import DEFAULT_LIMIT_READ, INSTANCE_QUERY_LIMIT
 
-ColumnNames = Literal["shopPath", "timeseries", "retrieve", "aggregation"]
+ColumnNames = Literal["objectType", "objectName", "attributeName", "timeseries"]
 
 
-class MappingTimeseriesQuery:
+class SHOPTimeSeriesTimeseriesQuery:
     def __init__(
         self,
         client: CogniteClient,
         view_id: dm.ViewId,
         timeseries_limit: int = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ):
@@ -36,15 +36,15 @@
         aggregates: Aggregate | list[Aggregate] | None = None,
         granularity: str | None = None,
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         limit: int | None = None,
         include_outside_points: bool = False,
     ) -> DatapointsList:
-        """`Retrieve datapoints for the `mapping.timeseries` timeseries.
+        """`Retrieve datapoints for the `shop_time_series.timeseries` timeseries.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -65,15 +65,15 @@
         Examples:
 
             In this example,
             we are using the time-ago format to get raw data for the 'my_timeseries' from 2 weeks ago up until now::
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> mapping_datapoints = client.mapping.timeseries(external_id="my_timeseries").retrieve(start="2w-ago")
+                >>> shop_time_series_datapoints = client.shop_time_series.timeseries(external_id="my_timeseries").retrieve(start="2w-ago")
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra()
         if external_ids:
             return self._client.time_series.data.retrieve(
                 external_id=list(external_ids),
                 start=start,
                 end=end,
@@ -95,15 +95,15 @@
         aggregates: Aggregate | list[Aggregate] | None = None,
         granularity: str | None = None,
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         limit: int | None = None,
         include_outside_points: bool = False,
     ) -> DatapointsArrayList:
-        """`Retrieve numpy arrays for the `mapping.timeseries` timeseries.
+        """`Retrieve numpy arrays for the `shop_time_series.timeseries` timeseries.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -124,15 +124,15 @@
         Examples:
 
             In this example,
             we are using the time-ago format to get raw data for the 'my_timeseries' from 2 weeks ago up until now::
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> mapping_datapoints = client.mapping.timeseries(external_id="my_timeseries").retrieve_array(start="2w-ago")
+                >>> shop_time_series_datapoints = client.shop_time_series.timeseries(external_id="my_timeseries").retrieve_array(start="2w-ago")
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra()
         if external_ids:
             return self._client.time_series.data.retrieve_arrays(
                 external_id=list(external_ids),
                 start=start,
                 end=end,
@@ -158,15 +158,15 @@
         limit: int | None = None,
         include_outside_points: bool = False,
         uniform_index: bool = False,
         include_aggregate_name: bool = True,
         include_granularity_name: bool = False,
         column_names: ColumnNames | list[ColumnNames] = "timeseries",
     ) -> pd.DataFrame:
-        """`Retrieve DataFrames for the `mapping.timeseries` timeseries.
+        """`Retrieve DataFrames for the `shop_time_series.timeseries` timeseries.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -192,15 +192,15 @@
         Examples:
 
             In this example,
             we are using the time-ago format to get raw data for the 'my_timeseries' from 2 weeks ago up until now::
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> mapping_datapoints = client.mapping.timeseries(external_id="my_timeseries").retrieve_dataframe(start="2w-ago")
+                >>> shop_time_series_datapoints = client.shop_time_series.timeseries(external_id="my_timeseries").retrieve_dataframe(start="2w-ago")
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra(column_names)
         if external_ids:
             df = self._client.time_series.data.retrieve_dataframe(
                 external_id=list(external_ids),
                 start=start,
                 end=end,
@@ -235,15 +235,15 @@
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         uniform_index: bool = False,
         include_aggregate_name: bool = True,
         include_granularity_name: bool = False,
         column_names: ColumnNames | list[ColumnNames] = "timeseries",
     ) -> pd.DataFrame:
-        """Retrieve DataFrames for the `mapping.timeseries` timeseries in Timezone.
+        """Retrieve DataFrames for the `shop_time_series.timeseries` timeseries in Timezone.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -270,15 +270,15 @@
 
             In this example,
             get weekly aggregates for the 'my_timeseries' for the first month of 2023 in Oslo time:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> from datetime import datetime, timezone
                 >>> client = PowerOpsModelsV1Client()
-                >>> mapping_datapoints = client.mapping.timeseries(
+                >>> shop_time_series_datapoints = client.shop_time_series.timeseries(
                 ...     external_id="my_timeseries").retrieve_dataframe_in_timezone(
                 ...         datetime(2023, 1, 1, tzinfo=ZoneInfo("Europe/Oslo")),
                 ...         datetime(2023, 1, 2, tzinfo=ZoneInfo("Europe/Oslo")),
                 ...         aggregates="average",
                 ...         granularity="1week",
                 ...     )
         """
@@ -348,126 +348,126 @@
             column_parts = (col.rsplit("|", maxsplit=splits) for col in df.columns)
             df.columns = [
                 "-".join(external_ids[external_id]) + "|" + "|".join(parts) for external_id, *parts in column_parts
             ]
         return df
 
 
-class MappingTimeseriesAPI:
+class SHOPTimeSeriesTimeseriesAPI:
     def __init__(self, client: CogniteClient, view_id: dm.ViewId):
         self._client = client
         self._view_id = view_id
 
     def __call__(
         self,
-        shop_path: str | list[str] | None = None,
-        shop_path_prefix: str | None = None,
-        retrieve: str | list[str] | None = None,
-        retrieve_prefix: str | None = None,
-        aggregation: str | list[str] | None = None,
-        aggregation_prefix: str | None = None,
+        object_type: str | list[str] | None = None,
+        object_type_prefix: str | None = None,
+        object_name: str | list[str] | None = None,
+        object_name_prefix: str | None = None,
+        attribute_name: str | list[str] | None = None,
+        attribute_name_prefix: str | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
-    ) -> MappingTimeseriesQuery:
-        """Query timeseries `mapping.timeseries`
+    ) -> SHOPTimeSeriesTimeseriesQuery:
+        """Query timeseries `shop_time_series.timeseries`
 
         Args:
-            shop_path: The shop path to filter on.
-            shop_path_prefix: The prefix of the shop path to filter on.
-            retrieve: The retrieve to filter on.
-            retrieve_prefix: The prefix of the retrieve to filter on.
-            aggregation: The aggregation to filter on.
-            aggregation_prefix: The prefix of the aggregation to filter on.
+            object_type: The object type to filter on.
+            object_type_prefix: The prefix of the object type to filter on.
+            object_name: The object name to filter on.
+            object_name_prefix: The prefix of the object name to filter on.
+            attribute_name: The attribute name to filter on.
+            attribute_name_prefix: The prefix of the attribute name to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of mappings to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of shop time series to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
-            A query object that can be used to retrieve datapoins for the mapping.timeseries timeseries
+            A query object that can be used to retrieve datapoins for the shop_time_series.timeseries timeseries
             selected in this method.
 
         Examples:
 
-            Retrieve all data for 5 mapping.timeseries timeseries:
+            Retrieve all data for 5 shop_time_series.timeseries timeseries:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> mappings = client.mapping.timeseries(limit=5).retrieve()
+                >>> shop_time_series_list = client.shop_time_series.timeseries(limit=5).retrieve()
 
         """
-        filter_ = _create_mapping_filter(
+        filter_ = _create_shop_time_series_filter(
             self._view_id,
-            shop_path,
-            shop_path_prefix,
-            retrieve,
-            retrieve_prefix,
-            aggregation,
-            aggregation_prefix,
+            object_type,
+            object_type_prefix,
+            object_name,
+            object_name_prefix,
+            attribute_name,
+            attribute_name_prefix,
             external_id_prefix,
             space,
             filter,
         )
 
-        return MappingTimeseriesQuery(
+        return SHOPTimeSeriesTimeseriesQuery(
             client=self._client,
             view_id=self._view_id,
             timeseries_limit=limit,
             filter=filter_,
         )
 
     def list(
         self,
-        shop_path: str | list[str] | None = None,
-        shop_path_prefix: str | None = None,
-        retrieve: str | list[str] | None = None,
-        retrieve_prefix: str | None = None,
-        aggregation: str | list[str] | None = None,
-        aggregation_prefix: str | None = None,
+        object_type: str | list[str] | None = None,
+        object_type_prefix: str | None = None,
+        object_name: str | list[str] | None = None,
+        object_name_prefix: str | None = None,
+        attribute_name: str | list[str] | None = None,
+        attribute_name_prefix: str | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> TimeSeriesList:
-        """List timeseries `mapping.timeseries`
+        """List timeseries `shop_time_series.timeseries`
 
         Args:
-            shop_path: The shop path to filter on.
-            shop_path_prefix: The prefix of the shop path to filter on.
-            retrieve: The retrieve to filter on.
-            retrieve_prefix: The prefix of the retrieve to filter on.
-            aggregation: The aggregation to filter on.
-            aggregation_prefix: The prefix of the aggregation to filter on.
+            object_type: The object type to filter on.
+            object_type_prefix: The prefix of the object type to filter on.
+            object_name: The object name to filter on.
+            object_name_prefix: The prefix of the object name to filter on.
+            attribute_name: The attribute name to filter on.
+            attribute_name_prefix: The prefix of the attribute name to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of mappings to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of shop time series to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
-            List of Timeseries mapping.timeseries.
+            List of Timeseries shop_time_series.timeseries.
 
         Examples:
 
-            List mapping.timeseries and limit to 5:
+            List shop_time_series.timeseries and limit to 5:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> mappings = client.mapping.timeseries.list(limit=5)
+                >>> shop_time_series_list = client.shop_time_series.timeseries.list(limit=5)
 
         """
-        filter_ = _create_mapping_filter(
+        filter_ = _create_shop_time_series_filter(
             self._view_id,
-            shop_path,
-            shop_path_prefix,
-            retrieve,
-            retrieve_prefix,
-            aggregation,
-            aggregation_prefix,
+            object_type,
+            object_type_prefix,
+            object_name,
+            object_name_prefix,
+            attribute_name,
+            attribute_name_prefix,
             external_id_prefix,
             space,
             filter,
         )
         external_ids = _retrieve_timeseries_external_ids_with_extra_timesery(
             self._client, self._view_id, filter_, limit
         )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/market_configuration.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/market_configuration.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/market_configuration_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/market_configuration_query.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/model_template.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/model_template.py`

 * *Files 11% similar despite different names*

```diff
@@ -49,49 +49,58 @@
             view_by_read_class=view_by_read_class,
         )
         self._view_id = view_id
         self.base_mappings_edge = ModelTemplateBaseMappingsAPI(client)
 
     def __call__(
         self,
+        name: str | list[str] | None = None,
+        name_prefix: str | None = None,
         version_: str | list[str] | None = None,
         version_prefix: str | None = None,
         shop_version: str | list[str] | None = None,
         shop_version_prefix: str | None = None,
-        watercourse: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        min_penalty_limit: float | None = None,
+        max_penalty_limit: float | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_QUERY_LIMIT,
         filter: dm.Filter | None = None,
     ) -> ModelTemplateQueryAPI[ModelTemplateList]:
         """Query starting at model templates.
 
         Args:
+            name: The name to filter on.
+            name_prefix: The prefix of the name to filter on.
             version_: The version to filter on.
             version_prefix: The prefix of the version to filter on.
             shop_version: The shop version to filter on.
             shop_version_prefix: The prefix of the shop version to filter on.
-            watercourse: The watercourse to filter on.
+            min_penalty_limit: The minimum value of the penalty limit to filter on.
+            max_penalty_limit: The maximum value of the penalty limit to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of model templates to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
             A query API for model templates.
 
         """
         has_data = dm.filters.HasData(views=[self._view_id])
         filter_ = _create_model_template_filter(
             self._view_id,
+            name,
+            name_prefix,
             version_,
             version_prefix,
             shop_version,
             shop_version_prefix,
-            watercourse,
+            min_penalty_limit,
+            max_penalty_limit,
             external_id_prefix,
             space,
             (filter and dm.filters.And(filter, has_data)) or has_data,
         )
         builder = QueryBuilder(ModelTemplateList)
         return ModelTemplateQueryAPI(self._client, builder, self._view_by_read_class, filter_, limit)
 
@@ -201,45 +210,51 @@
             external_id,
             space,
             retrieve_edges=True,
             edge_api_name_type_direction_view_id_penta=[
                 (
                     self.base_mappings_edge,
                     "base_mappings",
-                    dm.DirectRelationReference("sp_powerops_types", "ModelTemplate.baseMappings"),
+                    dm.DirectRelationReference("sp_powerops_types_temp", "ModelTemplate.baseMappings"),
                     "outwards",
-                    dm.ViewId("sp_powerops_models", "Mapping", "1"),
+                    dm.ViewId("sp_powerops_models_temp", "Mapping", "1"),
                 ),
             ],
         )
 
     def search(
         self,
         query: str,
         properties: ModelTemplateTextFields | Sequence[ModelTemplateTextFields] | None = None,
+        name: str | list[str] | None = None,
+        name_prefix: str | None = None,
         version_: str | list[str] | None = None,
         version_prefix: str | None = None,
         shop_version: str | list[str] | None = None,
         shop_version_prefix: str | None = None,
-        watercourse: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        min_penalty_limit: float | None = None,
+        max_penalty_limit: float | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> ModelTemplateList:
         """Search model templates
 
         Args:
             query: The search query,
             properties: The property to search, if nothing is passed all text fields will be searched.
+            name: The name to filter on.
+            name_prefix: The prefix of the name to filter on.
             version_: The version to filter on.
             version_prefix: The prefix of the version to filter on.
             shop_version: The shop version to filter on.
             shop_version_prefix: The prefix of the shop version to filter on.
-            watercourse: The watercourse to filter on.
+            min_penalty_limit: The minimum value of the penalty limit to filter on.
+            max_penalty_limit: The maximum value of the penalty limit to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of model templates to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
             Search results model templates matching the query.
@@ -251,19 +266,22 @@
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
                 >>> model_templates = client.model_template.search('my_model_template')
 
         """
         filter_ = _create_model_template_filter(
             self._view_id,
+            name,
+            name_prefix,
             version_,
             version_prefix,
             shop_version,
             shop_version_prefix,
-            watercourse,
+            min_penalty_limit,
+            max_penalty_limit,
             external_id_prefix,
             space,
             filter,
         )
         return self._search(self._view_id, query, _MODELTEMPLATE_PROPERTIES_BY_FIELD, properties, filter_, limit)
 
     @overload
@@ -275,19 +293,22 @@
             | Sequence[Aggregations]
             | Sequence[dm.aggregations.MetricAggregation]
         ),
         property: ModelTemplateFields | Sequence[ModelTemplateFields] | None = None,
         group_by: None = None,
         query: str | None = None,
         search_properties: ModelTemplateTextFields | Sequence[ModelTemplateTextFields] | None = None,
+        name: str | list[str] | None = None,
+        name_prefix: str | None = None,
         version_: str | list[str] | None = None,
         version_prefix: str | None = None,
         shop_version: str | list[str] | None = None,
         shop_version_prefix: str | None = None,
-        watercourse: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        min_penalty_limit: float | None = None,
+        max_penalty_limit: float | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> list[dm.aggregations.AggregatedNumberedValue]: ...
 
     @overload
@@ -299,19 +320,22 @@
             | Sequence[Aggregations]
             | Sequence[dm.aggregations.MetricAggregation]
         ),
         property: ModelTemplateFields | Sequence[ModelTemplateFields] | None = None,
         group_by: ModelTemplateFields | Sequence[ModelTemplateFields] = None,
         query: str | None = None,
         search_properties: ModelTemplateTextFields | Sequence[ModelTemplateTextFields] | None = None,
+        name: str | list[str] | None = None,
+        name_prefix: str | None = None,
         version_: str | list[str] | None = None,
         version_prefix: str | None = None,
         shop_version: str | list[str] | None = None,
         shop_version_prefix: str | None = None,
-        watercourse: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        min_penalty_limit: float | None = None,
+        max_penalty_limit: float | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> InstanceAggregationResultList: ...
 
     def aggregate(
@@ -322,37 +346,43 @@
             | Sequence[Aggregations]
             | Sequence[dm.aggregations.MetricAggregation]
         ),
         property: ModelTemplateFields | Sequence[ModelTemplateFields] | None = None,
         group_by: ModelTemplateFields | Sequence[ModelTemplateFields] | None = None,
         query: str | None = None,
         search_property: ModelTemplateTextFields | Sequence[ModelTemplateTextFields] | None = None,
+        name: str | list[str] | None = None,
+        name_prefix: str | None = None,
         version_: str | list[str] | None = None,
         version_prefix: str | None = None,
         shop_version: str | list[str] | None = None,
         shop_version_prefix: str | None = None,
-        watercourse: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        min_penalty_limit: float | None = None,
+        max_penalty_limit: float | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> list[dm.aggregations.AggregatedNumberedValue] | InstanceAggregationResultList:
         """Aggregate data across model templates
 
         Args:
             aggregate: The aggregation to perform.
             property: The property to perform aggregation on.
             group_by: The property to group by when doing the aggregation.
             query: The query to search for in the text field.
             search_property: The text field to search in.
+            name: The name to filter on.
+            name_prefix: The prefix of the name to filter on.
             version_: The version to filter on.
             version_prefix: The prefix of the version to filter on.
             shop_version: The shop version to filter on.
             shop_version_prefix: The prefix of the shop version to filter on.
-            watercourse: The watercourse to filter on.
+            min_penalty_limit: The minimum value of the penalty limit to filter on.
+            max_penalty_limit: The maximum value of the penalty limit to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of model templates to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
             Aggregation results.
@@ -365,19 +395,22 @@
                 >>> client = PowerOpsModelsV1Client()
                 >>> result = client.model_template.aggregate("count", space="my_space")
 
         """
 
         filter_ = _create_model_template_filter(
             self._view_id,
+            name,
+            name_prefix,
             version_,
             version_prefix,
             shop_version,
             shop_version_prefix,
-            watercourse,
+            min_penalty_limit,
+            max_penalty_limit,
             external_id_prefix,
             space,
             filter,
         )
         return self._aggregate(
             self._view_id,
             aggregate,
@@ -392,52 +425,61 @@
 
     def histogram(
         self,
         property: ModelTemplateFields,
         interval: float,
         query: str | None = None,
         search_property: ModelTemplateTextFields | Sequence[ModelTemplateTextFields] | None = None,
+        name: str | list[str] | None = None,
+        name_prefix: str | None = None,
         version_: str | list[str] | None = None,
         version_prefix: str | None = None,
         shop_version: str | list[str] | None = None,
         shop_version_prefix: str | None = None,
-        watercourse: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        min_penalty_limit: float | None = None,
+        max_penalty_limit: float | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> dm.aggregations.HistogramValue:
         """Produces histograms for model templates
 
         Args:
             property: The property to use as the value in the histogram.
             interval: The interval to use for the histogram bins.
             query: The query to search for in the text field.
             search_property: The text field to search in.
+            name: The name to filter on.
+            name_prefix: The prefix of the name to filter on.
             version_: The version to filter on.
             version_prefix: The prefix of the version to filter on.
             shop_version: The shop version to filter on.
             shop_version_prefix: The prefix of the shop version to filter on.
-            watercourse: The watercourse to filter on.
+            min_penalty_limit: The minimum value of the penalty limit to filter on.
+            max_penalty_limit: The maximum value of the penalty limit to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of model templates to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
             Bucketed histogram results.
 
         """
         filter_ = _create_model_template_filter(
             self._view_id,
+            name,
+            name_prefix,
             version_,
             version_prefix,
             shop_version,
             shop_version_prefix,
-            watercourse,
+            min_penalty_limit,
+            max_penalty_limit,
             external_id_prefix,
             space,
             filter,
         )
         return self._histogram(
             self._view_id,
             property,
@@ -447,33 +489,39 @@
             search_property,
             limit,
             filter_,
         )
 
     def list(
         self,
+        name: str | list[str] | None = None,
+        name_prefix: str | None = None,
         version_: str | list[str] | None = None,
         version_prefix: str | None = None,
         shop_version: str | list[str] | None = None,
         shop_version_prefix: str | None = None,
-        watercourse: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        min_penalty_limit: float | None = None,
+        max_penalty_limit: float | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
         retrieve_edges: bool = True,
     ) -> ModelTemplateList:
         """List/filter model templates
 
         Args:
+            name: The name to filter on.
+            name_prefix: The prefix of the name to filter on.
             version_: The version to filter on.
             version_prefix: The prefix of the version to filter on.
             shop_version: The shop version to filter on.
             shop_version_prefix: The prefix of the shop version to filter on.
-            watercourse: The watercourse to filter on.
+            min_penalty_limit: The minimum value of the penalty limit to filter on.
+            max_penalty_limit: The maximum value of the penalty limit to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of model templates to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
             retrieve_edges: Whether to retrieve `base_mappings` external ids for the model templates. Defaults to True.
 
         Returns:
@@ -486,31 +534,34 @@
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
                 >>> model_templates = client.model_template.list(limit=5)
 
         """
         filter_ = _create_model_template_filter(
             self._view_id,
+            name,
+            name_prefix,
             version_,
             version_prefix,
             shop_version,
             shop_version_prefix,
-            watercourse,
+            min_penalty_limit,
+            max_penalty_limit,
             external_id_prefix,
             space,
             filter,
         )
 
         return self._list(
             limit=limit,
             filter=filter_,
             retrieve_edges=retrieve_edges,
             edge_api_name_type_direction_view_id_penta=[
                 (
                     self.base_mappings_edge,
                     "base_mappings",
-                    dm.DirectRelationReference("sp_powerops_types", "ModelTemplate.baseMappings"),
+                    dm.DirectRelationReference("sp_powerops_types_temp", "ModelTemplate.baseMappings"),
                     "outwards",
-                    dm.ViewId("sp_powerops_models", "Mapping", "1"),
+                    dm.ViewId("sp_powerops_models_temp", "Mapping", "1"),
                 ),
             ],
         )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/model_template_base_mappings.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/model_template_base_mappings.py`

 * *Files 1% similar despite different names*

```diff
@@ -39,15 +39,15 @@
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
                 >>> model_template = client.model_template.base_mappings_edge.list("my_model_template", limit=5)
 
         """
         filter_ = _create_edge_filter(
-            dm.DirectRelationReference("sp_powerops_types", "ModelTemplate.baseMappings"),
+            dm.DirectRelationReference("sp_powerops_types_temp", "ModelTemplate.baseMappings"),
             from_model_template,
             from_model_template_space,
             to_mapping,
             to_mapping_space,
             external_id_prefix,
             space,
         )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/model_template_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/shop_based_partial_bid_configuration_query.py`

 * *Files 21% similar despite different names*

```diff
@@ -3,118 +3,97 @@
 import datetime
 from typing import TYPE_CHECKING
 
 from cognite.client import data_modeling as dm, CogniteClient
 
 from cognite.powerops.client._generated.v1.data_classes import (
     DomainModelCore,
-    ModelTemplate,
-    WatercourseShop,
+    ShopBasedPartialBidConfiguration,
+    PowerAsset,
+    ScenarioSet,
 )
 from ._core import DEFAULT_QUERY_LIMIT, QueryBuilder, QueryStep, QueryAPI, T_DomainModelList, _create_edge_filter
 
-if TYPE_CHECKING:
-    from .mapping_query import MappingQueryAPI
 
-
-class ModelTemplateQueryAPI(QueryAPI[T_DomainModelList]):
+class ShopBasedPartialBidConfigurationQueryAPI(QueryAPI[T_DomainModelList]):
     def __init__(
         self,
         client: CogniteClient,
         builder: QueryBuilder[T_DomainModelList],
         view_by_read_class: dict[type[DomainModelCore], dm.ViewId],
         filter_: dm.filters.Filter | None = None,
         limit: int = DEFAULT_QUERY_LIMIT,
     ):
         super().__init__(client, builder, view_by_read_class)
 
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("model_template"),
+                name=self._builder.next_name("shop_based_partial_bid_configuration"),
                 expression=dm.query.NodeResultSetExpression(
                     from_=self._builder[-1].name if self._builder else None,
                     filter=filter_,
                 ),
-                select=dm.query.Select([dm.query.SourceSelector(self._view_by_read_class[ModelTemplate], ["*"])]),
-                result_cls=ModelTemplate,
-                max_retrieve_limit=limit,
-            )
-        )
-
-    def base_mappings(
-        self,
-        external_id_prefix: str | None = None,
-        space: str | list[str] | None = None,
-        limit: int | None = DEFAULT_QUERY_LIMIT,
-        retrieve_watercourse: bool = False,
-    ) -> MappingQueryAPI[T_DomainModelList]:
-        """Query along the base mapping edges of the model template.
-
-        Args:
-            external_id_prefix: The prefix of the external ID to filter on.
-            space: The space to filter on.
-            limit: Maximum number of base mapping edges to return. Defaults to 25. Set to -1, float("inf") or None
-                to return all items.
-            retrieve_watercourse: Whether to retrieve the watercourse for each model template or not.
-
-        Returns:
-            MappingQueryAPI: The query API for the mapping.
-        """
-        from .mapping_query import MappingQueryAPI
-
-        from_ = self._builder[-1].name
-
-        edge_filter = _create_edge_filter(
-            dm.DirectRelationReference("sp_powerops_types", "ModelTemplate.baseMappings"),
-            external_id_prefix=external_id_prefix,
-            space=space,
-        )
-        self._builder.append(
-            QueryStep(
-                name=self._builder.next_name("base_mappings"),
-                expression=dm.query.EdgeResultSetExpression(
-                    filter=edge_filter,
-                    from_=from_,
-                    direction="outwards",
+                select=dm.query.Select(
+                    [dm.query.SourceSelector(self._view_by_read_class[ShopBasedPartialBidConfiguration], ["*"])]
                 ),
-                select=dm.query.Select(),
+                result_cls=ShopBasedPartialBidConfiguration,
                 max_retrieve_limit=limit,
             )
         )
-        if retrieve_watercourse:
-            self._query_append_watercourse(from_)
-        return MappingQueryAPI(self._client, self._builder, self._view_by_read_class, None, limit)
 
     def query(
         self,
-        retrieve_watercourse: bool = False,
+        retrieve_power_asset: bool = False,
+        retrieve_shop_scenarios: bool = False,
     ) -> T_DomainModelList:
         """Execute query and return the result.
 
         Args:
-            retrieve_watercourse: Whether to retrieve the watercourse for each model template or not.
+            retrieve_power_asset: Whether to retrieve the power asset for each shop based partial bid configuration or not.
+            retrieve_shop_scenarios: Whether to retrieve the shop scenario for each shop based partial bid configuration or not.
 
         Returns:
             The list of the source nodes of the query.
 
         """
         from_ = self._builder[-1].name
-        if retrieve_watercourse:
-            self._query_append_watercourse(from_)
+        if retrieve_power_asset:
+            self._query_append_power_asset(from_)
+        if retrieve_shop_scenarios:
+            self._query_append_shop_scenarios(from_)
         return self._query()
 
-    def _query_append_watercourse(self, from_: str) -> None:
-        view_id = self._view_by_read_class[WatercourseShop]
+    def _query_append_power_asset(self, from_: str) -> None:
+        view_id = self._view_by_read_class[PowerAsset]
+        self._builder.append(
+            QueryStep(
+                name=self._builder.next_name("power_asset"),
+                expression=dm.query.NodeResultSetExpression(
+                    filter=dm.filters.HasData(views=[view_id]),
+                    from_=from_,
+                    through=self._view_by_read_class[ShopBasedPartialBidConfiguration].as_property_ref("powerAsset"),
+                    direction="outwards",
+                ),
+                select=dm.query.Select([dm.query.SourceSelector(view_id, ["*"])]),
+                max_retrieve_limit=-1,
+                result_cls=PowerAsset,
+                is_single_direct_relation=True,
+            ),
+        )
+
+    def _query_append_shop_scenarios(self, from_: str) -> None:
+        view_id = self._view_by_read_class[ScenarioSet]
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("watercourse"),
+                name=self._builder.next_name("shop_scenarios"),
                 expression=dm.query.NodeResultSetExpression(
                     filter=dm.filters.HasData(views=[view_id]),
                     from_=from_,
-                    through=self._view_by_read_class[ModelTemplate].as_property_ref("watercourse"),
+                    through=self._view_by_read_class[ShopBasedPartialBidConfiguration].as_property_ref("shopScenarios"),
                     direction="outwards",
                 ),
                 select=dm.query.Select([dm.query.SourceSelector(view_id, ["*"])]),
                 max_retrieve_limit=-1,
-                result_cls=WatercourseShop,
+                result_cls=ScenarioSet,
+                is_single_direct_relation=True,
             ),
         )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/multi_scenario_matrix.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/multi_scenario_matrix_raw.py`

 * *Files 10% similar despite different names*

```diff
@@ -9,53 +9,55 @@
 from cognite.client.data_classes.data_modeling.instances import InstanceAggregationResultList
 
 from cognite.powerops.client._generated.v1.data_classes._core import DEFAULT_INSTANCE_SPACE
 from cognite.powerops.client._generated.v1.data_classes import (
     DomainModelCore,
     DomainModelWrite,
     ResourcesWriteResult,
-    MultiScenarioMatrix,
-    MultiScenarioMatrixWrite,
-    MultiScenarioMatrixFields,
-    MultiScenarioMatrixList,
-    MultiScenarioMatrixWriteList,
-    MultiScenarioMatrixTextFields,
+    MultiScenarioMatrixRaw,
+    MultiScenarioMatrixRawWrite,
+    MultiScenarioMatrixRawFields,
+    MultiScenarioMatrixRawList,
+    MultiScenarioMatrixRawWriteList,
+    MultiScenarioMatrixRawTextFields,
 )
-from cognite.powerops.client._generated.v1.data_classes._multi_scenario_matrix import (
-    _MULTISCENARIOMATRIX_PROPERTIES_BY_FIELD,
-    _create_multi_scenario_matrix_filter,
+from cognite.powerops.client._generated.v1.data_classes._multi_scenario_matrix_raw import (
+    _MULTISCENARIOMATRIXRAW_PROPERTIES_BY_FIELD,
+    _create_multi_scenario_matrix_raw_filter,
 )
 from ._core import (
     DEFAULT_LIMIT_READ,
     DEFAULT_QUERY_LIMIT,
     Aggregations,
     NodeAPI,
     SequenceNotStr,
     QueryStep,
     QueryBuilder,
 )
-from .multi_scenario_matrix_alerts import MultiScenarioMatrixAlertsAPI
-from .multi_scenario_matrix_scenario_results import MultiScenarioMatrixScenarioResultsAPI
-from .multi_scenario_matrix_query import MultiScenarioMatrixQueryAPI
+from .multi_scenario_matrix_raw_alerts import MultiScenarioMatrixRawAlertsAPI
+from .multi_scenario_matrix_raw_shop_results import MultiScenarioMatrixRawShopResultsAPI
+from .multi_scenario_matrix_raw_query import MultiScenarioMatrixRawQueryAPI
 
 
-class MultiScenarioMatrixAPI(NodeAPI[MultiScenarioMatrix, MultiScenarioMatrixWrite, MultiScenarioMatrixList]):
+class MultiScenarioMatrixRawAPI(
+    NodeAPI[MultiScenarioMatrixRaw, MultiScenarioMatrixRawWrite, MultiScenarioMatrixRawList]
+):
     def __init__(self, client: CogniteClient, view_by_read_class: dict[type[DomainModelCore], dm.ViewId]):
-        view_id = view_by_read_class[MultiScenarioMatrix]
+        view_id = view_by_read_class[MultiScenarioMatrixRaw]
         super().__init__(
             client=client,
             sources=view_id,
-            class_type=MultiScenarioMatrix,
-            class_list=MultiScenarioMatrixList,
-            class_write_list=MultiScenarioMatrixWriteList,
+            class_type=MultiScenarioMatrixRaw,
+            class_list=MultiScenarioMatrixRawList,
+            class_write_list=MultiScenarioMatrixRawWriteList,
             view_by_read_class=view_by_read_class,
         )
         self._view_id = view_id
-        self.alerts_edge = MultiScenarioMatrixAlertsAPI(client)
-        self.scenario_results_edge = MultiScenarioMatrixScenarioResultsAPI(client)
+        self.alerts_edge = MultiScenarioMatrixRawAlertsAPI(client)
+        self.shop_results_edge = MultiScenarioMatrixRawShopResultsAPI(client)
 
     def __call__(
         self,
         resource_cost: str | list[str] | None = None,
         resource_cost_prefix: str | None = None,
         asset_type: str | list[str] | None = None,
         asset_type_prefix: str | None = None,
@@ -63,155 +65,155 @@
         asset_id_prefix: str | None = None,
         is_processed: bool | None = None,
         method: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_QUERY_LIMIT,
         filter: dm.Filter | None = None,
-    ) -> MultiScenarioMatrixQueryAPI[MultiScenarioMatrixList]:
-        """Query starting at multi scenario matrixes.
+    ) -> MultiScenarioMatrixRawQueryAPI[MultiScenarioMatrixRawList]:
+        """Query starting at multi scenario matrix raws.
 
         Args:
             resource_cost: The resource cost to filter on.
             resource_cost_prefix: The prefix of the resource cost to filter on.
             asset_type: The asset type to filter on.
             asset_type_prefix: The prefix of the asset type to filter on.
             asset_id: The asset id to filter on.
             asset_id_prefix: The prefix of the asset id to filter on.
             is_processed: The is processed to filter on.
             method: The method to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of multi scenario matrixes to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of multi scenario matrix raws to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
-            A query API for multi scenario matrixes.
+            A query API for multi scenario matrix raws.
 
         """
         has_data = dm.filters.HasData(views=[self._view_id])
-        filter_ = _create_multi_scenario_matrix_filter(
+        filter_ = _create_multi_scenario_matrix_raw_filter(
             self._view_id,
             resource_cost,
             resource_cost_prefix,
             asset_type,
             asset_type_prefix,
             asset_id,
             asset_id_prefix,
             is_processed,
             method,
             external_id_prefix,
             space,
             (filter and dm.filters.And(filter, has_data)) or has_data,
         )
-        builder = QueryBuilder(MultiScenarioMatrixList)
-        return MultiScenarioMatrixQueryAPI(self._client, builder, self._view_by_read_class, filter_, limit)
+        builder = QueryBuilder(MultiScenarioMatrixRawList)
+        return MultiScenarioMatrixRawQueryAPI(self._client, builder, self._view_by_read_class, filter_, limit)
 
     def apply(
         self,
-        multi_scenario_matrix: MultiScenarioMatrixWrite | Sequence[MultiScenarioMatrixWrite],
+        multi_scenario_matrix_raw: MultiScenarioMatrixRawWrite | Sequence[MultiScenarioMatrixRawWrite],
         replace: bool = False,
         write_none: bool = False,
     ) -> ResourcesWriteResult:
-        """Add or update (upsert) multi scenario matrixes.
+        """Add or update (upsert) multi scenario matrix raws.
 
-        Note: This method iterates through all nodes and timeseries linked to multi_scenario_matrix and creates them including the edges
-        between the nodes. For example, if any of `alerts` or `scenario_results` are set, then these
+        Note: This method iterates through all nodes and timeseries linked to multi_scenario_matrix_raw and creates them including the edges
+        between the nodes. For example, if any of `alerts` or `shop_results` are set, then these
         nodes as well as any nodes linked to them, and all the edges linking these nodes will be created.
 
         Args:
-            multi_scenario_matrix: Multi scenario matrix or sequence of multi scenario matrixes to upsert.
+            multi_scenario_matrix_raw: Multi scenario matrix raw or sequence of multi scenario matrix raws to upsert.
             replace (bool): How do we behave when a property value exists? Do we replace all matching and existing values with the supplied values (true)?
                 Or should we merge in new values for properties together with the existing values (false)? Note: This setting applies for all nodes or edges specified in the ingestion call.
             write_none (bool): This method, will by default, skip properties that are set to None. However, if you want to set properties to None,
                 you can set this parameter to True. Note this only applies to properties that are nullable.
         Returns:
             Created instance(s), i.e., nodes, edges, and time series.
 
         Examples:
 
-            Create a new multi_scenario_matrix:
+            Create a new multi_scenario_matrix_raw:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
-                >>> from cognite.powerops.client._generated.v1.data_classes import MultiScenarioMatrixWrite
+                >>> from cognite.powerops.client._generated.v1.data_classes import MultiScenarioMatrixRawWrite
                 >>> client = PowerOpsModelsV1Client()
-                >>> multi_scenario_matrix = MultiScenarioMatrixWrite(external_id="my_multi_scenario_matrix", ...)
-                >>> result = client.multi_scenario_matrix.apply(multi_scenario_matrix)
+                >>> multi_scenario_matrix_raw = MultiScenarioMatrixRawWrite(external_id="my_multi_scenario_matrix_raw", ...)
+                >>> result = client.multi_scenario_matrix_raw.apply(multi_scenario_matrix_raw)
 
         """
         warnings.warn(
             "The .apply method is deprecated and will be removed in v1.0. "
             "Please use the .upsert method on the client instead. This means instead of "
-            "`my_client.multi_scenario_matrix.apply(my_items)` please use `my_client.upsert(my_items)`."
+            "`my_client.multi_scenario_matrix_raw.apply(my_items)` please use `my_client.upsert(my_items)`."
             "The motivation is that all apply methods are the same, and having one apply method per API "
             " class encourages users to create items in small batches, which is inefficient."
             "In addition, .upsert method is more descriptive of what the method does.",
             UserWarning,
             stacklevel=2,
         )
-        return self._apply(multi_scenario_matrix, replace, write_none)
+        return self._apply(multi_scenario_matrix_raw, replace, write_none)
 
     def delete(
         self, external_id: str | SequenceNotStr[str], space: str = DEFAULT_INSTANCE_SPACE
     ) -> dm.InstancesDeleteResult:
-        """Delete one or more multi scenario matrix.
+        """Delete one or more multi scenario matrix raw.
 
         Args:
-            external_id: External id of the multi scenario matrix to delete.
-            space: The space where all the multi scenario matrix are located.
+            external_id: External id of the multi scenario matrix raw to delete.
+            space: The space where all the multi scenario matrix raw are located.
 
         Returns:
             The instance(s), i.e., nodes and edges which has been deleted. Empty list if nothing was deleted.
 
         Examples:
 
-            Delete multi_scenario_matrix by id:
+            Delete multi_scenario_matrix_raw by id:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> client.multi_scenario_matrix.delete("my_multi_scenario_matrix")
+                >>> client.multi_scenario_matrix_raw.delete("my_multi_scenario_matrix_raw")
         """
         warnings.warn(
             "The .delete method is deprecated and will be removed in v1.0. "
             "Please use the .delete method on the client instead. This means instead of "
-            "`my_client.multi_scenario_matrix.delete(my_ids)` please use `my_client.delete(my_ids)`."
+            "`my_client.multi_scenario_matrix_raw.delete(my_ids)` please use `my_client.delete(my_ids)`."
             "The motivation is that all delete methods are the same, and having one delete method per API "
             " class encourages users to delete items in small batches, which is inefficient.",
             UserWarning,
             stacklevel=2,
         )
         return self._delete(external_id, space)
 
     @overload
-    def retrieve(self, external_id: str, space: str = DEFAULT_INSTANCE_SPACE) -> MultiScenarioMatrix | None: ...
+    def retrieve(self, external_id: str, space: str = DEFAULT_INSTANCE_SPACE) -> MultiScenarioMatrixRaw | None: ...
 
     @overload
     def retrieve(
         self, external_id: SequenceNotStr[str], space: str = DEFAULT_INSTANCE_SPACE
-    ) -> MultiScenarioMatrixList: ...
+    ) -> MultiScenarioMatrixRawList: ...
 
     def retrieve(
         self, external_id: str | SequenceNotStr[str], space: str = DEFAULT_INSTANCE_SPACE
-    ) -> MultiScenarioMatrix | MultiScenarioMatrixList | None:
-        """Retrieve one or more multi scenario matrixes by id(s).
+    ) -> MultiScenarioMatrixRaw | MultiScenarioMatrixRawList | None:
+        """Retrieve one or more multi scenario matrix raws by id(s).
 
         Args:
-            external_id: External id or list of external ids of the multi scenario matrixes.
-            space: The space where all the multi scenario matrixes are located.
+            external_id: External id or list of external ids of the multi scenario matrix raws.
+            space: The space where all the multi scenario matrix raws are located.
 
         Returns:
-            The requested multi scenario matrixes.
+            The requested multi scenario matrix raws.
 
         Examples:
 
-            Retrieve multi_scenario_matrix by id:
+            Retrieve multi_scenario_matrix_raw by id:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> multi_scenario_matrix = client.multi_scenario_matrix.retrieve("my_multi_scenario_matrix")
+                >>> multi_scenario_matrix_raw = client.multi_scenario_matrix_raw.retrieve("my_multi_scenario_matrix_raw")
 
         """
         return self._retrieve(
             external_id,
             space,
             retrieve_edges=True,
             edge_api_name_type_direction_view_id_penta=[
@@ -219,99 +221,101 @@
                     self.alerts_edge,
                     "alerts",
                     dm.DirectRelationReference("sp_powerops_types", "calculationIssue"),
                     "outwards",
                     dm.ViewId("sp_powerops_models", "Alert", "1"),
                 ),
                 (
-                    self.scenario_results_edge,
-                    "scenario_results",
-                    dm.DirectRelationReference("sp_powerops_types", "MultiScenarioMatrix.scenarioResults"),
+                    self.shop_results_edge,
+                    "shop_results",
+                    dm.DirectRelationReference("sp_powerops_types", "MultiScenarioMatrix.shopResults"),
                     "outwards",
-                    dm.ViewId("sp_powerops_models", "PriceProdCase", "1"),
+                    dm.ViewId("sp_powerops_models", "SHOPResultPriceProd", "1"),
                 ),
             ],
         )
 
     def search(
         self,
         query: str,
-        properties: MultiScenarioMatrixTextFields | Sequence[MultiScenarioMatrixTextFields] | None = None,
+        properties: MultiScenarioMatrixRawTextFields | Sequence[MultiScenarioMatrixRawTextFields] | None = None,
         resource_cost: str | list[str] | None = None,
         resource_cost_prefix: str | None = None,
         asset_type: str | list[str] | None = None,
         asset_type_prefix: str | None = None,
         asset_id: str | list[str] | None = None,
         asset_id_prefix: str | None = None,
         is_processed: bool | None = None,
         method: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
-    ) -> MultiScenarioMatrixList:
-        """Search multi scenario matrixes
+    ) -> MultiScenarioMatrixRawList:
+        """Search multi scenario matrix raws
 
         Args:
             query: The search query,
             properties: The property to search, if nothing is passed all text fields will be searched.
             resource_cost: The resource cost to filter on.
             resource_cost_prefix: The prefix of the resource cost to filter on.
             asset_type: The asset type to filter on.
             asset_type_prefix: The prefix of the asset type to filter on.
             asset_id: The asset id to filter on.
             asset_id_prefix: The prefix of the asset id to filter on.
             is_processed: The is processed to filter on.
             method: The method to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of multi scenario matrixes to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of multi scenario matrix raws to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
-            Search results multi scenario matrixes matching the query.
+            Search results multi scenario matrix raws matching the query.
 
         Examples:
 
-           Search for 'my_multi_scenario_matrix' in all text properties:
+           Search for 'my_multi_scenario_matrix_raw' in all text properties:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> multi_scenario_matrixes = client.multi_scenario_matrix.search('my_multi_scenario_matrix')
+                >>> multi_scenario_matrix_raws = client.multi_scenario_matrix_raw.search('my_multi_scenario_matrix_raw')
 
         """
-        filter_ = _create_multi_scenario_matrix_filter(
+        filter_ = _create_multi_scenario_matrix_raw_filter(
             self._view_id,
             resource_cost,
             resource_cost_prefix,
             asset_type,
             asset_type_prefix,
             asset_id,
             asset_id_prefix,
             is_processed,
             method,
             external_id_prefix,
             space,
             filter,
         )
-        return self._search(self._view_id, query, _MULTISCENARIOMATRIX_PROPERTIES_BY_FIELD, properties, filter_, limit)
+        return self._search(
+            self._view_id, query, _MULTISCENARIOMATRIXRAW_PROPERTIES_BY_FIELD, properties, filter_, limit
+        )
 
     @overload
     def aggregate(
         self,
         aggregations: (
             Aggregations
             | dm.aggregations.MetricAggregation
             | Sequence[Aggregations]
             | Sequence[dm.aggregations.MetricAggregation]
         ),
-        property: MultiScenarioMatrixFields | Sequence[MultiScenarioMatrixFields] | None = None,
+        property: MultiScenarioMatrixRawFields | Sequence[MultiScenarioMatrixRawFields] | None = None,
         group_by: None = None,
         query: str | None = None,
-        search_properties: MultiScenarioMatrixTextFields | Sequence[MultiScenarioMatrixTextFields] | None = None,
+        search_properties: MultiScenarioMatrixRawTextFields | Sequence[MultiScenarioMatrixRawTextFields] | None = None,
         resource_cost: str | list[str] | None = None,
         resource_cost_prefix: str | None = None,
         asset_type: str | list[str] | None = None,
         asset_type_prefix: str | None = None,
         asset_id: str | list[str] | None = None,
         asset_id_prefix: str | None = None,
         is_processed: bool | None = None,
@@ -327,18 +331,18 @@
         self,
         aggregations: (
             Aggregations
             | dm.aggregations.MetricAggregation
             | Sequence[Aggregations]
             | Sequence[dm.aggregations.MetricAggregation]
         ),
-        property: MultiScenarioMatrixFields | Sequence[MultiScenarioMatrixFields] | None = None,
-        group_by: MultiScenarioMatrixFields | Sequence[MultiScenarioMatrixFields] = None,
+        property: MultiScenarioMatrixRawFields | Sequence[MultiScenarioMatrixRawFields] | None = None,
+        group_by: MultiScenarioMatrixRawFields | Sequence[MultiScenarioMatrixRawFields] = None,
         query: str | None = None,
-        search_properties: MultiScenarioMatrixTextFields | Sequence[MultiScenarioMatrixTextFields] | None = None,
+        search_properties: MultiScenarioMatrixRawTextFields | Sequence[MultiScenarioMatrixRawTextFields] | None = None,
         resource_cost: str | list[str] | None = None,
         resource_cost_prefix: str | None = None,
         asset_type: str | list[str] | None = None,
         asset_type_prefix: str | None = None,
         asset_id: str | list[str] | None = None,
         asset_id_prefix: str | None = None,
         is_processed: bool | None = None,
@@ -353,32 +357,32 @@
         self,
         aggregate: (
             Aggregations
             | dm.aggregations.MetricAggregation
             | Sequence[Aggregations]
             | Sequence[dm.aggregations.MetricAggregation]
         ),
-        property: MultiScenarioMatrixFields | Sequence[MultiScenarioMatrixFields] | None = None,
-        group_by: MultiScenarioMatrixFields | Sequence[MultiScenarioMatrixFields] | None = None,
+        property: MultiScenarioMatrixRawFields | Sequence[MultiScenarioMatrixRawFields] | None = None,
+        group_by: MultiScenarioMatrixRawFields | Sequence[MultiScenarioMatrixRawFields] | None = None,
         query: str | None = None,
-        search_property: MultiScenarioMatrixTextFields | Sequence[MultiScenarioMatrixTextFields] | None = None,
+        search_property: MultiScenarioMatrixRawTextFields | Sequence[MultiScenarioMatrixRawTextFields] | None = None,
         resource_cost: str | list[str] | None = None,
         resource_cost_prefix: str | None = None,
         asset_type: str | list[str] | None = None,
         asset_type_prefix: str | None = None,
         asset_id: str | list[str] | None = None,
         asset_id_prefix: str | None = None,
         is_processed: bool | None = None,
         method: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> list[dm.aggregations.AggregatedNumberedValue] | InstanceAggregationResultList:
-        """Aggregate data across multi scenario matrixes
+        """Aggregate data across multi scenario matrix raws
 
         Args:
             aggregate: The aggregation to perform.
             property: The property to perform aggregation on.
             group_by: The property to group by when doing the aggregation.
             query: The query to search for in the text field.
             search_property: The text field to search in.
@@ -388,31 +392,31 @@
             asset_type_prefix: The prefix of the asset type to filter on.
             asset_id: The asset id to filter on.
             asset_id_prefix: The prefix of the asset id to filter on.
             is_processed: The is processed to filter on.
             method: The method to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of multi scenario matrixes to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of multi scenario matrix raws to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
             Aggregation results.
 
         Examples:
 
-            Count multi scenario matrixes in space `my_space`:
+            Count multi scenario matrix raws in space `my_space`:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> result = client.multi_scenario_matrix.aggregate("count", space="my_space")
+                >>> result = client.multi_scenario_matrix_raw.aggregate("count", space="my_space")
 
         """
 
-        filter_ = _create_multi_scenario_matrix_filter(
+        filter_ = _create_multi_scenario_matrix_raw_filter(
             self._view_id,
             resource_cost,
             resource_cost_prefix,
             asset_type,
             asset_type_prefix,
             asset_id,
             asset_id_prefix,
@@ -421,43 +425,43 @@
             external_id_prefix,
             space,
             filter,
         )
         return self._aggregate(
             self._view_id,
             aggregate,
-            _MULTISCENARIOMATRIX_PROPERTIES_BY_FIELD,
+            _MULTISCENARIOMATRIXRAW_PROPERTIES_BY_FIELD,
             property,
             group_by,
             query,
             search_property,
             limit,
             filter_,
         )
 
     def histogram(
         self,
-        property: MultiScenarioMatrixFields,
+        property: MultiScenarioMatrixRawFields,
         interval: float,
         query: str | None = None,
-        search_property: MultiScenarioMatrixTextFields | Sequence[MultiScenarioMatrixTextFields] | None = None,
+        search_property: MultiScenarioMatrixRawTextFields | Sequence[MultiScenarioMatrixRawTextFields] | None = None,
         resource_cost: str | list[str] | None = None,
         resource_cost_prefix: str | None = None,
         asset_type: str | list[str] | None = None,
         asset_type_prefix: str | None = None,
         asset_id: str | list[str] | None = None,
         asset_id_prefix: str | None = None,
         is_processed: bool | None = None,
         method: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> dm.aggregations.HistogramValue:
-        """Produces histograms for multi scenario matrixes
+        """Produces histograms for multi scenario matrix raws
 
         Args:
             property: The property to use as the value in the histogram.
             interval: The interval to use for the histogram bins.
             query: The query to search for in the text field.
             search_property: The text field to search in.
             resource_cost: The resource cost to filter on.
@@ -466,22 +470,22 @@
             asset_type_prefix: The prefix of the asset type to filter on.
             asset_id: The asset id to filter on.
             asset_id_prefix: The prefix of the asset id to filter on.
             is_processed: The is processed to filter on.
             method: The method to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of multi scenario matrixes to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of multi scenario matrix raws to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
             Bucketed histogram results.
 
         """
-        filter_ = _create_multi_scenario_matrix_filter(
+        filter_ = _create_multi_scenario_matrix_raw_filter(
             self._view_id,
             resource_cost,
             resource_cost_prefix,
             asset_type,
             asset_type_prefix,
             asset_id,
             asset_id_prefix,
@@ -491,15 +495,15 @@
             space,
             filter,
         )
         return self._histogram(
             self._view_id,
             property,
             interval,
-            _MULTISCENARIOMATRIX_PROPERTIES_BY_FIELD,
+            _MULTISCENARIOMATRIXRAW_PROPERTIES_BY_FIELD,
             query,
             search_property,
             limit,
             filter_,
         )
 
     def list(
@@ -513,45 +517,45 @@
         is_processed: bool | None = None,
         method: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
         retrieve_edges: bool = True,
-    ) -> MultiScenarioMatrixList:
-        """List/filter multi scenario matrixes
+    ) -> MultiScenarioMatrixRawList:
+        """List/filter multi scenario matrix raws
 
         Args:
             resource_cost: The resource cost to filter on.
             resource_cost_prefix: The prefix of the resource cost to filter on.
             asset_type: The asset type to filter on.
             asset_type_prefix: The prefix of the asset type to filter on.
             asset_id: The asset id to filter on.
             asset_id_prefix: The prefix of the asset id to filter on.
             is_processed: The is processed to filter on.
             method: The method to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of multi scenario matrixes to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of multi scenario matrix raws to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
-            retrieve_edges: Whether to retrieve `alerts` or `scenario_results` external ids for the multi scenario matrixes. Defaults to True.
+            retrieve_edges: Whether to retrieve `alerts` or `shop_results` external ids for the multi scenario matrix raws. Defaults to True.
 
         Returns:
-            List of requested multi scenario matrixes
+            List of requested multi scenario matrix raws
 
         Examples:
 
-            List multi scenario matrixes and limit to 5:
+            List multi scenario matrix raws and limit to 5:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> multi_scenario_matrixes = client.multi_scenario_matrix.list(limit=5)
+                >>> multi_scenario_matrix_raws = client.multi_scenario_matrix_raw.list(limit=5)
 
         """
-        filter_ = _create_multi_scenario_matrix_filter(
+        filter_ = _create_multi_scenario_matrix_raw_filter(
             self._view_id,
             resource_cost,
             resource_cost_prefix,
             asset_type,
             asset_type_prefix,
             asset_id,
             asset_id_prefix,
@@ -571,15 +575,15 @@
                     self.alerts_edge,
                     "alerts",
                     dm.DirectRelationReference("sp_powerops_types", "calculationIssue"),
                     "outwards",
                     dm.ViewId("sp_powerops_models", "Alert", "1"),
                 ),
                 (
-                    self.scenario_results_edge,
-                    "scenario_results",
-                    dm.DirectRelationReference("sp_powerops_types", "MultiScenarioMatrix.scenarioResults"),
+                    self.shop_results_edge,
+                    "shop_results",
+                    dm.DirectRelationReference("sp_powerops_types", "MultiScenarioMatrix.shopResults"),
                     "outwards",
-                    dm.ViewId("sp_powerops_models", "PriceProdCase", "1"),
+                    dm.ViewId("sp_powerops_models", "SHOPResultPriceProd", "1"),
                 ),
             ],
         )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/multi_scenario_matrix_alerts.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/multi_scenario_matrix_alerts.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/multi_scenario_matrix_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/partial_post_processing_output_query.py`

 * *Files 16% similar despite different names*

```diff
@@ -3,63 +3,65 @@
 import datetime
 from typing import TYPE_CHECKING
 
 from cognite.client import data_modeling as dm, CogniteClient
 
 from cognite.powerops.client._generated.v1.data_classes import (
     DomainModelCore,
-    MultiScenarioMatrix,
-    BidMethodSHOPMultiScenario,
+    PartialPostProcessingOutput,
+    PartialPostProcessingInput,
 )
 from ._core import DEFAULT_QUERY_LIMIT, QueryBuilder, QueryStep, QueryAPI, T_DomainModelList, _create_edge_filter
 
 if TYPE_CHECKING:
     from .alert_query import AlertQueryAPI
-    from .price_prod_case_query import PriceProdCaseQueryAPI
+    from .bid_matrix_query import BidMatrixQueryAPI
 
 
-class MultiScenarioMatrixQueryAPI(QueryAPI[T_DomainModelList]):
+class PartialPostProcessingOutputQueryAPI(QueryAPI[T_DomainModelList]):
     def __init__(
         self,
         client: CogniteClient,
         builder: QueryBuilder[T_DomainModelList],
         view_by_read_class: dict[type[DomainModelCore], dm.ViewId],
         filter_: dm.filters.Filter | None = None,
         limit: int = DEFAULT_QUERY_LIMIT,
     ):
         super().__init__(client, builder, view_by_read_class)
 
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("multi_scenario_matrix"),
+                name=self._builder.next_name("partial_post_processing_output"),
                 expression=dm.query.NodeResultSetExpression(
                     from_=self._builder[-1].name if self._builder else None,
                     filter=filter_,
                 ),
-                select=dm.query.Select([dm.query.SourceSelector(self._view_by_read_class[MultiScenarioMatrix], ["*"])]),
-                result_cls=MultiScenarioMatrix,
+                select=dm.query.Select(
+                    [dm.query.SourceSelector(self._view_by_read_class[PartialPostProcessingOutput], ["*"])]
+                ),
+                result_cls=PartialPostProcessingOutput,
                 max_retrieve_limit=limit,
             )
         )
 
     def alerts(
         self,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_QUERY_LIMIT,
-        retrieve_method: bool = False,
+        retrieve_input_: bool = False,
     ) -> AlertQueryAPI[T_DomainModelList]:
-        """Query along the alert edges of the multi scenario matrix.
+        """Query along the alert edges of the partial post processing output.
 
         Args:
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of alert edges to return. Defaults to 25. Set to -1, float("inf") or None
                 to return all items.
-            retrieve_method: Whether to retrieve the method for each multi scenario matrix or not.
+            retrieve_input_: Whether to retrieve the input for each partial post processing output or not.
 
         Returns:
             AlertQueryAPI: The query API for the alert.
         """
         from .alert_query import AlertQueryAPI
 
         from_ = self._builder[-1].name
@@ -77,89 +79,89 @@
                     from_=from_,
                     direction="outwards",
                 ),
                 select=dm.query.Select(),
                 max_retrieve_limit=limit,
             )
         )
-        if retrieve_method:
-            self._query_append_method(from_)
+        if retrieve_input_:
+            self._query_append_input_(from_)
         return AlertQueryAPI(self._client, self._builder, self._view_by_read_class, None, limit)
 
-    def scenario_results(
+    def partial_matrices(
         self,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_QUERY_LIMIT,
-        retrieve_method: bool = False,
-    ) -> PriceProdCaseQueryAPI[T_DomainModelList]:
-        """Query along the scenario result edges of the multi scenario matrix.
+        retrieve_input_: bool = False,
+    ) -> BidMatrixQueryAPI[T_DomainModelList]:
+        """Query along the partial matrice edges of the partial post processing output.
 
         Args:
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of scenario result edges to return. Defaults to 25. Set to -1, float("inf") or None
+            limit: Maximum number of partial matrice edges to return. Defaults to 25. Set to -1, float("inf") or None
                 to return all items.
-            retrieve_method: Whether to retrieve the method for each multi scenario matrix or not.
+            retrieve_input_: Whether to retrieve the input for each partial post processing output or not.
 
         Returns:
-            PriceProdCaseQueryAPI: The query API for the price prod case.
+            BidMatrixQueryAPI: The query API for the bid matrix.
         """
-        from .price_prod_case_query import PriceProdCaseQueryAPI
+        from .bid_matrix_query import BidMatrixQueryAPI
 
         from_ = self._builder[-1].name
 
         edge_filter = _create_edge_filter(
-            dm.DirectRelationReference("sp_powerops_types", "MultiScenarioMatrix.scenarioResults"),
+            dm.DirectRelationReference("sp_powerops_types", "BidMatrix"),
             external_id_prefix=external_id_prefix,
             space=space,
         )
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("scenario_results"),
+                name=self._builder.next_name("partial_matrices"),
                 expression=dm.query.EdgeResultSetExpression(
                     filter=edge_filter,
                     from_=from_,
                     direction="outwards",
                 ),
                 select=dm.query.Select(),
                 max_retrieve_limit=limit,
             )
         )
-        if retrieve_method:
-            self._query_append_method(from_)
-        return PriceProdCaseQueryAPI(self._client, self._builder, self._view_by_read_class, None, limit)
+        if retrieve_input_:
+            self._query_append_input_(from_)
+        return BidMatrixQueryAPI(self._client, self._builder, self._view_by_read_class, None, limit)
 
     def query(
         self,
-        retrieve_method: bool = False,
+        retrieve_input_: bool = False,
     ) -> T_DomainModelList:
         """Execute query and return the result.
 
         Args:
-            retrieve_method: Whether to retrieve the method for each multi scenario matrix or not.
+            retrieve_input_: Whether to retrieve the input for each partial post processing output or not.
 
         Returns:
             The list of the source nodes of the query.
 
         """
         from_ = self._builder[-1].name
-        if retrieve_method:
-            self._query_append_method(from_)
+        if retrieve_input_:
+            self._query_append_input_(from_)
         return self._query()
 
-    def _query_append_method(self, from_: str) -> None:
-        view_id = self._view_by_read_class[BidMethodSHOPMultiScenario]
+    def _query_append_input_(self, from_: str) -> None:
+        view_id = self._view_by_read_class[PartialPostProcessingInput]
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("method"),
+                name=self._builder.next_name("input_"),
                 expression=dm.query.NodeResultSetExpression(
                     filter=dm.filters.HasData(views=[view_id]),
                     from_=from_,
-                    through=self._view_by_read_class[MultiScenarioMatrix].as_property_ref("method"),
+                    through=self._view_by_read_class[PartialPostProcessingOutput].as_property_ref("input"),
                     direction="outwards",
                 ),
                 select=dm.query.Select([dm.query.SourceSelector(view_id, ["*"])]),
                 max_retrieve_limit=-1,
-                result_cls=BidMethodSHOPMultiScenario,
+                result_cls=PartialPostProcessingInput,
             ),
         )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/multi_scenario_matrix_raw.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_document.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,329 +1,336 @@
 from __future__ import annotations
 
+import datetime
 from collections.abc import Sequence
 from typing import overload
 import warnings
 
 from cognite.client import CogniteClient
 from cognite.client import data_modeling as dm
 from cognite.client.data_classes.data_modeling.instances import InstanceAggregationResultList
 
 from cognite.powerops.client._generated.v1.data_classes._core import DEFAULT_INSTANCE_SPACE
 from cognite.powerops.client._generated.v1.data_classes import (
     DomainModelCore,
     DomainModelWrite,
     ResourcesWriteResult,
-    MultiScenarioMatrixRaw,
-    MultiScenarioMatrixRawWrite,
-    MultiScenarioMatrixRawFields,
-    MultiScenarioMatrixRawList,
-    MultiScenarioMatrixRawWriteList,
-    MultiScenarioMatrixRawTextFields,
+    BidDocument,
+    BidDocumentWrite,
+    BidDocumentFields,
+    BidDocumentList,
+    BidDocumentWriteList,
+    BidDocumentTextFields,
 )
-from cognite.powerops.client._generated.v1.data_classes._multi_scenario_matrix_raw import (
-    _MULTISCENARIOMATRIXRAW_PROPERTIES_BY_FIELD,
-    _create_multi_scenario_matrix_raw_filter,
+from cognite.powerops.client._generated.v1.data_classes._bid_document import (
+    _BIDDOCUMENT_PROPERTIES_BY_FIELD,
+    _create_bid_document_filter,
 )
 from ._core import (
     DEFAULT_LIMIT_READ,
     DEFAULT_QUERY_LIMIT,
     Aggregations,
     NodeAPI,
     SequenceNotStr,
     QueryStep,
     QueryBuilder,
 )
-from .multi_scenario_matrix_raw_alerts import MultiScenarioMatrixRawAlertsAPI
-from .multi_scenario_matrix_raw_shop_results import MultiScenarioMatrixRawShopResultsAPI
-from .multi_scenario_matrix_raw_query import MultiScenarioMatrixRawQueryAPI
+from .bid_document_alerts import BidDocumentAlertsAPI
+from .bid_document_query import BidDocumentQueryAPI
 
 
-class MultiScenarioMatrixRawAPI(
-    NodeAPI[MultiScenarioMatrixRaw, MultiScenarioMatrixRawWrite, MultiScenarioMatrixRawList]
-):
+class BidDocumentAPI(NodeAPI[BidDocument, BidDocumentWrite, BidDocumentList]):
     def __init__(self, client: CogniteClient, view_by_read_class: dict[type[DomainModelCore], dm.ViewId]):
-        view_id = view_by_read_class[MultiScenarioMatrixRaw]
+        view_id = view_by_read_class[BidDocument]
         super().__init__(
             client=client,
             sources=view_id,
-            class_type=MultiScenarioMatrixRaw,
-            class_list=MultiScenarioMatrixRawList,
-            class_write_list=MultiScenarioMatrixRawWriteList,
+            class_type=BidDocument,
+            class_list=BidDocumentList,
+            class_write_list=BidDocumentWriteList,
             view_by_read_class=view_by_read_class,
         )
         self._view_id = view_id
-        self.alerts_edge = MultiScenarioMatrixRawAlertsAPI(client)
-        self.shop_results_edge = MultiScenarioMatrixRawShopResultsAPI(client)
+        self.alerts_edge = BidDocumentAlertsAPI(client)
 
     def __call__(
         self,
-        resource_cost: str | list[str] | None = None,
-        resource_cost_prefix: str | None = None,
-        asset_type: str | list[str] | None = None,
-        asset_type_prefix: str | None = None,
-        asset_id: str | list[str] | None = None,
-        asset_id_prefix: str | None = None,
-        is_processed: bool | None = None,
-        method: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        name: str | list[str] | None = None,
+        name_prefix: str | None = None,
+        process_id: str | list[str] | None = None,
+        process_id_prefix: str | None = None,
+        min_delivery_date: datetime.date | None = None,
+        max_delivery_date: datetime.date | None = None,
+        min_start_calculation: datetime.datetime | None = None,
+        max_start_calculation: datetime.datetime | None = None,
+        min_end_calculation: datetime.datetime | None = None,
+        max_end_calculation: datetime.datetime | None = None,
+        is_complete: bool | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_QUERY_LIMIT,
         filter: dm.Filter | None = None,
-    ) -> MultiScenarioMatrixRawQueryAPI[MultiScenarioMatrixRawList]:
-        """Query starting at multi scenario matrix raws.
+    ) -> BidDocumentQueryAPI[BidDocumentList]:
+        """Query starting at bid documents.
 
         Args:
-            resource_cost: The resource cost to filter on.
-            resource_cost_prefix: The prefix of the resource cost to filter on.
-            asset_type: The asset type to filter on.
-            asset_type_prefix: The prefix of the asset type to filter on.
-            asset_id: The asset id to filter on.
-            asset_id_prefix: The prefix of the asset id to filter on.
-            is_processed: The is processed to filter on.
-            method: The method to filter on.
+            name: The name to filter on.
+            name_prefix: The prefix of the name to filter on.
+            process_id: The process id to filter on.
+            process_id_prefix: The prefix of the process id to filter on.
+            min_delivery_date: The minimum value of the delivery date to filter on.
+            max_delivery_date: The maximum value of the delivery date to filter on.
+            min_start_calculation: The minimum value of the start calculation to filter on.
+            max_start_calculation: The maximum value of the start calculation to filter on.
+            min_end_calculation: The minimum value of the end calculation to filter on.
+            max_end_calculation: The maximum value of the end calculation to filter on.
+            is_complete: The is complete to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of multi scenario matrix raws to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of bid documents to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
-            A query API for multi scenario matrix raws.
+            A query API for bid documents.
 
         """
         has_data = dm.filters.HasData(views=[self._view_id])
-        filter_ = _create_multi_scenario_matrix_raw_filter(
+        filter_ = _create_bid_document_filter(
             self._view_id,
-            resource_cost,
-            resource_cost_prefix,
-            asset_type,
-            asset_type_prefix,
-            asset_id,
-            asset_id_prefix,
-            is_processed,
-            method,
+            name,
+            name_prefix,
+            process_id,
+            process_id_prefix,
+            min_delivery_date,
+            max_delivery_date,
+            min_start_calculation,
+            max_start_calculation,
+            min_end_calculation,
+            max_end_calculation,
+            is_complete,
             external_id_prefix,
             space,
             (filter and dm.filters.And(filter, has_data)) or has_data,
         )
-        builder = QueryBuilder(MultiScenarioMatrixRawList)
-        return MultiScenarioMatrixRawQueryAPI(self._client, builder, self._view_by_read_class, filter_, limit)
+        builder = QueryBuilder(BidDocumentList)
+        return BidDocumentQueryAPI(self._client, builder, self._view_by_read_class, filter_, limit)
 
     def apply(
         self,
-        multi_scenario_matrix_raw: MultiScenarioMatrixRawWrite | Sequence[MultiScenarioMatrixRawWrite],
+        bid_document: BidDocumentWrite | Sequence[BidDocumentWrite],
         replace: bool = False,
         write_none: bool = False,
     ) -> ResourcesWriteResult:
-        """Add or update (upsert) multi scenario matrix raws.
+        """Add or update (upsert) bid documents.
 
-        Note: This method iterates through all nodes and timeseries linked to multi_scenario_matrix_raw and creates them including the edges
-        between the nodes. For example, if any of `alerts` or `shop_results` are set, then these
+        Note: This method iterates through all nodes and timeseries linked to bid_document and creates them including the edges
+        between the nodes. For example, if any of `alerts` are set, then these
         nodes as well as any nodes linked to them, and all the edges linking these nodes will be created.
 
         Args:
-            multi_scenario_matrix_raw: Multi scenario matrix raw or sequence of multi scenario matrix raws to upsert.
+            bid_document: Bid document or sequence of bid documents to upsert.
             replace (bool): How do we behave when a property value exists? Do we replace all matching and existing values with the supplied values (true)?
                 Or should we merge in new values for properties together with the existing values (false)? Note: This setting applies for all nodes or edges specified in the ingestion call.
             write_none (bool): This method, will by default, skip properties that are set to None. However, if you want to set properties to None,
                 you can set this parameter to True. Note this only applies to properties that are nullable.
         Returns:
             Created instance(s), i.e., nodes, edges, and time series.
 
         Examples:
 
-            Create a new multi_scenario_matrix_raw:
+            Create a new bid_document:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
-                >>> from cognite.powerops.client._generated.v1.data_classes import MultiScenarioMatrixRawWrite
+                >>> from cognite.powerops.client._generated.v1.data_classes import BidDocumentWrite
                 >>> client = PowerOpsModelsV1Client()
-                >>> multi_scenario_matrix_raw = MultiScenarioMatrixRawWrite(external_id="my_multi_scenario_matrix_raw", ...)
-                >>> result = client.multi_scenario_matrix_raw.apply(multi_scenario_matrix_raw)
+                >>> bid_document = BidDocumentWrite(external_id="my_bid_document", ...)
+                >>> result = client.bid_document.apply(bid_document)
 
         """
         warnings.warn(
             "The .apply method is deprecated and will be removed in v1.0. "
             "Please use the .upsert method on the client instead. This means instead of "
-            "`my_client.multi_scenario_matrix_raw.apply(my_items)` please use `my_client.upsert(my_items)`."
+            "`my_client.bid_document.apply(my_items)` please use `my_client.upsert(my_items)`."
             "The motivation is that all apply methods are the same, and having one apply method per API "
             " class encourages users to create items in small batches, which is inefficient."
             "In addition, .upsert method is more descriptive of what the method does.",
             UserWarning,
             stacklevel=2,
         )
-        return self._apply(multi_scenario_matrix_raw, replace, write_none)
+        return self._apply(bid_document, replace, write_none)
 
     def delete(
         self, external_id: str | SequenceNotStr[str], space: str = DEFAULT_INSTANCE_SPACE
     ) -> dm.InstancesDeleteResult:
-        """Delete one or more multi scenario matrix raw.
+        """Delete one or more bid document.
 
         Args:
-            external_id: External id of the multi scenario matrix raw to delete.
-            space: The space where all the multi scenario matrix raw are located.
+            external_id: External id of the bid document to delete.
+            space: The space where all the bid document are located.
 
         Returns:
             The instance(s), i.e., nodes and edges which has been deleted. Empty list if nothing was deleted.
 
         Examples:
 
-            Delete multi_scenario_matrix_raw by id:
+            Delete bid_document by id:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> client.multi_scenario_matrix_raw.delete("my_multi_scenario_matrix_raw")
+                >>> client.bid_document.delete("my_bid_document")
         """
         warnings.warn(
             "The .delete method is deprecated and will be removed in v1.0. "
             "Please use the .delete method on the client instead. This means instead of "
-            "`my_client.multi_scenario_matrix_raw.delete(my_ids)` please use `my_client.delete(my_ids)`."
+            "`my_client.bid_document.delete(my_ids)` please use `my_client.delete(my_ids)`."
             "The motivation is that all delete methods are the same, and having one delete method per API "
             " class encourages users to delete items in small batches, which is inefficient.",
             UserWarning,
             stacklevel=2,
         )
         return self._delete(external_id, space)
 
     @overload
-    def retrieve(self, external_id: str, space: str = DEFAULT_INSTANCE_SPACE) -> MultiScenarioMatrixRaw | None: ...
+    def retrieve(self, external_id: str, space: str = DEFAULT_INSTANCE_SPACE) -> BidDocument | None: ...
 
     @overload
-    def retrieve(
-        self, external_id: SequenceNotStr[str], space: str = DEFAULT_INSTANCE_SPACE
-    ) -> MultiScenarioMatrixRawList: ...
+    def retrieve(self, external_id: SequenceNotStr[str], space: str = DEFAULT_INSTANCE_SPACE) -> BidDocumentList: ...
 
     def retrieve(
         self, external_id: str | SequenceNotStr[str], space: str = DEFAULT_INSTANCE_SPACE
-    ) -> MultiScenarioMatrixRaw | MultiScenarioMatrixRawList | None:
-        """Retrieve one or more multi scenario matrix raws by id(s).
+    ) -> BidDocument | BidDocumentList | None:
+        """Retrieve one or more bid documents by id(s).
 
         Args:
-            external_id: External id or list of external ids of the multi scenario matrix raws.
-            space: The space where all the multi scenario matrix raws are located.
+            external_id: External id or list of external ids of the bid documents.
+            space: The space where all the bid documents are located.
 
         Returns:
-            The requested multi scenario matrix raws.
+            The requested bid documents.
 
         Examples:
 
-            Retrieve multi_scenario_matrix_raw by id:
+            Retrieve bid_document by id:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> multi_scenario_matrix_raw = client.multi_scenario_matrix_raw.retrieve("my_multi_scenario_matrix_raw")
+                >>> bid_document = client.bid_document.retrieve("my_bid_document")
 
         """
         return self._retrieve(
             external_id,
             space,
             retrieve_edges=True,
             edge_api_name_type_direction_view_id_penta=[
                 (
                     self.alerts_edge,
                     "alerts",
-                    dm.DirectRelationReference("sp_powerops_types", "calculationIssue"),
-                    "outwards",
-                    dm.ViewId("sp_powerops_models", "Alert", "1"),
-                ),
-                (
-                    self.shop_results_edge,
-                    "shop_results",
-                    dm.DirectRelationReference("sp_powerops_types", "MultiScenarioMatrix.shopResults"),
+                    dm.DirectRelationReference("sp_powerops_types_temp", "calculationIssue"),
                     "outwards",
-                    dm.ViewId("sp_powerops_models", "SHOPResultPriceProd", "1"),
+                    dm.ViewId("sp_powerops_models_temp", "Alert", "1"),
                 ),
             ],
         )
 
     def search(
         self,
         query: str,
-        properties: MultiScenarioMatrixRawTextFields | Sequence[MultiScenarioMatrixRawTextFields] | None = None,
-        resource_cost: str | list[str] | None = None,
-        resource_cost_prefix: str | None = None,
-        asset_type: str | list[str] | None = None,
-        asset_type_prefix: str | None = None,
-        asset_id: str | list[str] | None = None,
-        asset_id_prefix: str | None = None,
-        is_processed: bool | None = None,
-        method: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        properties: BidDocumentTextFields | Sequence[BidDocumentTextFields] | None = None,
+        name: str | list[str] | None = None,
+        name_prefix: str | None = None,
+        process_id: str | list[str] | None = None,
+        process_id_prefix: str | None = None,
+        min_delivery_date: datetime.date | None = None,
+        max_delivery_date: datetime.date | None = None,
+        min_start_calculation: datetime.datetime | None = None,
+        max_start_calculation: datetime.datetime | None = None,
+        min_end_calculation: datetime.datetime | None = None,
+        max_end_calculation: datetime.datetime | None = None,
+        is_complete: bool | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
-    ) -> MultiScenarioMatrixRawList:
-        """Search multi scenario matrix raws
+    ) -> BidDocumentList:
+        """Search bid documents
 
         Args:
             query: The search query,
             properties: The property to search, if nothing is passed all text fields will be searched.
-            resource_cost: The resource cost to filter on.
-            resource_cost_prefix: The prefix of the resource cost to filter on.
-            asset_type: The asset type to filter on.
-            asset_type_prefix: The prefix of the asset type to filter on.
-            asset_id: The asset id to filter on.
-            asset_id_prefix: The prefix of the asset id to filter on.
-            is_processed: The is processed to filter on.
-            method: The method to filter on.
+            name: The name to filter on.
+            name_prefix: The prefix of the name to filter on.
+            process_id: The process id to filter on.
+            process_id_prefix: The prefix of the process id to filter on.
+            min_delivery_date: The minimum value of the delivery date to filter on.
+            max_delivery_date: The maximum value of the delivery date to filter on.
+            min_start_calculation: The minimum value of the start calculation to filter on.
+            max_start_calculation: The maximum value of the start calculation to filter on.
+            min_end_calculation: The minimum value of the end calculation to filter on.
+            max_end_calculation: The maximum value of the end calculation to filter on.
+            is_complete: The is complete to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of multi scenario matrix raws to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of bid documents to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
-            Search results multi scenario matrix raws matching the query.
+            Search results bid documents matching the query.
 
         Examples:
 
-           Search for 'my_multi_scenario_matrix_raw' in all text properties:
+           Search for 'my_bid_document' in all text properties:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> multi_scenario_matrix_raws = client.multi_scenario_matrix_raw.search('my_multi_scenario_matrix_raw')
+                >>> bid_documents = client.bid_document.search('my_bid_document')
 
         """
-        filter_ = _create_multi_scenario_matrix_raw_filter(
+        filter_ = _create_bid_document_filter(
             self._view_id,
-            resource_cost,
-            resource_cost_prefix,
-            asset_type,
-            asset_type_prefix,
-            asset_id,
-            asset_id_prefix,
-            is_processed,
-            method,
+            name,
+            name_prefix,
+            process_id,
+            process_id_prefix,
+            min_delivery_date,
+            max_delivery_date,
+            min_start_calculation,
+            max_start_calculation,
+            min_end_calculation,
+            max_end_calculation,
+            is_complete,
             external_id_prefix,
             space,
             filter,
         )
-        return self._search(
-            self._view_id, query, _MULTISCENARIOMATRIXRAW_PROPERTIES_BY_FIELD, properties, filter_, limit
-        )
+        return self._search(self._view_id, query, _BIDDOCUMENT_PROPERTIES_BY_FIELD, properties, filter_, limit)
 
     @overload
     def aggregate(
         self,
         aggregations: (
             Aggregations
             | dm.aggregations.MetricAggregation
             | Sequence[Aggregations]
             | Sequence[dm.aggregations.MetricAggregation]
         ),
-        property: MultiScenarioMatrixRawFields | Sequence[MultiScenarioMatrixRawFields] | None = None,
+        property: BidDocumentFields | Sequence[BidDocumentFields] | None = None,
         group_by: None = None,
         query: str | None = None,
-        search_properties: MultiScenarioMatrixRawTextFields | Sequence[MultiScenarioMatrixRawTextFields] | None = None,
-        resource_cost: str | list[str] | None = None,
-        resource_cost_prefix: str | None = None,
-        asset_type: str | list[str] | None = None,
-        asset_type_prefix: str | None = None,
-        asset_id: str | list[str] | None = None,
-        asset_id_prefix: str | None = None,
-        is_processed: bool | None = None,
-        method: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        search_properties: BidDocumentTextFields | Sequence[BidDocumentTextFields] | None = None,
+        name: str | list[str] | None = None,
+        name_prefix: str | None = None,
+        process_id: str | list[str] | None = None,
+        process_id_prefix: str | None = None,
+        min_delivery_date: datetime.date | None = None,
+        max_delivery_date: datetime.date | None = None,
+        min_start_calculation: datetime.datetime | None = None,
+        max_start_calculation: datetime.datetime | None = None,
+        min_end_calculation: datetime.datetime | None = None,
+        max_end_calculation: datetime.datetime | None = None,
+        is_complete: bool | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> list[dm.aggregations.AggregatedNumberedValue]: ...
 
     @overload
@@ -331,259 +338,282 @@
         self,
         aggregations: (
             Aggregations
             | dm.aggregations.MetricAggregation
             | Sequence[Aggregations]
             | Sequence[dm.aggregations.MetricAggregation]
         ),
-        property: MultiScenarioMatrixRawFields | Sequence[MultiScenarioMatrixRawFields] | None = None,
-        group_by: MultiScenarioMatrixRawFields | Sequence[MultiScenarioMatrixRawFields] = None,
+        property: BidDocumentFields | Sequence[BidDocumentFields] | None = None,
+        group_by: BidDocumentFields | Sequence[BidDocumentFields] = None,
         query: str | None = None,
-        search_properties: MultiScenarioMatrixRawTextFields | Sequence[MultiScenarioMatrixRawTextFields] | None = None,
-        resource_cost: str | list[str] | None = None,
-        resource_cost_prefix: str | None = None,
-        asset_type: str | list[str] | None = None,
-        asset_type_prefix: str | None = None,
-        asset_id: str | list[str] | None = None,
-        asset_id_prefix: str | None = None,
-        is_processed: bool | None = None,
-        method: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        search_properties: BidDocumentTextFields | Sequence[BidDocumentTextFields] | None = None,
+        name: str | list[str] | None = None,
+        name_prefix: str | None = None,
+        process_id: str | list[str] | None = None,
+        process_id_prefix: str | None = None,
+        min_delivery_date: datetime.date | None = None,
+        max_delivery_date: datetime.date | None = None,
+        min_start_calculation: datetime.datetime | None = None,
+        max_start_calculation: datetime.datetime | None = None,
+        min_end_calculation: datetime.datetime | None = None,
+        max_end_calculation: datetime.datetime | None = None,
+        is_complete: bool | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> InstanceAggregationResultList: ...
 
     def aggregate(
         self,
         aggregate: (
             Aggregations
             | dm.aggregations.MetricAggregation
             | Sequence[Aggregations]
             | Sequence[dm.aggregations.MetricAggregation]
         ),
-        property: MultiScenarioMatrixRawFields | Sequence[MultiScenarioMatrixRawFields] | None = None,
-        group_by: MultiScenarioMatrixRawFields | Sequence[MultiScenarioMatrixRawFields] | None = None,
+        property: BidDocumentFields | Sequence[BidDocumentFields] | None = None,
+        group_by: BidDocumentFields | Sequence[BidDocumentFields] | None = None,
         query: str | None = None,
-        search_property: MultiScenarioMatrixRawTextFields | Sequence[MultiScenarioMatrixRawTextFields] | None = None,
-        resource_cost: str | list[str] | None = None,
-        resource_cost_prefix: str | None = None,
-        asset_type: str | list[str] | None = None,
-        asset_type_prefix: str | None = None,
-        asset_id: str | list[str] | None = None,
-        asset_id_prefix: str | None = None,
-        is_processed: bool | None = None,
-        method: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        search_property: BidDocumentTextFields | Sequence[BidDocumentTextFields] | None = None,
+        name: str | list[str] | None = None,
+        name_prefix: str | None = None,
+        process_id: str | list[str] | None = None,
+        process_id_prefix: str | None = None,
+        min_delivery_date: datetime.date | None = None,
+        max_delivery_date: datetime.date | None = None,
+        min_start_calculation: datetime.datetime | None = None,
+        max_start_calculation: datetime.datetime | None = None,
+        min_end_calculation: datetime.datetime | None = None,
+        max_end_calculation: datetime.datetime | None = None,
+        is_complete: bool | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> list[dm.aggregations.AggregatedNumberedValue] | InstanceAggregationResultList:
-        """Aggregate data across multi scenario matrix raws
+        """Aggregate data across bid documents
 
         Args:
             aggregate: The aggregation to perform.
             property: The property to perform aggregation on.
             group_by: The property to group by when doing the aggregation.
             query: The query to search for in the text field.
             search_property: The text field to search in.
-            resource_cost: The resource cost to filter on.
-            resource_cost_prefix: The prefix of the resource cost to filter on.
-            asset_type: The asset type to filter on.
-            asset_type_prefix: The prefix of the asset type to filter on.
-            asset_id: The asset id to filter on.
-            asset_id_prefix: The prefix of the asset id to filter on.
-            is_processed: The is processed to filter on.
-            method: The method to filter on.
+            name: The name to filter on.
+            name_prefix: The prefix of the name to filter on.
+            process_id: The process id to filter on.
+            process_id_prefix: The prefix of the process id to filter on.
+            min_delivery_date: The minimum value of the delivery date to filter on.
+            max_delivery_date: The maximum value of the delivery date to filter on.
+            min_start_calculation: The minimum value of the start calculation to filter on.
+            max_start_calculation: The maximum value of the start calculation to filter on.
+            min_end_calculation: The minimum value of the end calculation to filter on.
+            max_end_calculation: The maximum value of the end calculation to filter on.
+            is_complete: The is complete to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of multi scenario matrix raws to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of bid documents to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
             Aggregation results.
 
         Examples:
 
-            Count multi scenario matrix raws in space `my_space`:
+            Count bid documents in space `my_space`:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> result = client.multi_scenario_matrix_raw.aggregate("count", space="my_space")
+                >>> result = client.bid_document.aggregate("count", space="my_space")
 
         """
 
-        filter_ = _create_multi_scenario_matrix_raw_filter(
+        filter_ = _create_bid_document_filter(
             self._view_id,
-            resource_cost,
-            resource_cost_prefix,
-            asset_type,
-            asset_type_prefix,
-            asset_id,
-            asset_id_prefix,
-            is_processed,
-            method,
+            name,
+            name_prefix,
+            process_id,
+            process_id_prefix,
+            min_delivery_date,
+            max_delivery_date,
+            min_start_calculation,
+            max_start_calculation,
+            min_end_calculation,
+            max_end_calculation,
+            is_complete,
             external_id_prefix,
             space,
             filter,
         )
         return self._aggregate(
             self._view_id,
             aggregate,
-            _MULTISCENARIOMATRIXRAW_PROPERTIES_BY_FIELD,
+            _BIDDOCUMENT_PROPERTIES_BY_FIELD,
             property,
             group_by,
             query,
             search_property,
             limit,
             filter_,
         )
 
     def histogram(
         self,
-        property: MultiScenarioMatrixRawFields,
+        property: BidDocumentFields,
         interval: float,
         query: str | None = None,
-        search_property: MultiScenarioMatrixRawTextFields | Sequence[MultiScenarioMatrixRawTextFields] | None = None,
-        resource_cost: str | list[str] | None = None,
-        resource_cost_prefix: str | None = None,
-        asset_type: str | list[str] | None = None,
-        asset_type_prefix: str | None = None,
-        asset_id: str | list[str] | None = None,
-        asset_id_prefix: str | None = None,
-        is_processed: bool | None = None,
-        method: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        search_property: BidDocumentTextFields | Sequence[BidDocumentTextFields] | None = None,
+        name: str | list[str] | None = None,
+        name_prefix: str | None = None,
+        process_id: str | list[str] | None = None,
+        process_id_prefix: str | None = None,
+        min_delivery_date: datetime.date | None = None,
+        max_delivery_date: datetime.date | None = None,
+        min_start_calculation: datetime.datetime | None = None,
+        max_start_calculation: datetime.datetime | None = None,
+        min_end_calculation: datetime.datetime | None = None,
+        max_end_calculation: datetime.datetime | None = None,
+        is_complete: bool | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> dm.aggregations.HistogramValue:
-        """Produces histograms for multi scenario matrix raws
+        """Produces histograms for bid documents
 
         Args:
             property: The property to use as the value in the histogram.
             interval: The interval to use for the histogram bins.
             query: The query to search for in the text field.
             search_property: The text field to search in.
-            resource_cost: The resource cost to filter on.
-            resource_cost_prefix: The prefix of the resource cost to filter on.
-            asset_type: The asset type to filter on.
-            asset_type_prefix: The prefix of the asset type to filter on.
-            asset_id: The asset id to filter on.
-            asset_id_prefix: The prefix of the asset id to filter on.
-            is_processed: The is processed to filter on.
-            method: The method to filter on.
+            name: The name to filter on.
+            name_prefix: The prefix of the name to filter on.
+            process_id: The process id to filter on.
+            process_id_prefix: The prefix of the process id to filter on.
+            min_delivery_date: The minimum value of the delivery date to filter on.
+            max_delivery_date: The maximum value of the delivery date to filter on.
+            min_start_calculation: The minimum value of the start calculation to filter on.
+            max_start_calculation: The maximum value of the start calculation to filter on.
+            min_end_calculation: The minimum value of the end calculation to filter on.
+            max_end_calculation: The maximum value of the end calculation to filter on.
+            is_complete: The is complete to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of multi scenario matrix raws to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of bid documents to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
             Bucketed histogram results.
 
         """
-        filter_ = _create_multi_scenario_matrix_raw_filter(
+        filter_ = _create_bid_document_filter(
             self._view_id,
-            resource_cost,
-            resource_cost_prefix,
-            asset_type,
-            asset_type_prefix,
-            asset_id,
-            asset_id_prefix,
-            is_processed,
-            method,
+            name,
+            name_prefix,
+            process_id,
+            process_id_prefix,
+            min_delivery_date,
+            max_delivery_date,
+            min_start_calculation,
+            max_start_calculation,
+            min_end_calculation,
+            max_end_calculation,
+            is_complete,
             external_id_prefix,
             space,
             filter,
         )
         return self._histogram(
             self._view_id,
             property,
             interval,
-            _MULTISCENARIOMATRIXRAW_PROPERTIES_BY_FIELD,
+            _BIDDOCUMENT_PROPERTIES_BY_FIELD,
             query,
             search_property,
             limit,
             filter_,
         )
 
     def list(
         self,
-        resource_cost: str | list[str] | None = None,
-        resource_cost_prefix: str | None = None,
-        asset_type: str | list[str] | None = None,
-        asset_type_prefix: str | None = None,
-        asset_id: str | list[str] | None = None,
-        asset_id_prefix: str | None = None,
-        is_processed: bool | None = None,
-        method: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        name: str | list[str] | None = None,
+        name_prefix: str | None = None,
+        process_id: str | list[str] | None = None,
+        process_id_prefix: str | None = None,
+        min_delivery_date: datetime.date | None = None,
+        max_delivery_date: datetime.date | None = None,
+        min_start_calculation: datetime.datetime | None = None,
+        max_start_calculation: datetime.datetime | None = None,
+        min_end_calculation: datetime.datetime | None = None,
+        max_end_calculation: datetime.datetime | None = None,
+        is_complete: bool | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
         retrieve_edges: bool = True,
-    ) -> MultiScenarioMatrixRawList:
-        """List/filter multi scenario matrix raws
+    ) -> BidDocumentList:
+        """List/filter bid documents
 
         Args:
-            resource_cost: The resource cost to filter on.
-            resource_cost_prefix: The prefix of the resource cost to filter on.
-            asset_type: The asset type to filter on.
-            asset_type_prefix: The prefix of the asset type to filter on.
-            asset_id: The asset id to filter on.
-            asset_id_prefix: The prefix of the asset id to filter on.
-            is_processed: The is processed to filter on.
-            method: The method to filter on.
+            name: The name to filter on.
+            name_prefix: The prefix of the name to filter on.
+            process_id: The process id to filter on.
+            process_id_prefix: The prefix of the process id to filter on.
+            min_delivery_date: The minimum value of the delivery date to filter on.
+            max_delivery_date: The maximum value of the delivery date to filter on.
+            min_start_calculation: The minimum value of the start calculation to filter on.
+            max_start_calculation: The maximum value of the start calculation to filter on.
+            min_end_calculation: The minimum value of the end calculation to filter on.
+            max_end_calculation: The maximum value of the end calculation to filter on.
+            is_complete: The is complete to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of multi scenario matrix raws to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of bid documents to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
-            retrieve_edges: Whether to retrieve `alerts` or `shop_results` external ids for the multi scenario matrix raws. Defaults to True.
+            retrieve_edges: Whether to retrieve `alerts` external ids for the bid documents. Defaults to True.
 
         Returns:
-            List of requested multi scenario matrix raws
+            List of requested bid documents
 
         Examples:
 
-            List multi scenario matrix raws and limit to 5:
+            List bid documents and limit to 5:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> multi_scenario_matrix_raws = client.multi_scenario_matrix_raw.list(limit=5)
+                >>> bid_documents = client.bid_document.list(limit=5)
 
         """
-        filter_ = _create_multi_scenario_matrix_raw_filter(
+        filter_ = _create_bid_document_filter(
             self._view_id,
-            resource_cost,
-            resource_cost_prefix,
-            asset_type,
-            asset_type_prefix,
-            asset_id,
-            asset_id_prefix,
-            is_processed,
-            method,
+            name,
+            name_prefix,
+            process_id,
+            process_id_prefix,
+            min_delivery_date,
+            max_delivery_date,
+            min_start_calculation,
+            max_start_calculation,
+            min_end_calculation,
+            max_end_calculation,
+            is_complete,
             external_id_prefix,
             space,
             filter,
         )
 
         return self._list(
             limit=limit,
             filter=filter_,
             retrieve_edges=retrieve_edges,
             edge_api_name_type_direction_view_id_penta=[
                 (
                     self.alerts_edge,
                     "alerts",
-                    dm.DirectRelationReference("sp_powerops_types", "calculationIssue"),
-                    "outwards",
-                    dm.ViewId("sp_powerops_models", "Alert", "1"),
-                ),
-                (
-                    self.shop_results_edge,
-                    "shop_results",
-                    dm.DirectRelationReference("sp_powerops_types", "MultiScenarioMatrix.shopResults"),
+                    dm.DirectRelationReference("sp_powerops_types_temp", "calculationIssue"),
                     "outwards",
-                    dm.ViewId("sp_powerops_models", "SHOPResultPriceProd", "1"),
+                    dm.ViewId("sp_powerops_models_temp", "Alert", "1"),
                 ),
             ],
         )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/multi_scenario_matrix_raw_alerts.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/multi_scenario_matrix_raw_alerts.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/multi_scenario_matrix_raw_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/scenario_query.py`

 * *Files 16% similar despite different names*

```diff
@@ -3,165 +3,182 @@
 import datetime
 from typing import TYPE_CHECKING
 
 from cognite.client import data_modeling as dm, CogniteClient
 
 from cognite.powerops.client._generated.v1.data_classes import (
     DomainModelCore,
-    MultiScenarioMatrixRaw,
-    BidMethodSHOPMultiScenario,
+    Scenario,
+    ModelTemplate,
+    Commands,
+)
+from cognite.powerops.client._generated.v1.data_classes._mapping import (
+    Mapping,
+    _create_mapping_filter,
 )
 from ._core import DEFAULT_QUERY_LIMIT, QueryBuilder, QueryStep, QueryAPI, T_DomainModelList, _create_edge_filter
 
 if TYPE_CHECKING:
-    from .alert_query import AlertQueryAPI
-    from .shop_result_price_prod_query import SHOPResultPriceProdQueryAPI
+    from .mapping_query import MappingQueryAPI
 
 
-class MultiScenarioMatrixRawQueryAPI(QueryAPI[T_DomainModelList]):
+class ScenarioQueryAPI(QueryAPI[T_DomainModelList]):
     def __init__(
         self,
         client: CogniteClient,
         builder: QueryBuilder[T_DomainModelList],
         view_by_read_class: dict[type[DomainModelCore], dm.ViewId],
         filter_: dm.filters.Filter | None = None,
         limit: int = DEFAULT_QUERY_LIMIT,
     ):
         super().__init__(client, builder, view_by_read_class)
 
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("multi_scenario_matrix_raw"),
+                name=self._builder.next_name("scenario"),
                 expression=dm.query.NodeResultSetExpression(
                     from_=self._builder[-1].name if self._builder else None,
                     filter=filter_,
                 ),
-                select=dm.query.Select(
-                    [dm.query.SourceSelector(self._view_by_read_class[MultiScenarioMatrixRaw], ["*"])]
-                ),
-                result_cls=MultiScenarioMatrixRaw,
+                select=dm.query.Select([dm.query.SourceSelector(self._view_by_read_class[Scenario], ["*"])]),
+                result_cls=Scenario,
                 max_retrieve_limit=limit,
             )
         )
 
-    def alerts(
+    def mappings_override(
         self,
+        shop_path: str | list[str] | None = None,
+        shop_path_prefix: str | None = None,
+        retrieve: str | list[str] | None = None,
+        retrieve_prefix: str | None = None,
+        aggregation: str | list[str] | None = None,
+        aggregation_prefix: str | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
+        external_id_prefix_edge: str | None = None,
+        space_edge: str | list[str] | None = None,
+        filter: dm.Filter | None = None,
         limit: int | None = DEFAULT_QUERY_LIMIT,
-        retrieve_method: bool = False,
-    ) -> AlertQueryAPI[T_DomainModelList]:
-        """Query along the alert edges of the multi scenario matrix raw.
+        retrieve_model_template: bool = False,
+        retrieve_commands: bool = False,
+    ) -> MappingQueryAPI[T_DomainModelList]:
+        """Query along the mappings override edges of the scenario.
 
         Args:
+            shop_path: The shop path to filter on.
+            shop_path_prefix: The prefix of the shop path to filter on.
+            retrieve: The retrieve to filter on.
+            retrieve_prefix: The prefix of the retrieve to filter on.
+            aggregation: The aggregation to filter on.
+            aggregation_prefix: The prefix of the aggregation to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of alert edges to return. Defaults to 25. Set to -1, float("inf") or None
+            external_id_prefix_edge: The prefix of the external ID to filter on.
+            space_edge: The space to filter on.
+            filter: (Advanced) Filter applied to node. If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
+            limit: Maximum number of mappings override edges to return. Defaults to 3. Set to -1, float("inf") or None
                 to return all items.
-            retrieve_method: Whether to retrieve the method for each multi scenario matrix raw or not.
+            retrieve_model_template: Whether to retrieve the model template for each scenario or not.
+            retrieve_commands: Whether to retrieve the command for each scenario or not.
 
         Returns:
-            AlertQueryAPI: The query API for the alert.
+            MappingQueryAPI: The query API for the mapping.
         """
-        from .alert_query import AlertQueryAPI
+        from .mapping_query import MappingQueryAPI
 
         from_ = self._builder[-1].name
-
         edge_filter = _create_edge_filter(
-            dm.DirectRelationReference("sp_powerops_types", "calculationIssue"),
-            external_id_prefix=external_id_prefix,
-            space=space,
+            dm.DirectRelationReference("sp_powerops_types_temp", "Mapping"),
+            external_id_prefix=external_id_prefix_edge,
+            space=space_edge,
         )
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("alerts"),
+                name=self._builder.next_name("mappings_override"),
                 expression=dm.query.EdgeResultSetExpression(
                     filter=edge_filter,
                     from_=from_,
                     direction="outwards",
                 ),
                 select=dm.query.Select(),
                 max_retrieve_limit=limit,
             )
         )
-        if retrieve_method:
-            self._query_append_method(from_)
-        return AlertQueryAPI(self._client, self._builder, self._view_by_read_class, None, limit)
-
-    def shop_results(
-        self,
-        external_id_prefix: str | None = None,
-        space: str | list[str] | None = None,
-        limit: int | None = DEFAULT_QUERY_LIMIT,
-        retrieve_method: bool = False,
-    ) -> SHOPResultPriceProdQueryAPI[T_DomainModelList]:
-        """Query along the shop result edges of the multi scenario matrix raw.
-
-        Args:
-            external_id_prefix: The prefix of the external ID to filter on.
-            space: The space to filter on.
-            limit: Maximum number of shop result edges to return. Defaults to 25. Set to -1, float("inf") or None
-                to return all items.
-            retrieve_method: Whether to retrieve the method for each multi scenario matrix raw or not.
-
-        Returns:
-            SHOPResultPriceProdQueryAPI: The query API for the shop result price prod.
-        """
-        from .shop_result_price_prod_query import SHOPResultPriceProdQueryAPI
 
-        from_ = self._builder[-1].name
-
-        edge_filter = _create_edge_filter(
-            dm.DirectRelationReference("sp_powerops_types", "MultiScenarioMatrix.shopResults"),
-            external_id_prefix=external_id_prefix,
-            space=space,
-        )
-        self._builder.append(
-            QueryStep(
-                name=self._builder.next_name("shop_results"),
-                expression=dm.query.EdgeResultSetExpression(
-                    filter=edge_filter,
-                    from_=from_,
-                    direction="outwards",
-                ),
-                select=dm.query.Select(),
-                max_retrieve_limit=limit,
-            )
-        )
-        if retrieve_method:
-            self._query_append_method(from_)
-        return SHOPResultPriceProdQueryAPI(self._client, self._builder, self._view_by_read_class, None, limit)
+        view_id = self._view_by_read_class[Mapping]
+        has_data = dm.filters.HasData(views=[view_id])
+        node_filer = _create_mapping_filter(
+            view_id,
+            shop_path,
+            shop_path_prefix,
+            retrieve,
+            retrieve_prefix,
+            aggregation,
+            aggregation_prefix,
+            external_id_prefix,
+            space,
+            (filter and dm.filters.And(filter, has_data)) or has_data,
+        )
+        if retrieve_model_template:
+            self._query_append_model_template(from_)
+        if retrieve_commands:
+            self._query_append_commands(from_)
+        return MappingQueryAPI(self._client, self._builder, self._view_by_read_class, node_filer, limit)
 
     def query(
         self,
-        retrieve_method: bool = False,
+        retrieve_model_template: bool = False,
+        retrieve_commands: bool = False,
     ) -> T_DomainModelList:
         """Execute query and return the result.
 
         Args:
-            retrieve_method: Whether to retrieve the method for each multi scenario matrix raw or not.
+            retrieve_model_template: Whether to retrieve the model template for each scenario or not.
+            retrieve_commands: Whether to retrieve the command for each scenario or not.
 
         Returns:
             The list of the source nodes of the query.
 
         """
         from_ = self._builder[-1].name
-        if retrieve_method:
-            self._query_append_method(from_)
+        if retrieve_model_template:
+            self._query_append_model_template(from_)
+        if retrieve_commands:
+            self._query_append_commands(from_)
         return self._query()
 
-    def _query_append_method(self, from_: str) -> None:
-        view_id = self._view_by_read_class[BidMethodSHOPMultiScenario]
+    def _query_append_model_template(self, from_: str) -> None:
+        view_id = self._view_by_read_class[ModelTemplate]
+        self._builder.append(
+            QueryStep(
+                name=self._builder.next_name("model_template"),
+                expression=dm.query.NodeResultSetExpression(
+                    filter=dm.filters.HasData(views=[view_id]),
+                    from_=from_,
+                    through=self._view_by_read_class[Scenario].as_property_ref("modelTemplate"),
+                    direction="outwards",
+                ),
+                select=dm.query.Select([dm.query.SourceSelector(view_id, ["*"])]),
+                max_retrieve_limit=-1,
+                result_cls=ModelTemplate,
+                is_single_direct_relation=True,
+            ),
+        )
+
+    def _query_append_commands(self, from_: str) -> None:
+        view_id = self._view_by_read_class[Commands]
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("method"),
+                name=self._builder.next_name("commands"),
                 expression=dm.query.NodeResultSetExpression(
                     filter=dm.filters.HasData(views=[view_id]),
                     from_=from_,
-                    through=self._view_by_read_class[MultiScenarioMatrixRaw].as_property_ref("method"),
+                    through=self._view_by_read_class[Scenario].as_property_ref("commands"),
                     direction="outwards",
                 ),
                 select=dm.query.Select([dm.query.SourceSelector(view_id, ["*"])]),
                 max_retrieve_limit=-1,
-                result_cls=BidMethodSHOPMultiScenario,
+                result_cls=Commands,
+                is_single_direct_relation=True,
             ),
         )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/multi_scenario_matrix_raw_shop_results.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/multi_scenario_matrix_raw_shop_results.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/multi_scenario_matrix_scenario_results.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/multi_scenario_matrix_scenario_results.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/multi_scenario_matrix_shop_results.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/multi_scenario_matrix_shop_results.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/partial_post_processing_input.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/partial_post_processing_input.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/partial_post_processing_input_partial_bid_matrices_raw.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/partial_post_processing_input_partial_bid_matrices_raw.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/partial_post_processing_input_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/scenario_set_query.py`

 * *Files 19% similar despite different names*

```diff
@@ -3,120 +3,125 @@
 import datetime
 from typing import TYPE_CHECKING
 
 from cognite.client import data_modeling as dm, CogniteClient
 
 from cognite.powerops.client._generated.v1.data_classes import (
     DomainModelCore,
-    PartialPostProcessingInput,
-    MarketConfiguration,
+    ScenarioSet,
+)
+from cognite.powerops.client._generated.v1.data_classes._scenario import (
+    Scenario,
+    _create_scenario_filter,
 )
 from ._core import DEFAULT_QUERY_LIMIT, QueryBuilder, QueryStep, QueryAPI, T_DomainModelList, _create_edge_filter
 
 if TYPE_CHECKING:
-    from .bid_matrix_raw_query import BidMatrixRawQueryAPI
+    from .scenario_query import ScenarioQueryAPI
 
 
-class PartialPostProcessingInputQueryAPI(QueryAPI[T_DomainModelList]):
+class ScenarioSetQueryAPI(QueryAPI[T_DomainModelList]):
     def __init__(
         self,
         client: CogniteClient,
         builder: QueryBuilder[T_DomainModelList],
         view_by_read_class: dict[type[DomainModelCore], dm.ViewId],
         filter_: dm.filters.Filter | None = None,
         limit: int = DEFAULT_QUERY_LIMIT,
     ):
         super().__init__(client, builder, view_by_read_class)
 
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("partial_post_processing_input"),
+                name=self._builder.next_name("scenario_set"),
                 expression=dm.query.NodeResultSetExpression(
                     from_=self._builder[-1].name if self._builder else None,
                     filter=filter_,
                 ),
-                select=dm.query.Select(
-                    [dm.query.SourceSelector(self._view_by_read_class[PartialPostProcessingInput], ["*"])]
-                ),
-                result_cls=PartialPostProcessingInput,
+                select=dm.query.Select([dm.query.SourceSelector(self._view_by_read_class[ScenarioSet], ["*"])]),
+                result_cls=ScenarioSet,
                 max_retrieve_limit=limit,
             )
         )
 
-    def partial_bid_matrices_raw(
+    def shop_scenarios(
         self,
+        name: str | list[str] | None = None,
+        name_prefix: str | None = None,
+        model_template: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        commands: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        source: str | list[str] | None = None,
+        source_prefix: str | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
+        external_id_prefix_edge: str | None = None,
+        space_edge: str | list[str] | None = None,
+        filter: dm.Filter | None = None,
         limit: int | None = DEFAULT_QUERY_LIMIT,
-        retrieve_market_config: bool = False,
-    ) -> BidMatrixRawQueryAPI[T_DomainModelList]:
-        """Query along the partial bid matrices raw edges of the partial post processing input.
+    ) -> ScenarioQueryAPI[T_DomainModelList]:
+        """Query along the shop scenario edges of the scenario set.
 
         Args:
+            name: The name to filter on.
+            name_prefix: The prefix of the name to filter on.
+            model_template: The model template to filter on.
+            commands: The command to filter on.
+            source: The source to filter on.
+            source_prefix: The prefix of the source to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of partial bid matrices raw edges to return. Defaults to 25. Set to -1, float("inf") or None
+            external_id_prefix_edge: The prefix of the external ID to filter on.
+            space_edge: The space to filter on.
+            filter: (Advanced) Filter applied to node. If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
+            limit: Maximum number of shop scenario edges to return. Defaults to 3. Set to -1, float("inf") or None
                 to return all items.
-            retrieve_market_config: Whether to retrieve the market config for each partial post processing input or not.
 
         Returns:
-            BidMatrixRawQueryAPI: The query API for the bid matrix raw.
+            ScenarioQueryAPI: The query API for the scenario.
         """
-        from .bid_matrix_raw_query import BidMatrixRawQueryAPI
+        from .scenario_query import ScenarioQueryAPI
 
         from_ = self._builder[-1].name
-
         edge_filter = _create_edge_filter(
-            dm.DirectRelationReference("sp_powerops_types", "partialBidMatricesRaw"),
-            external_id_prefix=external_id_prefix,
-            space=space,
+            dm.DirectRelationReference("sp_powerops_types_temp", "ScenarioSet.scenarios"),
+            external_id_prefix=external_id_prefix_edge,
+            space=space_edge,
         )
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("partial_bid_matrices_raw"),
+                name=self._builder.next_name("shop_scenarios"),
                 expression=dm.query.EdgeResultSetExpression(
                     filter=edge_filter,
                     from_=from_,
                     direction="outwards",
                 ),
                 select=dm.query.Select(),
                 max_retrieve_limit=limit,
             )
         )
-        if retrieve_market_config:
-            self._query_append_market_config(from_)
-        return BidMatrixRawQueryAPI(self._client, self._builder, self._view_by_read_class, None, limit)
+
+        view_id = self._view_by_read_class[Scenario]
+        has_data = dm.filters.HasData(views=[view_id])
+        node_filer = _create_scenario_filter(
+            view_id,
+            name,
+            name_prefix,
+            model_template,
+            commands,
+            source,
+            source_prefix,
+            external_id_prefix,
+            space,
+            (filter and dm.filters.And(filter, has_data)) or has_data,
+        )
+        return ScenarioQueryAPI(self._client, self._builder, self._view_by_read_class, node_filer, limit)
 
     def query(
         self,
-        retrieve_market_config: bool = False,
     ) -> T_DomainModelList:
         """Execute query and return the result.
 
-        Args:
-            retrieve_market_config: Whether to retrieve the market config for each partial post processing input or not.
-
         Returns:
             The list of the source nodes of the query.
 
         """
-        from_ = self._builder[-1].name
-        if retrieve_market_config:
-            self._query_append_market_config(from_)
         return self._query()
-
-    def _query_append_market_config(self, from_: str) -> None:
-        view_id = self._view_by_read_class[MarketConfiguration]
-        self._builder.append(
-            QueryStep(
-                name=self._builder.next_name("market_config"),
-                expression=dm.query.NodeResultSetExpression(
-                    filter=dm.filters.HasData(views=[view_id]),
-                    from_=from_,
-                    through=self._view_by_read_class[PartialPostProcessingInput].as_property_ref("marketConfig"),
-                    direction="outwards",
-                ),
-                select=dm.query.Select([dm.query.SourceSelector(view_id, ["*"])]),
-                max_retrieve_limit=-1,
-                result_cls=MarketConfiguration,
-            ),
-        )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/partial_post_processing_output.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/partial_post_processing_output.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/partial_post_processing_output_alerts.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/partial_post_processing_output_alerts.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/partial_post_processing_output_partial_matrices.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/partial_post_processing_output_partial_matrices.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/partial_post_processing_output_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/basic_bid_matrix_query.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,167 +1,181 @@
 from __future__ import annotations
 
 import datetime
 from typing import TYPE_CHECKING
 
 from cognite.client import data_modeling as dm, CogniteClient
 
-from cognite.powerops.client._generated.v1.data_classes import (
+from cognite.powerops.client._generated.day_ahead_bid.data_classes import (
     DomainModelCore,
-    PartialPostProcessingOutput,
-    PartialPostProcessingInput,
+    BasicBidMatrix,
+    BidMethod,
+)
+from cognite.powerops.client._generated.day_ahead_bid.data_classes._alert import (
+    Alert,
+    _create_alert_filter,
 )
 from ._core import DEFAULT_QUERY_LIMIT, QueryBuilder, QueryStep, QueryAPI, T_DomainModelList, _create_edge_filter
 
 if TYPE_CHECKING:
     from .alert_query import AlertQueryAPI
-    from .bid_matrix_query import BidMatrixQueryAPI
 
 
-class PartialPostProcessingOutputQueryAPI(QueryAPI[T_DomainModelList]):
+class BasicBidMatrixQueryAPI(QueryAPI[T_DomainModelList]):
     def __init__(
         self,
         client: CogniteClient,
         builder: QueryBuilder[T_DomainModelList],
         view_by_read_class: dict[type[DomainModelCore], dm.ViewId],
         filter_: dm.filters.Filter | None = None,
         limit: int = DEFAULT_QUERY_LIMIT,
     ):
         super().__init__(client, builder, view_by_read_class)
 
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("partial_post_processing_output"),
+                name=self._builder.next_name("basic_bid_matrix"),
                 expression=dm.query.NodeResultSetExpression(
                     from_=self._builder[-1].name if self._builder else None,
                     filter=filter_,
                 ),
-                select=dm.query.Select(
-                    [dm.query.SourceSelector(self._view_by_read_class[PartialPostProcessingOutput], ["*"])]
-                ),
-                result_cls=PartialPostProcessingOutput,
+                select=dm.query.Select([dm.query.SourceSelector(self._view_by_read_class[BasicBidMatrix], ["*"])]),
+                result_cls=BasicBidMatrix,
                 max_retrieve_limit=limit,
             )
         )
 
     def alerts(
         self,
+        min_time: datetime.datetime | None = None,
+        max_time: datetime.datetime | None = None,
+        title: str | list[str] | None = None,
+        title_prefix: str | None = None,
+        description: str | list[str] | None = None,
+        description_prefix: str | None = None,
+        severity: str | list[str] | None = None,
+        severity_prefix: str | None = None,
+        alert_type: str | list[str] | None = None,
+        alert_type_prefix: str | None = None,
+        min_status_code: int | None = None,
+        max_status_code: int | None = None,
+        calculation_run: str | list[str] | None = None,
+        calculation_run_prefix: str | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
+        external_id_prefix_edge: str | None = None,
+        space_edge: str | list[str] | None = None,
+        filter: dm.Filter | None = None,
         limit: int | None = DEFAULT_QUERY_LIMIT,
-        retrieve_input_: bool = False,
+        retrieve_method: bool = False,
     ) -> AlertQueryAPI[T_DomainModelList]:
-        """Query along the alert edges of the partial post processing output.
+        """Query along the alert edges of the basic bid matrix.
 
         Args:
+            min_time: The minimum value of the time to filter on.
+            max_time: The maximum value of the time to filter on.
+            title: The title to filter on.
+            title_prefix: The prefix of the title to filter on.
+            description: The description to filter on.
+            description_prefix: The prefix of the description to filter on.
+            severity: The severity to filter on.
+            severity_prefix: The prefix of the severity to filter on.
+            alert_type: The alert type to filter on.
+            alert_type_prefix: The prefix of the alert type to filter on.
+            min_status_code: The minimum value of the status code to filter on.
+            max_status_code: The maximum value of the status code to filter on.
+            calculation_run: The calculation run to filter on.
+            calculation_run_prefix: The prefix of the calculation run to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of alert edges to return. Defaults to 25. Set to -1, float("inf") or None
+            external_id_prefix_edge: The prefix of the external ID to filter on.
+            space_edge: The space to filter on.
+            filter: (Advanced) Filter applied to node. If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
+            limit: Maximum number of alert edges to return. Defaults to 3. Set to -1, float("inf") or None
                 to return all items.
-            retrieve_input_: Whether to retrieve the input for each partial post processing output or not.
+            retrieve_method: Whether to retrieve the method for each basic bid matrix or not.
 
         Returns:
             AlertQueryAPI: The query API for the alert.
         """
         from .alert_query import AlertQueryAPI
 
         from_ = self._builder[-1].name
-
         edge_filter = _create_edge_filter(
-            dm.DirectRelationReference("sp_powerops_types", "calculationIssue"),
-            external_id_prefix=external_id_prefix,
-            space=space,
+            dm.DirectRelationReference("power-ops-types", "calculationIssue"),
+            external_id_prefix=external_id_prefix_edge,
+            space=space_edge,
         )
         self._builder.append(
             QueryStep(
                 name=self._builder.next_name("alerts"),
                 expression=dm.query.EdgeResultSetExpression(
                     filter=edge_filter,
                     from_=from_,
                     direction="outwards",
                 ),
                 select=dm.query.Select(),
                 max_retrieve_limit=limit,
             )
         )
-        if retrieve_input_:
-            self._query_append_input_(from_)
-        return AlertQueryAPI(self._client, self._builder, self._view_by_read_class, None, limit)
-
-    def partial_matrices(
-        self,
-        external_id_prefix: str | None = None,
-        space: str | list[str] | None = None,
-        limit: int | None = DEFAULT_QUERY_LIMIT,
-        retrieve_input_: bool = False,
-    ) -> BidMatrixQueryAPI[T_DomainModelList]:
-        """Query along the partial matrice edges of the partial post processing output.
-
-        Args:
-            external_id_prefix: The prefix of the external ID to filter on.
-            space: The space to filter on.
-            limit: Maximum number of partial matrice edges to return. Defaults to 25. Set to -1, float("inf") or None
-                to return all items.
-            retrieve_input_: Whether to retrieve the input for each partial post processing output or not.
-
-        Returns:
-            BidMatrixQueryAPI: The query API for the bid matrix.
-        """
-        from .bid_matrix_query import BidMatrixQueryAPI
 
-        from_ = self._builder[-1].name
-
-        edge_filter = _create_edge_filter(
-            dm.DirectRelationReference("sp_powerops_types", "BidMatrix"),
-            external_id_prefix=external_id_prefix,
-            space=space,
-        )
-        self._builder.append(
-            QueryStep(
-                name=self._builder.next_name("partial_matrices"),
-                expression=dm.query.EdgeResultSetExpression(
-                    filter=edge_filter,
-                    from_=from_,
-                    direction="outwards",
-                ),
-                select=dm.query.Select(),
-                max_retrieve_limit=limit,
-            )
-        )
-        if retrieve_input_:
-            self._query_append_input_(from_)
-        return BidMatrixQueryAPI(self._client, self._builder, self._view_by_read_class, None, limit)
+        view_id = self._view_by_read_class[Alert]
+        has_data = dm.filters.HasData(views=[view_id])
+        node_filer = _create_alert_filter(
+            view_id,
+            min_time,
+            max_time,
+            title,
+            title_prefix,
+            description,
+            description_prefix,
+            severity,
+            severity_prefix,
+            alert_type,
+            alert_type_prefix,
+            min_status_code,
+            max_status_code,
+            calculation_run,
+            calculation_run_prefix,
+            external_id_prefix,
+            space,
+            (filter and dm.filters.And(filter, has_data)) or has_data,
+        )
+        if retrieve_method:
+            self._query_append_method(from_)
+        return AlertQueryAPI(self._client, self._builder, self._view_by_read_class, node_filer, limit)
 
     def query(
         self,
-        retrieve_input_: bool = False,
+        retrieve_method: bool = False,
     ) -> T_DomainModelList:
         """Execute query and return the result.
 
         Args:
-            retrieve_input_: Whether to retrieve the input for each partial post processing output or not.
+            retrieve_method: Whether to retrieve the method for each basic bid matrix or not.
 
         Returns:
             The list of the source nodes of the query.
 
         """
         from_ = self._builder[-1].name
-        if retrieve_input_:
-            self._query_append_input_(from_)
+        if retrieve_method:
+            self._query_append_method(from_)
         return self._query()
 
-    def _query_append_input_(self, from_: str) -> None:
-        view_id = self._view_by_read_class[PartialPostProcessingInput]
+    def _query_append_method(self, from_: str) -> None:
+        view_id = self._view_by_read_class[BidMethod]
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("input_"),
+                name=self._builder.next_name("method"),
                 expression=dm.query.NodeResultSetExpression(
                     filter=dm.filters.HasData(views=[view_id]),
                     from_=from_,
-                    through=self._view_by_read_class[PartialPostProcessingOutput].as_property_ref("input"),
+                    through=self._view_by_read_class[BasicBidMatrix].as_property_ref("method"),
                     direction="outwards",
                 ),
                 select=dm.query.Select([dm.query.SourceSelector(view_id, ["*"])]),
                 max_retrieve_limit=-1,
-                result_cls=PartialPostProcessingInput,
+                result_cls=BidMethod,
+                is_single_direct_relation=True,
             ),
         )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/plant.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/plant.py`

 * *Files 10% similar despite different names*

```diff
@@ -30,16 +30,16 @@
     Aggregations,
     NodeAPI,
     SequenceNotStr,
     QueryStep,
     QueryBuilder,
 )
 from .plant_generators import PlantGeneratorsAPI
-from .plant_p_max_time_series import PlantPMaxTimeSeriesAPI
-from .plant_p_min_time_series import PlantPMinTimeSeriesAPI
+from .plant_production_max_time_series import PlantProductionMaxTimeSeriesAPI
+from .plant_production_min_time_series import PlantProductionMinTimeSeriesAPI
 from .plant_water_value_time_series import PlantWaterValueTimeSeriesAPI
 from .plant_feeding_fee_time_series import PlantFeedingFeeTimeSeriesAPI
 from .plant_outlet_level_time_series import PlantOutletLevelTimeSeriesAPI
 from .plant_inlet_level_time_series import PlantInletLevelTimeSeriesAPI
 from .plant_head_direct_time_series import PlantHeadDirectTimeSeriesAPI
 from .plant_query import PlantQueryAPI
 
@@ -53,68 +53,68 @@
             class_type=Plant,
             class_list=PlantList,
             class_write_list=PlantWriteList,
             view_by_read_class=view_by_read_class,
         )
         self._view_id = view_id
         self.generators_edge = PlantGeneratorsAPI(client)
-        self.p_max_time_series = PlantPMaxTimeSeriesAPI(client, view_id)
-        self.p_min_time_series = PlantPMinTimeSeriesAPI(client, view_id)
+        self.production_max_time_series = PlantProductionMaxTimeSeriesAPI(client, view_id)
+        self.production_min_time_series = PlantProductionMinTimeSeriesAPI(client, view_id)
         self.water_value_time_series = PlantWaterValueTimeSeriesAPI(client, view_id)
         self.feeding_fee_time_series = PlantFeedingFeeTimeSeriesAPI(client, view_id)
         self.outlet_level_time_series = PlantOutletLevelTimeSeriesAPI(client, view_id)
         self.inlet_level_time_series = PlantInletLevelTimeSeriesAPI(client, view_id)
         self.head_direct_time_series = PlantHeadDirectTimeSeriesAPI(client, view_id)
 
     def __call__(
         self,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
         display_name: str | list[str] | None = None,
         display_name_prefix: str | None = None,
         min_ordering: int | None = None,
         max_ordering: int | None = None,
+        asset_type: str | list[str] | None = None,
+        asset_type_prefix: str | None = None,
         min_head_loss_factor: float | None = None,
         max_head_loss_factor: float | None = None,
         min_outlet_level: float | None = None,
         max_outlet_level: float | None = None,
-        min_p_max: float | None = None,
-        max_p_max: float | None = None,
-        min_p_min: float | None = None,
-        max_p_min: float | None = None,
-        watercourse: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        min_production_max: float | None = None,
+        max_production_max: float | None = None,
+        min_production_min: float | None = None,
+        max_production_min: float | None = None,
         min_connection_losses: float | None = None,
         max_connection_losses: float | None = None,
-        inlet_reservoir: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_QUERY_LIMIT,
         filter: dm.Filter | None = None,
     ) -> PlantQueryAPI[PlantList]:
         """Query starting at plants.
 
         Args:
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
             display_name: The display name to filter on.
             display_name_prefix: The prefix of the display name to filter on.
             min_ordering: The minimum value of the ordering to filter on.
             max_ordering: The maximum value of the ordering to filter on.
+            asset_type: The asset type to filter on.
+            asset_type_prefix: The prefix of the asset type to filter on.
             min_head_loss_factor: The minimum value of the head loss factor to filter on.
             max_head_loss_factor: The maximum value of the head loss factor to filter on.
             min_outlet_level: The minimum value of the outlet level to filter on.
             max_outlet_level: The maximum value of the outlet level to filter on.
-            min_p_max: The minimum value of the p max to filter on.
-            max_p_max: The maximum value of the p max to filter on.
-            min_p_min: The minimum value of the p min to filter on.
-            max_p_min: The maximum value of the p min to filter on.
-            watercourse: The watercourse to filter on.
+            min_production_max: The minimum value of the production max to filter on.
+            max_production_max: The maximum value of the production max to filter on.
+            min_production_min: The minimum value of the production min to filter on.
+            max_production_min: The maximum value of the production min to filter on.
             min_connection_losses: The minimum value of the connection loss to filter on.
             max_connection_losses: The maximum value of the connection loss to filter on.
-            inlet_reservoir: The inlet reservoir to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of plants to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
             A query API for plants.
@@ -125,26 +125,26 @@
             self._view_id,
             name,
             name_prefix,
             display_name,
             display_name_prefix,
             min_ordering,
             max_ordering,
+            asset_type,
+            asset_type_prefix,
             min_head_loss_factor,
             max_head_loss_factor,
             min_outlet_level,
             max_outlet_level,
-            min_p_max,
-            max_p_max,
-            min_p_min,
-            max_p_min,
-            watercourse,
+            min_production_max,
+            max_production_max,
+            min_production_min,
+            max_production_min,
             min_connection_losses,
             max_connection_losses,
-            inlet_reservoir,
             external_id_prefix,
             space,
             (filter and dm.filters.And(filter, has_data)) or has_data,
         )
         builder = QueryBuilder(PlantList)
         return PlantQueryAPI(self._client, builder, self._view_by_read_class, filter_, limit)
 
@@ -254,43 +254,43 @@
             external_id,
             space,
             retrieve_edges=True,
             edge_api_name_type_direction_view_id_penta=[
                 (
                     self.generators_edge,
                     "generators",
-                    dm.DirectRelationReference("sp_powerops_types", "isSubAssetOf"),
+                    dm.DirectRelationReference("sp_powerops_types_temp", "isSubAssetOf"),
                     "outwards",
-                    dm.ViewId("sp_powerops_models", "Generator", "1"),
+                    dm.ViewId("sp_powerops_models_temp", "Generator", "1"),
                 ),
             ],
         )
 
     def search(
         self,
         query: str,
         properties: PlantTextFields | Sequence[PlantTextFields] | None = None,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
         display_name: str | list[str] | None = None,
         display_name_prefix: str | None = None,
         min_ordering: int | None = None,
         max_ordering: int | None = None,
+        asset_type: str | list[str] | None = None,
+        asset_type_prefix: str | None = None,
         min_head_loss_factor: float | None = None,
         max_head_loss_factor: float | None = None,
         min_outlet_level: float | None = None,
         max_outlet_level: float | None = None,
-        min_p_max: float | None = None,
-        max_p_max: float | None = None,
-        min_p_min: float | None = None,
-        max_p_min: float | None = None,
-        watercourse: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        min_production_max: float | None = None,
+        max_production_max: float | None = None,
+        min_production_min: float | None = None,
+        max_production_min: float | None = None,
         min_connection_losses: float | None = None,
         max_connection_losses: float | None = None,
-        inlet_reservoir: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> PlantList:
         """Search plants
 
@@ -299,26 +299,26 @@
             properties: The property to search, if nothing is passed all text fields will be searched.
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
             display_name: The display name to filter on.
             display_name_prefix: The prefix of the display name to filter on.
             min_ordering: The minimum value of the ordering to filter on.
             max_ordering: The maximum value of the ordering to filter on.
+            asset_type: The asset type to filter on.
+            asset_type_prefix: The prefix of the asset type to filter on.
             min_head_loss_factor: The minimum value of the head loss factor to filter on.
             max_head_loss_factor: The maximum value of the head loss factor to filter on.
             min_outlet_level: The minimum value of the outlet level to filter on.
             max_outlet_level: The maximum value of the outlet level to filter on.
-            min_p_max: The minimum value of the p max to filter on.
-            max_p_max: The maximum value of the p max to filter on.
-            min_p_min: The minimum value of the p min to filter on.
-            max_p_min: The maximum value of the p min to filter on.
-            watercourse: The watercourse to filter on.
+            min_production_max: The minimum value of the production max to filter on.
+            max_production_max: The maximum value of the production max to filter on.
+            min_production_min: The minimum value of the production min to filter on.
+            max_production_min: The maximum value of the production min to filter on.
             min_connection_losses: The minimum value of the connection loss to filter on.
             max_connection_losses: The maximum value of the connection loss to filter on.
-            inlet_reservoir: The inlet reservoir to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of plants to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
             Search results plants matching the query.
@@ -336,26 +336,26 @@
             self._view_id,
             name,
             name_prefix,
             display_name,
             display_name_prefix,
             min_ordering,
             max_ordering,
+            asset_type,
+            asset_type_prefix,
             min_head_loss_factor,
             max_head_loss_factor,
             min_outlet_level,
             max_outlet_level,
-            min_p_max,
-            max_p_max,
-            min_p_min,
-            max_p_min,
-            watercourse,
+            min_production_max,
+            max_production_max,
+            min_production_min,
+            max_production_min,
             min_connection_losses,
             max_connection_losses,
-            inlet_reservoir,
             external_id_prefix,
             space,
             filter,
         )
         return self._search(self._view_id, query, _PLANT_PROPERTIES_BY_FIELD, properties, filter_, limit)
 
     @overload
@@ -373,26 +373,26 @@
         search_properties: PlantTextFields | Sequence[PlantTextFields] | None = None,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
         display_name: str | list[str] | None = None,
         display_name_prefix: str | None = None,
         min_ordering: int | None = None,
         max_ordering: int | None = None,
+        asset_type: str | list[str] | None = None,
+        asset_type_prefix: str | None = None,
         min_head_loss_factor: float | None = None,
         max_head_loss_factor: float | None = None,
         min_outlet_level: float | None = None,
         max_outlet_level: float | None = None,
-        min_p_max: float | None = None,
-        max_p_max: float | None = None,
-        min_p_min: float | None = None,
-        max_p_min: float | None = None,
-        watercourse: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        min_production_max: float | None = None,
+        max_production_max: float | None = None,
+        min_production_min: float | None = None,
+        max_production_min: float | None = None,
         min_connection_losses: float | None = None,
         max_connection_losses: float | None = None,
-        inlet_reservoir: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> list[dm.aggregations.AggregatedNumberedValue]: ...
 
     @overload
@@ -410,26 +410,26 @@
         search_properties: PlantTextFields | Sequence[PlantTextFields] | None = None,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
         display_name: str | list[str] | None = None,
         display_name_prefix: str | None = None,
         min_ordering: int | None = None,
         max_ordering: int | None = None,
+        asset_type: str | list[str] | None = None,
+        asset_type_prefix: str | None = None,
         min_head_loss_factor: float | None = None,
         max_head_loss_factor: float | None = None,
         min_outlet_level: float | None = None,
         max_outlet_level: float | None = None,
-        min_p_max: float | None = None,
-        max_p_max: float | None = None,
-        min_p_min: float | None = None,
-        max_p_min: float | None = None,
-        watercourse: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        min_production_max: float | None = None,
+        max_production_max: float | None = None,
+        min_production_min: float | None = None,
+        max_production_min: float | None = None,
         min_connection_losses: float | None = None,
         max_connection_losses: float | None = None,
-        inlet_reservoir: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> InstanceAggregationResultList: ...
 
     def aggregate(
@@ -446,26 +446,26 @@
         search_property: PlantTextFields | Sequence[PlantTextFields] | None = None,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
         display_name: str | list[str] | None = None,
         display_name_prefix: str | None = None,
         min_ordering: int | None = None,
         max_ordering: int | None = None,
+        asset_type: str | list[str] | None = None,
+        asset_type_prefix: str | None = None,
         min_head_loss_factor: float | None = None,
         max_head_loss_factor: float | None = None,
         min_outlet_level: float | None = None,
         max_outlet_level: float | None = None,
-        min_p_max: float | None = None,
-        max_p_max: float | None = None,
-        min_p_min: float | None = None,
-        max_p_min: float | None = None,
-        watercourse: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        min_production_max: float | None = None,
+        max_production_max: float | None = None,
+        min_production_min: float | None = None,
+        max_production_min: float | None = None,
         min_connection_losses: float | None = None,
         max_connection_losses: float | None = None,
-        inlet_reservoir: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> list[dm.aggregations.AggregatedNumberedValue] | InstanceAggregationResultList:
         """Aggregate data across plants
 
@@ -477,26 +477,26 @@
             search_property: The text field to search in.
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
             display_name: The display name to filter on.
             display_name_prefix: The prefix of the display name to filter on.
             min_ordering: The minimum value of the ordering to filter on.
             max_ordering: The maximum value of the ordering to filter on.
+            asset_type: The asset type to filter on.
+            asset_type_prefix: The prefix of the asset type to filter on.
             min_head_loss_factor: The minimum value of the head loss factor to filter on.
             max_head_loss_factor: The maximum value of the head loss factor to filter on.
             min_outlet_level: The minimum value of the outlet level to filter on.
             max_outlet_level: The maximum value of the outlet level to filter on.
-            min_p_max: The minimum value of the p max to filter on.
-            max_p_max: The maximum value of the p max to filter on.
-            min_p_min: The minimum value of the p min to filter on.
-            max_p_min: The maximum value of the p min to filter on.
-            watercourse: The watercourse to filter on.
+            min_production_max: The minimum value of the production max to filter on.
+            max_production_max: The maximum value of the production max to filter on.
+            min_production_min: The minimum value of the production min to filter on.
+            max_production_min: The maximum value of the production min to filter on.
             min_connection_losses: The minimum value of the connection loss to filter on.
             max_connection_losses: The maximum value of the connection loss to filter on.
-            inlet_reservoir: The inlet reservoir to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of plants to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
             Aggregation results.
@@ -515,26 +515,26 @@
             self._view_id,
             name,
             name_prefix,
             display_name,
             display_name_prefix,
             min_ordering,
             max_ordering,
+            asset_type,
+            asset_type_prefix,
             min_head_loss_factor,
             max_head_loss_factor,
             min_outlet_level,
             max_outlet_level,
-            min_p_max,
-            max_p_max,
-            min_p_min,
-            max_p_min,
-            watercourse,
+            min_production_max,
+            max_production_max,
+            min_production_min,
+            max_production_min,
             min_connection_losses,
             max_connection_losses,
-            inlet_reservoir,
             external_id_prefix,
             space,
             filter,
         )
         return self._aggregate(
             self._view_id,
             aggregate,
@@ -555,26 +555,26 @@
         search_property: PlantTextFields | Sequence[PlantTextFields] | None = None,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
         display_name: str | list[str] | None = None,
         display_name_prefix: str | None = None,
         min_ordering: int | None = None,
         max_ordering: int | None = None,
+        asset_type: str | list[str] | None = None,
+        asset_type_prefix: str | None = None,
         min_head_loss_factor: float | None = None,
         max_head_loss_factor: float | None = None,
         min_outlet_level: float | None = None,
         max_outlet_level: float | None = None,
-        min_p_max: float | None = None,
-        max_p_max: float | None = None,
-        min_p_min: float | None = None,
-        max_p_min: float | None = None,
-        watercourse: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        min_production_max: float | None = None,
+        max_production_max: float | None = None,
+        min_production_min: float | None = None,
+        max_production_min: float | None = None,
         min_connection_losses: float | None = None,
         max_connection_losses: float | None = None,
-        inlet_reservoir: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> dm.aggregations.HistogramValue:
         """Produces histograms for plants
 
@@ -585,26 +585,26 @@
             search_property: The text field to search in.
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
             display_name: The display name to filter on.
             display_name_prefix: The prefix of the display name to filter on.
             min_ordering: The minimum value of the ordering to filter on.
             max_ordering: The maximum value of the ordering to filter on.
+            asset_type: The asset type to filter on.
+            asset_type_prefix: The prefix of the asset type to filter on.
             min_head_loss_factor: The minimum value of the head loss factor to filter on.
             max_head_loss_factor: The maximum value of the head loss factor to filter on.
             min_outlet_level: The minimum value of the outlet level to filter on.
             max_outlet_level: The maximum value of the outlet level to filter on.
-            min_p_max: The minimum value of the p max to filter on.
-            max_p_max: The maximum value of the p max to filter on.
-            min_p_min: The minimum value of the p min to filter on.
-            max_p_min: The maximum value of the p min to filter on.
-            watercourse: The watercourse to filter on.
+            min_production_max: The minimum value of the production max to filter on.
+            max_production_max: The maximum value of the production max to filter on.
+            min_production_min: The minimum value of the production min to filter on.
+            max_production_min: The maximum value of the production min to filter on.
             min_connection_losses: The minimum value of the connection loss to filter on.
             max_connection_losses: The maximum value of the connection loss to filter on.
-            inlet_reservoir: The inlet reservoir to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of plants to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
             Bucketed histogram results.
@@ -614,26 +614,26 @@
             self._view_id,
             name,
             name_prefix,
             display_name,
             display_name_prefix,
             min_ordering,
             max_ordering,
+            asset_type,
+            asset_type_prefix,
             min_head_loss_factor,
             max_head_loss_factor,
             min_outlet_level,
             max_outlet_level,
-            min_p_max,
-            max_p_max,
-            min_p_min,
-            max_p_min,
-            watercourse,
+            min_production_max,
+            max_production_max,
+            min_production_min,
+            max_production_min,
             min_connection_losses,
             max_connection_losses,
-            inlet_reservoir,
             external_id_prefix,
             space,
             filter,
         )
         return self._histogram(
             self._view_id,
             property,
@@ -649,26 +649,26 @@
         self,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
         display_name: str | list[str] | None = None,
         display_name_prefix: str | None = None,
         min_ordering: int | None = None,
         max_ordering: int | None = None,
+        asset_type: str | list[str] | None = None,
+        asset_type_prefix: str | None = None,
         min_head_loss_factor: float | None = None,
         max_head_loss_factor: float | None = None,
         min_outlet_level: float | None = None,
         max_outlet_level: float | None = None,
-        min_p_max: float | None = None,
-        max_p_max: float | None = None,
-        min_p_min: float | None = None,
-        max_p_min: float | None = None,
-        watercourse: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        min_production_max: float | None = None,
+        max_production_max: float | None = None,
+        min_production_min: float | None = None,
+        max_production_min: float | None = None,
         min_connection_losses: float | None = None,
         max_connection_losses: float | None = None,
-        inlet_reservoir: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
         retrieve_edges: bool = True,
     ) -> PlantList:
         """List/filter plants
@@ -676,26 +676,26 @@
         Args:
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
             display_name: The display name to filter on.
             display_name_prefix: The prefix of the display name to filter on.
             min_ordering: The minimum value of the ordering to filter on.
             max_ordering: The maximum value of the ordering to filter on.
+            asset_type: The asset type to filter on.
+            asset_type_prefix: The prefix of the asset type to filter on.
             min_head_loss_factor: The minimum value of the head loss factor to filter on.
             max_head_loss_factor: The maximum value of the head loss factor to filter on.
             min_outlet_level: The minimum value of the outlet level to filter on.
             max_outlet_level: The maximum value of the outlet level to filter on.
-            min_p_max: The minimum value of the p max to filter on.
-            max_p_max: The maximum value of the p max to filter on.
-            min_p_min: The minimum value of the p min to filter on.
-            max_p_min: The maximum value of the p min to filter on.
-            watercourse: The watercourse to filter on.
+            min_production_max: The minimum value of the production max to filter on.
+            max_production_max: The maximum value of the production max to filter on.
+            min_production_min: The minimum value of the production min to filter on.
+            max_production_min: The maximum value of the production min to filter on.
             min_connection_losses: The minimum value of the connection loss to filter on.
             max_connection_losses: The maximum value of the connection loss to filter on.
-            inlet_reservoir: The inlet reservoir to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of plants to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
             retrieve_edges: Whether to retrieve `generators` external ids for the plants. Defaults to True.
 
         Returns:
@@ -714,38 +714,38 @@
             self._view_id,
             name,
             name_prefix,
             display_name,
             display_name_prefix,
             min_ordering,
             max_ordering,
+            asset_type,
+            asset_type_prefix,
             min_head_loss_factor,
             max_head_loss_factor,
             min_outlet_level,
             max_outlet_level,
-            min_p_max,
-            max_p_max,
-            min_p_min,
-            max_p_min,
-            watercourse,
+            min_production_max,
+            max_production_max,
+            min_production_min,
+            max_production_min,
             min_connection_losses,
             max_connection_losses,
-            inlet_reservoir,
             external_id_prefix,
             space,
             filter,
         )
 
         return self._list(
             limit=limit,
             filter=filter_,
             retrieve_edges=retrieve_edges,
             edge_api_name_type_direction_view_id_penta=[
                 (
                     self.generators_edge,
                     "generators",
-                    dm.DirectRelationReference("sp_powerops_types", "isSubAssetOf"),
+                    dm.DirectRelationReference("sp_powerops_types_temp", "isSubAssetOf"),
                     "outwards",
-                    dm.ViewId("sp_powerops_models", "Generator", "1"),
+                    dm.ViewId("sp_powerops_models_temp", "Generator", "1"),
                 ),
             ],
         )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/plant_feeding_fee_time_series.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/plant_head_direct_time_series.py`

 * *Files 6% similar despite different names*

```diff
@@ -12,31 +12,32 @@
 from cognite.powerops.client._generated.v1.data_classes._plant import _create_plant_filter
 from ._core import DEFAULT_LIMIT_READ, INSTANCE_QUERY_LIMIT
 
 ColumnNames = Literal[
     "name",
     "displayName",
     "ordering",
+    "assetType",
     "headLossFactor",
     "outletLevel",
-    "pMax",
-    "pMin",
+    "productionMax",
+    "productionMin",
     "penstockHeadLossFactors",
     "connectionLosses",
-    "pMaxTimeSeries",
-    "pMinTimeSeries",
+    "productionMaxTimeSeries",
+    "productionMinTimeSeries",
     "waterValueTimeSeries",
     "feedingFeeTimeSeries",
     "outletLevelTimeSeries",
     "inletLevelTimeSeries",
     "headDirectTimeSeries",
 ]
 
 
-class PlantFeedingFeeTimeSeriesQuery:
+class PlantHeadDirectTimeSeriesQuery:
     def __init__(
         self,
         client: CogniteClient,
         view_id: dm.ViewId,
         timeseries_limit: int = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ):
@@ -53,15 +54,15 @@
         aggregates: Aggregate | list[Aggregate] | None = None,
         granularity: str | None = None,
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         limit: int | None = None,
         include_outside_points: bool = False,
     ) -> DatapointsList:
-        """`Retrieve datapoints for the `plant.feeding_fee_time_series` timeseries.
+        """`Retrieve datapoints for the `plant.head_direct_time_series` timeseries.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -78,19 +79,19 @@
 
         Returns:
             A ``DatapointsList`` with the requested datapoints.
 
         Examples:
 
             In this example,
-            we are using the time-ago format to get raw data for the 'my_feeding_fee_time_series' from 2 weeks ago up until now::
+            we are using the time-ago format to get raw data for the 'my_head_direct_time_series' from 2 weeks ago up until now::
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> plant_datapoints = client.plant.feeding_fee_time_series(external_id="my_feeding_fee_time_series").retrieve(start="2w-ago")
+                >>> plant_datapoints = client.plant.head_direct_time_series(external_id="my_head_direct_time_series").retrieve(start="2w-ago")
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra()
         if external_ids:
             return self._client.time_series.data.retrieve(
                 external_id=list(external_ids),
                 start=start,
                 end=end,
@@ -112,15 +113,15 @@
         aggregates: Aggregate | list[Aggregate] | None = None,
         granularity: str | None = None,
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         limit: int | None = None,
         include_outside_points: bool = False,
     ) -> DatapointsArrayList:
-        """`Retrieve numpy arrays for the `plant.feeding_fee_time_series` timeseries.
+        """`Retrieve numpy arrays for the `plant.head_direct_time_series` timeseries.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -137,19 +138,19 @@
 
         Returns:
             A ``DatapointsArrayList`` with the requested datapoints.
 
         Examples:
 
             In this example,
-            we are using the time-ago format to get raw data for the 'my_feeding_fee_time_series' from 2 weeks ago up until now::
+            we are using the time-ago format to get raw data for the 'my_head_direct_time_series' from 2 weeks ago up until now::
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> plant_datapoints = client.plant.feeding_fee_time_series(external_id="my_feeding_fee_time_series").retrieve_array(start="2w-ago")
+                >>> plant_datapoints = client.plant.head_direct_time_series(external_id="my_head_direct_time_series").retrieve_array(start="2w-ago")
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra()
         if external_ids:
             return self._client.time_series.data.retrieve_arrays(
                 external_id=list(external_ids),
                 start=start,
                 end=end,
@@ -173,17 +174,17 @@
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         limit: int | None = None,
         include_outside_points: bool = False,
         uniform_index: bool = False,
         include_aggregate_name: bool = True,
         include_granularity_name: bool = False,
-        column_names: ColumnNames | list[ColumnNames] = "feedingFeeTimeSeries",
+        column_names: ColumnNames | list[ColumnNames] = "headDirectTimeSeries",
     ) -> pd.DataFrame:
-        """`Retrieve DataFrames for the `plant.feeding_fee_time_series` timeseries.
+        """`Retrieve DataFrames for the `plant.head_direct_time_series` timeseries.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -196,28 +197,28 @@
             target_unit: The unit_external_id of the data points returned. If the time series does not have an unit_external_id that can be converted to the target_unit, an error will be returned. Cannot be used with target_unit_system.
             target_unit_system: The unit system of the data points returned. Cannot be used with target_unit.
             limit: Maximum number of datapoints to return for each time series. Default: None (no limit)
             include_outside_points: Whether to include outside points. Not allowed when fetching aggregates. Default: False
             uniform_index: If only querying aggregates AND a single granularity is used, AND no limit is used, specifying `uniform_index=True` will return a dataframe with an equidistant datetime index from the earliest `start` to the latest `end` (missing values will be NaNs). If these requirements are not met, a ValueError is raised. Default: False
             include_aggregate_name: Include 'aggregate' in the column name, e.g. `my-ts|average`. Ignored for raw time series. Default: True
             include_granularity_name: Include 'granularity' in the column name, e.g. `my-ts|12h`. Added after 'aggregate' when present. Ignored for raw time series. Default: False
-            column_names: Which property to use for column names. Defauts to feedingFeeTimeSeries
+            column_names: Which property to use for column names. Defauts to headDirectTimeSeries
 
 
         Returns:
             A ``DataFrame`` with the requested datapoints.
 
         Examples:
 
             In this example,
-            we are using the time-ago format to get raw data for the 'my_feeding_fee_time_series' from 2 weeks ago up until now::
+            we are using the time-ago format to get raw data for the 'my_head_direct_time_series' from 2 weeks ago up until now::
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> plant_datapoints = client.plant.feeding_fee_time_series(external_id="my_feeding_fee_time_series").retrieve_dataframe(start="2w-ago")
+                >>> plant_datapoints = client.plant.head_direct_time_series(external_id="my_head_direct_time_series").retrieve_dataframe(start="2w-ago")
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra(column_names)
         if external_ids:
             df = self._client.time_series.data.retrieve_dataframe(
                 external_id=list(external_ids),
                 start=start,
                 end=end,
@@ -250,17 +251,17 @@
         aggregates: Aggregate | Sequence[Aggregate] | None = None,
         granularity: str | None = None,
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         uniform_index: bool = False,
         include_aggregate_name: bool = True,
         include_granularity_name: bool = False,
-        column_names: ColumnNames | list[ColumnNames] = "feedingFeeTimeSeries",
+        column_names: ColumnNames | list[ColumnNames] = "headDirectTimeSeries",
     ) -> pd.DataFrame:
-        """Retrieve DataFrames for the `plant.feeding_fee_time_series` timeseries in Timezone.
+        """Retrieve DataFrames for the `plant.head_direct_time_series` timeseries in Timezone.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -273,30 +274,30 @@
             target_unit: The unit_external_id of the data points returned. If the time series does not have an unit_external_id that can be converted to the target_unit, an error will be returned. Cannot be used with target_unit_system.
             target_unit_system: The unit system of the data points returned. Cannot be used with target_unit.
             limit: Maximum number of datapoints to return for each time series. Default: None (no limit)
             include_outside_points: Whether to include outside points. Not allowed when fetching aggregates. Default: False
             uniform_index: If only querying aggregates AND a single granularity is used, AND no limit is used, specifying `uniform_index=True` will return a dataframe with an equidistant datetime index from the earliest `start` to the latest `end` (missing values will be NaNs). If these requirements are not met, a ValueError is raised. Default: False
             include_aggregate_name: Include 'aggregate' in the column name, e.g. `my-ts|average`. Ignored for raw time series. Default: True
             include_granularity_name: Include 'granularity' in the column name, e.g. `my-ts|12h`. Added after 'aggregate' when present. Ignored for raw time series. Default: False
-            column_names: Which property to use for column names. Defauts to feedingFeeTimeSeries
+            column_names: Which property to use for column names. Defauts to headDirectTimeSeries
 
 
         Returns:
             A ``DataFrame`` with the requested datapoints.
 
         Examples:
 
             In this example,
-            get weekly aggregates for the 'my_feeding_fee_time_series' for the first month of 2023 in Oslo time:
+            get weekly aggregates for the 'my_head_direct_time_series' for the first month of 2023 in Oslo time:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> from datetime import datetime, timezone
                 >>> client = PowerOpsModelsV1Client()
-                >>> plant_datapoints = client.plant.feeding_fee_time_series(
-                ...     external_id="my_feeding_fee_time_series").retrieve_dataframe_in_timezone(
+                >>> plant_datapoints = client.plant.head_direct_time_series(
+                ...     external_id="my_head_direct_time_series").retrieve_dataframe_in_timezone(
                 ...         datetime(2023, 1, 1, tzinfo=ZoneInfo("Europe/Oslo")),
                 ...         datetime(2023, 1, 2, tzinfo=ZoneInfo("Europe/Oslo")),
                 ...         aggregates="average",
                 ...         granularity="1week",
                 ...     )
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra(column_names)
@@ -334,17 +335,17 @@
                 external_id=list(external_ids),
                 before=before,
             )
         else:
             return None
 
     def _retrieve_timeseries_external_ids_with_extra(
-        self, extra_properties: ColumnNames | list[ColumnNames] = "feedingFeeTimeSeries"
+        self, extra_properties: ColumnNames | list[ColumnNames] = "headDirectTimeSeries"
     ) -> dict[str, list[str]]:
-        return _retrieve_timeseries_external_ids_with_extra_feeding_fee_time_series(
+        return _retrieve_timeseries_external_ids_with_extra_head_direct_time_series(
             self._client,
             self._view_id,
             self._filter,
             self._timeseries_limit,
             extra_properties,
         )
 
@@ -352,248 +353,248 @@
     def _rename_columns(
         external_ids: dict[str, list[str]],
         df: pd.DataFrame,
         column_names: ColumnNames | list[ColumnNames],
         include_aggregate_name: bool,
         include_granularity_name: bool,
     ) -> pd.DataFrame:
-        if isinstance(column_names, str) and column_names == "feedingFeeTimeSeries":
+        if isinstance(column_names, str) and column_names == "headDirectTimeSeries":
             return df
         splits = sum(included for included in [include_aggregate_name, include_granularity_name])
         if splits == 0:
             df.columns = ["-".join(external_ids[external_id]) for external_id in df.columns]
         else:
             column_parts = (col.rsplit("|", maxsplit=splits) for col in df.columns)
             df.columns = [
                 "-".join(external_ids[external_id]) + "|" + "|".join(parts) for external_id, *parts in column_parts
             ]
         return df
 
 
-class PlantFeedingFeeTimeSeriesAPI:
+class PlantHeadDirectTimeSeriesAPI:
     def __init__(self, client: CogniteClient, view_id: dm.ViewId):
         self._client = client
         self._view_id = view_id
 
     def __call__(
         self,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
         display_name: str | list[str] | None = None,
         display_name_prefix: str | None = None,
         min_ordering: int | None = None,
         max_ordering: int | None = None,
+        asset_type: str | list[str] | None = None,
+        asset_type_prefix: str | None = None,
         min_head_loss_factor: float | None = None,
         max_head_loss_factor: float | None = None,
         min_outlet_level: float | None = None,
         max_outlet_level: float | None = None,
-        min_p_max: float | None = None,
-        max_p_max: float | None = None,
-        min_p_min: float | None = None,
-        max_p_min: float | None = None,
-        watercourse: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        min_production_max: float | None = None,
+        max_production_max: float | None = None,
+        min_production_min: float | None = None,
+        max_production_min: float | None = None,
         min_connection_losses: float | None = None,
         max_connection_losses: float | None = None,
-        inlet_reservoir: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
-    ) -> PlantFeedingFeeTimeSeriesQuery:
-        """Query timeseries `plant.feeding_fee_time_series`
+    ) -> PlantHeadDirectTimeSeriesQuery:
+        """Query timeseries `plant.head_direct_time_series`
 
         Args:
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
             display_name: The display name to filter on.
             display_name_prefix: The prefix of the display name to filter on.
             min_ordering: The minimum value of the ordering to filter on.
             max_ordering: The maximum value of the ordering to filter on.
+            asset_type: The asset type to filter on.
+            asset_type_prefix: The prefix of the asset type to filter on.
             min_head_loss_factor: The minimum value of the head loss factor to filter on.
             max_head_loss_factor: The maximum value of the head loss factor to filter on.
             min_outlet_level: The minimum value of the outlet level to filter on.
             max_outlet_level: The maximum value of the outlet level to filter on.
-            min_p_max: The minimum value of the p max to filter on.
-            max_p_max: The maximum value of the p max to filter on.
-            min_p_min: The minimum value of the p min to filter on.
-            max_p_min: The maximum value of the p min to filter on.
-            watercourse: The watercourse to filter on.
+            min_production_max: The minimum value of the production max to filter on.
+            max_production_max: The maximum value of the production max to filter on.
+            min_production_min: The minimum value of the production min to filter on.
+            max_production_min: The maximum value of the production min to filter on.
             min_connection_losses: The minimum value of the connection loss to filter on.
             max_connection_losses: The maximum value of the connection loss to filter on.
-            inlet_reservoir: The inlet reservoir to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of plants to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
-            A query object that can be used to retrieve datapoins for the plant.feeding_fee_time_series timeseries
+            A query object that can be used to retrieve datapoins for the plant.head_direct_time_series timeseries
             selected in this method.
 
         Examples:
 
-            Retrieve all data for 5 plant.feeding_fee_time_series timeseries:
+            Retrieve all data for 5 plant.head_direct_time_series timeseries:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> plants = client.plant.feeding_fee_time_series(limit=5).retrieve()
+                >>> plants = client.plant.head_direct_time_series(limit=5).retrieve()
 
         """
         filter_ = _create_plant_filter(
             self._view_id,
             name,
             name_prefix,
             display_name,
             display_name_prefix,
             min_ordering,
             max_ordering,
+            asset_type,
+            asset_type_prefix,
             min_head_loss_factor,
             max_head_loss_factor,
             min_outlet_level,
             max_outlet_level,
-            min_p_max,
-            max_p_max,
-            min_p_min,
-            max_p_min,
-            watercourse,
+            min_production_max,
+            max_production_max,
+            min_production_min,
+            max_production_min,
             min_connection_losses,
             max_connection_losses,
-            inlet_reservoir,
             external_id_prefix,
             space,
             filter,
         )
 
-        return PlantFeedingFeeTimeSeriesQuery(
+        return PlantHeadDirectTimeSeriesQuery(
             client=self._client,
             view_id=self._view_id,
             timeseries_limit=limit,
             filter=filter_,
         )
 
     def list(
         self,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
         display_name: str | list[str] | None = None,
         display_name_prefix: str | None = None,
         min_ordering: int | None = None,
         max_ordering: int | None = None,
+        asset_type: str | list[str] | None = None,
+        asset_type_prefix: str | None = None,
         min_head_loss_factor: float | None = None,
         max_head_loss_factor: float | None = None,
         min_outlet_level: float | None = None,
         max_outlet_level: float | None = None,
-        min_p_max: float | None = None,
-        max_p_max: float | None = None,
-        min_p_min: float | None = None,
-        max_p_min: float | None = None,
-        watercourse: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        min_production_max: float | None = None,
+        max_production_max: float | None = None,
+        min_production_min: float | None = None,
+        max_production_min: float | None = None,
         min_connection_losses: float | None = None,
         max_connection_losses: float | None = None,
-        inlet_reservoir: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> TimeSeriesList:
-        """List timeseries `plant.feeding_fee_time_series`
+        """List timeseries `plant.head_direct_time_series`
 
         Args:
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
             display_name: The display name to filter on.
             display_name_prefix: The prefix of the display name to filter on.
             min_ordering: The minimum value of the ordering to filter on.
             max_ordering: The maximum value of the ordering to filter on.
+            asset_type: The asset type to filter on.
+            asset_type_prefix: The prefix of the asset type to filter on.
             min_head_loss_factor: The minimum value of the head loss factor to filter on.
             max_head_loss_factor: The maximum value of the head loss factor to filter on.
             min_outlet_level: The minimum value of the outlet level to filter on.
             max_outlet_level: The maximum value of the outlet level to filter on.
-            min_p_max: The minimum value of the p max to filter on.
-            max_p_max: The maximum value of the p max to filter on.
-            min_p_min: The minimum value of the p min to filter on.
-            max_p_min: The maximum value of the p min to filter on.
-            watercourse: The watercourse to filter on.
+            min_production_max: The minimum value of the production max to filter on.
+            max_production_max: The maximum value of the production max to filter on.
+            min_production_min: The minimum value of the production min to filter on.
+            max_production_min: The maximum value of the production min to filter on.
             min_connection_losses: The minimum value of the connection loss to filter on.
             max_connection_losses: The maximum value of the connection loss to filter on.
-            inlet_reservoir: The inlet reservoir to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of plants to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
-            List of Timeseries plant.feeding_fee_time_series.
+            List of Timeseries plant.head_direct_time_series.
 
         Examples:
 
-            List plant.feeding_fee_time_series and limit to 5:
+            List plant.head_direct_time_series and limit to 5:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> plants = client.plant.feeding_fee_time_series.list(limit=5)
+                >>> plants = client.plant.head_direct_time_series.list(limit=5)
 
         """
         filter_ = _create_plant_filter(
             self._view_id,
             name,
             name_prefix,
             display_name,
             display_name_prefix,
             min_ordering,
             max_ordering,
+            asset_type,
+            asset_type_prefix,
             min_head_loss_factor,
             max_head_loss_factor,
             min_outlet_level,
             max_outlet_level,
-            min_p_max,
-            max_p_max,
-            min_p_min,
-            max_p_min,
-            watercourse,
+            min_production_max,
+            max_production_max,
+            min_production_min,
+            max_production_min,
             min_connection_losses,
             max_connection_losses,
-            inlet_reservoir,
             external_id_prefix,
             space,
             filter,
         )
-        external_ids = _retrieve_timeseries_external_ids_with_extra_feeding_fee_time_series(
+        external_ids = _retrieve_timeseries_external_ids_with_extra_head_direct_time_series(
             self._client, self._view_id, filter_, limit
         )
         if external_ids:
             return self._client.time_series.retrieve_multiple(external_ids=list(external_ids))
         else:
             return TimeSeriesList([])
 
 
-def _retrieve_timeseries_external_ids_with_extra_feeding_fee_time_series(
+def _retrieve_timeseries_external_ids_with_extra_head_direct_time_series(
     client: CogniteClient,
     view_id: dm.ViewId,
     filter_: dm.Filter | None,
     limit: int,
-    extra_properties: ColumnNames | list[ColumnNames] = "feedingFeeTimeSeries",
+    extra_properties: ColumnNames | list[ColumnNames] = "headDirectTimeSeries",
 ) -> dict[str, list[str]]:
     limit = float("inf") if limit is None or limit == -1 else limit
-    properties = ["feedingFeeTimeSeries"]
-    if extra_properties == "feedingFeeTimeSeries":
+    properties = ["headDirectTimeSeries"]
+    if extra_properties == "headDirectTimeSeries":
         ...
-    elif isinstance(extra_properties, str) and extra_properties != "feedingFeeTimeSeries":
+    elif isinstance(extra_properties, str) and extra_properties != "headDirectTimeSeries":
         properties.append(extra_properties)
     elif isinstance(extra_properties, list):
-        properties.extend([prop for prop in extra_properties if prop != "feedingFeeTimeSeries"])
+        properties.extend([prop for prop in extra_properties if prop != "headDirectTimeSeries"])
     else:
         raise ValueError(f"Invalid value for extra_properties: {extra_properties}")
 
     if isinstance(extra_properties, str):
         extra_list = [extra_properties]
     else:
         extra_list = extra_properties
     has_data = dm.filters.HasData(views=[view_id])
-    has_property = dm.filters.Exists(property=view_id.as_property_ref("feedingFeeTimeSeries"))
+    has_property = dm.filters.Exists(property=view_id.as_property_ref("headDirectTimeSeries"))
     filter_ = dm.filters.And(filter_, has_data, has_property) if filter_ else dm.filters.And(has_data, has_property)
 
     cursor = None
     external_ids: dict[str, list[str]] = {}
     total_retrieved = 0
     while True:
         query_limit = max(min(INSTANCE_QUERY_LIMIT, limit - total_retrieved), 0)
@@ -607,15 +608,15 @@
                     [dm.query.SourceSelector(view_id, properties)],
                 )
             },
             cursors={"nodes": cursor},
         )
         result = client.data_modeling.instances.query(query)
         batch_external_ids = {
-            node.properties[view_id]["feedingFeeTimeSeries"]: [
+            node.properties[view_id]["headDirectTimeSeries"]: [
                 node.properties[view_id].get(prop, "") for prop in extra_list
             ]
             for node in result.data["nodes"].data
         }
         total_retrieved += len(batch_external_ids)
         external_ids.update(batch_external_ids)
         cursor = result.cursors["nodes"]
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/plant_generators.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/plant_generators.py`

 * *Files 2% similar despite different names*

```diff
@@ -39,15 +39,15 @@
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
                 >>> plant = client.plant.generators_edge.list("my_plant", limit=5)
 
         """
         filter_ = _create_edge_filter(
-            dm.DirectRelationReference("sp_powerops_types", "isSubAssetOf"),
+            dm.DirectRelationReference("sp_powerops_types_temp", "isSubAssetOf"),
             from_plant,
             from_plant_space,
             to_generator,
             to_generator_space,
             external_id_prefix,
             space,
         )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/plant_head_direct_time_series.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/plant_p_min_time_series.py`

 * *Files 4% similar despite different names*

```diff
@@ -28,15 +28,15 @@
     "feedingFeeTimeSeries",
     "outletLevelTimeSeries",
     "inletLevelTimeSeries",
     "headDirectTimeSeries",
 ]
 
 
-class PlantHeadDirectTimeSeriesQuery:
+class PlantPMinTimeSeriesQuery:
     def __init__(
         self,
         client: CogniteClient,
         view_id: dm.ViewId,
         timeseries_limit: int = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ):
@@ -53,15 +53,15 @@
         aggregates: Aggregate | list[Aggregate] | None = None,
         granularity: str | None = None,
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         limit: int | None = None,
         include_outside_points: bool = False,
     ) -> DatapointsList:
-        """`Retrieve datapoints for the `plant.head_direct_time_series` timeseries.
+        """`Retrieve datapoints for the `plant.p_min_time_series` timeseries.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -78,19 +78,19 @@
 
         Returns:
             A ``DatapointsList`` with the requested datapoints.
 
         Examples:
 
             In this example,
-            we are using the time-ago format to get raw data for the 'my_head_direct_time_series' from 2 weeks ago up until now::
+            we are using the time-ago format to get raw data for the 'my_p_min_time_series' from 2 weeks ago up until now::
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> plant_datapoints = client.plant.head_direct_time_series(external_id="my_head_direct_time_series").retrieve(start="2w-ago")
+                >>> plant_datapoints = client.plant.p_min_time_series(external_id="my_p_min_time_series").retrieve(start="2w-ago")
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra()
         if external_ids:
             return self._client.time_series.data.retrieve(
                 external_id=list(external_ids),
                 start=start,
                 end=end,
@@ -112,15 +112,15 @@
         aggregates: Aggregate | list[Aggregate] | None = None,
         granularity: str | None = None,
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         limit: int | None = None,
         include_outside_points: bool = False,
     ) -> DatapointsArrayList:
-        """`Retrieve numpy arrays for the `plant.head_direct_time_series` timeseries.
+        """`Retrieve numpy arrays for the `plant.p_min_time_series` timeseries.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -137,19 +137,19 @@
 
         Returns:
             A ``DatapointsArrayList`` with the requested datapoints.
 
         Examples:
 
             In this example,
-            we are using the time-ago format to get raw data for the 'my_head_direct_time_series' from 2 weeks ago up until now::
+            we are using the time-ago format to get raw data for the 'my_p_min_time_series' from 2 weeks ago up until now::
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> plant_datapoints = client.plant.head_direct_time_series(external_id="my_head_direct_time_series").retrieve_array(start="2w-ago")
+                >>> plant_datapoints = client.plant.p_min_time_series(external_id="my_p_min_time_series").retrieve_array(start="2w-ago")
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra()
         if external_ids:
             return self._client.time_series.data.retrieve_arrays(
                 external_id=list(external_ids),
                 start=start,
                 end=end,
@@ -173,17 +173,17 @@
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         limit: int | None = None,
         include_outside_points: bool = False,
         uniform_index: bool = False,
         include_aggregate_name: bool = True,
         include_granularity_name: bool = False,
-        column_names: ColumnNames | list[ColumnNames] = "headDirectTimeSeries",
+        column_names: ColumnNames | list[ColumnNames] = "pMinTimeSeries",
     ) -> pd.DataFrame:
-        """`Retrieve DataFrames for the `plant.head_direct_time_series` timeseries.
+        """`Retrieve DataFrames for the `plant.p_min_time_series` timeseries.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -196,28 +196,28 @@
             target_unit: The unit_external_id of the data points returned. If the time series does not have an unit_external_id that can be converted to the target_unit, an error will be returned. Cannot be used with target_unit_system.
             target_unit_system: The unit system of the data points returned. Cannot be used with target_unit.
             limit: Maximum number of datapoints to return for each time series. Default: None (no limit)
             include_outside_points: Whether to include outside points. Not allowed when fetching aggregates. Default: False
             uniform_index: If only querying aggregates AND a single granularity is used, AND no limit is used, specifying `uniform_index=True` will return a dataframe with an equidistant datetime index from the earliest `start` to the latest `end` (missing values will be NaNs). If these requirements are not met, a ValueError is raised. Default: False
             include_aggregate_name: Include 'aggregate' in the column name, e.g. `my-ts|average`. Ignored for raw time series. Default: True
             include_granularity_name: Include 'granularity' in the column name, e.g. `my-ts|12h`. Added after 'aggregate' when present. Ignored for raw time series. Default: False
-            column_names: Which property to use for column names. Defauts to headDirectTimeSeries
+            column_names: Which property to use for column names. Defauts to pMinTimeSeries
 
 
         Returns:
             A ``DataFrame`` with the requested datapoints.
 
         Examples:
 
             In this example,
-            we are using the time-ago format to get raw data for the 'my_head_direct_time_series' from 2 weeks ago up until now::
+            we are using the time-ago format to get raw data for the 'my_p_min_time_series' from 2 weeks ago up until now::
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> plant_datapoints = client.plant.head_direct_time_series(external_id="my_head_direct_time_series").retrieve_dataframe(start="2w-ago")
+                >>> plant_datapoints = client.plant.p_min_time_series(external_id="my_p_min_time_series").retrieve_dataframe(start="2w-ago")
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra(column_names)
         if external_ids:
             df = self._client.time_series.data.retrieve_dataframe(
                 external_id=list(external_ids),
                 start=start,
                 end=end,
@@ -250,17 +250,17 @@
         aggregates: Aggregate | Sequence[Aggregate] | None = None,
         granularity: str | None = None,
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         uniform_index: bool = False,
         include_aggregate_name: bool = True,
         include_granularity_name: bool = False,
-        column_names: ColumnNames | list[ColumnNames] = "headDirectTimeSeries",
+        column_names: ColumnNames | list[ColumnNames] = "pMinTimeSeries",
     ) -> pd.DataFrame:
-        """Retrieve DataFrames for the `plant.head_direct_time_series` timeseries in Timezone.
+        """Retrieve DataFrames for the `plant.p_min_time_series` timeseries in Timezone.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -273,30 +273,30 @@
             target_unit: The unit_external_id of the data points returned. If the time series does not have an unit_external_id that can be converted to the target_unit, an error will be returned. Cannot be used with target_unit_system.
             target_unit_system: The unit system of the data points returned. Cannot be used with target_unit.
             limit: Maximum number of datapoints to return for each time series. Default: None (no limit)
             include_outside_points: Whether to include outside points. Not allowed when fetching aggregates. Default: False
             uniform_index: If only querying aggregates AND a single granularity is used, AND no limit is used, specifying `uniform_index=True` will return a dataframe with an equidistant datetime index from the earliest `start` to the latest `end` (missing values will be NaNs). If these requirements are not met, a ValueError is raised. Default: False
             include_aggregate_name: Include 'aggregate' in the column name, e.g. `my-ts|average`. Ignored for raw time series. Default: True
             include_granularity_name: Include 'granularity' in the column name, e.g. `my-ts|12h`. Added after 'aggregate' when present. Ignored for raw time series. Default: False
-            column_names: Which property to use for column names. Defauts to headDirectTimeSeries
+            column_names: Which property to use for column names. Defauts to pMinTimeSeries
 
 
         Returns:
             A ``DataFrame`` with the requested datapoints.
 
         Examples:
 
             In this example,
-            get weekly aggregates for the 'my_head_direct_time_series' for the first month of 2023 in Oslo time:
+            get weekly aggregates for the 'my_p_min_time_series' for the first month of 2023 in Oslo time:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> from datetime import datetime, timezone
                 >>> client = PowerOpsModelsV1Client()
-                >>> plant_datapoints = client.plant.head_direct_time_series(
-                ...     external_id="my_head_direct_time_series").retrieve_dataframe_in_timezone(
+                >>> plant_datapoints = client.plant.p_min_time_series(
+                ...     external_id="my_p_min_time_series").retrieve_dataframe_in_timezone(
                 ...         datetime(2023, 1, 1, tzinfo=ZoneInfo("Europe/Oslo")),
                 ...         datetime(2023, 1, 2, tzinfo=ZoneInfo("Europe/Oslo")),
                 ...         aggregates="average",
                 ...         granularity="1week",
                 ...     )
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra(column_names)
@@ -334,17 +334,17 @@
                 external_id=list(external_ids),
                 before=before,
             )
         else:
             return None
 
     def _retrieve_timeseries_external_ids_with_extra(
-        self, extra_properties: ColumnNames | list[ColumnNames] = "headDirectTimeSeries"
+        self, extra_properties: ColumnNames | list[ColumnNames] = "pMinTimeSeries"
     ) -> dict[str, list[str]]:
-        return _retrieve_timeseries_external_ids_with_extra_head_direct_time_series(
+        return _retrieve_timeseries_external_ids_with_extra_p_min_time_series(
             self._client,
             self._view_id,
             self._filter,
             self._timeseries_limit,
             extra_properties,
         )
 
@@ -352,28 +352,28 @@
     def _rename_columns(
         external_ids: dict[str, list[str]],
         df: pd.DataFrame,
         column_names: ColumnNames | list[ColumnNames],
         include_aggregate_name: bool,
         include_granularity_name: bool,
     ) -> pd.DataFrame:
-        if isinstance(column_names, str) and column_names == "headDirectTimeSeries":
+        if isinstance(column_names, str) and column_names == "pMinTimeSeries":
             return df
         splits = sum(included for included in [include_aggregate_name, include_granularity_name])
         if splits == 0:
             df.columns = ["-".join(external_ids[external_id]) for external_id in df.columns]
         else:
             column_parts = (col.rsplit("|", maxsplit=splits) for col in df.columns)
             df.columns = [
                 "-".join(external_ids[external_id]) + "|" + "|".join(parts) for external_id, *parts in column_parts
             ]
         return df
 
 
-class PlantHeadDirectTimeSeriesAPI:
+class PlantPMinTimeSeriesAPI:
     def __init__(self, client: CogniteClient, view_id: dm.ViewId):
         self._client = client
         self._view_id = view_id
 
     def __call__(
         self,
         name: str | list[str] | None = None,
@@ -394,16 +394,16 @@
         min_connection_losses: float | None = None,
         max_connection_losses: float | None = None,
         inlet_reservoir: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
-    ) -> PlantHeadDirectTimeSeriesQuery:
-        """Query timeseries `plant.head_direct_time_series`
+    ) -> PlantPMinTimeSeriesQuery:
+        """Query timeseries `plant.p_min_time_series`
 
         Args:
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
             display_name: The display name to filter on.
             display_name_prefix: The prefix of the display name to filter on.
             min_ordering: The minimum value of the ordering to filter on.
@@ -422,24 +422,24 @@
             inlet_reservoir: The inlet reservoir to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of plants to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
-            A query object that can be used to retrieve datapoins for the plant.head_direct_time_series timeseries
+            A query object that can be used to retrieve datapoins for the plant.p_min_time_series timeseries
             selected in this method.
 
         Examples:
 
-            Retrieve all data for 5 plant.head_direct_time_series timeseries:
+            Retrieve all data for 5 plant.p_min_time_series timeseries:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> plants = client.plant.head_direct_time_series(limit=5).retrieve()
+                >>> plants = client.plant.p_min_time_series(limit=5).retrieve()
 
         """
         filter_ = _create_plant_filter(
             self._view_id,
             name,
             name_prefix,
             display_name,
@@ -459,15 +459,15 @@
             max_connection_losses,
             inlet_reservoir,
             external_id_prefix,
             space,
             filter,
         )
 
-        return PlantHeadDirectTimeSeriesQuery(
+        return PlantPMinTimeSeriesQuery(
             client=self._client,
             view_id=self._view_id,
             timeseries_limit=limit,
             filter=filter_,
         )
 
     def list(
@@ -491,15 +491,15 @@
         max_connection_losses: float | None = None,
         inlet_reservoir: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> TimeSeriesList:
-        """List timeseries `plant.head_direct_time_series`
+        """List timeseries `plant.p_min_time_series`
 
         Args:
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
             display_name: The display name to filter on.
             display_name_prefix: The prefix of the display name to filter on.
             min_ordering: The minimum value of the ordering to filter on.
@@ -518,23 +518,23 @@
             inlet_reservoir: The inlet reservoir to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of plants to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
-            List of Timeseries plant.head_direct_time_series.
+            List of Timeseries plant.p_min_time_series.
 
         Examples:
 
-            List plant.head_direct_time_series and limit to 5:
+            List plant.p_min_time_series and limit to 5:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> plants = client.plant.head_direct_time_series.list(limit=5)
+                >>> plants = client.plant.p_min_time_series.list(limit=5)
 
         """
         filter_ = _create_plant_filter(
             self._view_id,
             name,
             name_prefix,
             display_name,
@@ -553,47 +553,47 @@
             min_connection_losses,
             max_connection_losses,
             inlet_reservoir,
             external_id_prefix,
             space,
             filter,
         )
-        external_ids = _retrieve_timeseries_external_ids_with_extra_head_direct_time_series(
+        external_ids = _retrieve_timeseries_external_ids_with_extra_p_min_time_series(
             self._client, self._view_id, filter_, limit
         )
         if external_ids:
             return self._client.time_series.retrieve_multiple(external_ids=list(external_ids))
         else:
             return TimeSeriesList([])
 
 
-def _retrieve_timeseries_external_ids_with_extra_head_direct_time_series(
+def _retrieve_timeseries_external_ids_with_extra_p_min_time_series(
     client: CogniteClient,
     view_id: dm.ViewId,
     filter_: dm.Filter | None,
     limit: int,
-    extra_properties: ColumnNames | list[ColumnNames] = "headDirectTimeSeries",
+    extra_properties: ColumnNames | list[ColumnNames] = "pMinTimeSeries",
 ) -> dict[str, list[str]]:
     limit = float("inf") if limit is None or limit == -1 else limit
-    properties = ["headDirectTimeSeries"]
-    if extra_properties == "headDirectTimeSeries":
+    properties = ["pMinTimeSeries"]
+    if extra_properties == "pMinTimeSeries":
         ...
-    elif isinstance(extra_properties, str) and extra_properties != "headDirectTimeSeries":
+    elif isinstance(extra_properties, str) and extra_properties != "pMinTimeSeries":
         properties.append(extra_properties)
     elif isinstance(extra_properties, list):
-        properties.extend([prop for prop in extra_properties if prop != "headDirectTimeSeries"])
+        properties.extend([prop for prop in extra_properties if prop != "pMinTimeSeries"])
     else:
         raise ValueError(f"Invalid value for extra_properties: {extra_properties}")
 
     if isinstance(extra_properties, str):
         extra_list = [extra_properties]
     else:
         extra_list = extra_properties
     has_data = dm.filters.HasData(views=[view_id])
-    has_property = dm.filters.Exists(property=view_id.as_property_ref("headDirectTimeSeries"))
+    has_property = dm.filters.Exists(property=view_id.as_property_ref("pMinTimeSeries"))
     filter_ = dm.filters.And(filter_, has_data, has_property) if filter_ else dm.filters.And(has_data, has_property)
 
     cursor = None
     external_ids: dict[str, list[str]] = {}
     total_retrieved = 0
     while True:
         query_limit = max(min(INSTANCE_QUERY_LIMIT, limit - total_retrieved), 0)
@@ -607,17 +607,15 @@
                     [dm.query.SourceSelector(view_id, properties)],
                 )
             },
             cursors={"nodes": cursor},
         )
         result = client.data_modeling.instances.query(query)
         batch_external_ids = {
-            node.properties[view_id]["headDirectTimeSeries"]: [
-                node.properties[view_id].get(prop, "") for prop in extra_list
-            ]
+            node.properties[view_id]["pMinTimeSeries"]: [node.properties[view_id].get(prop, "") for prop in extra_list]
             for node in result.data["nodes"].data
         }
         total_retrieved += len(batch_external_ids)
         external_ids.update(batch_external_ids)
         cursor = result.cursors["nodes"]
         if total_retrieved >= limit or cursor is None:
             break
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/plant_inlet_level_time_series.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/plant_p_max_time_series.py`

 * *Files 6% similar despite different names*

```diff
@@ -28,15 +28,15 @@
     "feedingFeeTimeSeries",
     "outletLevelTimeSeries",
     "inletLevelTimeSeries",
     "headDirectTimeSeries",
 ]
 
 
-class PlantInletLevelTimeSeriesQuery:
+class PlantPMaxTimeSeriesQuery:
     def __init__(
         self,
         client: CogniteClient,
         view_id: dm.ViewId,
         timeseries_limit: int = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ):
@@ -53,15 +53,15 @@
         aggregates: Aggregate | list[Aggregate] | None = None,
         granularity: str | None = None,
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         limit: int | None = None,
         include_outside_points: bool = False,
     ) -> DatapointsList:
-        """`Retrieve datapoints for the `plant.inlet_level_time_series` timeseries.
+        """`Retrieve datapoints for the `plant.p_max_time_series` timeseries.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -78,19 +78,19 @@
 
         Returns:
             A ``DatapointsList`` with the requested datapoints.
 
         Examples:
 
             In this example,
-            we are using the time-ago format to get raw data for the 'my_inlet_level_time_series' from 2 weeks ago up until now::
+            we are using the time-ago format to get raw data for the 'my_p_max_time_series' from 2 weeks ago up until now::
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> plant_datapoints = client.plant.inlet_level_time_series(external_id="my_inlet_level_time_series").retrieve(start="2w-ago")
+                >>> plant_datapoints = client.plant.p_max_time_series(external_id="my_p_max_time_series").retrieve(start="2w-ago")
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra()
         if external_ids:
             return self._client.time_series.data.retrieve(
                 external_id=list(external_ids),
                 start=start,
                 end=end,
@@ -112,15 +112,15 @@
         aggregates: Aggregate | list[Aggregate] | None = None,
         granularity: str | None = None,
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         limit: int | None = None,
         include_outside_points: bool = False,
     ) -> DatapointsArrayList:
-        """`Retrieve numpy arrays for the `plant.inlet_level_time_series` timeseries.
+        """`Retrieve numpy arrays for the `plant.p_max_time_series` timeseries.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -137,19 +137,19 @@
 
         Returns:
             A ``DatapointsArrayList`` with the requested datapoints.
 
         Examples:
 
             In this example,
-            we are using the time-ago format to get raw data for the 'my_inlet_level_time_series' from 2 weeks ago up until now::
+            we are using the time-ago format to get raw data for the 'my_p_max_time_series' from 2 weeks ago up until now::
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> plant_datapoints = client.plant.inlet_level_time_series(external_id="my_inlet_level_time_series").retrieve_array(start="2w-ago")
+                >>> plant_datapoints = client.plant.p_max_time_series(external_id="my_p_max_time_series").retrieve_array(start="2w-ago")
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra()
         if external_ids:
             return self._client.time_series.data.retrieve_arrays(
                 external_id=list(external_ids),
                 start=start,
                 end=end,
@@ -173,17 +173,17 @@
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         limit: int | None = None,
         include_outside_points: bool = False,
         uniform_index: bool = False,
         include_aggregate_name: bool = True,
         include_granularity_name: bool = False,
-        column_names: ColumnNames | list[ColumnNames] = "inletLevelTimeSeries",
+        column_names: ColumnNames | list[ColumnNames] = "pMaxTimeSeries",
     ) -> pd.DataFrame:
-        """`Retrieve DataFrames for the `plant.inlet_level_time_series` timeseries.
+        """`Retrieve DataFrames for the `plant.p_max_time_series` timeseries.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -196,28 +196,28 @@
             target_unit: The unit_external_id of the data points returned. If the time series does not have an unit_external_id that can be converted to the target_unit, an error will be returned. Cannot be used with target_unit_system.
             target_unit_system: The unit system of the data points returned. Cannot be used with target_unit.
             limit: Maximum number of datapoints to return for each time series. Default: None (no limit)
             include_outside_points: Whether to include outside points. Not allowed when fetching aggregates. Default: False
             uniform_index: If only querying aggregates AND a single granularity is used, AND no limit is used, specifying `uniform_index=True` will return a dataframe with an equidistant datetime index from the earliest `start` to the latest `end` (missing values will be NaNs). If these requirements are not met, a ValueError is raised. Default: False
             include_aggregate_name: Include 'aggregate' in the column name, e.g. `my-ts|average`. Ignored for raw time series. Default: True
             include_granularity_name: Include 'granularity' in the column name, e.g. `my-ts|12h`. Added after 'aggregate' when present. Ignored for raw time series. Default: False
-            column_names: Which property to use for column names. Defauts to inletLevelTimeSeries
+            column_names: Which property to use for column names. Defauts to pMaxTimeSeries
 
 
         Returns:
             A ``DataFrame`` with the requested datapoints.
 
         Examples:
 
             In this example,
-            we are using the time-ago format to get raw data for the 'my_inlet_level_time_series' from 2 weeks ago up until now::
+            we are using the time-ago format to get raw data for the 'my_p_max_time_series' from 2 weeks ago up until now::
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> plant_datapoints = client.plant.inlet_level_time_series(external_id="my_inlet_level_time_series").retrieve_dataframe(start="2w-ago")
+                >>> plant_datapoints = client.plant.p_max_time_series(external_id="my_p_max_time_series").retrieve_dataframe(start="2w-ago")
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra(column_names)
         if external_ids:
             df = self._client.time_series.data.retrieve_dataframe(
                 external_id=list(external_ids),
                 start=start,
                 end=end,
@@ -250,17 +250,17 @@
         aggregates: Aggregate | Sequence[Aggregate] | None = None,
         granularity: str | None = None,
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         uniform_index: bool = False,
         include_aggregate_name: bool = True,
         include_granularity_name: bool = False,
-        column_names: ColumnNames | list[ColumnNames] = "inletLevelTimeSeries",
+        column_names: ColumnNames | list[ColumnNames] = "pMaxTimeSeries",
     ) -> pd.DataFrame:
-        """Retrieve DataFrames for the `plant.inlet_level_time_series` timeseries in Timezone.
+        """Retrieve DataFrames for the `plant.p_max_time_series` timeseries in Timezone.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -273,30 +273,30 @@
             target_unit: The unit_external_id of the data points returned. If the time series does not have an unit_external_id that can be converted to the target_unit, an error will be returned. Cannot be used with target_unit_system.
             target_unit_system: The unit system of the data points returned. Cannot be used with target_unit.
             limit: Maximum number of datapoints to return for each time series. Default: None (no limit)
             include_outside_points: Whether to include outside points. Not allowed when fetching aggregates. Default: False
             uniform_index: If only querying aggregates AND a single granularity is used, AND no limit is used, specifying `uniform_index=True` will return a dataframe with an equidistant datetime index from the earliest `start` to the latest `end` (missing values will be NaNs). If these requirements are not met, a ValueError is raised. Default: False
             include_aggregate_name: Include 'aggregate' in the column name, e.g. `my-ts|average`. Ignored for raw time series. Default: True
             include_granularity_name: Include 'granularity' in the column name, e.g. `my-ts|12h`. Added after 'aggregate' when present. Ignored for raw time series. Default: False
-            column_names: Which property to use for column names. Defauts to inletLevelTimeSeries
+            column_names: Which property to use for column names. Defauts to pMaxTimeSeries
 
 
         Returns:
             A ``DataFrame`` with the requested datapoints.
 
         Examples:
 
             In this example,
-            get weekly aggregates for the 'my_inlet_level_time_series' for the first month of 2023 in Oslo time:
+            get weekly aggregates for the 'my_p_max_time_series' for the first month of 2023 in Oslo time:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> from datetime import datetime, timezone
                 >>> client = PowerOpsModelsV1Client()
-                >>> plant_datapoints = client.plant.inlet_level_time_series(
-                ...     external_id="my_inlet_level_time_series").retrieve_dataframe_in_timezone(
+                >>> plant_datapoints = client.plant.p_max_time_series(
+                ...     external_id="my_p_max_time_series").retrieve_dataframe_in_timezone(
                 ...         datetime(2023, 1, 1, tzinfo=ZoneInfo("Europe/Oslo")),
                 ...         datetime(2023, 1, 2, tzinfo=ZoneInfo("Europe/Oslo")),
                 ...         aggregates="average",
                 ...         granularity="1week",
                 ...     )
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra(column_names)
@@ -334,17 +334,17 @@
                 external_id=list(external_ids),
                 before=before,
             )
         else:
             return None
 
     def _retrieve_timeseries_external_ids_with_extra(
-        self, extra_properties: ColumnNames | list[ColumnNames] = "inletLevelTimeSeries"
+        self, extra_properties: ColumnNames | list[ColumnNames] = "pMaxTimeSeries"
     ) -> dict[str, list[str]]:
-        return _retrieve_timeseries_external_ids_with_extra_inlet_level_time_series(
+        return _retrieve_timeseries_external_ids_with_extra_p_max_time_series(
             self._client,
             self._view_id,
             self._filter,
             self._timeseries_limit,
             extra_properties,
         )
 
@@ -352,28 +352,28 @@
     def _rename_columns(
         external_ids: dict[str, list[str]],
         df: pd.DataFrame,
         column_names: ColumnNames | list[ColumnNames],
         include_aggregate_name: bool,
         include_granularity_name: bool,
     ) -> pd.DataFrame:
-        if isinstance(column_names, str) and column_names == "inletLevelTimeSeries":
+        if isinstance(column_names, str) and column_names == "pMaxTimeSeries":
             return df
         splits = sum(included for included in [include_aggregate_name, include_granularity_name])
         if splits == 0:
             df.columns = ["-".join(external_ids[external_id]) for external_id in df.columns]
         else:
             column_parts = (col.rsplit("|", maxsplit=splits) for col in df.columns)
             df.columns = [
                 "-".join(external_ids[external_id]) + "|" + "|".join(parts) for external_id, *parts in column_parts
             ]
         return df
 
 
-class PlantInletLevelTimeSeriesAPI:
+class PlantPMaxTimeSeriesAPI:
     def __init__(self, client: CogniteClient, view_id: dm.ViewId):
         self._client = client
         self._view_id = view_id
 
     def __call__(
         self,
         name: str | list[str] | None = None,
@@ -394,16 +394,16 @@
         min_connection_losses: float | None = None,
         max_connection_losses: float | None = None,
         inlet_reservoir: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
-    ) -> PlantInletLevelTimeSeriesQuery:
-        """Query timeseries `plant.inlet_level_time_series`
+    ) -> PlantPMaxTimeSeriesQuery:
+        """Query timeseries `plant.p_max_time_series`
 
         Args:
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
             display_name: The display name to filter on.
             display_name_prefix: The prefix of the display name to filter on.
             min_ordering: The minimum value of the ordering to filter on.
@@ -422,24 +422,24 @@
             inlet_reservoir: The inlet reservoir to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of plants to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
-            A query object that can be used to retrieve datapoins for the plant.inlet_level_time_series timeseries
+            A query object that can be used to retrieve datapoins for the plant.p_max_time_series timeseries
             selected in this method.
 
         Examples:
 
-            Retrieve all data for 5 plant.inlet_level_time_series timeseries:
+            Retrieve all data for 5 plant.p_max_time_series timeseries:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> plants = client.plant.inlet_level_time_series(limit=5).retrieve()
+                >>> plants = client.plant.p_max_time_series(limit=5).retrieve()
 
         """
         filter_ = _create_plant_filter(
             self._view_id,
             name,
             name_prefix,
             display_name,
@@ -459,15 +459,15 @@
             max_connection_losses,
             inlet_reservoir,
             external_id_prefix,
             space,
             filter,
         )
 
-        return PlantInletLevelTimeSeriesQuery(
+        return PlantPMaxTimeSeriesQuery(
             client=self._client,
             view_id=self._view_id,
             timeseries_limit=limit,
             filter=filter_,
         )
 
     def list(
@@ -491,15 +491,15 @@
         max_connection_losses: float | None = None,
         inlet_reservoir: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> TimeSeriesList:
-        """List timeseries `plant.inlet_level_time_series`
+        """List timeseries `plant.p_max_time_series`
 
         Args:
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
             display_name: The display name to filter on.
             display_name_prefix: The prefix of the display name to filter on.
             min_ordering: The minimum value of the ordering to filter on.
@@ -518,23 +518,23 @@
             inlet_reservoir: The inlet reservoir to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of plants to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
-            List of Timeseries plant.inlet_level_time_series.
+            List of Timeseries plant.p_max_time_series.
 
         Examples:
 
-            List plant.inlet_level_time_series and limit to 5:
+            List plant.p_max_time_series and limit to 5:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> plants = client.plant.inlet_level_time_series.list(limit=5)
+                >>> plants = client.plant.p_max_time_series.list(limit=5)
 
         """
         filter_ = _create_plant_filter(
             self._view_id,
             name,
             name_prefix,
             display_name,
@@ -553,47 +553,47 @@
             min_connection_losses,
             max_connection_losses,
             inlet_reservoir,
             external_id_prefix,
             space,
             filter,
         )
-        external_ids = _retrieve_timeseries_external_ids_with_extra_inlet_level_time_series(
+        external_ids = _retrieve_timeseries_external_ids_with_extra_p_max_time_series(
             self._client, self._view_id, filter_, limit
         )
         if external_ids:
             return self._client.time_series.retrieve_multiple(external_ids=list(external_ids))
         else:
             return TimeSeriesList([])
 
 
-def _retrieve_timeseries_external_ids_with_extra_inlet_level_time_series(
+def _retrieve_timeseries_external_ids_with_extra_p_max_time_series(
     client: CogniteClient,
     view_id: dm.ViewId,
     filter_: dm.Filter | None,
     limit: int,
-    extra_properties: ColumnNames | list[ColumnNames] = "inletLevelTimeSeries",
+    extra_properties: ColumnNames | list[ColumnNames] = "pMaxTimeSeries",
 ) -> dict[str, list[str]]:
     limit = float("inf") if limit is None or limit == -1 else limit
-    properties = ["inletLevelTimeSeries"]
-    if extra_properties == "inletLevelTimeSeries":
+    properties = ["pMaxTimeSeries"]
+    if extra_properties == "pMaxTimeSeries":
         ...
-    elif isinstance(extra_properties, str) and extra_properties != "inletLevelTimeSeries":
+    elif isinstance(extra_properties, str) and extra_properties != "pMaxTimeSeries":
         properties.append(extra_properties)
     elif isinstance(extra_properties, list):
-        properties.extend([prop for prop in extra_properties if prop != "inletLevelTimeSeries"])
+        properties.extend([prop for prop in extra_properties if prop != "pMaxTimeSeries"])
     else:
         raise ValueError(f"Invalid value for extra_properties: {extra_properties}")
 
     if isinstance(extra_properties, str):
         extra_list = [extra_properties]
     else:
         extra_list = extra_properties
     has_data = dm.filters.HasData(views=[view_id])
-    has_property = dm.filters.Exists(property=view_id.as_property_ref("inletLevelTimeSeries"))
+    has_property = dm.filters.Exists(property=view_id.as_property_ref("pMaxTimeSeries"))
     filter_ = dm.filters.And(filter_, has_data, has_property) if filter_ else dm.filters.And(has_data, has_property)
 
     cursor = None
     external_ids: dict[str, list[str]] = {}
     total_retrieved = 0
     while True:
         query_limit = max(min(INSTANCE_QUERY_LIMIT, limit - total_retrieved), 0)
@@ -607,17 +607,15 @@
                     [dm.query.SourceSelector(view_id, properties)],
                 )
             },
             cursors={"nodes": cursor},
         )
         result = client.data_modeling.instances.query(query)
         batch_external_ids = {
-            node.properties[view_id]["inletLevelTimeSeries"]: [
-                node.properties[view_id].get(prop, "") for prop in extra_list
-            ]
+            node.properties[view_id]["pMaxTimeSeries"]: [node.properties[view_id].get(prop, "") for prop in extra_list]
             for node in result.data["nodes"].data
         }
         total_retrieved += len(batch_external_ids)
         external_ids.update(batch_external_ids)
         cursor = result.cursors["nodes"]
         if total_retrieved >= limit or cursor is None:
             break
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/plant_outlet_level_time_series.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/plant_outlet_level_time_series.py`

 * *Files 6% similar despite different names*

```diff
@@ -12,22 +12,23 @@
 from cognite.powerops.client._generated.v1.data_classes._plant import _create_plant_filter
 from ._core import DEFAULT_LIMIT_READ, INSTANCE_QUERY_LIMIT
 
 ColumnNames = Literal[
     "name",
     "displayName",
     "ordering",
+    "assetType",
     "headLossFactor",
     "outletLevel",
-    "pMax",
-    "pMin",
+    "productionMax",
+    "productionMin",
     "penstockHeadLossFactors",
     "connectionLosses",
-    "pMaxTimeSeries",
-    "pMinTimeSeries",
+    "productionMaxTimeSeries",
+    "productionMinTimeSeries",
     "waterValueTimeSeries",
     "feedingFeeTimeSeries",
     "outletLevelTimeSeries",
     "inletLevelTimeSeries",
     "headDirectTimeSeries",
 ]
 
@@ -378,52 +379,52 @@
         self,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
         display_name: str | list[str] | None = None,
         display_name_prefix: str | None = None,
         min_ordering: int | None = None,
         max_ordering: int | None = None,
+        asset_type: str | list[str] | None = None,
+        asset_type_prefix: str | None = None,
         min_head_loss_factor: float | None = None,
         max_head_loss_factor: float | None = None,
         min_outlet_level: float | None = None,
         max_outlet_level: float | None = None,
-        min_p_max: float | None = None,
-        max_p_max: float | None = None,
-        min_p_min: float | None = None,
-        max_p_min: float | None = None,
-        watercourse: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        min_production_max: float | None = None,
+        max_production_max: float | None = None,
+        min_production_min: float | None = None,
+        max_production_min: float | None = None,
         min_connection_losses: float | None = None,
         max_connection_losses: float | None = None,
-        inlet_reservoir: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> PlantOutletLevelTimeSeriesQuery:
         """Query timeseries `plant.outlet_level_time_series`
 
         Args:
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
             display_name: The display name to filter on.
             display_name_prefix: The prefix of the display name to filter on.
             min_ordering: The minimum value of the ordering to filter on.
             max_ordering: The maximum value of the ordering to filter on.
+            asset_type: The asset type to filter on.
+            asset_type_prefix: The prefix of the asset type to filter on.
             min_head_loss_factor: The minimum value of the head loss factor to filter on.
             max_head_loss_factor: The maximum value of the head loss factor to filter on.
             min_outlet_level: The minimum value of the outlet level to filter on.
             max_outlet_level: The maximum value of the outlet level to filter on.
-            min_p_max: The minimum value of the p max to filter on.
-            max_p_max: The maximum value of the p max to filter on.
-            min_p_min: The minimum value of the p min to filter on.
-            max_p_min: The maximum value of the p min to filter on.
-            watercourse: The watercourse to filter on.
+            min_production_max: The minimum value of the production max to filter on.
+            max_production_max: The maximum value of the production max to filter on.
+            min_production_min: The minimum value of the production min to filter on.
+            max_production_min: The maximum value of the production min to filter on.
             min_connection_losses: The minimum value of the connection loss to filter on.
             max_connection_losses: The maximum value of the connection loss to filter on.
-            inlet_reservoir: The inlet reservoir to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of plants to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
             A query object that can be used to retrieve datapoins for the plant.outlet_level_time_series timeseries
@@ -442,26 +443,26 @@
             self._view_id,
             name,
             name_prefix,
             display_name,
             display_name_prefix,
             min_ordering,
             max_ordering,
+            asset_type,
+            asset_type_prefix,
             min_head_loss_factor,
             max_head_loss_factor,
             min_outlet_level,
             max_outlet_level,
-            min_p_max,
-            max_p_max,
-            min_p_min,
-            max_p_min,
-            watercourse,
+            min_production_max,
+            max_production_max,
+            min_production_min,
+            max_production_min,
             min_connection_losses,
             max_connection_losses,
-            inlet_reservoir,
             external_id_prefix,
             space,
             filter,
         )
 
         return PlantOutletLevelTimeSeriesQuery(
             client=self._client,
@@ -474,52 +475,52 @@
         self,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
         display_name: str | list[str] | None = None,
         display_name_prefix: str | None = None,
         min_ordering: int | None = None,
         max_ordering: int | None = None,
+        asset_type: str | list[str] | None = None,
+        asset_type_prefix: str | None = None,
         min_head_loss_factor: float | None = None,
         max_head_loss_factor: float | None = None,
         min_outlet_level: float | None = None,
         max_outlet_level: float | None = None,
-        min_p_max: float | None = None,
-        max_p_max: float | None = None,
-        min_p_min: float | None = None,
-        max_p_min: float | None = None,
-        watercourse: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        min_production_max: float | None = None,
+        max_production_max: float | None = None,
+        min_production_min: float | None = None,
+        max_production_min: float | None = None,
         min_connection_losses: float | None = None,
         max_connection_losses: float | None = None,
-        inlet_reservoir: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> TimeSeriesList:
         """List timeseries `plant.outlet_level_time_series`
 
         Args:
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
             display_name: The display name to filter on.
             display_name_prefix: The prefix of the display name to filter on.
             min_ordering: The minimum value of the ordering to filter on.
             max_ordering: The maximum value of the ordering to filter on.
+            asset_type: The asset type to filter on.
+            asset_type_prefix: The prefix of the asset type to filter on.
             min_head_loss_factor: The minimum value of the head loss factor to filter on.
             max_head_loss_factor: The maximum value of the head loss factor to filter on.
             min_outlet_level: The minimum value of the outlet level to filter on.
             max_outlet_level: The maximum value of the outlet level to filter on.
-            min_p_max: The minimum value of the p max to filter on.
-            max_p_max: The maximum value of the p max to filter on.
-            min_p_min: The minimum value of the p min to filter on.
-            max_p_min: The maximum value of the p min to filter on.
-            watercourse: The watercourse to filter on.
+            min_production_max: The minimum value of the production max to filter on.
+            max_production_max: The maximum value of the production max to filter on.
+            min_production_min: The minimum value of the production min to filter on.
+            max_production_min: The maximum value of the production min to filter on.
             min_connection_losses: The minimum value of the connection loss to filter on.
             max_connection_losses: The maximum value of the connection loss to filter on.
-            inlet_reservoir: The inlet reservoir to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of plants to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
             List of Timeseries plant.outlet_level_time_series.
@@ -537,26 +538,26 @@
             self._view_id,
             name,
             name_prefix,
             display_name,
             display_name_prefix,
             min_ordering,
             max_ordering,
+            asset_type,
+            asset_type_prefix,
             min_head_loss_factor,
             max_head_loss_factor,
             min_outlet_level,
             max_outlet_level,
-            min_p_max,
-            max_p_max,
-            min_p_min,
-            max_p_min,
-            watercourse,
+            min_production_max,
+            max_production_max,
+            min_production_min,
+            max_production_min,
             min_connection_losses,
             max_connection_losses,
-            inlet_reservoir,
             external_id_prefix,
             space,
             filter,
         )
         external_ids = _retrieve_timeseries_external_ids_with_extra_outlet_level_time_series(
             self._client, self._view_id, filter_, limit
         )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/plant_p_max_time_series.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/generator_availability_time_series.py`

 * *Files 6% similar despite different names*

```diff
@@ -5,38 +5,31 @@
 from typing import Literal
 
 import pandas as pd
 from cognite.client import CogniteClient
 from cognite.client import data_modeling as dm
 from cognite.client.data_classes import Datapoints, DatapointsArrayList, DatapointsList, TimeSeriesList
 from cognite.client.data_classes.datapoints import Aggregate
-from cognite.powerops.client._generated.v1.data_classes._plant import _create_plant_filter
+from cognite.powerops.client._generated.v1.data_classes._generator import _create_generator_filter
 from ._core import DEFAULT_LIMIT_READ, INSTANCE_QUERY_LIMIT
 
 ColumnNames = Literal[
     "name",
     "displayName",
     "ordering",
-    "headLossFactor",
-    "outletLevel",
-    "pMax",
-    "pMin",
-    "penstockHeadLossFactors",
-    "connectionLosses",
-    "pMaxTimeSeries",
-    "pMinTimeSeries",
-    "waterValueTimeSeries",
-    "feedingFeeTimeSeries",
-    "outletLevelTimeSeries",
-    "inletLevelTimeSeries",
-    "headDirectTimeSeries",
+    "assetType",
+    "productionMin",
+    "penstockNumber",
+    "startCost",
+    "startStopCost",
+    "availabilityTimeSeries",
 ]
 
 
-class PlantPMaxTimeSeriesQuery:
+class GeneratorAvailabilityTimeSeriesQuery:
     def __init__(
         self,
         client: CogniteClient,
         view_id: dm.ViewId,
         timeseries_limit: int = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ):
@@ -53,15 +46,15 @@
         aggregates: Aggregate | list[Aggregate] | None = None,
         granularity: str | None = None,
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         limit: int | None = None,
         include_outside_points: bool = False,
     ) -> DatapointsList:
-        """`Retrieve datapoints for the `plant.p_max_time_series` timeseries.
+        """`Retrieve datapoints for the `generator.availability_time_series` timeseries.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -78,19 +71,19 @@
 
         Returns:
             A ``DatapointsList`` with the requested datapoints.
 
         Examples:
 
             In this example,
-            we are using the time-ago format to get raw data for the 'my_p_max_time_series' from 2 weeks ago up until now::
+            we are using the time-ago format to get raw data for the 'my_availability_time_series' from 2 weeks ago up until now::
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> plant_datapoints = client.plant.p_max_time_series(external_id="my_p_max_time_series").retrieve(start="2w-ago")
+                >>> generator_datapoints = client.generator.availability_time_series(external_id="my_availability_time_series").retrieve(start="2w-ago")
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra()
         if external_ids:
             return self._client.time_series.data.retrieve(
                 external_id=list(external_ids),
                 start=start,
                 end=end,
@@ -112,15 +105,15 @@
         aggregates: Aggregate | list[Aggregate] | None = None,
         granularity: str | None = None,
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         limit: int | None = None,
         include_outside_points: bool = False,
     ) -> DatapointsArrayList:
-        """`Retrieve numpy arrays for the `plant.p_max_time_series` timeseries.
+        """`Retrieve numpy arrays for the `generator.availability_time_series` timeseries.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -137,19 +130,19 @@
 
         Returns:
             A ``DatapointsArrayList`` with the requested datapoints.
 
         Examples:
 
             In this example,
-            we are using the time-ago format to get raw data for the 'my_p_max_time_series' from 2 weeks ago up until now::
+            we are using the time-ago format to get raw data for the 'my_availability_time_series' from 2 weeks ago up until now::
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> plant_datapoints = client.plant.p_max_time_series(external_id="my_p_max_time_series").retrieve_array(start="2w-ago")
+                >>> generator_datapoints = client.generator.availability_time_series(external_id="my_availability_time_series").retrieve_array(start="2w-ago")
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra()
         if external_ids:
             return self._client.time_series.data.retrieve_arrays(
                 external_id=list(external_ids),
                 start=start,
                 end=end,
@@ -173,17 +166,17 @@
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         limit: int | None = None,
         include_outside_points: bool = False,
         uniform_index: bool = False,
         include_aggregate_name: bool = True,
         include_granularity_name: bool = False,
-        column_names: ColumnNames | list[ColumnNames] = "pMaxTimeSeries",
+        column_names: ColumnNames | list[ColumnNames] = "availabilityTimeSeries",
     ) -> pd.DataFrame:
-        """`Retrieve DataFrames for the `plant.p_max_time_series` timeseries.
+        """`Retrieve DataFrames for the `generator.availability_time_series` timeseries.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -196,28 +189,28 @@
             target_unit: The unit_external_id of the data points returned. If the time series does not have an unit_external_id that can be converted to the target_unit, an error will be returned. Cannot be used with target_unit_system.
             target_unit_system: The unit system of the data points returned. Cannot be used with target_unit.
             limit: Maximum number of datapoints to return for each time series. Default: None (no limit)
             include_outside_points: Whether to include outside points. Not allowed when fetching aggregates. Default: False
             uniform_index: If only querying aggregates AND a single granularity is used, AND no limit is used, specifying `uniform_index=True` will return a dataframe with an equidistant datetime index from the earliest `start` to the latest `end` (missing values will be NaNs). If these requirements are not met, a ValueError is raised. Default: False
             include_aggregate_name: Include 'aggregate' in the column name, e.g. `my-ts|average`. Ignored for raw time series. Default: True
             include_granularity_name: Include 'granularity' in the column name, e.g. `my-ts|12h`. Added after 'aggregate' when present. Ignored for raw time series. Default: False
-            column_names: Which property to use for column names. Defauts to pMaxTimeSeries
+            column_names: Which property to use for column names. Defauts to availabilityTimeSeries
 
 
         Returns:
             A ``DataFrame`` with the requested datapoints.
 
         Examples:
 
             In this example,
-            we are using the time-ago format to get raw data for the 'my_p_max_time_series' from 2 weeks ago up until now::
+            we are using the time-ago format to get raw data for the 'my_availability_time_series' from 2 weeks ago up until now::
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> plant_datapoints = client.plant.p_max_time_series(external_id="my_p_max_time_series").retrieve_dataframe(start="2w-ago")
+                >>> generator_datapoints = client.generator.availability_time_series(external_id="my_availability_time_series").retrieve_dataframe(start="2w-ago")
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra(column_names)
         if external_ids:
             df = self._client.time_series.data.retrieve_dataframe(
                 external_id=list(external_ids),
                 start=start,
                 end=end,
@@ -250,17 +243,17 @@
         aggregates: Aggregate | Sequence[Aggregate] | None = None,
         granularity: str | None = None,
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         uniform_index: bool = False,
         include_aggregate_name: bool = True,
         include_granularity_name: bool = False,
-        column_names: ColumnNames | list[ColumnNames] = "pMaxTimeSeries",
+        column_names: ColumnNames | list[ColumnNames] = "availabilityTimeSeries",
     ) -> pd.DataFrame:
-        """Retrieve DataFrames for the `plant.p_max_time_series` timeseries in Timezone.
+        """Retrieve DataFrames for the `generator.availability_time_series` timeseries in Timezone.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -273,30 +266,30 @@
             target_unit: The unit_external_id of the data points returned. If the time series does not have an unit_external_id that can be converted to the target_unit, an error will be returned. Cannot be used with target_unit_system.
             target_unit_system: The unit system of the data points returned. Cannot be used with target_unit.
             limit: Maximum number of datapoints to return for each time series. Default: None (no limit)
             include_outside_points: Whether to include outside points. Not allowed when fetching aggregates. Default: False
             uniform_index: If only querying aggregates AND a single granularity is used, AND no limit is used, specifying `uniform_index=True` will return a dataframe with an equidistant datetime index from the earliest `start` to the latest `end` (missing values will be NaNs). If these requirements are not met, a ValueError is raised. Default: False
             include_aggregate_name: Include 'aggregate' in the column name, e.g. `my-ts|average`. Ignored for raw time series. Default: True
             include_granularity_name: Include 'granularity' in the column name, e.g. `my-ts|12h`. Added after 'aggregate' when present. Ignored for raw time series. Default: False
-            column_names: Which property to use for column names. Defauts to pMaxTimeSeries
+            column_names: Which property to use for column names. Defauts to availabilityTimeSeries
 
 
         Returns:
             A ``DataFrame`` with the requested datapoints.
 
         Examples:
 
             In this example,
-            get weekly aggregates for the 'my_p_max_time_series' for the first month of 2023 in Oslo time:
+            get weekly aggregates for the 'my_availability_time_series' for the first month of 2023 in Oslo time:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> from datetime import datetime, timezone
                 >>> client = PowerOpsModelsV1Client()
-                >>> plant_datapoints = client.plant.p_max_time_series(
-                ...     external_id="my_p_max_time_series").retrieve_dataframe_in_timezone(
+                >>> generator_datapoints = client.generator.availability_time_series(
+                ...     external_id="my_availability_time_series").retrieve_dataframe_in_timezone(
                 ...         datetime(2023, 1, 1, tzinfo=ZoneInfo("Europe/Oslo")),
                 ...         datetime(2023, 1, 2, tzinfo=ZoneInfo("Europe/Oslo")),
                 ...         aggregates="average",
                 ...         granularity="1week",
                 ...     )
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra(column_names)
@@ -334,17 +327,17 @@
                 external_id=list(external_ids),
                 before=before,
             )
         else:
             return None
 
     def _retrieve_timeseries_external_ids_with_extra(
-        self, extra_properties: ColumnNames | list[ColumnNames] = "pMaxTimeSeries"
+        self, extra_properties: ColumnNames | list[ColumnNames] = "availabilityTimeSeries"
     ) -> dict[str, list[str]]:
-        return _retrieve_timeseries_external_ids_with_extra_p_max_time_series(
+        return _retrieve_timeseries_external_ids_with_extra_availability_time_series(
             self._client,
             self._view_id,
             self._filter,
             self._timeseries_limit,
             extra_properties,
         )
 
@@ -352,248 +345,230 @@
     def _rename_columns(
         external_ids: dict[str, list[str]],
         df: pd.DataFrame,
         column_names: ColumnNames | list[ColumnNames],
         include_aggregate_name: bool,
         include_granularity_name: bool,
     ) -> pd.DataFrame:
-        if isinstance(column_names, str) and column_names == "pMaxTimeSeries":
+        if isinstance(column_names, str) and column_names == "availabilityTimeSeries":
             return df
         splits = sum(included for included in [include_aggregate_name, include_granularity_name])
         if splits == 0:
             df.columns = ["-".join(external_ids[external_id]) for external_id in df.columns]
         else:
             column_parts = (col.rsplit("|", maxsplit=splits) for col in df.columns)
             df.columns = [
                 "-".join(external_ids[external_id]) + "|" + "|".join(parts) for external_id, *parts in column_parts
             ]
         return df
 
 
-class PlantPMaxTimeSeriesAPI:
+class GeneratorAvailabilityTimeSeriesAPI:
     def __init__(self, client: CogniteClient, view_id: dm.ViewId):
         self._client = client
         self._view_id = view_id
 
     def __call__(
         self,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
         display_name: str | list[str] | None = None,
         display_name_prefix: str | None = None,
         min_ordering: int | None = None,
         max_ordering: int | None = None,
-        min_head_loss_factor: float | None = None,
-        max_head_loss_factor: float | None = None,
-        min_outlet_level: float | None = None,
-        max_outlet_level: float | None = None,
-        min_p_max: float | None = None,
-        max_p_max: float | None = None,
-        min_p_min: float | None = None,
-        max_p_min: float | None = None,
-        watercourse: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
-        min_connection_losses: float | None = None,
-        max_connection_losses: float | None = None,
-        inlet_reservoir: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        asset_type: str | list[str] | None = None,
+        asset_type_prefix: str | None = None,
+        min_production_min: float | None = None,
+        max_production_min: float | None = None,
+        min_penstock_number: int | None = None,
+        max_penstock_number: int | None = None,
+        min_start_cost: float | None = None,
+        max_start_cost: float | None = None,
+        generator_efficiency_curve: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
-    ) -> PlantPMaxTimeSeriesQuery:
-        """Query timeseries `plant.p_max_time_series`
+    ) -> GeneratorAvailabilityTimeSeriesQuery:
+        """Query timeseries `generator.availability_time_series`
 
         Args:
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
             display_name: The display name to filter on.
             display_name_prefix: The prefix of the display name to filter on.
             min_ordering: The minimum value of the ordering to filter on.
             max_ordering: The maximum value of the ordering to filter on.
-            min_head_loss_factor: The minimum value of the head loss factor to filter on.
-            max_head_loss_factor: The maximum value of the head loss factor to filter on.
-            min_outlet_level: The minimum value of the outlet level to filter on.
-            max_outlet_level: The maximum value of the outlet level to filter on.
-            min_p_max: The minimum value of the p max to filter on.
-            max_p_max: The maximum value of the p max to filter on.
-            min_p_min: The minimum value of the p min to filter on.
-            max_p_min: The maximum value of the p min to filter on.
-            watercourse: The watercourse to filter on.
-            min_connection_losses: The minimum value of the connection loss to filter on.
-            max_connection_losses: The maximum value of the connection loss to filter on.
-            inlet_reservoir: The inlet reservoir to filter on.
+            asset_type: The asset type to filter on.
+            asset_type_prefix: The prefix of the asset type to filter on.
+            min_production_min: The minimum value of the production min to filter on.
+            max_production_min: The maximum value of the production min to filter on.
+            min_penstock_number: The minimum value of the penstock number to filter on.
+            max_penstock_number: The maximum value of the penstock number to filter on.
+            min_start_cost: The minimum value of the start cost to filter on.
+            max_start_cost: The maximum value of the start cost to filter on.
+            generator_efficiency_curve: The generator efficiency curve to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of plants to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of generators to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
-            A query object that can be used to retrieve datapoins for the plant.p_max_time_series timeseries
+            A query object that can be used to retrieve datapoins for the generator.availability_time_series timeseries
             selected in this method.
 
         Examples:
 
-            Retrieve all data for 5 plant.p_max_time_series timeseries:
+            Retrieve all data for 5 generator.availability_time_series timeseries:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> plants = client.plant.p_max_time_series(limit=5).retrieve()
+                >>> generators = client.generator.availability_time_series(limit=5).retrieve()
 
         """
-        filter_ = _create_plant_filter(
+        filter_ = _create_generator_filter(
             self._view_id,
             name,
             name_prefix,
             display_name,
             display_name_prefix,
             min_ordering,
             max_ordering,
-            min_head_loss_factor,
-            max_head_loss_factor,
-            min_outlet_level,
-            max_outlet_level,
-            min_p_max,
-            max_p_max,
-            min_p_min,
-            max_p_min,
-            watercourse,
-            min_connection_losses,
-            max_connection_losses,
-            inlet_reservoir,
+            asset_type,
+            asset_type_prefix,
+            min_production_min,
+            max_production_min,
+            min_penstock_number,
+            max_penstock_number,
+            min_start_cost,
+            max_start_cost,
+            generator_efficiency_curve,
             external_id_prefix,
             space,
             filter,
         )
 
-        return PlantPMaxTimeSeriesQuery(
+        return GeneratorAvailabilityTimeSeriesQuery(
             client=self._client,
             view_id=self._view_id,
             timeseries_limit=limit,
             filter=filter_,
         )
 
     def list(
         self,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
         display_name: str | list[str] | None = None,
         display_name_prefix: str | None = None,
         min_ordering: int | None = None,
         max_ordering: int | None = None,
-        min_head_loss_factor: float | None = None,
-        max_head_loss_factor: float | None = None,
-        min_outlet_level: float | None = None,
-        max_outlet_level: float | None = None,
-        min_p_max: float | None = None,
-        max_p_max: float | None = None,
-        min_p_min: float | None = None,
-        max_p_min: float | None = None,
-        watercourse: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
-        min_connection_losses: float | None = None,
-        max_connection_losses: float | None = None,
-        inlet_reservoir: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        asset_type: str | list[str] | None = None,
+        asset_type_prefix: str | None = None,
+        min_production_min: float | None = None,
+        max_production_min: float | None = None,
+        min_penstock_number: int | None = None,
+        max_penstock_number: int | None = None,
+        min_start_cost: float | None = None,
+        max_start_cost: float | None = None,
+        generator_efficiency_curve: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> TimeSeriesList:
-        """List timeseries `plant.p_max_time_series`
+        """List timeseries `generator.availability_time_series`
 
         Args:
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
             display_name: The display name to filter on.
             display_name_prefix: The prefix of the display name to filter on.
             min_ordering: The minimum value of the ordering to filter on.
             max_ordering: The maximum value of the ordering to filter on.
-            min_head_loss_factor: The minimum value of the head loss factor to filter on.
-            max_head_loss_factor: The maximum value of the head loss factor to filter on.
-            min_outlet_level: The minimum value of the outlet level to filter on.
-            max_outlet_level: The maximum value of the outlet level to filter on.
-            min_p_max: The minimum value of the p max to filter on.
-            max_p_max: The maximum value of the p max to filter on.
-            min_p_min: The minimum value of the p min to filter on.
-            max_p_min: The maximum value of the p min to filter on.
-            watercourse: The watercourse to filter on.
-            min_connection_losses: The minimum value of the connection loss to filter on.
-            max_connection_losses: The maximum value of the connection loss to filter on.
-            inlet_reservoir: The inlet reservoir to filter on.
+            asset_type: The asset type to filter on.
+            asset_type_prefix: The prefix of the asset type to filter on.
+            min_production_min: The minimum value of the production min to filter on.
+            max_production_min: The maximum value of the production min to filter on.
+            min_penstock_number: The minimum value of the penstock number to filter on.
+            max_penstock_number: The maximum value of the penstock number to filter on.
+            min_start_cost: The minimum value of the start cost to filter on.
+            max_start_cost: The maximum value of the start cost to filter on.
+            generator_efficiency_curve: The generator efficiency curve to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of plants to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of generators to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
-            List of Timeseries plant.p_max_time_series.
+            List of Timeseries generator.availability_time_series.
 
         Examples:
 
-            List plant.p_max_time_series and limit to 5:
+            List generator.availability_time_series and limit to 5:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> plants = client.plant.p_max_time_series.list(limit=5)
+                >>> generators = client.generator.availability_time_series.list(limit=5)
 
         """
-        filter_ = _create_plant_filter(
+        filter_ = _create_generator_filter(
             self._view_id,
             name,
             name_prefix,
             display_name,
             display_name_prefix,
             min_ordering,
             max_ordering,
-            min_head_loss_factor,
-            max_head_loss_factor,
-            min_outlet_level,
-            max_outlet_level,
-            min_p_max,
-            max_p_max,
-            min_p_min,
-            max_p_min,
-            watercourse,
-            min_connection_losses,
-            max_connection_losses,
-            inlet_reservoir,
+            asset_type,
+            asset_type_prefix,
+            min_production_min,
+            max_production_min,
+            min_penstock_number,
+            max_penstock_number,
+            min_start_cost,
+            max_start_cost,
+            generator_efficiency_curve,
             external_id_prefix,
             space,
             filter,
         )
-        external_ids = _retrieve_timeseries_external_ids_with_extra_p_max_time_series(
+        external_ids = _retrieve_timeseries_external_ids_with_extra_availability_time_series(
             self._client, self._view_id, filter_, limit
         )
         if external_ids:
             return self._client.time_series.retrieve_multiple(external_ids=list(external_ids))
         else:
             return TimeSeriesList([])
 
 
-def _retrieve_timeseries_external_ids_with_extra_p_max_time_series(
+def _retrieve_timeseries_external_ids_with_extra_availability_time_series(
     client: CogniteClient,
     view_id: dm.ViewId,
     filter_: dm.Filter | None,
     limit: int,
-    extra_properties: ColumnNames | list[ColumnNames] = "pMaxTimeSeries",
+    extra_properties: ColumnNames | list[ColumnNames] = "availabilityTimeSeries",
 ) -> dict[str, list[str]]:
     limit = float("inf") if limit is None or limit == -1 else limit
-    properties = ["pMaxTimeSeries"]
-    if extra_properties == "pMaxTimeSeries":
+    properties = ["availabilityTimeSeries"]
+    if extra_properties == "availabilityTimeSeries":
         ...
-    elif isinstance(extra_properties, str) and extra_properties != "pMaxTimeSeries":
+    elif isinstance(extra_properties, str) and extra_properties != "availabilityTimeSeries":
         properties.append(extra_properties)
     elif isinstance(extra_properties, list):
-        properties.extend([prop for prop in extra_properties if prop != "pMaxTimeSeries"])
+        properties.extend([prop for prop in extra_properties if prop != "availabilityTimeSeries"])
     else:
         raise ValueError(f"Invalid value for extra_properties: {extra_properties}")
 
     if isinstance(extra_properties, str):
         extra_list = [extra_properties]
     else:
         extra_list = extra_properties
     has_data = dm.filters.HasData(views=[view_id])
-    has_property = dm.filters.Exists(property=view_id.as_property_ref("pMaxTimeSeries"))
+    has_property = dm.filters.Exists(property=view_id.as_property_ref("availabilityTimeSeries"))
     filter_ = dm.filters.And(filter_, has_data, has_property) if filter_ else dm.filters.And(has_data, has_property)
 
     cursor = None
     external_ids: dict[str, list[str]] = {}
     total_retrieved = 0
     while True:
         query_limit = max(min(INSTANCE_QUERY_LIMIT, limit - total_retrieved), 0)
@@ -607,15 +582,17 @@
                     [dm.query.SourceSelector(view_id, properties)],
                 )
             },
             cursors={"nodes": cursor},
         )
         result = client.data_modeling.instances.query(query)
         batch_external_ids = {
-            node.properties[view_id]["pMaxTimeSeries"]: [node.properties[view_id].get(prop, "") for prop in extra_list]
+            node.properties[view_id]["availabilityTimeSeries"]: [
+                node.properties[view_id].get(prop, "") for prop in extra_list
+            ]
             for node in result.data["nodes"].data
         }
         total_retrieved += len(batch_external_ids)
         external_ids.update(batch_external_ids)
         cursor = result.cursors["nodes"]
         if total_retrieved >= limit or cursor is None:
             break
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/plant_p_min_time_series.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/plant_production_min_time_series.py`

 * *Files 7% similar despite different names*

```diff
@@ -12,31 +12,32 @@
 from cognite.powerops.client._generated.v1.data_classes._plant import _create_plant_filter
 from ._core import DEFAULT_LIMIT_READ, INSTANCE_QUERY_LIMIT
 
 ColumnNames = Literal[
     "name",
     "displayName",
     "ordering",
+    "assetType",
     "headLossFactor",
     "outletLevel",
-    "pMax",
-    "pMin",
+    "productionMax",
+    "productionMin",
     "penstockHeadLossFactors",
     "connectionLosses",
-    "pMaxTimeSeries",
-    "pMinTimeSeries",
+    "productionMaxTimeSeries",
+    "productionMinTimeSeries",
     "waterValueTimeSeries",
     "feedingFeeTimeSeries",
     "outletLevelTimeSeries",
     "inletLevelTimeSeries",
     "headDirectTimeSeries",
 ]
 
 
-class PlantPMinTimeSeriesQuery:
+class PlantProductionMinTimeSeriesQuery:
     def __init__(
         self,
         client: CogniteClient,
         view_id: dm.ViewId,
         timeseries_limit: int = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ):
@@ -53,15 +54,15 @@
         aggregates: Aggregate | list[Aggregate] | None = None,
         granularity: str | None = None,
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         limit: int | None = None,
         include_outside_points: bool = False,
     ) -> DatapointsList:
-        """`Retrieve datapoints for the `plant.p_min_time_series` timeseries.
+        """`Retrieve datapoints for the `plant.production_min_time_series` timeseries.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -78,19 +79,19 @@
 
         Returns:
             A ``DatapointsList`` with the requested datapoints.
 
         Examples:
 
             In this example,
-            we are using the time-ago format to get raw data for the 'my_p_min_time_series' from 2 weeks ago up until now::
+            we are using the time-ago format to get raw data for the 'my_production_min_time_series' from 2 weeks ago up until now::
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> plant_datapoints = client.plant.p_min_time_series(external_id="my_p_min_time_series").retrieve(start="2w-ago")
+                >>> plant_datapoints = client.plant.production_min_time_series(external_id="my_production_min_time_series").retrieve(start="2w-ago")
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra()
         if external_ids:
             return self._client.time_series.data.retrieve(
                 external_id=list(external_ids),
                 start=start,
                 end=end,
@@ -112,15 +113,15 @@
         aggregates: Aggregate | list[Aggregate] | None = None,
         granularity: str | None = None,
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         limit: int | None = None,
         include_outside_points: bool = False,
     ) -> DatapointsArrayList:
-        """`Retrieve numpy arrays for the `plant.p_min_time_series` timeseries.
+        """`Retrieve numpy arrays for the `plant.production_min_time_series` timeseries.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -137,19 +138,19 @@
 
         Returns:
             A ``DatapointsArrayList`` with the requested datapoints.
 
         Examples:
 
             In this example,
-            we are using the time-ago format to get raw data for the 'my_p_min_time_series' from 2 weeks ago up until now::
+            we are using the time-ago format to get raw data for the 'my_production_min_time_series' from 2 weeks ago up until now::
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> plant_datapoints = client.plant.p_min_time_series(external_id="my_p_min_time_series").retrieve_array(start="2w-ago")
+                >>> plant_datapoints = client.plant.production_min_time_series(external_id="my_production_min_time_series").retrieve_array(start="2w-ago")
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra()
         if external_ids:
             return self._client.time_series.data.retrieve_arrays(
                 external_id=list(external_ids),
                 start=start,
                 end=end,
@@ -173,17 +174,17 @@
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         limit: int | None = None,
         include_outside_points: bool = False,
         uniform_index: bool = False,
         include_aggregate_name: bool = True,
         include_granularity_name: bool = False,
-        column_names: ColumnNames | list[ColumnNames] = "pMinTimeSeries",
+        column_names: ColumnNames | list[ColumnNames] = "productionMinTimeSeries",
     ) -> pd.DataFrame:
-        """`Retrieve DataFrames for the `plant.p_min_time_series` timeseries.
+        """`Retrieve DataFrames for the `plant.production_min_time_series` timeseries.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -196,28 +197,28 @@
             target_unit: The unit_external_id of the data points returned. If the time series does not have an unit_external_id that can be converted to the target_unit, an error will be returned. Cannot be used with target_unit_system.
             target_unit_system: The unit system of the data points returned. Cannot be used with target_unit.
             limit: Maximum number of datapoints to return for each time series. Default: None (no limit)
             include_outside_points: Whether to include outside points. Not allowed when fetching aggregates. Default: False
             uniform_index: If only querying aggregates AND a single granularity is used, AND no limit is used, specifying `uniform_index=True` will return a dataframe with an equidistant datetime index from the earliest `start` to the latest `end` (missing values will be NaNs). If these requirements are not met, a ValueError is raised. Default: False
             include_aggregate_name: Include 'aggregate' in the column name, e.g. `my-ts|average`. Ignored for raw time series. Default: True
             include_granularity_name: Include 'granularity' in the column name, e.g. `my-ts|12h`. Added after 'aggregate' when present. Ignored for raw time series. Default: False
-            column_names: Which property to use for column names. Defauts to pMinTimeSeries
+            column_names: Which property to use for column names. Defauts to productionMinTimeSeries
 
 
         Returns:
             A ``DataFrame`` with the requested datapoints.
 
         Examples:
 
             In this example,
-            we are using the time-ago format to get raw data for the 'my_p_min_time_series' from 2 weeks ago up until now::
+            we are using the time-ago format to get raw data for the 'my_production_min_time_series' from 2 weeks ago up until now::
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> plant_datapoints = client.plant.p_min_time_series(external_id="my_p_min_time_series").retrieve_dataframe(start="2w-ago")
+                >>> plant_datapoints = client.plant.production_min_time_series(external_id="my_production_min_time_series").retrieve_dataframe(start="2w-ago")
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra(column_names)
         if external_ids:
             df = self._client.time_series.data.retrieve_dataframe(
                 external_id=list(external_ids),
                 start=start,
                 end=end,
@@ -250,17 +251,17 @@
         aggregates: Aggregate | Sequence[Aggregate] | None = None,
         granularity: str | None = None,
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         uniform_index: bool = False,
         include_aggregate_name: bool = True,
         include_granularity_name: bool = False,
-        column_names: ColumnNames | list[ColumnNames] = "pMinTimeSeries",
+        column_names: ColumnNames | list[ColumnNames] = "productionMinTimeSeries",
     ) -> pd.DataFrame:
-        """Retrieve DataFrames for the `plant.p_min_time_series` timeseries in Timezone.
+        """Retrieve DataFrames for the `plant.production_min_time_series` timeseries in Timezone.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -273,30 +274,30 @@
             target_unit: The unit_external_id of the data points returned. If the time series does not have an unit_external_id that can be converted to the target_unit, an error will be returned. Cannot be used with target_unit_system.
             target_unit_system: The unit system of the data points returned. Cannot be used with target_unit.
             limit: Maximum number of datapoints to return for each time series. Default: None (no limit)
             include_outside_points: Whether to include outside points. Not allowed when fetching aggregates. Default: False
             uniform_index: If only querying aggregates AND a single granularity is used, AND no limit is used, specifying `uniform_index=True` will return a dataframe with an equidistant datetime index from the earliest `start` to the latest `end` (missing values will be NaNs). If these requirements are not met, a ValueError is raised. Default: False
             include_aggregate_name: Include 'aggregate' in the column name, e.g. `my-ts|average`. Ignored for raw time series. Default: True
             include_granularity_name: Include 'granularity' in the column name, e.g. `my-ts|12h`. Added after 'aggregate' when present. Ignored for raw time series. Default: False
-            column_names: Which property to use for column names. Defauts to pMinTimeSeries
+            column_names: Which property to use for column names. Defauts to productionMinTimeSeries
 
 
         Returns:
             A ``DataFrame`` with the requested datapoints.
 
         Examples:
 
             In this example,
-            get weekly aggregates for the 'my_p_min_time_series' for the first month of 2023 in Oslo time:
+            get weekly aggregates for the 'my_production_min_time_series' for the first month of 2023 in Oslo time:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> from datetime import datetime, timezone
                 >>> client = PowerOpsModelsV1Client()
-                >>> plant_datapoints = client.plant.p_min_time_series(
-                ...     external_id="my_p_min_time_series").retrieve_dataframe_in_timezone(
+                >>> plant_datapoints = client.plant.production_min_time_series(
+                ...     external_id="my_production_min_time_series").retrieve_dataframe_in_timezone(
                 ...         datetime(2023, 1, 1, tzinfo=ZoneInfo("Europe/Oslo")),
                 ...         datetime(2023, 1, 2, tzinfo=ZoneInfo("Europe/Oslo")),
                 ...         aggregates="average",
                 ...         granularity="1week",
                 ...     )
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra(column_names)
@@ -334,17 +335,17 @@
                 external_id=list(external_ids),
                 before=before,
             )
         else:
             return None
 
     def _retrieve_timeseries_external_ids_with_extra(
-        self, extra_properties: ColumnNames | list[ColumnNames] = "pMinTimeSeries"
+        self, extra_properties: ColumnNames | list[ColumnNames] = "productionMinTimeSeries"
     ) -> dict[str, list[str]]:
-        return _retrieve_timeseries_external_ids_with_extra_p_min_time_series(
+        return _retrieve_timeseries_external_ids_with_extra_production_min_time_series(
             self._client,
             self._view_id,
             self._filter,
             self._timeseries_limit,
             extra_properties,
         )
 
@@ -352,248 +353,248 @@
     def _rename_columns(
         external_ids: dict[str, list[str]],
         df: pd.DataFrame,
         column_names: ColumnNames | list[ColumnNames],
         include_aggregate_name: bool,
         include_granularity_name: bool,
     ) -> pd.DataFrame:
-        if isinstance(column_names, str) and column_names == "pMinTimeSeries":
+        if isinstance(column_names, str) and column_names == "productionMinTimeSeries":
             return df
         splits = sum(included for included in [include_aggregate_name, include_granularity_name])
         if splits == 0:
             df.columns = ["-".join(external_ids[external_id]) for external_id in df.columns]
         else:
             column_parts = (col.rsplit("|", maxsplit=splits) for col in df.columns)
             df.columns = [
                 "-".join(external_ids[external_id]) + "|" + "|".join(parts) for external_id, *parts in column_parts
             ]
         return df
 
 
-class PlantPMinTimeSeriesAPI:
+class PlantProductionMinTimeSeriesAPI:
     def __init__(self, client: CogniteClient, view_id: dm.ViewId):
         self._client = client
         self._view_id = view_id
 
     def __call__(
         self,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
         display_name: str | list[str] | None = None,
         display_name_prefix: str | None = None,
         min_ordering: int | None = None,
         max_ordering: int | None = None,
+        asset_type: str | list[str] | None = None,
+        asset_type_prefix: str | None = None,
         min_head_loss_factor: float | None = None,
         max_head_loss_factor: float | None = None,
         min_outlet_level: float | None = None,
         max_outlet_level: float | None = None,
-        min_p_max: float | None = None,
-        max_p_max: float | None = None,
-        min_p_min: float | None = None,
-        max_p_min: float | None = None,
-        watercourse: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        min_production_max: float | None = None,
+        max_production_max: float | None = None,
+        min_production_min: float | None = None,
+        max_production_min: float | None = None,
         min_connection_losses: float | None = None,
         max_connection_losses: float | None = None,
-        inlet_reservoir: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
-    ) -> PlantPMinTimeSeriesQuery:
-        """Query timeseries `plant.p_min_time_series`
+    ) -> PlantProductionMinTimeSeriesQuery:
+        """Query timeseries `plant.production_min_time_series`
 
         Args:
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
             display_name: The display name to filter on.
             display_name_prefix: The prefix of the display name to filter on.
             min_ordering: The minimum value of the ordering to filter on.
             max_ordering: The maximum value of the ordering to filter on.
+            asset_type: The asset type to filter on.
+            asset_type_prefix: The prefix of the asset type to filter on.
             min_head_loss_factor: The minimum value of the head loss factor to filter on.
             max_head_loss_factor: The maximum value of the head loss factor to filter on.
             min_outlet_level: The minimum value of the outlet level to filter on.
             max_outlet_level: The maximum value of the outlet level to filter on.
-            min_p_max: The minimum value of the p max to filter on.
-            max_p_max: The maximum value of the p max to filter on.
-            min_p_min: The minimum value of the p min to filter on.
-            max_p_min: The maximum value of the p min to filter on.
-            watercourse: The watercourse to filter on.
+            min_production_max: The minimum value of the production max to filter on.
+            max_production_max: The maximum value of the production max to filter on.
+            min_production_min: The minimum value of the production min to filter on.
+            max_production_min: The maximum value of the production min to filter on.
             min_connection_losses: The minimum value of the connection loss to filter on.
             max_connection_losses: The maximum value of the connection loss to filter on.
-            inlet_reservoir: The inlet reservoir to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of plants to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
-            A query object that can be used to retrieve datapoins for the plant.p_min_time_series timeseries
+            A query object that can be used to retrieve datapoins for the plant.production_min_time_series timeseries
             selected in this method.
 
         Examples:
 
-            Retrieve all data for 5 plant.p_min_time_series timeseries:
+            Retrieve all data for 5 plant.production_min_time_series timeseries:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> plants = client.plant.p_min_time_series(limit=5).retrieve()
+                >>> plants = client.plant.production_min_time_series(limit=5).retrieve()
 
         """
         filter_ = _create_plant_filter(
             self._view_id,
             name,
             name_prefix,
             display_name,
             display_name_prefix,
             min_ordering,
             max_ordering,
+            asset_type,
+            asset_type_prefix,
             min_head_loss_factor,
             max_head_loss_factor,
             min_outlet_level,
             max_outlet_level,
-            min_p_max,
-            max_p_max,
-            min_p_min,
-            max_p_min,
-            watercourse,
+            min_production_max,
+            max_production_max,
+            min_production_min,
+            max_production_min,
             min_connection_losses,
             max_connection_losses,
-            inlet_reservoir,
             external_id_prefix,
             space,
             filter,
         )
 
-        return PlantPMinTimeSeriesQuery(
+        return PlantProductionMinTimeSeriesQuery(
             client=self._client,
             view_id=self._view_id,
             timeseries_limit=limit,
             filter=filter_,
         )
 
     def list(
         self,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
         display_name: str | list[str] | None = None,
         display_name_prefix: str | None = None,
         min_ordering: int | None = None,
         max_ordering: int | None = None,
+        asset_type: str | list[str] | None = None,
+        asset_type_prefix: str | None = None,
         min_head_loss_factor: float | None = None,
         max_head_loss_factor: float | None = None,
         min_outlet_level: float | None = None,
         max_outlet_level: float | None = None,
-        min_p_max: float | None = None,
-        max_p_max: float | None = None,
-        min_p_min: float | None = None,
-        max_p_min: float | None = None,
-        watercourse: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        min_production_max: float | None = None,
+        max_production_max: float | None = None,
+        min_production_min: float | None = None,
+        max_production_min: float | None = None,
         min_connection_losses: float | None = None,
         max_connection_losses: float | None = None,
-        inlet_reservoir: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> TimeSeriesList:
-        """List timeseries `plant.p_min_time_series`
+        """List timeseries `plant.production_min_time_series`
 
         Args:
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
             display_name: The display name to filter on.
             display_name_prefix: The prefix of the display name to filter on.
             min_ordering: The minimum value of the ordering to filter on.
             max_ordering: The maximum value of the ordering to filter on.
+            asset_type: The asset type to filter on.
+            asset_type_prefix: The prefix of the asset type to filter on.
             min_head_loss_factor: The minimum value of the head loss factor to filter on.
             max_head_loss_factor: The maximum value of the head loss factor to filter on.
             min_outlet_level: The minimum value of the outlet level to filter on.
             max_outlet_level: The maximum value of the outlet level to filter on.
-            min_p_max: The minimum value of the p max to filter on.
-            max_p_max: The maximum value of the p max to filter on.
-            min_p_min: The minimum value of the p min to filter on.
-            max_p_min: The maximum value of the p min to filter on.
-            watercourse: The watercourse to filter on.
+            min_production_max: The minimum value of the production max to filter on.
+            max_production_max: The maximum value of the production max to filter on.
+            min_production_min: The minimum value of the production min to filter on.
+            max_production_min: The maximum value of the production min to filter on.
             min_connection_losses: The minimum value of the connection loss to filter on.
             max_connection_losses: The maximum value of the connection loss to filter on.
-            inlet_reservoir: The inlet reservoir to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of plants to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
-            List of Timeseries plant.p_min_time_series.
+            List of Timeseries plant.production_min_time_series.
 
         Examples:
 
-            List plant.p_min_time_series and limit to 5:
+            List plant.production_min_time_series and limit to 5:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> plants = client.plant.p_min_time_series.list(limit=5)
+                >>> plants = client.plant.production_min_time_series.list(limit=5)
 
         """
         filter_ = _create_plant_filter(
             self._view_id,
             name,
             name_prefix,
             display_name,
             display_name_prefix,
             min_ordering,
             max_ordering,
+            asset_type,
+            asset_type_prefix,
             min_head_loss_factor,
             max_head_loss_factor,
             min_outlet_level,
             max_outlet_level,
-            min_p_max,
-            max_p_max,
-            min_p_min,
-            max_p_min,
-            watercourse,
+            min_production_max,
+            max_production_max,
+            min_production_min,
+            max_production_min,
             min_connection_losses,
             max_connection_losses,
-            inlet_reservoir,
             external_id_prefix,
             space,
             filter,
         )
-        external_ids = _retrieve_timeseries_external_ids_with_extra_p_min_time_series(
+        external_ids = _retrieve_timeseries_external_ids_with_extra_production_min_time_series(
             self._client, self._view_id, filter_, limit
         )
         if external_ids:
             return self._client.time_series.retrieve_multiple(external_ids=list(external_ids))
         else:
             return TimeSeriesList([])
 
 
-def _retrieve_timeseries_external_ids_with_extra_p_min_time_series(
+def _retrieve_timeseries_external_ids_with_extra_production_min_time_series(
     client: CogniteClient,
     view_id: dm.ViewId,
     filter_: dm.Filter | None,
     limit: int,
-    extra_properties: ColumnNames | list[ColumnNames] = "pMinTimeSeries",
+    extra_properties: ColumnNames | list[ColumnNames] = "productionMinTimeSeries",
 ) -> dict[str, list[str]]:
     limit = float("inf") if limit is None or limit == -1 else limit
-    properties = ["pMinTimeSeries"]
-    if extra_properties == "pMinTimeSeries":
+    properties = ["productionMinTimeSeries"]
+    if extra_properties == "productionMinTimeSeries":
         ...
-    elif isinstance(extra_properties, str) and extra_properties != "pMinTimeSeries":
+    elif isinstance(extra_properties, str) and extra_properties != "productionMinTimeSeries":
         properties.append(extra_properties)
     elif isinstance(extra_properties, list):
-        properties.extend([prop for prop in extra_properties if prop != "pMinTimeSeries"])
+        properties.extend([prop for prop in extra_properties if prop != "productionMinTimeSeries"])
     else:
         raise ValueError(f"Invalid value for extra_properties: {extra_properties}")
 
     if isinstance(extra_properties, str):
         extra_list = [extra_properties]
     else:
         extra_list = extra_properties
     has_data = dm.filters.HasData(views=[view_id])
-    has_property = dm.filters.Exists(property=view_id.as_property_ref("pMinTimeSeries"))
+    has_property = dm.filters.Exists(property=view_id.as_property_ref("productionMinTimeSeries"))
     filter_ = dm.filters.And(filter_, has_data, has_property) if filter_ else dm.filters.And(has_data, has_property)
 
     cursor = None
     external_ids: dict[str, list[str]] = {}
     total_retrieved = 0
     while True:
         query_limit = max(min(INSTANCE_QUERY_LIMIT, limit - total_retrieved), 0)
@@ -607,15 +608,17 @@
                     [dm.query.SourceSelector(view_id, properties)],
                 )
             },
             cursors={"nodes": cursor},
         )
         result = client.data_modeling.instances.query(query)
         batch_external_ids = {
-            node.properties[view_id]["pMinTimeSeries"]: [node.properties[view_id].get(prop, "") for prop in extra_list]
+            node.properties[view_id]["productionMinTimeSeries"]: [
+                node.properties[view_id].get(prop, "") for prop in extra_list
+            ]
             for node in result.data["nodes"].data
         }
         total_retrieved += len(batch_external_ids)
         external_ids.update(batch_external_ids)
         cursor = result.cursors["nodes"]
         if total_retrieved >= limit or cursor is None:
             break
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/plant_shop.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/plant_shop.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/plant_shop_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/plant_shop_query.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/plant_water_value_time_series.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/plant_water_value_time_series.py`

 * *Files 4% similar despite different names*

```diff
@@ -12,22 +12,23 @@
 from cognite.powerops.client._generated.v1.data_classes._plant import _create_plant_filter
 from ._core import DEFAULT_LIMIT_READ, INSTANCE_QUERY_LIMIT
 
 ColumnNames = Literal[
     "name",
     "displayName",
     "ordering",
+    "assetType",
     "headLossFactor",
     "outletLevel",
-    "pMax",
-    "pMin",
+    "productionMax",
+    "productionMin",
     "penstockHeadLossFactors",
     "connectionLosses",
-    "pMaxTimeSeries",
-    "pMinTimeSeries",
+    "productionMaxTimeSeries",
+    "productionMinTimeSeries",
     "waterValueTimeSeries",
     "feedingFeeTimeSeries",
     "outletLevelTimeSeries",
     "inletLevelTimeSeries",
     "headDirectTimeSeries",
 ]
 
@@ -378,52 +379,52 @@
         self,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
         display_name: str | list[str] | None = None,
         display_name_prefix: str | None = None,
         min_ordering: int | None = None,
         max_ordering: int | None = None,
+        asset_type: str | list[str] | None = None,
+        asset_type_prefix: str | None = None,
         min_head_loss_factor: float | None = None,
         max_head_loss_factor: float | None = None,
         min_outlet_level: float | None = None,
         max_outlet_level: float | None = None,
-        min_p_max: float | None = None,
-        max_p_max: float | None = None,
-        min_p_min: float | None = None,
-        max_p_min: float | None = None,
-        watercourse: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        min_production_max: float | None = None,
+        max_production_max: float | None = None,
+        min_production_min: float | None = None,
+        max_production_min: float | None = None,
         min_connection_losses: float | None = None,
         max_connection_losses: float | None = None,
-        inlet_reservoir: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> PlantWaterValueTimeSeriesQuery:
         """Query timeseries `plant.water_value_time_series`
 
         Args:
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
             display_name: The display name to filter on.
             display_name_prefix: The prefix of the display name to filter on.
             min_ordering: The minimum value of the ordering to filter on.
             max_ordering: The maximum value of the ordering to filter on.
+            asset_type: The asset type to filter on.
+            asset_type_prefix: The prefix of the asset type to filter on.
             min_head_loss_factor: The minimum value of the head loss factor to filter on.
             max_head_loss_factor: The maximum value of the head loss factor to filter on.
             min_outlet_level: The minimum value of the outlet level to filter on.
             max_outlet_level: The maximum value of the outlet level to filter on.
-            min_p_max: The minimum value of the p max to filter on.
-            max_p_max: The maximum value of the p max to filter on.
-            min_p_min: The minimum value of the p min to filter on.
-            max_p_min: The maximum value of the p min to filter on.
-            watercourse: The watercourse to filter on.
+            min_production_max: The minimum value of the production max to filter on.
+            max_production_max: The maximum value of the production max to filter on.
+            min_production_min: The minimum value of the production min to filter on.
+            max_production_min: The maximum value of the production min to filter on.
             min_connection_losses: The minimum value of the connection loss to filter on.
             max_connection_losses: The maximum value of the connection loss to filter on.
-            inlet_reservoir: The inlet reservoir to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of plants to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
             A query object that can be used to retrieve datapoins for the plant.water_value_time_series timeseries
@@ -442,26 +443,26 @@
             self._view_id,
             name,
             name_prefix,
             display_name,
             display_name_prefix,
             min_ordering,
             max_ordering,
+            asset_type,
+            asset_type_prefix,
             min_head_loss_factor,
             max_head_loss_factor,
             min_outlet_level,
             max_outlet_level,
-            min_p_max,
-            max_p_max,
-            min_p_min,
-            max_p_min,
-            watercourse,
+            min_production_max,
+            max_production_max,
+            min_production_min,
+            max_production_min,
             min_connection_losses,
             max_connection_losses,
-            inlet_reservoir,
             external_id_prefix,
             space,
             filter,
         )
 
         return PlantWaterValueTimeSeriesQuery(
             client=self._client,
@@ -474,52 +475,52 @@
         self,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
         display_name: str | list[str] | None = None,
         display_name_prefix: str | None = None,
         min_ordering: int | None = None,
         max_ordering: int | None = None,
+        asset_type: str | list[str] | None = None,
+        asset_type_prefix: str | None = None,
         min_head_loss_factor: float | None = None,
         max_head_loss_factor: float | None = None,
         min_outlet_level: float | None = None,
         max_outlet_level: float | None = None,
-        min_p_max: float | None = None,
-        max_p_max: float | None = None,
-        min_p_min: float | None = None,
-        max_p_min: float | None = None,
-        watercourse: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        min_production_max: float | None = None,
+        max_production_max: float | None = None,
+        min_production_min: float | None = None,
+        max_production_min: float | None = None,
         min_connection_losses: float | None = None,
         max_connection_losses: float | None = None,
-        inlet_reservoir: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> TimeSeriesList:
         """List timeseries `plant.water_value_time_series`
 
         Args:
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
             display_name: The display name to filter on.
             display_name_prefix: The prefix of the display name to filter on.
             min_ordering: The minimum value of the ordering to filter on.
             max_ordering: The maximum value of the ordering to filter on.
+            asset_type: The asset type to filter on.
+            asset_type_prefix: The prefix of the asset type to filter on.
             min_head_loss_factor: The minimum value of the head loss factor to filter on.
             max_head_loss_factor: The maximum value of the head loss factor to filter on.
             min_outlet_level: The minimum value of the outlet level to filter on.
             max_outlet_level: The maximum value of the outlet level to filter on.
-            min_p_max: The minimum value of the p max to filter on.
-            max_p_max: The maximum value of the p max to filter on.
-            min_p_min: The minimum value of the p min to filter on.
-            max_p_min: The maximum value of the p min to filter on.
-            watercourse: The watercourse to filter on.
+            min_production_max: The minimum value of the production max to filter on.
+            max_production_max: The maximum value of the production max to filter on.
+            min_production_min: The minimum value of the production min to filter on.
+            max_production_min: The maximum value of the production min to filter on.
             min_connection_losses: The minimum value of the connection loss to filter on.
             max_connection_losses: The maximum value of the connection loss to filter on.
-            inlet_reservoir: The inlet reservoir to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of plants to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
             List of Timeseries plant.water_value_time_series.
@@ -537,26 +538,26 @@
             self._view_id,
             name,
             name_prefix,
             display_name,
             display_name_prefix,
             min_ordering,
             max_ordering,
+            asset_type,
+            asset_type_prefix,
             min_head_loss_factor,
             max_head_loss_factor,
             min_outlet_level,
             max_outlet_level,
-            min_p_max,
-            max_p_max,
-            min_p_min,
-            max_p_min,
-            watercourse,
+            min_production_max,
+            max_production_max,
+            min_production_min,
+            max_production_min,
             min_connection_losses,
             max_connection_losses,
-            inlet_reservoir,
             external_id_prefix,
             space,
             filter,
         )
         external_ids = _retrieve_timeseries_external_ids_with_extra_water_value_time_series(
             self._client, self._view_id, filter_, limit
         )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/preprocessor_input.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/preprocessor_input.py`

 * *Files 2% similar despite different names*

```diff
@@ -57,18 +57,18 @@
         min_process_step: int | None = None,
         max_process_step: int | None = None,
         function_name: str | list[str] | None = None,
         function_name_prefix: str | None = None,
         function_call_id: str | list[str] | None = None,
         function_call_id_prefix: str | None = None,
         scenario: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
-        min_shop_start: datetime.date | None = None,
-        max_shop_start: datetime.date | None = None,
-        min_shop_end: datetime.date | None = None,
-        max_shop_end: datetime.date | None = None,
+        min_shop_start: datetime.datetime | None = None,
+        max_shop_start: datetime.datetime | None = None,
+        min_shop_end: datetime.datetime | None = None,
+        max_shop_end: datetime.datetime | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_QUERY_LIMIT,
         filter: dm.Filter | None = None,
     ) -> PreprocessorInputQueryAPI[PreprocessorInputList]:
         """Query starting at preprocessor inputs.
 
@@ -229,18 +229,18 @@
         min_process_step: int | None = None,
         max_process_step: int | None = None,
         function_name: str | list[str] | None = None,
         function_name_prefix: str | None = None,
         function_call_id: str | list[str] | None = None,
         function_call_id_prefix: str | None = None,
         scenario: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
-        min_shop_start: datetime.date | None = None,
-        max_shop_start: datetime.date | None = None,
-        min_shop_end: datetime.date | None = None,
-        max_shop_end: datetime.date | None = None,
+        min_shop_start: datetime.datetime | None = None,
+        max_shop_start: datetime.datetime | None = None,
+        min_shop_end: datetime.datetime | None = None,
+        max_shop_end: datetime.datetime | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> PreprocessorInputList:
         """Search preprocessor inputs
 
@@ -316,18 +316,18 @@
         min_process_step: int | None = None,
         max_process_step: int | None = None,
         function_name: str | list[str] | None = None,
         function_name_prefix: str | None = None,
         function_call_id: str | list[str] | None = None,
         function_call_id_prefix: str | None = None,
         scenario: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
-        min_shop_start: datetime.date | None = None,
-        max_shop_start: datetime.date | None = None,
-        min_shop_end: datetime.date | None = None,
-        max_shop_end: datetime.date | None = None,
+        min_shop_start: datetime.datetime | None = None,
+        max_shop_start: datetime.datetime | None = None,
+        min_shop_end: datetime.datetime | None = None,
+        max_shop_end: datetime.datetime | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> list[dm.aggregations.AggregatedNumberedValue]: ...
 
     @overload
@@ -348,18 +348,18 @@
         min_process_step: int | None = None,
         max_process_step: int | None = None,
         function_name: str | list[str] | None = None,
         function_name_prefix: str | None = None,
         function_call_id: str | list[str] | None = None,
         function_call_id_prefix: str | None = None,
         scenario: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
-        min_shop_start: datetime.date | None = None,
-        max_shop_start: datetime.date | None = None,
-        min_shop_end: datetime.date | None = None,
-        max_shop_end: datetime.date | None = None,
+        min_shop_start: datetime.datetime | None = None,
+        max_shop_start: datetime.datetime | None = None,
+        min_shop_end: datetime.datetime | None = None,
+        max_shop_end: datetime.datetime | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> InstanceAggregationResultList: ...
 
     def aggregate(
@@ -379,18 +379,18 @@
         min_process_step: int | None = None,
         max_process_step: int | None = None,
         function_name: str | list[str] | None = None,
         function_name_prefix: str | None = None,
         function_call_id: str | list[str] | None = None,
         function_call_id_prefix: str | None = None,
         scenario: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
-        min_shop_start: datetime.date | None = None,
-        max_shop_start: datetime.date | None = None,
-        min_shop_end: datetime.date | None = None,
-        max_shop_end: datetime.date | None = None,
+        min_shop_start: datetime.datetime | None = None,
+        max_shop_start: datetime.datetime | None = None,
+        min_shop_end: datetime.datetime | None = None,
+        max_shop_end: datetime.datetime | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> list[dm.aggregations.AggregatedNumberedValue] | InstanceAggregationResultList:
         """Aggregate data across preprocessor inputs
 
@@ -473,18 +473,18 @@
         min_process_step: int | None = None,
         max_process_step: int | None = None,
         function_name: str | list[str] | None = None,
         function_name_prefix: str | None = None,
         function_call_id: str | list[str] | None = None,
         function_call_id_prefix: str | None = None,
         scenario: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
-        min_shop_start: datetime.date | None = None,
-        max_shop_start: datetime.date | None = None,
-        min_shop_end: datetime.date | None = None,
-        max_shop_end: datetime.date | None = None,
+        min_shop_start: datetime.datetime | None = None,
+        max_shop_start: datetime.datetime | None = None,
+        min_shop_end: datetime.datetime | None = None,
+        max_shop_end: datetime.datetime | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> dm.aggregations.HistogramValue:
         """Produces histograms for preprocessor inputs
 
@@ -552,18 +552,18 @@
         min_process_step: int | None = None,
         max_process_step: int | None = None,
         function_name: str | list[str] | None = None,
         function_name_prefix: str | None = None,
         function_call_id: str | list[str] | None = None,
         function_call_id_prefix: str | None = None,
         scenario: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
-        min_shop_start: datetime.date | None = None,
-        max_shop_start: datetime.date | None = None,
-        min_shop_end: datetime.date | None = None,
-        max_shop_end: datetime.date | None = None,
+        min_shop_start: datetime.datetime | None = None,
+        max_shop_start: datetime.datetime | None = None,
+        min_shop_end: datetime.datetime | None = None,
+        max_shop_end: datetime.datetime | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> PreprocessorInputList:
         """List/filter preprocessor inputs
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/preprocessor_input_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/price_production_query.py`

 * *Files 11% similar despite different names*

```diff
@@ -3,71 +3,72 @@
 import datetime
 from typing import TYPE_CHECKING
 
 from cognite.client import data_modeling as dm, CogniteClient
 
 from cognite.powerops.client._generated.v1.data_classes import (
     DomainModelCore,
-    PreprocessorInput,
-    Scenario,
+    PriceProduction,
+    SHOPResult,
 )
 from ._core import DEFAULT_QUERY_LIMIT, QueryBuilder, QueryStep, QueryAPI, T_DomainModelList, _create_edge_filter
 
 
-class PreprocessorInputQueryAPI(QueryAPI[T_DomainModelList]):
+class PriceProductionQueryAPI(QueryAPI[T_DomainModelList]):
     def __init__(
         self,
         client: CogniteClient,
         builder: QueryBuilder[T_DomainModelList],
         view_by_read_class: dict[type[DomainModelCore], dm.ViewId],
         filter_: dm.filters.Filter | None = None,
         limit: int = DEFAULT_QUERY_LIMIT,
     ):
         super().__init__(client, builder, view_by_read_class)
 
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("preprocessor_input"),
+                name=self._builder.next_name("price_production"),
                 expression=dm.query.NodeResultSetExpression(
                     from_=self._builder[-1].name if self._builder else None,
                     filter=filter_,
                 ),
-                select=dm.query.Select([dm.query.SourceSelector(self._view_by_read_class[PreprocessorInput], ["*"])]),
-                result_cls=PreprocessorInput,
+                select=dm.query.Select([dm.query.SourceSelector(self._view_by_read_class[PriceProduction], ["*"])]),
+                result_cls=PriceProduction,
                 max_retrieve_limit=limit,
             )
         )
 
     def query(
         self,
-        retrieve_scenario: bool = False,
+        retrieve_shop_result: bool = False,
     ) -> T_DomainModelList:
         """Execute query and return the result.
 
         Args:
-            retrieve_scenario: Whether to retrieve the scenario for each preprocessor input or not.
+            retrieve_shop_result: Whether to retrieve the shop result for each price production or not.
 
         Returns:
             The list of the source nodes of the query.
 
         """
         from_ = self._builder[-1].name
-        if retrieve_scenario:
-            self._query_append_scenario(from_)
+        if retrieve_shop_result:
+            self._query_append_shop_result(from_)
         return self._query()
 
-    def _query_append_scenario(self, from_: str) -> None:
-        view_id = self._view_by_read_class[Scenario]
+    def _query_append_shop_result(self, from_: str) -> None:
+        view_id = self._view_by_read_class[SHOPResult]
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("scenario"),
+                name=self._builder.next_name("shop_result"),
                 expression=dm.query.NodeResultSetExpression(
                     filter=dm.filters.HasData(views=[view_id]),
                     from_=from_,
-                    through=self._view_by_read_class[PreprocessorInput].as_property_ref("scenario"),
+                    through=self._view_by_read_class[PriceProduction].as_property_ref("shopResult"),
                     direction="outwards",
                 ),
                 select=dm.query.Select([dm.query.SourceSelector(view_id, ["*"])]),
                 max_retrieve_limit=-1,
-                result_cls=Scenario,
+                result_cls=SHOPResult,
+                is_single_direct_relation=True,
             ),
         )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/preprocessor_output.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/preprocessor_output.py`

 * *Files 0% similar despite different names*

```diff
@@ -218,17 +218,17 @@
             external_id,
             space,
             retrieve_edges=True,
             edge_api_name_type_direction_view_id_penta=[
                 (
                     self.alerts_edge,
                     "alerts",
-                    dm.DirectRelationReference("sp_powerops_types", "calculationIssue"),
+                    dm.DirectRelationReference("sp_powerops_types_temp", "calculationIssue"),
                     "outwards",
-                    dm.ViewId("sp_powerops_models", "Alert", "1"),
+                    dm.ViewId("sp_powerops_models_temp", "Alert", "1"),
                 ),
             ],
         )
 
     def search(
         self,
         query: str,
@@ -591,13 +591,13 @@
             limit=limit,
             filter=filter_,
             retrieve_edges=retrieve_edges,
             edge_api_name_type_direction_view_id_penta=[
                 (
                     self.alerts_edge,
                     "alerts",
-                    dm.DirectRelationReference("sp_powerops_types", "calculationIssue"),
+                    dm.DirectRelationReference("sp_powerops_types_temp", "calculationIssue"),
                     "outwards",
-                    dm.ViewId("sp_powerops_models", "Alert", "1"),
+                    dm.ViewId("sp_powerops_models_temp", "Alert", "1"),
                 ),
             ],
         )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/preprocessor_output_alerts.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/preprocessor_output_alerts.py`

 * *Files 0% similar despite different names*

```diff
@@ -39,15 +39,15 @@
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
                 >>> preprocessor_output = client.preprocessor_output.alerts_edge.list("my_preprocessor_output", limit=5)
 
         """
         filter_ = _create_edge_filter(
-            dm.DirectRelationReference("sp_powerops_types", "calculationIssue"),
+            dm.DirectRelationReference("sp_powerops_types_temp", "calculationIssue"),
             from_preprocessor_output,
             from_preprocessor_output_space,
             to_alert,
             to_alert_space,
             external_id_prefix,
             space,
         )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/preprocessor_output_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/assets/_api/generator_query.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,146 +1,145 @@
 from __future__ import annotations
 
 import datetime
 from typing import TYPE_CHECKING
 
 from cognite.client import data_modeling as dm, CogniteClient
 
-from cognite.powerops.client._generated.v1.data_classes import (
+from cognite.powerops.client._generated.assets.data_classes import (
     DomainModelCore,
-    PreprocessorOutput,
-    Case,
-    PreprocessorInput,
+    Generator,
+    GeneratorEfficiencyCurve,
+)
+from cognite.powerops.client._generated.assets.data_classes._turbine_efficiency_curve import (
+    TurbineEfficiencyCurve,
+    _create_turbine_efficiency_curve_filter,
 )
 from ._core import DEFAULT_QUERY_LIMIT, QueryBuilder, QueryStep, QueryAPI, T_DomainModelList, _create_edge_filter
 
 if TYPE_CHECKING:
-    from .alert_query import AlertQueryAPI
+    from .turbine_efficiency_curve_query import TurbineEfficiencyCurveQueryAPI
 
 
-class PreprocessorOutputQueryAPI(QueryAPI[T_DomainModelList]):
+class GeneratorQueryAPI(QueryAPI[T_DomainModelList]):
     def __init__(
         self,
         client: CogniteClient,
         builder: QueryBuilder[T_DomainModelList],
         view_by_read_class: dict[type[DomainModelCore], dm.ViewId],
         filter_: dm.filters.Filter | None = None,
         limit: int = DEFAULT_QUERY_LIMIT,
     ):
         super().__init__(client, builder, view_by_read_class)
 
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("preprocessor_output"),
+                name=self._builder.next_name("generator"),
                 expression=dm.query.NodeResultSetExpression(
                     from_=self._builder[-1].name if self._builder else None,
                     filter=filter_,
                 ),
-                select=dm.query.Select([dm.query.SourceSelector(self._view_by_read_class[PreprocessorOutput], ["*"])]),
-                result_cls=PreprocessorOutput,
+                select=dm.query.Select([dm.query.SourceSelector(self._view_by_read_class[Generator], ["*"])]),
+                result_cls=Generator,
                 max_retrieve_limit=limit,
             )
         )
 
-    def alerts(
+    def turbine_curves(
         self,
+        min_head: float | None = None,
+        max_head: float | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
+        external_id_prefix_edge: str | None = None,
+        space_edge: str | list[str] | None = None,
+        filter: dm.Filter | None = None,
         limit: int | None = DEFAULT_QUERY_LIMIT,
-        retrieve_case: bool = False,
-        retrieve_input_: bool = False,
-    ) -> AlertQueryAPI[T_DomainModelList]:
-        """Query along the alert edges of the preprocessor output.
+        retrieve_efficiency_curve: bool = False,
+    ) -> TurbineEfficiencyCurveQueryAPI[T_DomainModelList]:
+        """Query along the turbine curve edges of the generator.
 
         Args:
+            min_head: The minimum value of the head to filter on.
+            max_head: The maximum value of the head to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of alert edges to return. Defaults to 25. Set to -1, float("inf") or None
+            external_id_prefix_edge: The prefix of the external ID to filter on.
+            space_edge: The space to filter on.
+            filter: (Advanced) Filter applied to node. If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
+            limit: Maximum number of turbine curve edges to return. Defaults to 3. Set to -1, float("inf") or None
                 to return all items.
-            retrieve_case: Whether to retrieve the case for each preprocessor output or not.
-            retrieve_input_: Whether to retrieve the input for each preprocessor output or not.
+            retrieve_efficiency_curve: Whether to retrieve the efficiency curve for each generator or not.
 
         Returns:
-            AlertQueryAPI: The query API for the alert.
+            TurbineEfficiencyCurveQueryAPI: The query API for the turbine efficiency curve.
         """
-        from .alert_query import AlertQueryAPI
+        from .turbine_efficiency_curve_query import TurbineEfficiencyCurveQueryAPI
 
         from_ = self._builder[-1].name
-
         edge_filter = _create_edge_filter(
-            dm.DirectRelationReference("sp_powerops_types", "calculationIssue"),
-            external_id_prefix=external_id_prefix,
-            space=space,
+            dm.DirectRelationReference("power-ops-types", "isSubAssetOf"),
+            external_id_prefix=external_id_prefix_edge,
+            space=space_edge,
         )
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("alerts"),
+                name=self._builder.next_name("turbine_curves"),
                 expression=dm.query.EdgeResultSetExpression(
                     filter=edge_filter,
                     from_=from_,
                     direction="outwards",
                 ),
                 select=dm.query.Select(),
                 max_retrieve_limit=limit,
             )
         )
-        if retrieve_case:
-            self._query_append_case(from_)
-        if retrieve_input_:
-            self._query_append_input_(from_)
-        return AlertQueryAPI(self._client, self._builder, self._view_by_read_class, None, limit)
+
+        view_id = self._view_by_read_class[TurbineEfficiencyCurve]
+        has_data = dm.filters.HasData(views=[view_id])
+        node_filer = _create_turbine_efficiency_curve_filter(
+            view_id,
+            min_head,
+            max_head,
+            external_id_prefix,
+            space,
+            (filter and dm.filters.And(filter, has_data)) or has_data,
+        )
+        if retrieve_efficiency_curve:
+            self._query_append_efficiency_curve(from_)
+        return TurbineEfficiencyCurveQueryAPI(self._client, self._builder, self._view_by_read_class, node_filer, limit)
 
     def query(
         self,
-        retrieve_case: bool = False,
-        retrieve_input_: bool = False,
+        retrieve_efficiency_curve: bool = False,
     ) -> T_DomainModelList:
         """Execute query and return the result.
 
         Args:
-            retrieve_case: Whether to retrieve the case for each preprocessor output or not.
-            retrieve_input_: Whether to retrieve the input for each preprocessor output or not.
+            retrieve_efficiency_curve: Whether to retrieve the efficiency curve for each generator or not.
 
         Returns:
             The list of the source nodes of the query.
 
         """
         from_ = self._builder[-1].name
-        if retrieve_case:
-            self._query_append_case(from_)
-        if retrieve_input_:
-            self._query_append_input_(from_)
+        if retrieve_efficiency_curve:
+            self._query_append_efficiency_curve(from_)
         return self._query()
 
-    def _query_append_case(self, from_: str) -> None:
-        view_id = self._view_by_read_class[Case]
-        self._builder.append(
-            QueryStep(
-                name=self._builder.next_name("case"),
-                expression=dm.query.NodeResultSetExpression(
-                    filter=dm.filters.HasData(views=[view_id]),
-                    from_=from_,
-                    through=self._view_by_read_class[PreprocessorOutput].as_property_ref("case"),
-                    direction="outwards",
-                ),
-                select=dm.query.Select([dm.query.SourceSelector(view_id, ["*"])]),
-                max_retrieve_limit=-1,
-                result_cls=Case,
-            ),
-        )
-
-    def _query_append_input_(self, from_: str) -> None:
-        view_id = self._view_by_read_class[PreprocessorInput]
+    def _query_append_efficiency_curve(self, from_: str) -> None:
+        view_id = self._view_by_read_class[GeneratorEfficiencyCurve]
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("input_"),
+                name=self._builder.next_name("efficiency_curve"),
                 expression=dm.query.NodeResultSetExpression(
                     filter=dm.filters.HasData(views=[view_id]),
                     from_=from_,
-                    through=self._view_by_read_class[PreprocessorOutput].as_property_ref("input"),
+                    through=self._view_by_read_class[Generator].as_property_ref("efficiencyCurve"),
                     direction="outwards",
                 ),
                 select=dm.query.Select([dm.query.SourceSelector(view_id, ["*"])]),
                 max_retrieve_limit=-1,
-                result_cls=PreprocessorInput,
+                result_cls=GeneratorEfficiencyCurve,
+                is_single_direct_relation=True,
             ),
         )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/price_area.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/price_area_asset.py`

 * *Files 15% similar despite different names*

```diff
@@ -9,255 +9,283 @@
 from cognite.client.data_classes.data_modeling.instances import InstanceAggregationResultList
 
 from cognite.powerops.client._generated.v1.data_classes._core import DEFAULT_INSTANCE_SPACE
 from cognite.powerops.client._generated.v1.data_classes import (
     DomainModelCore,
     DomainModelWrite,
     ResourcesWriteResult,
-    PriceArea,
-    PriceAreaWrite,
-    PriceAreaFields,
-    PriceAreaList,
-    PriceAreaWriteList,
-    PriceAreaTextFields,
+    PriceAreaAsset,
+    PriceAreaAssetWrite,
+    PriceAreaAssetFields,
+    PriceAreaAssetList,
+    PriceAreaAssetWriteList,
+    PriceAreaAssetTextFields,
 )
-from cognite.powerops.client._generated.v1.data_classes._price_area import (
-    _PRICEAREA_PROPERTIES_BY_FIELD,
-    _create_price_area_filter,
+from cognite.powerops.client._generated.v1.data_classes._price_area_asset import (
+    _PRICEAREAASSET_PROPERTIES_BY_FIELD,
+    _create_price_area_asset_filter,
 )
 from ._core import (
     DEFAULT_LIMIT_READ,
     DEFAULT_QUERY_LIMIT,
     Aggregations,
     NodeAPI,
     SequenceNotStr,
     QueryStep,
     QueryBuilder,
 )
-from .price_area_query import PriceAreaQueryAPI
+from .price_area_asset_plants import PriceAreaAssetPlantsAPI
+from .price_area_asset_watercourses import PriceAreaAssetWatercoursesAPI
+from .price_area_asset_query import PriceAreaAssetQueryAPI
 
 
-class PriceAreaAPI(NodeAPI[PriceArea, PriceAreaWrite, PriceAreaList]):
+class PriceAreaAssetAPI(NodeAPI[PriceAreaAsset, PriceAreaAssetWrite, PriceAreaAssetList]):
     def __init__(self, client: CogniteClient, view_by_read_class: dict[type[DomainModelCore], dm.ViewId]):
-        view_id = view_by_read_class[PriceArea]
+        view_id = view_by_read_class[PriceAreaAsset]
         super().__init__(
             client=client,
             sources=view_id,
-            class_type=PriceArea,
-            class_list=PriceAreaList,
-            class_write_list=PriceAreaWriteList,
+            class_type=PriceAreaAsset,
+            class_list=PriceAreaAssetList,
+            class_write_list=PriceAreaAssetWriteList,
             view_by_read_class=view_by_read_class,
         )
         self._view_id = view_id
+        self.plants_edge = PriceAreaAssetPlantsAPI(client)
+        self.watercourses_edge = PriceAreaAssetWatercoursesAPI(client)
 
     def __call__(
         self,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
         timezone: str | list[str] | None = None,
         timezone_prefix: str | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_QUERY_LIMIT,
         filter: dm.Filter | None = None,
-    ) -> PriceAreaQueryAPI[PriceAreaList]:
-        """Query starting at price areas.
+    ) -> PriceAreaAssetQueryAPI[PriceAreaAssetList]:
+        """Query starting at price area assets.
 
         Args:
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
             timezone: The timezone to filter on.
             timezone_prefix: The prefix of the timezone to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of price areas to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of price area assets to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
-            A query API for price areas.
+            A query API for price area assets.
 
         """
         has_data = dm.filters.HasData(views=[self._view_id])
-        filter_ = _create_price_area_filter(
+        filter_ = _create_price_area_asset_filter(
             self._view_id,
             name,
             name_prefix,
             timezone,
             timezone_prefix,
             external_id_prefix,
             space,
             (filter and dm.filters.And(filter, has_data)) or has_data,
         )
-        builder = QueryBuilder(PriceAreaList)
-        return PriceAreaQueryAPI(self._client, builder, self._view_by_read_class, filter_, limit)
+        builder = QueryBuilder(PriceAreaAssetList)
+        return PriceAreaAssetQueryAPI(self._client, builder, self._view_by_read_class, filter_, limit)
 
     def apply(
         self,
-        price_area: PriceAreaWrite | Sequence[PriceAreaWrite],
+        price_area_asset: PriceAreaAssetWrite | Sequence[PriceAreaAssetWrite],
         replace: bool = False,
         write_none: bool = False,
     ) -> ResourcesWriteResult:
-        """Add or update (upsert) price areas.
+        """Add or update (upsert) price area assets.
+
+        Note: This method iterates through all nodes and timeseries linked to price_area_asset and creates them including the edges
+        between the nodes. For example, if any of `plants` or `watercourses` are set, then these
+        nodes as well as any nodes linked to them, and all the edges linking these nodes will be created.
 
         Args:
-            price_area: Price area or sequence of price areas to upsert.
+            price_area_asset: Price area asset or sequence of price area assets to upsert.
             replace (bool): How do we behave when a property value exists? Do we replace all matching and existing values with the supplied values (true)?
                 Or should we merge in new values for properties together with the existing values (false)? Note: This setting applies for all nodes or edges specified in the ingestion call.
             write_none (bool): This method, will by default, skip properties that are set to None. However, if you want to set properties to None,
                 you can set this parameter to True. Note this only applies to properties that are nullable.
         Returns:
             Created instance(s), i.e., nodes, edges, and time series.
 
         Examples:
 
-            Create a new price_area:
+            Create a new price_area_asset:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
-                >>> from cognite.powerops.client._generated.v1.data_classes import PriceAreaWrite
+                >>> from cognite.powerops.client._generated.v1.data_classes import PriceAreaAssetWrite
                 >>> client = PowerOpsModelsV1Client()
-                >>> price_area = PriceAreaWrite(external_id="my_price_area", ...)
-                >>> result = client.price_area.apply(price_area)
+                >>> price_area_asset = PriceAreaAssetWrite(external_id="my_price_area_asset", ...)
+                >>> result = client.price_area_asset.apply(price_area_asset)
 
         """
         warnings.warn(
             "The .apply method is deprecated and will be removed in v1.0. "
             "Please use the .upsert method on the client instead. This means instead of "
-            "`my_client.price_area.apply(my_items)` please use `my_client.upsert(my_items)`."
+            "`my_client.price_area_asset.apply(my_items)` please use `my_client.upsert(my_items)`."
             "The motivation is that all apply methods are the same, and having one apply method per API "
             " class encourages users to create items in small batches, which is inefficient."
             "In addition, .upsert method is more descriptive of what the method does.",
             UserWarning,
             stacklevel=2,
         )
-        return self._apply(price_area, replace, write_none)
+        return self._apply(price_area_asset, replace, write_none)
 
     def delete(
         self, external_id: str | SequenceNotStr[str], space: str = DEFAULT_INSTANCE_SPACE
     ) -> dm.InstancesDeleteResult:
-        """Delete one or more price area.
+        """Delete one or more price area asset.
 
         Args:
-            external_id: External id of the price area to delete.
-            space: The space where all the price area are located.
+            external_id: External id of the price area asset to delete.
+            space: The space where all the price area asset are located.
 
         Returns:
             The instance(s), i.e., nodes and edges which has been deleted. Empty list if nothing was deleted.
 
         Examples:
 
-            Delete price_area by id:
+            Delete price_area_asset by id:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> client.price_area.delete("my_price_area")
+                >>> client.price_area_asset.delete("my_price_area_asset")
         """
         warnings.warn(
             "The .delete method is deprecated and will be removed in v1.0. "
             "Please use the .delete method on the client instead. This means instead of "
-            "`my_client.price_area.delete(my_ids)` please use `my_client.delete(my_ids)`."
+            "`my_client.price_area_asset.delete(my_ids)` please use `my_client.delete(my_ids)`."
             "The motivation is that all delete methods are the same, and having one delete method per API "
             " class encourages users to delete items in small batches, which is inefficient.",
             UserWarning,
             stacklevel=2,
         )
         return self._delete(external_id, space)
 
     @overload
-    def retrieve(self, external_id: str, space: str = DEFAULT_INSTANCE_SPACE) -> PriceArea | None: ...
+    def retrieve(self, external_id: str, space: str = DEFAULT_INSTANCE_SPACE) -> PriceAreaAsset | None: ...
 
     @overload
-    def retrieve(self, external_id: SequenceNotStr[str], space: str = DEFAULT_INSTANCE_SPACE) -> PriceAreaList: ...
+    def retrieve(self, external_id: SequenceNotStr[str], space: str = DEFAULT_INSTANCE_SPACE) -> PriceAreaAssetList: ...
 
     def retrieve(
         self, external_id: str | SequenceNotStr[str], space: str = DEFAULT_INSTANCE_SPACE
-    ) -> PriceArea | PriceAreaList | None:
-        """Retrieve one or more price areas by id(s).
+    ) -> PriceAreaAsset | PriceAreaAssetList | None:
+        """Retrieve one or more price area assets by id(s).
 
         Args:
-            external_id: External id or list of external ids of the price areas.
-            space: The space where all the price areas are located.
+            external_id: External id or list of external ids of the price area assets.
+            space: The space where all the price area assets are located.
 
         Returns:
-            The requested price areas.
+            The requested price area assets.
 
         Examples:
 
-            Retrieve price_area by id:
+            Retrieve price_area_asset by id:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> price_area = client.price_area.retrieve("my_price_area")
+                >>> price_area_asset = client.price_area_asset.retrieve("my_price_area_asset")
 
         """
-        return self._retrieve(external_id, space)
+        return self._retrieve(
+            external_id,
+            space,
+            retrieve_edges=True,
+            edge_api_name_type_direction_view_id_penta=[
+                (
+                    self.plants_edge,
+                    "plants",
+                    dm.DirectRelationReference("sp_powerops_types", "isPlantOf"),
+                    "outwards",
+                    dm.ViewId("sp_powerops_models", "Plant", "1"),
+                ),
+                (
+                    self.watercourses_edge,
+                    "watercourses",
+                    dm.DirectRelationReference("sp_powerops_types", "isWatercourseOf"),
+                    "outwards",
+                    dm.ViewId("sp_powerops_models", "Watercourse", "1"),
+                ),
+            ],
+        )
 
     def search(
         self,
         query: str,
-        properties: PriceAreaTextFields | Sequence[PriceAreaTextFields] | None = None,
+        properties: PriceAreaAssetTextFields | Sequence[PriceAreaAssetTextFields] | None = None,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
         timezone: str | list[str] | None = None,
         timezone_prefix: str | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
-    ) -> PriceAreaList:
-        """Search price areas
+    ) -> PriceAreaAssetList:
+        """Search price area assets
 
         Args:
             query: The search query,
             properties: The property to search, if nothing is passed all text fields will be searched.
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
             timezone: The timezone to filter on.
             timezone_prefix: The prefix of the timezone to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of price areas to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of price area assets to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
-            Search results price areas matching the query.
+            Search results price area assets matching the query.
 
         Examples:
 
-           Search for 'my_price_area' in all text properties:
+           Search for 'my_price_area_asset' in all text properties:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> price_areas = client.price_area.search('my_price_area')
+                >>> price_area_assets = client.price_area_asset.search('my_price_area_asset')
 
         """
-        filter_ = _create_price_area_filter(
+        filter_ = _create_price_area_asset_filter(
             self._view_id,
             name,
             name_prefix,
             timezone,
             timezone_prefix,
             external_id_prefix,
             space,
             filter,
         )
-        return self._search(self._view_id, query, _PRICEAREA_PROPERTIES_BY_FIELD, properties, filter_, limit)
+        return self._search(self._view_id, query, _PRICEAREAASSET_PROPERTIES_BY_FIELD, properties, filter_, limit)
 
     @overload
     def aggregate(
         self,
         aggregations: (
             Aggregations
             | dm.aggregations.MetricAggregation
             | Sequence[Aggregations]
             | Sequence[dm.aggregations.MetricAggregation]
         ),
-        property: PriceAreaFields | Sequence[PriceAreaFields] | None = None,
+        property: PriceAreaAssetFields | Sequence[PriceAreaAssetFields] | None = None,
         group_by: None = None,
         query: str | None = None,
-        search_properties: PriceAreaTextFields | Sequence[PriceAreaTextFields] | None = None,
+        search_properties: PriceAreaAssetTextFields | Sequence[PriceAreaAssetTextFields] | None = None,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
         timezone: str | list[str] | None = None,
         timezone_prefix: str | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
@@ -269,18 +297,18 @@
         self,
         aggregations: (
             Aggregations
             | dm.aggregations.MetricAggregation
             | Sequence[Aggregations]
             | Sequence[dm.aggregations.MetricAggregation]
         ),
-        property: PriceAreaFields | Sequence[PriceAreaFields] | None = None,
-        group_by: PriceAreaFields | Sequence[PriceAreaFields] = None,
+        property: PriceAreaAssetFields | Sequence[PriceAreaAssetFields] | None = None,
+        group_by: PriceAreaAssetFields | Sequence[PriceAreaAssetFields] = None,
         query: str | None = None,
-        search_properties: PriceAreaTextFields | Sequence[PriceAreaTextFields] | None = None,
+        search_properties: PriceAreaAssetTextFields | Sequence[PriceAreaAssetTextFields] | None = None,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
         timezone: str | list[str] | None = None,
         timezone_prefix: str | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
@@ -291,129 +319,129 @@
         self,
         aggregate: (
             Aggregations
             | dm.aggregations.MetricAggregation
             | Sequence[Aggregations]
             | Sequence[dm.aggregations.MetricAggregation]
         ),
-        property: PriceAreaFields | Sequence[PriceAreaFields] | None = None,
-        group_by: PriceAreaFields | Sequence[PriceAreaFields] | None = None,
+        property: PriceAreaAssetFields | Sequence[PriceAreaAssetFields] | None = None,
+        group_by: PriceAreaAssetFields | Sequence[PriceAreaAssetFields] | None = None,
         query: str | None = None,
-        search_property: PriceAreaTextFields | Sequence[PriceAreaTextFields] | None = None,
+        search_property: PriceAreaAssetTextFields | Sequence[PriceAreaAssetTextFields] | None = None,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
         timezone: str | list[str] | None = None,
         timezone_prefix: str | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> list[dm.aggregations.AggregatedNumberedValue] | InstanceAggregationResultList:
-        """Aggregate data across price areas
+        """Aggregate data across price area assets
 
         Args:
             aggregate: The aggregation to perform.
             property: The property to perform aggregation on.
             group_by: The property to group by when doing the aggregation.
             query: The query to search for in the text field.
             search_property: The text field to search in.
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
             timezone: The timezone to filter on.
             timezone_prefix: The prefix of the timezone to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of price areas to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of price area assets to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
             Aggregation results.
 
         Examples:
 
-            Count price areas in space `my_space`:
+            Count price area assets in space `my_space`:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> result = client.price_area.aggregate("count", space="my_space")
+                >>> result = client.price_area_asset.aggregate("count", space="my_space")
 
         """
 
-        filter_ = _create_price_area_filter(
+        filter_ = _create_price_area_asset_filter(
             self._view_id,
             name,
             name_prefix,
             timezone,
             timezone_prefix,
             external_id_prefix,
             space,
             filter,
         )
         return self._aggregate(
             self._view_id,
             aggregate,
-            _PRICEAREA_PROPERTIES_BY_FIELD,
+            _PRICEAREAASSET_PROPERTIES_BY_FIELD,
             property,
             group_by,
             query,
             search_property,
             limit,
             filter_,
         )
 
     def histogram(
         self,
-        property: PriceAreaFields,
+        property: PriceAreaAssetFields,
         interval: float,
         query: str | None = None,
-        search_property: PriceAreaTextFields | Sequence[PriceAreaTextFields] | None = None,
+        search_property: PriceAreaAssetTextFields | Sequence[PriceAreaAssetTextFields] | None = None,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
         timezone: str | list[str] | None = None,
         timezone_prefix: str | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> dm.aggregations.HistogramValue:
-        """Produces histograms for price areas
+        """Produces histograms for price area assets
 
         Args:
             property: The property to use as the value in the histogram.
             interval: The interval to use for the histogram bins.
             query: The query to search for in the text field.
             search_property: The text field to search in.
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
             timezone: The timezone to filter on.
             timezone_prefix: The prefix of the timezone to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of price areas to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of price area assets to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
             Bucketed histogram results.
 
         """
-        filter_ = _create_price_area_filter(
+        filter_ = _create_price_area_asset_filter(
             self._view_id,
             name,
             name_prefix,
             timezone,
             timezone_prefix,
             external_id_prefix,
             space,
             filter,
         )
         return self._histogram(
             self._view_id,
             property,
             interval,
-            _PRICEAREA_PROPERTIES_BY_FIELD,
+            _PRICEAREAASSET_PROPERTIES_BY_FIELD,
             query,
             search_property,
             limit,
             filter_,
         )
 
     def list(
@@ -422,43 +450,66 @@
         name_prefix: str | None = None,
         timezone: str | list[str] | None = None,
         timezone_prefix: str | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
-    ) -> PriceAreaList:
-        """List/filter price areas
+        retrieve_edges: bool = True,
+    ) -> PriceAreaAssetList:
+        """List/filter price area assets
 
         Args:
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
             timezone: The timezone to filter on.
             timezone_prefix: The prefix of the timezone to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of price areas to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of price area assets to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
+            retrieve_edges: Whether to retrieve `plants` or `watercourses` external ids for the price area assets. Defaults to True.
 
         Returns:
-            List of requested price areas
+            List of requested price area assets
 
         Examples:
 
-            List price areas and limit to 5:
+            List price area assets and limit to 5:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> price_areas = client.price_area.list(limit=5)
+                >>> price_area_assets = client.price_area_asset.list(limit=5)
 
         """
-        filter_ = _create_price_area_filter(
+        filter_ = _create_price_area_asset_filter(
             self._view_id,
             name,
             name_prefix,
             timezone,
             timezone_prefix,
             external_id_prefix,
             space,
             filter,
         )
-        return self._list(limit=limit, filter=filter_)
+
+        return self._list(
+            limit=limit,
+            filter=filter_,
+            retrieve_edges=retrieve_edges,
+            edge_api_name_type_direction_view_id_penta=[
+                (
+                    self.plants_edge,
+                    "plants",
+                    dm.DirectRelationReference("sp_powerops_types", "isPlantOf"),
+                    "outwards",
+                    dm.ViewId("sp_powerops_models", "Plant", "1"),
+                ),
+                (
+                    self.watercourses_edge,
+                    "watercourses",
+                    dm.DirectRelationReference("sp_powerops_types", "isWatercourseOf"),
+                    "outwards",
+                    dm.ViewId("sp_powerops_models", "Watercourse", "1"),
+                ),
+            ],
+        )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/price_area_afrr.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/scenario.py`

 * *Files 18% similar despite different names*

```diff
@@ -9,277 +9,292 @@
 from cognite.client.data_classes.data_modeling.instances import InstanceAggregationResultList
 
 from cognite.powerops.client._generated.v1.data_classes._core import DEFAULT_INSTANCE_SPACE
 from cognite.powerops.client._generated.v1.data_classes import (
     DomainModelCore,
     DomainModelWrite,
     ResourcesWriteResult,
-    PriceAreaAFRR,
-    PriceAreaAFRRWrite,
-    PriceAreaAFRRFields,
-    PriceAreaAFRRList,
-    PriceAreaAFRRWriteList,
-    PriceAreaAFRRTextFields,
+    Scenario,
+    ScenarioWrite,
+    ScenarioFields,
+    ScenarioList,
+    ScenarioWriteList,
+    ScenarioTextFields,
 )
-from cognite.powerops.client._generated.v1.data_classes._price_area_afrr import (
-    _PRICEAREAAFRR_PROPERTIES_BY_FIELD,
-    _create_price_area_afrr_filter,
+from cognite.powerops.client._generated.v1.data_classes._scenario import (
+    _SCENARIO_PROPERTIES_BY_FIELD,
+    _create_scenario_filter,
 )
 from ._core import (
     DEFAULT_LIMIT_READ,
     DEFAULT_QUERY_LIMIT,
     Aggregations,
     NodeAPI,
     SequenceNotStr,
     QueryStep,
     QueryBuilder,
 )
-from .price_area_afrr_capacity_price_up import PriceAreaAFRRCapacityPriceUpAPI
-from .price_area_afrr_capacity_price_down import PriceAreaAFRRCapacityPriceDownAPI
-from .price_area_afrr_activation_price_up import PriceAreaAFRRActivationPriceUpAPI
-from .price_area_afrr_activation_price_down import PriceAreaAFRRActivationPriceDownAPI
-from .price_area_afrr_relative_activation import PriceAreaAFRRRelativeActivationAPI
-from .price_area_afrr_total_capacity_allocation_up import PriceAreaAFRRTotalCapacityAllocationUpAPI
-from .price_area_afrr_total_capacity_allocation_down import PriceAreaAFRRTotalCapacityAllocationDownAPI
-from .price_area_afrr_own_capacity_allocation_up import PriceAreaAFRROwnCapacityAllocationUpAPI
-from .price_area_afrr_own_capacity_allocation_down import PriceAreaAFRROwnCapacityAllocationDownAPI
-from .price_area_afrr_query import PriceAreaAFRRQueryAPI
+from .scenario_mappings_override import ScenarioMappingsOverrideAPI
+from .scenario_query import ScenarioQueryAPI
 
 
-class PriceAreaAFRRAPI(NodeAPI[PriceAreaAFRR, PriceAreaAFRRWrite, PriceAreaAFRRList]):
+class ScenarioAPI(NodeAPI[Scenario, ScenarioWrite, ScenarioList]):
     def __init__(self, client: CogniteClient, view_by_read_class: dict[type[DomainModelCore], dm.ViewId]):
-        view_id = view_by_read_class[PriceAreaAFRR]
+        view_id = view_by_read_class[Scenario]
         super().__init__(
             client=client,
             sources=view_id,
-            class_type=PriceAreaAFRR,
-            class_list=PriceAreaAFRRList,
-            class_write_list=PriceAreaAFRRWriteList,
+            class_type=Scenario,
+            class_list=ScenarioList,
+            class_write_list=ScenarioWriteList,
             view_by_read_class=view_by_read_class,
         )
         self._view_id = view_id
-        self.capacity_price_up = PriceAreaAFRRCapacityPriceUpAPI(client, view_id)
-        self.capacity_price_down = PriceAreaAFRRCapacityPriceDownAPI(client, view_id)
-        self.activation_price_up = PriceAreaAFRRActivationPriceUpAPI(client, view_id)
-        self.activation_price_down = PriceAreaAFRRActivationPriceDownAPI(client, view_id)
-        self.relative_activation = PriceAreaAFRRRelativeActivationAPI(client, view_id)
-        self.total_capacity_allocation_up = PriceAreaAFRRTotalCapacityAllocationUpAPI(client, view_id)
-        self.total_capacity_allocation_down = PriceAreaAFRRTotalCapacityAllocationDownAPI(client, view_id)
-        self.own_capacity_allocation_up = PriceAreaAFRROwnCapacityAllocationUpAPI(client, view_id)
-        self.own_capacity_allocation_down = PriceAreaAFRROwnCapacityAllocationDownAPI(client, view_id)
+        self.mappings_override_edge = ScenarioMappingsOverrideAPI(client)
 
     def __call__(
         self,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
-        timezone: str | list[str] | None = None,
-        timezone_prefix: str | None = None,
+        model_template: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        commands: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        source: str | list[str] | None = None,
+        source_prefix: str | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_QUERY_LIMIT,
         filter: dm.Filter | None = None,
-    ) -> PriceAreaAFRRQueryAPI[PriceAreaAFRRList]:
-        """Query starting at price area afrrs.
+    ) -> ScenarioQueryAPI[ScenarioList]:
+        """Query starting at scenarios.
 
         Args:
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
-            timezone: The timezone to filter on.
-            timezone_prefix: The prefix of the timezone to filter on.
+            model_template: The model template to filter on.
+            commands: The command to filter on.
+            source: The source to filter on.
+            source_prefix: The prefix of the source to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of price area afrrs to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of scenarios to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
-            A query API for price area afrrs.
+            A query API for scenarios.
 
         """
         has_data = dm.filters.HasData(views=[self._view_id])
-        filter_ = _create_price_area_afrr_filter(
+        filter_ = _create_scenario_filter(
             self._view_id,
             name,
             name_prefix,
-            timezone,
-            timezone_prefix,
+            model_template,
+            commands,
+            source,
+            source_prefix,
             external_id_prefix,
             space,
             (filter and dm.filters.And(filter, has_data)) or has_data,
         )
-        builder = QueryBuilder(PriceAreaAFRRList)
-        return PriceAreaAFRRQueryAPI(self._client, builder, self._view_by_read_class, filter_, limit)
+        builder = QueryBuilder(ScenarioList)
+        return ScenarioQueryAPI(self._client, builder, self._view_by_read_class, filter_, limit)
 
     def apply(
         self,
-        price_area_afrr: PriceAreaAFRRWrite | Sequence[PriceAreaAFRRWrite],
+        scenario: ScenarioWrite | Sequence[ScenarioWrite],
         replace: bool = False,
         write_none: bool = False,
     ) -> ResourcesWriteResult:
-        """Add or update (upsert) price area afrrs.
+        """Add or update (upsert) scenarios.
+
+        Note: This method iterates through all nodes and timeseries linked to scenario and creates them including the edges
+        between the nodes. For example, if any of `mappings_override` are set, then these
+        nodes as well as any nodes linked to them, and all the edges linking these nodes will be created.
 
         Args:
-            price_area_afrr: Price area afrr or sequence of price area afrrs to upsert.
+            scenario: Scenario or sequence of scenarios to upsert.
             replace (bool): How do we behave when a property value exists? Do we replace all matching and existing values with the supplied values (true)?
                 Or should we merge in new values for properties together with the existing values (false)? Note: This setting applies for all nodes or edges specified in the ingestion call.
             write_none (bool): This method, will by default, skip properties that are set to None. However, if you want to set properties to None,
                 you can set this parameter to True. Note this only applies to properties that are nullable.
         Returns:
             Created instance(s), i.e., nodes, edges, and time series.
 
         Examples:
 
-            Create a new price_area_afrr:
+            Create a new scenario:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
-                >>> from cognite.powerops.client._generated.v1.data_classes import PriceAreaAFRRWrite
+                >>> from cognite.powerops.client._generated.v1.data_classes import ScenarioWrite
                 >>> client = PowerOpsModelsV1Client()
-                >>> price_area_afrr = PriceAreaAFRRWrite(external_id="my_price_area_afrr", ...)
-                >>> result = client.price_area_afrr.apply(price_area_afrr)
+                >>> scenario = ScenarioWrite(external_id="my_scenario", ...)
+                >>> result = client.scenario.apply(scenario)
 
         """
         warnings.warn(
             "The .apply method is deprecated and will be removed in v1.0. "
             "Please use the .upsert method on the client instead. This means instead of "
-            "`my_client.price_area_afrr.apply(my_items)` please use `my_client.upsert(my_items)`."
+            "`my_client.scenario.apply(my_items)` please use `my_client.upsert(my_items)`."
             "The motivation is that all apply methods are the same, and having one apply method per API "
             " class encourages users to create items in small batches, which is inefficient."
             "In addition, .upsert method is more descriptive of what the method does.",
             UserWarning,
             stacklevel=2,
         )
-        return self._apply(price_area_afrr, replace, write_none)
+        return self._apply(scenario, replace, write_none)
 
     def delete(
         self, external_id: str | SequenceNotStr[str], space: str = DEFAULT_INSTANCE_SPACE
     ) -> dm.InstancesDeleteResult:
-        """Delete one or more price area afrr.
+        """Delete one or more scenario.
 
         Args:
-            external_id: External id of the price area afrr to delete.
-            space: The space where all the price area afrr are located.
+            external_id: External id of the scenario to delete.
+            space: The space where all the scenario are located.
 
         Returns:
             The instance(s), i.e., nodes and edges which has been deleted. Empty list if nothing was deleted.
 
         Examples:
 
-            Delete price_area_afrr by id:
+            Delete scenario by id:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> client.price_area_afrr.delete("my_price_area_afrr")
+                >>> client.scenario.delete("my_scenario")
         """
         warnings.warn(
             "The .delete method is deprecated and will be removed in v1.0. "
             "Please use the .delete method on the client instead. This means instead of "
-            "`my_client.price_area_afrr.delete(my_ids)` please use `my_client.delete(my_ids)`."
+            "`my_client.scenario.delete(my_ids)` please use `my_client.delete(my_ids)`."
             "The motivation is that all delete methods are the same, and having one delete method per API "
             " class encourages users to delete items in small batches, which is inefficient.",
             UserWarning,
             stacklevel=2,
         )
         return self._delete(external_id, space)
 
     @overload
-    def retrieve(self, external_id: str, space: str = DEFAULT_INSTANCE_SPACE) -> PriceAreaAFRR | None: ...
+    def retrieve(self, external_id: str, space: str = DEFAULT_INSTANCE_SPACE) -> Scenario | None: ...
 
     @overload
-    def retrieve(self, external_id: SequenceNotStr[str], space: str = DEFAULT_INSTANCE_SPACE) -> PriceAreaAFRRList: ...
+    def retrieve(self, external_id: SequenceNotStr[str], space: str = DEFAULT_INSTANCE_SPACE) -> ScenarioList: ...
 
     def retrieve(
         self, external_id: str | SequenceNotStr[str], space: str = DEFAULT_INSTANCE_SPACE
-    ) -> PriceAreaAFRR | PriceAreaAFRRList | None:
-        """Retrieve one or more price area afrrs by id(s).
+    ) -> Scenario | ScenarioList | None:
+        """Retrieve one or more scenarios by id(s).
 
         Args:
-            external_id: External id or list of external ids of the price area afrrs.
-            space: The space where all the price area afrrs are located.
+            external_id: External id or list of external ids of the scenarios.
+            space: The space where all the scenarios are located.
 
         Returns:
-            The requested price area afrrs.
+            The requested scenarios.
 
         Examples:
 
-            Retrieve price_area_afrr by id:
+            Retrieve scenario by id:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> price_area_afrr = client.price_area_afrr.retrieve("my_price_area_afrr")
+                >>> scenario = client.scenario.retrieve("my_scenario")
 
         """
-        return self._retrieve(external_id, space)
+        return self._retrieve(
+            external_id,
+            space,
+            retrieve_edges=True,
+            edge_api_name_type_direction_view_id_penta=[
+                (
+                    self.mappings_override_edge,
+                    "mappings_override",
+                    dm.DirectRelationReference("sp_powerops_types_temp", "Mapping"),
+                    "outwards",
+                    dm.ViewId("sp_powerops_models_temp", "Mapping", "1"),
+                ),
+            ],
+        )
 
     def search(
         self,
         query: str,
-        properties: PriceAreaAFRRTextFields | Sequence[PriceAreaAFRRTextFields] | None = None,
+        properties: ScenarioTextFields | Sequence[ScenarioTextFields] | None = None,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
-        timezone: str | list[str] | None = None,
-        timezone_prefix: str | None = None,
+        model_template: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        commands: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        source: str | list[str] | None = None,
+        source_prefix: str | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
-    ) -> PriceAreaAFRRList:
-        """Search price area afrrs
+    ) -> ScenarioList:
+        """Search scenarios
 
         Args:
             query: The search query,
             properties: The property to search, if nothing is passed all text fields will be searched.
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
-            timezone: The timezone to filter on.
-            timezone_prefix: The prefix of the timezone to filter on.
+            model_template: The model template to filter on.
+            commands: The command to filter on.
+            source: The source to filter on.
+            source_prefix: The prefix of the source to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of price area afrrs to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of scenarios to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
-            Search results price area afrrs matching the query.
+            Search results scenarios matching the query.
 
         Examples:
 
-           Search for 'my_price_area_afrr' in all text properties:
+           Search for 'my_scenario' in all text properties:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> price_area_afrrs = client.price_area_afrr.search('my_price_area_afrr')
+                >>> scenarios = client.scenario.search('my_scenario')
 
         """
-        filter_ = _create_price_area_afrr_filter(
+        filter_ = _create_scenario_filter(
             self._view_id,
             name,
             name_prefix,
-            timezone,
-            timezone_prefix,
+            model_template,
+            commands,
+            source,
+            source_prefix,
             external_id_prefix,
             space,
             filter,
         )
-        return self._search(self._view_id, query, _PRICEAREAAFRR_PROPERTIES_BY_FIELD, properties, filter_, limit)
+        return self._search(self._view_id, query, _SCENARIO_PROPERTIES_BY_FIELD, properties, filter_, limit)
 
     @overload
     def aggregate(
         self,
         aggregations: (
             Aggregations
             | dm.aggregations.MetricAggregation
             | Sequence[Aggregations]
             | Sequence[dm.aggregations.MetricAggregation]
         ),
-        property: PriceAreaAFRRFields | Sequence[PriceAreaAFRRFields] | None = None,
+        property: ScenarioFields | Sequence[ScenarioFields] | None = None,
         group_by: None = None,
         query: str | None = None,
-        search_properties: PriceAreaAFRRTextFields | Sequence[PriceAreaAFRRTextFields] | None = None,
+        search_properties: ScenarioTextFields | Sequence[ScenarioTextFields] | None = None,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
-        timezone: str | list[str] | None = None,
-        timezone_prefix: str | None = None,
+        model_template: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        commands: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        source: str | list[str] | None = None,
+        source_prefix: str | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> list[dm.aggregations.AggregatedNumberedValue]: ...
 
     @overload
@@ -287,196 +302,232 @@
         self,
         aggregations: (
             Aggregations
             | dm.aggregations.MetricAggregation
             | Sequence[Aggregations]
             | Sequence[dm.aggregations.MetricAggregation]
         ),
-        property: PriceAreaAFRRFields | Sequence[PriceAreaAFRRFields] | None = None,
-        group_by: PriceAreaAFRRFields | Sequence[PriceAreaAFRRFields] = None,
+        property: ScenarioFields | Sequence[ScenarioFields] | None = None,
+        group_by: ScenarioFields | Sequence[ScenarioFields] = None,
         query: str | None = None,
-        search_properties: PriceAreaAFRRTextFields | Sequence[PriceAreaAFRRTextFields] | None = None,
+        search_properties: ScenarioTextFields | Sequence[ScenarioTextFields] | None = None,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
-        timezone: str | list[str] | None = None,
-        timezone_prefix: str | None = None,
+        model_template: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        commands: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        source: str | list[str] | None = None,
+        source_prefix: str | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> InstanceAggregationResultList: ...
 
     def aggregate(
         self,
         aggregate: (
             Aggregations
             | dm.aggregations.MetricAggregation
             | Sequence[Aggregations]
             | Sequence[dm.aggregations.MetricAggregation]
         ),
-        property: PriceAreaAFRRFields | Sequence[PriceAreaAFRRFields] | None = None,
-        group_by: PriceAreaAFRRFields | Sequence[PriceAreaAFRRFields] | None = None,
+        property: ScenarioFields | Sequence[ScenarioFields] | None = None,
+        group_by: ScenarioFields | Sequence[ScenarioFields] | None = None,
         query: str | None = None,
-        search_property: PriceAreaAFRRTextFields | Sequence[PriceAreaAFRRTextFields] | None = None,
+        search_property: ScenarioTextFields | Sequence[ScenarioTextFields] | None = None,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
-        timezone: str | list[str] | None = None,
-        timezone_prefix: str | None = None,
+        model_template: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        commands: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        source: str | list[str] | None = None,
+        source_prefix: str | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> list[dm.aggregations.AggregatedNumberedValue] | InstanceAggregationResultList:
-        """Aggregate data across price area afrrs
+        """Aggregate data across scenarios
 
         Args:
             aggregate: The aggregation to perform.
             property: The property to perform aggregation on.
             group_by: The property to group by when doing the aggregation.
             query: The query to search for in the text field.
             search_property: The text field to search in.
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
-            timezone: The timezone to filter on.
-            timezone_prefix: The prefix of the timezone to filter on.
+            model_template: The model template to filter on.
+            commands: The command to filter on.
+            source: The source to filter on.
+            source_prefix: The prefix of the source to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of price area afrrs to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of scenarios to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
             Aggregation results.
 
         Examples:
 
-            Count price area afrrs in space `my_space`:
+            Count scenarios in space `my_space`:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> result = client.price_area_afrr.aggregate("count", space="my_space")
+                >>> result = client.scenario.aggregate("count", space="my_space")
 
         """
 
-        filter_ = _create_price_area_afrr_filter(
+        filter_ = _create_scenario_filter(
             self._view_id,
             name,
             name_prefix,
-            timezone,
-            timezone_prefix,
+            model_template,
+            commands,
+            source,
+            source_prefix,
             external_id_prefix,
             space,
             filter,
         )
         return self._aggregate(
             self._view_id,
             aggregate,
-            _PRICEAREAAFRR_PROPERTIES_BY_FIELD,
+            _SCENARIO_PROPERTIES_BY_FIELD,
             property,
             group_by,
             query,
             search_property,
             limit,
             filter_,
         )
 
     def histogram(
         self,
-        property: PriceAreaAFRRFields,
+        property: ScenarioFields,
         interval: float,
         query: str | None = None,
-        search_property: PriceAreaAFRRTextFields | Sequence[PriceAreaAFRRTextFields] | None = None,
+        search_property: ScenarioTextFields | Sequence[ScenarioTextFields] | None = None,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
-        timezone: str | list[str] | None = None,
-        timezone_prefix: str | None = None,
+        model_template: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        commands: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        source: str | list[str] | None = None,
+        source_prefix: str | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> dm.aggregations.HistogramValue:
-        """Produces histograms for price area afrrs
+        """Produces histograms for scenarios
 
         Args:
             property: The property to use as the value in the histogram.
             interval: The interval to use for the histogram bins.
             query: The query to search for in the text field.
             search_property: The text field to search in.
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
-            timezone: The timezone to filter on.
-            timezone_prefix: The prefix of the timezone to filter on.
+            model_template: The model template to filter on.
+            commands: The command to filter on.
+            source: The source to filter on.
+            source_prefix: The prefix of the source to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of price area afrrs to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of scenarios to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
             Bucketed histogram results.
 
         """
-        filter_ = _create_price_area_afrr_filter(
+        filter_ = _create_scenario_filter(
             self._view_id,
             name,
             name_prefix,
-            timezone,
-            timezone_prefix,
+            model_template,
+            commands,
+            source,
+            source_prefix,
             external_id_prefix,
             space,
             filter,
         )
         return self._histogram(
             self._view_id,
             property,
             interval,
-            _PRICEAREAAFRR_PROPERTIES_BY_FIELD,
+            _SCENARIO_PROPERTIES_BY_FIELD,
             query,
             search_property,
             limit,
             filter_,
         )
 
     def list(
         self,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
-        timezone: str | list[str] | None = None,
-        timezone_prefix: str | None = None,
+        model_template: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        commands: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        source: str | list[str] | None = None,
+        source_prefix: str | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
-    ) -> PriceAreaAFRRList:
-        """List/filter price area afrrs
+        retrieve_edges: bool = True,
+    ) -> ScenarioList:
+        """List/filter scenarios
 
         Args:
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
-            timezone: The timezone to filter on.
-            timezone_prefix: The prefix of the timezone to filter on.
+            model_template: The model template to filter on.
+            commands: The command to filter on.
+            source: The source to filter on.
+            source_prefix: The prefix of the source to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of price area afrrs to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of scenarios to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
+            retrieve_edges: Whether to retrieve `mappings_override` external ids for the scenarios. Defaults to True.
 
         Returns:
-            List of requested price area afrrs
+            List of requested scenarios
 
         Examples:
 
-            List price area afrrs and limit to 5:
+            List scenarios and limit to 5:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> price_area_afrrs = client.price_area_afrr.list(limit=5)
+                >>> scenarios = client.scenario.list(limit=5)
 
         """
-        filter_ = _create_price_area_afrr_filter(
+        filter_ = _create_scenario_filter(
             self._view_id,
             name,
             name_prefix,
-            timezone,
-            timezone_prefix,
+            model_template,
+            commands,
+            source,
+            source_prefix,
             external_id_prefix,
             space,
             filter,
         )
-        return self._list(limit=limit, filter=filter_)
+
+        return self._list(
+            limit=limit,
+            filter=filter_,
+            retrieve_edges=retrieve_edges,
+            edge_api_name_type_direction_view_id_penta=[
+                (
+                    self.mappings_override_edge,
+                    "mappings_override",
+                    dm.DirectRelationReference("sp_powerops_types_temp", "Mapping"),
+                    "outwards",
+                    dm.ViewId("sp_powerops_models_temp", "Mapping", "1"),
+                ),
+            ],
+        )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/price_area_afrr_activation_price_down.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/price_area_afrr_activation_price_up.py`

 * *Files 6% similar despite different names*

```diff
@@ -10,28 +10,31 @@
 from cognite.client.data_classes import Datapoints, DatapointsArrayList, DatapointsList, TimeSeriesList
 from cognite.client.data_classes.datapoints import Aggregate
 from cognite.powerops.client._generated.v1.data_classes._price_area_afrr import _create_price_area_afrr_filter
 from ._core import DEFAULT_LIMIT_READ, INSTANCE_QUERY_LIMIT
 
 ColumnNames = Literal[
     "name",
+    "displayName",
+    "ordering",
+    "assetType",
     "timezone",
     "capacityPriceUp",
     "capacityPriceDown",
     "activationPriceUp",
     "activationPriceDown",
     "relativeActivation",
     "totalCapacityAllocationUp",
     "totalCapacityAllocationDown",
     "ownCapacityAllocationUp",
     "ownCapacityAllocationDown",
 ]
 
 
-class PriceAreaAFRRActivationPriceDownQuery:
+class PriceAreaAFRRActivationPriceUpQuery:
     def __init__(
         self,
         client: CogniteClient,
         view_id: dm.ViewId,
         timeseries_limit: int = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ):
@@ -48,15 +51,15 @@
         aggregates: Aggregate | list[Aggregate] | None = None,
         granularity: str | None = None,
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         limit: int | None = None,
         include_outside_points: bool = False,
     ) -> DatapointsList:
-        """`Retrieve datapoints for the `price_area_afrr.activation_price_down` timeseries.
+        """`Retrieve datapoints for the `price_area_afrr.activation_price_up` timeseries.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -73,19 +76,19 @@
 
         Returns:
             A ``DatapointsList`` with the requested datapoints.
 
         Examples:
 
             In this example,
-            we are using the time-ago format to get raw data for the 'my_activation_price_down' from 2 weeks ago up until now::
+            we are using the time-ago format to get raw data for the 'my_activation_price_up' from 2 weeks ago up until now::
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> price_area_afrr_datapoints = client.price_area_afrr.activation_price_down(external_id="my_activation_price_down").retrieve(start="2w-ago")
+                >>> price_area_afrr_datapoints = client.price_area_afrr.activation_price_up(external_id="my_activation_price_up").retrieve(start="2w-ago")
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra()
         if external_ids:
             return self._client.time_series.data.retrieve(
                 external_id=list(external_ids),
                 start=start,
                 end=end,
@@ -107,15 +110,15 @@
         aggregates: Aggregate | list[Aggregate] | None = None,
         granularity: str | None = None,
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         limit: int | None = None,
         include_outside_points: bool = False,
     ) -> DatapointsArrayList:
-        """`Retrieve numpy arrays for the `price_area_afrr.activation_price_down` timeseries.
+        """`Retrieve numpy arrays for the `price_area_afrr.activation_price_up` timeseries.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -132,19 +135,19 @@
 
         Returns:
             A ``DatapointsArrayList`` with the requested datapoints.
 
         Examples:
 
             In this example,
-            we are using the time-ago format to get raw data for the 'my_activation_price_down' from 2 weeks ago up until now::
+            we are using the time-ago format to get raw data for the 'my_activation_price_up' from 2 weeks ago up until now::
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> price_area_afrr_datapoints = client.price_area_afrr.activation_price_down(external_id="my_activation_price_down").retrieve_array(start="2w-ago")
+                >>> price_area_afrr_datapoints = client.price_area_afrr.activation_price_up(external_id="my_activation_price_up").retrieve_array(start="2w-ago")
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra()
         if external_ids:
             return self._client.time_series.data.retrieve_arrays(
                 external_id=list(external_ids),
                 start=start,
                 end=end,
@@ -168,17 +171,17 @@
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         limit: int | None = None,
         include_outside_points: bool = False,
         uniform_index: bool = False,
         include_aggregate_name: bool = True,
         include_granularity_name: bool = False,
-        column_names: ColumnNames | list[ColumnNames] = "activationPriceDown",
+        column_names: ColumnNames | list[ColumnNames] = "activationPriceUp",
     ) -> pd.DataFrame:
-        """`Retrieve DataFrames for the `price_area_afrr.activation_price_down` timeseries.
+        """`Retrieve DataFrames for the `price_area_afrr.activation_price_up` timeseries.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -191,28 +194,28 @@
             target_unit: The unit_external_id of the data points returned. If the time series does not have an unit_external_id that can be converted to the target_unit, an error will be returned. Cannot be used with target_unit_system.
             target_unit_system: The unit system of the data points returned. Cannot be used with target_unit.
             limit: Maximum number of datapoints to return for each time series. Default: None (no limit)
             include_outside_points: Whether to include outside points. Not allowed when fetching aggregates. Default: False
             uniform_index: If only querying aggregates AND a single granularity is used, AND no limit is used, specifying `uniform_index=True` will return a dataframe with an equidistant datetime index from the earliest `start` to the latest `end` (missing values will be NaNs). If these requirements are not met, a ValueError is raised. Default: False
             include_aggregate_name: Include 'aggregate' in the column name, e.g. `my-ts|average`. Ignored for raw time series. Default: True
             include_granularity_name: Include 'granularity' in the column name, e.g. `my-ts|12h`. Added after 'aggregate' when present. Ignored for raw time series. Default: False
-            column_names: Which property to use for column names. Defauts to activationPriceDown
+            column_names: Which property to use for column names. Defauts to activationPriceUp
 
 
         Returns:
             A ``DataFrame`` with the requested datapoints.
 
         Examples:
 
             In this example,
-            we are using the time-ago format to get raw data for the 'my_activation_price_down' from 2 weeks ago up until now::
+            we are using the time-ago format to get raw data for the 'my_activation_price_up' from 2 weeks ago up until now::
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> price_area_afrr_datapoints = client.price_area_afrr.activation_price_down(external_id="my_activation_price_down").retrieve_dataframe(start="2w-ago")
+                >>> price_area_afrr_datapoints = client.price_area_afrr.activation_price_up(external_id="my_activation_price_up").retrieve_dataframe(start="2w-ago")
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra(column_names)
         if external_ids:
             df = self._client.time_series.data.retrieve_dataframe(
                 external_id=list(external_ids),
                 start=start,
                 end=end,
@@ -245,17 +248,17 @@
         aggregates: Aggregate | Sequence[Aggregate] | None = None,
         granularity: str | None = None,
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         uniform_index: bool = False,
         include_aggregate_name: bool = True,
         include_granularity_name: bool = False,
-        column_names: ColumnNames | list[ColumnNames] = "activationPriceDown",
+        column_names: ColumnNames | list[ColumnNames] = "activationPriceUp",
     ) -> pd.DataFrame:
-        """Retrieve DataFrames for the `price_area_afrr.activation_price_down` timeseries in Timezone.
+        """Retrieve DataFrames for the `price_area_afrr.activation_price_up` timeseries in Timezone.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -268,30 +271,30 @@
             target_unit: The unit_external_id of the data points returned. If the time series does not have an unit_external_id that can be converted to the target_unit, an error will be returned. Cannot be used with target_unit_system.
             target_unit_system: The unit system of the data points returned. Cannot be used with target_unit.
             limit: Maximum number of datapoints to return for each time series. Default: None (no limit)
             include_outside_points: Whether to include outside points. Not allowed when fetching aggregates. Default: False
             uniform_index: If only querying aggregates AND a single granularity is used, AND no limit is used, specifying `uniform_index=True` will return a dataframe with an equidistant datetime index from the earliest `start` to the latest `end` (missing values will be NaNs). If these requirements are not met, a ValueError is raised. Default: False
             include_aggregate_name: Include 'aggregate' in the column name, e.g. `my-ts|average`. Ignored for raw time series. Default: True
             include_granularity_name: Include 'granularity' in the column name, e.g. `my-ts|12h`. Added after 'aggregate' when present. Ignored for raw time series. Default: False
-            column_names: Which property to use for column names. Defauts to activationPriceDown
+            column_names: Which property to use for column names. Defauts to activationPriceUp
 
 
         Returns:
             A ``DataFrame`` with the requested datapoints.
 
         Examples:
 
             In this example,
-            get weekly aggregates for the 'my_activation_price_down' for the first month of 2023 in Oslo time:
+            get weekly aggregates for the 'my_activation_price_up' for the first month of 2023 in Oslo time:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> from datetime import datetime, timezone
                 >>> client = PowerOpsModelsV1Client()
-                >>> price_area_afrr_datapoints = client.price_area_afrr.activation_price_down(
-                ...     external_id="my_activation_price_down").retrieve_dataframe_in_timezone(
+                >>> price_area_afrr_datapoints = client.price_area_afrr.activation_price_up(
+                ...     external_id="my_activation_price_up").retrieve_dataframe_in_timezone(
                 ...         datetime(2023, 1, 1, tzinfo=ZoneInfo("Europe/Oslo")),
                 ...         datetime(2023, 1, 2, tzinfo=ZoneInfo("Europe/Oslo")),
                 ...         aggregates="average",
                 ...         granularity="1week",
                 ...     )
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra(column_names)
@@ -329,17 +332,17 @@
                 external_id=list(external_ids),
                 before=before,
             )
         else:
             return None
 
     def _retrieve_timeseries_external_ids_with_extra(
-        self, extra_properties: ColumnNames | list[ColumnNames] = "activationPriceDown"
+        self, extra_properties: ColumnNames | list[ColumnNames] = "activationPriceUp"
     ) -> dict[str, list[str]]:
-        return _retrieve_timeseries_external_ids_with_extra_activation_price_down(
+        return _retrieve_timeseries_external_ids_with_extra_activation_price_up(
             self._client,
             self._view_id,
             self._filter,
             self._timeseries_limit,
             extra_properties,
         )
 
@@ -347,164 +350,200 @@
     def _rename_columns(
         external_ids: dict[str, list[str]],
         df: pd.DataFrame,
         column_names: ColumnNames | list[ColumnNames],
         include_aggregate_name: bool,
         include_granularity_name: bool,
     ) -> pd.DataFrame:
-        if isinstance(column_names, str) and column_names == "activationPriceDown":
+        if isinstance(column_names, str) and column_names == "activationPriceUp":
             return df
         splits = sum(included for included in [include_aggregate_name, include_granularity_name])
         if splits == 0:
             df.columns = ["-".join(external_ids[external_id]) for external_id in df.columns]
         else:
             column_parts = (col.rsplit("|", maxsplit=splits) for col in df.columns)
             df.columns = [
                 "-".join(external_ids[external_id]) + "|" + "|".join(parts) for external_id, *parts in column_parts
             ]
         return df
 
 
-class PriceAreaAFRRActivationPriceDownAPI:
+class PriceAreaAFRRActivationPriceUpAPI:
     def __init__(self, client: CogniteClient, view_id: dm.ViewId):
         self._client = client
         self._view_id = view_id
 
     def __call__(
         self,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
+        display_name: str | list[str] | None = None,
+        display_name_prefix: str | None = None,
+        min_ordering: int | None = None,
+        max_ordering: int | None = None,
+        asset_type: str | list[str] | None = None,
+        asset_type_prefix: str | None = None,
         timezone: str | list[str] | None = None,
         timezone_prefix: str | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
-    ) -> PriceAreaAFRRActivationPriceDownQuery:
-        """Query timeseries `price_area_afrr.activation_price_down`
+    ) -> PriceAreaAFRRActivationPriceUpQuery:
+        """Query timeseries `price_area_afrr.activation_price_up`
 
         Args:
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
+            display_name: The display name to filter on.
+            display_name_prefix: The prefix of the display name to filter on.
+            min_ordering: The minimum value of the ordering to filter on.
+            max_ordering: The maximum value of the ordering to filter on.
+            asset_type: The asset type to filter on.
+            asset_type_prefix: The prefix of the asset type to filter on.
             timezone: The timezone to filter on.
             timezone_prefix: The prefix of the timezone to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of price area afrrs to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
-            A query object that can be used to retrieve datapoins for the price_area_afrr.activation_price_down timeseries
+            A query object that can be used to retrieve datapoins for the price_area_afrr.activation_price_up timeseries
             selected in this method.
 
         Examples:
 
-            Retrieve all data for 5 price_area_afrr.activation_price_down timeseries:
+            Retrieve all data for 5 price_area_afrr.activation_price_up timeseries:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> price_area_afrrs = client.price_area_afrr.activation_price_down(limit=5).retrieve()
+                >>> price_area_afrrs = client.price_area_afrr.activation_price_up(limit=5).retrieve()
 
         """
         filter_ = _create_price_area_afrr_filter(
             self._view_id,
             name,
             name_prefix,
+            display_name,
+            display_name_prefix,
+            min_ordering,
+            max_ordering,
+            asset_type,
+            asset_type_prefix,
             timezone,
             timezone_prefix,
             external_id_prefix,
             space,
             filter,
         )
 
-        return PriceAreaAFRRActivationPriceDownQuery(
+        return PriceAreaAFRRActivationPriceUpQuery(
             client=self._client,
             view_id=self._view_id,
             timeseries_limit=limit,
             filter=filter_,
         )
 
     def list(
         self,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
+        display_name: str | list[str] | None = None,
+        display_name_prefix: str | None = None,
+        min_ordering: int | None = None,
+        max_ordering: int | None = None,
+        asset_type: str | list[str] | None = None,
+        asset_type_prefix: str | None = None,
         timezone: str | list[str] | None = None,
         timezone_prefix: str | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> TimeSeriesList:
-        """List timeseries `price_area_afrr.activation_price_down`
+        """List timeseries `price_area_afrr.activation_price_up`
 
         Args:
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
+            display_name: The display name to filter on.
+            display_name_prefix: The prefix of the display name to filter on.
+            min_ordering: The minimum value of the ordering to filter on.
+            max_ordering: The maximum value of the ordering to filter on.
+            asset_type: The asset type to filter on.
+            asset_type_prefix: The prefix of the asset type to filter on.
             timezone: The timezone to filter on.
             timezone_prefix: The prefix of the timezone to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of price area afrrs to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
-            List of Timeseries price_area_afrr.activation_price_down.
+            List of Timeseries price_area_afrr.activation_price_up.
 
         Examples:
 
-            List price_area_afrr.activation_price_down and limit to 5:
+            List price_area_afrr.activation_price_up and limit to 5:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> price_area_afrrs = client.price_area_afrr.activation_price_down.list(limit=5)
+                >>> price_area_afrrs = client.price_area_afrr.activation_price_up.list(limit=5)
 
         """
         filter_ = _create_price_area_afrr_filter(
             self._view_id,
             name,
             name_prefix,
+            display_name,
+            display_name_prefix,
+            min_ordering,
+            max_ordering,
+            asset_type,
+            asset_type_prefix,
             timezone,
             timezone_prefix,
             external_id_prefix,
             space,
             filter,
         )
-        external_ids = _retrieve_timeseries_external_ids_with_extra_activation_price_down(
+        external_ids = _retrieve_timeseries_external_ids_with_extra_activation_price_up(
             self._client, self._view_id, filter_, limit
         )
         if external_ids:
             return self._client.time_series.retrieve_multiple(external_ids=list(external_ids))
         else:
             return TimeSeriesList([])
 
 
-def _retrieve_timeseries_external_ids_with_extra_activation_price_down(
+def _retrieve_timeseries_external_ids_with_extra_activation_price_up(
     client: CogniteClient,
     view_id: dm.ViewId,
     filter_: dm.Filter | None,
     limit: int,
-    extra_properties: ColumnNames | list[ColumnNames] = "activationPriceDown",
+    extra_properties: ColumnNames | list[ColumnNames] = "activationPriceUp",
 ) -> dict[str, list[str]]:
     limit = float("inf") if limit is None or limit == -1 else limit
-    properties = ["activationPriceDown"]
-    if extra_properties == "activationPriceDown":
+    properties = ["activationPriceUp"]
+    if extra_properties == "activationPriceUp":
         ...
-    elif isinstance(extra_properties, str) and extra_properties != "activationPriceDown":
+    elif isinstance(extra_properties, str) and extra_properties != "activationPriceUp":
         properties.append(extra_properties)
     elif isinstance(extra_properties, list):
-        properties.extend([prop for prop in extra_properties if prop != "activationPriceDown"])
+        properties.extend([prop for prop in extra_properties if prop != "activationPriceUp"])
     else:
         raise ValueError(f"Invalid value for extra_properties: {extra_properties}")
 
     if isinstance(extra_properties, str):
         extra_list = [extra_properties]
     else:
         extra_list = extra_properties
     has_data = dm.filters.HasData(views=[view_id])
-    has_property = dm.filters.Exists(property=view_id.as_property_ref("activationPriceDown"))
+    has_property = dm.filters.Exists(property=view_id.as_property_ref("activationPriceUp"))
     filter_ = dm.filters.And(filter_, has_data, has_property) if filter_ else dm.filters.And(has_data, has_property)
 
     cursor = None
     external_ids: dict[str, list[str]] = {}
     total_retrieved = 0
     while True:
         query_limit = max(min(INSTANCE_QUERY_LIMIT, limit - total_retrieved), 0)
@@ -518,15 +557,15 @@
                     [dm.query.SourceSelector(view_id, properties)],
                 )
             },
             cursors={"nodes": cursor},
         )
         result = client.data_modeling.instances.query(query)
         batch_external_ids = {
-            node.properties[view_id]["activationPriceDown"]: [
+            node.properties[view_id]["activationPriceUp"]: [
                 node.properties[view_id].get(prop, "") for prop in extra_list
             ]
             for node in result.data["nodes"].data
         }
         total_retrieved += len(batch_external_ids)
         external_ids.update(batch_external_ids)
         cursor = result.cursors["nodes"]
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/price_area_afrr_activation_price_up.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/price_area_afrr_activation_price_down.py`

 * *Files 9% similar despite different names*

```diff
@@ -10,28 +10,31 @@
 from cognite.client.data_classes import Datapoints, DatapointsArrayList, DatapointsList, TimeSeriesList
 from cognite.client.data_classes.datapoints import Aggregate
 from cognite.powerops.client._generated.v1.data_classes._price_area_afrr import _create_price_area_afrr_filter
 from ._core import DEFAULT_LIMIT_READ, INSTANCE_QUERY_LIMIT
 
 ColumnNames = Literal[
     "name",
+    "displayName",
+    "ordering",
+    "assetType",
     "timezone",
     "capacityPriceUp",
     "capacityPriceDown",
     "activationPriceUp",
     "activationPriceDown",
     "relativeActivation",
     "totalCapacityAllocationUp",
     "totalCapacityAllocationDown",
     "ownCapacityAllocationUp",
     "ownCapacityAllocationDown",
 ]
 
 
-class PriceAreaAFRRActivationPriceUpQuery:
+class PriceAreaAFRRActivationPriceDownQuery:
     def __init__(
         self,
         client: CogniteClient,
         view_id: dm.ViewId,
         timeseries_limit: int = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ):
@@ -48,15 +51,15 @@
         aggregates: Aggregate | list[Aggregate] | None = None,
         granularity: str | None = None,
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         limit: int | None = None,
         include_outside_points: bool = False,
     ) -> DatapointsList:
-        """`Retrieve datapoints for the `price_area_afrr.activation_price_up` timeseries.
+        """`Retrieve datapoints for the `price_area_afrr.activation_price_down` timeseries.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -73,19 +76,19 @@
 
         Returns:
             A ``DatapointsList`` with the requested datapoints.
 
         Examples:
 
             In this example,
-            we are using the time-ago format to get raw data for the 'my_activation_price_up' from 2 weeks ago up until now::
+            we are using the time-ago format to get raw data for the 'my_activation_price_down' from 2 weeks ago up until now::
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> price_area_afrr_datapoints = client.price_area_afrr.activation_price_up(external_id="my_activation_price_up").retrieve(start="2w-ago")
+                >>> price_area_afrr_datapoints = client.price_area_afrr.activation_price_down(external_id="my_activation_price_down").retrieve(start="2w-ago")
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra()
         if external_ids:
             return self._client.time_series.data.retrieve(
                 external_id=list(external_ids),
                 start=start,
                 end=end,
@@ -107,15 +110,15 @@
         aggregates: Aggregate | list[Aggregate] | None = None,
         granularity: str | None = None,
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         limit: int | None = None,
         include_outside_points: bool = False,
     ) -> DatapointsArrayList:
-        """`Retrieve numpy arrays for the `price_area_afrr.activation_price_up` timeseries.
+        """`Retrieve numpy arrays for the `price_area_afrr.activation_price_down` timeseries.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -132,19 +135,19 @@
 
         Returns:
             A ``DatapointsArrayList`` with the requested datapoints.
 
         Examples:
 
             In this example,
-            we are using the time-ago format to get raw data for the 'my_activation_price_up' from 2 weeks ago up until now::
+            we are using the time-ago format to get raw data for the 'my_activation_price_down' from 2 weeks ago up until now::
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> price_area_afrr_datapoints = client.price_area_afrr.activation_price_up(external_id="my_activation_price_up").retrieve_array(start="2w-ago")
+                >>> price_area_afrr_datapoints = client.price_area_afrr.activation_price_down(external_id="my_activation_price_down").retrieve_array(start="2w-ago")
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra()
         if external_ids:
             return self._client.time_series.data.retrieve_arrays(
                 external_id=list(external_ids),
                 start=start,
                 end=end,
@@ -168,17 +171,17 @@
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         limit: int | None = None,
         include_outside_points: bool = False,
         uniform_index: bool = False,
         include_aggregate_name: bool = True,
         include_granularity_name: bool = False,
-        column_names: ColumnNames | list[ColumnNames] = "activationPriceUp",
+        column_names: ColumnNames | list[ColumnNames] = "activationPriceDown",
     ) -> pd.DataFrame:
-        """`Retrieve DataFrames for the `price_area_afrr.activation_price_up` timeseries.
+        """`Retrieve DataFrames for the `price_area_afrr.activation_price_down` timeseries.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -191,28 +194,28 @@
             target_unit: The unit_external_id of the data points returned. If the time series does not have an unit_external_id that can be converted to the target_unit, an error will be returned. Cannot be used with target_unit_system.
             target_unit_system: The unit system of the data points returned. Cannot be used with target_unit.
             limit: Maximum number of datapoints to return for each time series. Default: None (no limit)
             include_outside_points: Whether to include outside points. Not allowed when fetching aggregates. Default: False
             uniform_index: If only querying aggregates AND a single granularity is used, AND no limit is used, specifying `uniform_index=True` will return a dataframe with an equidistant datetime index from the earliest `start` to the latest `end` (missing values will be NaNs). If these requirements are not met, a ValueError is raised. Default: False
             include_aggregate_name: Include 'aggregate' in the column name, e.g. `my-ts|average`. Ignored for raw time series. Default: True
             include_granularity_name: Include 'granularity' in the column name, e.g. `my-ts|12h`. Added after 'aggregate' when present. Ignored for raw time series. Default: False
-            column_names: Which property to use for column names. Defauts to activationPriceUp
+            column_names: Which property to use for column names. Defauts to activationPriceDown
 
 
         Returns:
             A ``DataFrame`` with the requested datapoints.
 
         Examples:
 
             In this example,
-            we are using the time-ago format to get raw data for the 'my_activation_price_up' from 2 weeks ago up until now::
+            we are using the time-ago format to get raw data for the 'my_activation_price_down' from 2 weeks ago up until now::
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> price_area_afrr_datapoints = client.price_area_afrr.activation_price_up(external_id="my_activation_price_up").retrieve_dataframe(start="2w-ago")
+                >>> price_area_afrr_datapoints = client.price_area_afrr.activation_price_down(external_id="my_activation_price_down").retrieve_dataframe(start="2w-ago")
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra(column_names)
         if external_ids:
             df = self._client.time_series.data.retrieve_dataframe(
                 external_id=list(external_ids),
                 start=start,
                 end=end,
@@ -245,17 +248,17 @@
         aggregates: Aggregate | Sequence[Aggregate] | None = None,
         granularity: str | None = None,
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         uniform_index: bool = False,
         include_aggregate_name: bool = True,
         include_granularity_name: bool = False,
-        column_names: ColumnNames | list[ColumnNames] = "activationPriceUp",
+        column_names: ColumnNames | list[ColumnNames] = "activationPriceDown",
     ) -> pd.DataFrame:
-        """Retrieve DataFrames for the `price_area_afrr.activation_price_up` timeseries in Timezone.
+        """Retrieve DataFrames for the `price_area_afrr.activation_price_down` timeseries in Timezone.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -268,30 +271,30 @@
             target_unit: The unit_external_id of the data points returned. If the time series does not have an unit_external_id that can be converted to the target_unit, an error will be returned. Cannot be used with target_unit_system.
             target_unit_system: The unit system of the data points returned. Cannot be used with target_unit.
             limit: Maximum number of datapoints to return for each time series. Default: None (no limit)
             include_outside_points: Whether to include outside points. Not allowed when fetching aggregates. Default: False
             uniform_index: If only querying aggregates AND a single granularity is used, AND no limit is used, specifying `uniform_index=True` will return a dataframe with an equidistant datetime index from the earliest `start` to the latest `end` (missing values will be NaNs). If these requirements are not met, a ValueError is raised. Default: False
             include_aggregate_name: Include 'aggregate' in the column name, e.g. `my-ts|average`. Ignored for raw time series. Default: True
             include_granularity_name: Include 'granularity' in the column name, e.g. `my-ts|12h`. Added after 'aggregate' when present. Ignored for raw time series. Default: False
-            column_names: Which property to use for column names. Defauts to activationPriceUp
+            column_names: Which property to use for column names. Defauts to activationPriceDown
 
 
         Returns:
             A ``DataFrame`` with the requested datapoints.
 
         Examples:
 
             In this example,
-            get weekly aggregates for the 'my_activation_price_up' for the first month of 2023 in Oslo time:
+            get weekly aggregates for the 'my_activation_price_down' for the first month of 2023 in Oslo time:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> from datetime import datetime, timezone
                 >>> client = PowerOpsModelsV1Client()
-                >>> price_area_afrr_datapoints = client.price_area_afrr.activation_price_up(
-                ...     external_id="my_activation_price_up").retrieve_dataframe_in_timezone(
+                >>> price_area_afrr_datapoints = client.price_area_afrr.activation_price_down(
+                ...     external_id="my_activation_price_down").retrieve_dataframe_in_timezone(
                 ...         datetime(2023, 1, 1, tzinfo=ZoneInfo("Europe/Oslo")),
                 ...         datetime(2023, 1, 2, tzinfo=ZoneInfo("Europe/Oslo")),
                 ...         aggregates="average",
                 ...         granularity="1week",
                 ...     )
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra(column_names)
@@ -329,17 +332,17 @@
                 external_id=list(external_ids),
                 before=before,
             )
         else:
             return None
 
     def _retrieve_timeseries_external_ids_with_extra(
-        self, extra_properties: ColumnNames | list[ColumnNames] = "activationPriceUp"
+        self, extra_properties: ColumnNames | list[ColumnNames] = "activationPriceDown"
     ) -> dict[str, list[str]]:
-        return _retrieve_timeseries_external_ids_with_extra_activation_price_up(
+        return _retrieve_timeseries_external_ids_with_extra_activation_price_down(
             self._client,
             self._view_id,
             self._filter,
             self._timeseries_limit,
             extra_properties,
         )
 
@@ -347,164 +350,200 @@
     def _rename_columns(
         external_ids: dict[str, list[str]],
         df: pd.DataFrame,
         column_names: ColumnNames | list[ColumnNames],
         include_aggregate_name: bool,
         include_granularity_name: bool,
     ) -> pd.DataFrame:
-        if isinstance(column_names, str) and column_names == "activationPriceUp":
+        if isinstance(column_names, str) and column_names == "activationPriceDown":
             return df
         splits = sum(included for included in [include_aggregate_name, include_granularity_name])
         if splits == 0:
             df.columns = ["-".join(external_ids[external_id]) for external_id in df.columns]
         else:
             column_parts = (col.rsplit("|", maxsplit=splits) for col in df.columns)
             df.columns = [
                 "-".join(external_ids[external_id]) + "|" + "|".join(parts) for external_id, *parts in column_parts
             ]
         return df
 
 
-class PriceAreaAFRRActivationPriceUpAPI:
+class PriceAreaAFRRActivationPriceDownAPI:
     def __init__(self, client: CogniteClient, view_id: dm.ViewId):
         self._client = client
         self._view_id = view_id
 
     def __call__(
         self,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
+        display_name: str | list[str] | None = None,
+        display_name_prefix: str | None = None,
+        min_ordering: int | None = None,
+        max_ordering: int | None = None,
+        asset_type: str | list[str] | None = None,
+        asset_type_prefix: str | None = None,
         timezone: str | list[str] | None = None,
         timezone_prefix: str | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
-    ) -> PriceAreaAFRRActivationPriceUpQuery:
-        """Query timeseries `price_area_afrr.activation_price_up`
+    ) -> PriceAreaAFRRActivationPriceDownQuery:
+        """Query timeseries `price_area_afrr.activation_price_down`
 
         Args:
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
+            display_name: The display name to filter on.
+            display_name_prefix: The prefix of the display name to filter on.
+            min_ordering: The minimum value of the ordering to filter on.
+            max_ordering: The maximum value of the ordering to filter on.
+            asset_type: The asset type to filter on.
+            asset_type_prefix: The prefix of the asset type to filter on.
             timezone: The timezone to filter on.
             timezone_prefix: The prefix of the timezone to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of price area afrrs to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
-            A query object that can be used to retrieve datapoins for the price_area_afrr.activation_price_up timeseries
+            A query object that can be used to retrieve datapoins for the price_area_afrr.activation_price_down timeseries
             selected in this method.
 
         Examples:
 
-            Retrieve all data for 5 price_area_afrr.activation_price_up timeseries:
+            Retrieve all data for 5 price_area_afrr.activation_price_down timeseries:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> price_area_afrrs = client.price_area_afrr.activation_price_up(limit=5).retrieve()
+                >>> price_area_afrrs = client.price_area_afrr.activation_price_down(limit=5).retrieve()
 
         """
         filter_ = _create_price_area_afrr_filter(
             self._view_id,
             name,
             name_prefix,
+            display_name,
+            display_name_prefix,
+            min_ordering,
+            max_ordering,
+            asset_type,
+            asset_type_prefix,
             timezone,
             timezone_prefix,
             external_id_prefix,
             space,
             filter,
         )
 
-        return PriceAreaAFRRActivationPriceUpQuery(
+        return PriceAreaAFRRActivationPriceDownQuery(
             client=self._client,
             view_id=self._view_id,
             timeseries_limit=limit,
             filter=filter_,
         )
 
     def list(
         self,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
+        display_name: str | list[str] | None = None,
+        display_name_prefix: str | None = None,
+        min_ordering: int | None = None,
+        max_ordering: int | None = None,
+        asset_type: str | list[str] | None = None,
+        asset_type_prefix: str | None = None,
         timezone: str | list[str] | None = None,
         timezone_prefix: str | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> TimeSeriesList:
-        """List timeseries `price_area_afrr.activation_price_up`
+        """List timeseries `price_area_afrr.activation_price_down`
 
         Args:
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
+            display_name: The display name to filter on.
+            display_name_prefix: The prefix of the display name to filter on.
+            min_ordering: The minimum value of the ordering to filter on.
+            max_ordering: The maximum value of the ordering to filter on.
+            asset_type: The asset type to filter on.
+            asset_type_prefix: The prefix of the asset type to filter on.
             timezone: The timezone to filter on.
             timezone_prefix: The prefix of the timezone to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of price area afrrs to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
-            List of Timeseries price_area_afrr.activation_price_up.
+            List of Timeseries price_area_afrr.activation_price_down.
 
         Examples:
 
-            List price_area_afrr.activation_price_up and limit to 5:
+            List price_area_afrr.activation_price_down and limit to 5:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> price_area_afrrs = client.price_area_afrr.activation_price_up.list(limit=5)
+                >>> price_area_afrrs = client.price_area_afrr.activation_price_down.list(limit=5)
 
         """
         filter_ = _create_price_area_afrr_filter(
             self._view_id,
             name,
             name_prefix,
+            display_name,
+            display_name_prefix,
+            min_ordering,
+            max_ordering,
+            asset_type,
+            asset_type_prefix,
             timezone,
             timezone_prefix,
             external_id_prefix,
             space,
             filter,
         )
-        external_ids = _retrieve_timeseries_external_ids_with_extra_activation_price_up(
+        external_ids = _retrieve_timeseries_external_ids_with_extra_activation_price_down(
             self._client, self._view_id, filter_, limit
         )
         if external_ids:
             return self._client.time_series.retrieve_multiple(external_ids=list(external_ids))
         else:
             return TimeSeriesList([])
 
 
-def _retrieve_timeseries_external_ids_with_extra_activation_price_up(
+def _retrieve_timeseries_external_ids_with_extra_activation_price_down(
     client: CogniteClient,
     view_id: dm.ViewId,
     filter_: dm.Filter | None,
     limit: int,
-    extra_properties: ColumnNames | list[ColumnNames] = "activationPriceUp",
+    extra_properties: ColumnNames | list[ColumnNames] = "activationPriceDown",
 ) -> dict[str, list[str]]:
     limit = float("inf") if limit is None or limit == -1 else limit
-    properties = ["activationPriceUp"]
-    if extra_properties == "activationPriceUp":
+    properties = ["activationPriceDown"]
+    if extra_properties == "activationPriceDown":
         ...
-    elif isinstance(extra_properties, str) and extra_properties != "activationPriceUp":
+    elif isinstance(extra_properties, str) and extra_properties != "activationPriceDown":
         properties.append(extra_properties)
     elif isinstance(extra_properties, list):
-        properties.extend([prop for prop in extra_properties if prop != "activationPriceUp"])
+        properties.extend([prop for prop in extra_properties if prop != "activationPriceDown"])
     else:
         raise ValueError(f"Invalid value for extra_properties: {extra_properties}")
 
     if isinstance(extra_properties, str):
         extra_list = [extra_properties]
     else:
         extra_list = extra_properties
     has_data = dm.filters.HasData(views=[view_id])
-    has_property = dm.filters.Exists(property=view_id.as_property_ref("activationPriceUp"))
+    has_property = dm.filters.Exists(property=view_id.as_property_ref("activationPriceDown"))
     filter_ = dm.filters.And(filter_, has_data, has_property) if filter_ else dm.filters.And(has_data, has_property)
 
     cursor = None
     external_ids: dict[str, list[str]] = {}
     total_retrieved = 0
     while True:
         query_limit = max(min(INSTANCE_QUERY_LIMIT, limit - total_retrieved), 0)
@@ -518,15 +557,15 @@
                     [dm.query.SourceSelector(view_id, properties)],
                 )
             },
             cursors={"nodes": cursor},
         )
         result = client.data_modeling.instances.query(query)
         batch_external_ids = {
-            node.properties[view_id]["activationPriceUp"]: [
+            node.properties[view_id]["activationPriceDown"]: [
                 node.properties[view_id].get(prop, "") for prop in extra_list
             ]
             for node in result.data["nodes"].data
         }
         total_retrieved += len(batch_external_ids)
         external_ids.update(batch_external_ids)
         cursor = result.cursors["nodes"]
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/price_area_afrr_capacity_price_down.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/price_area_afrr_capacity_price_up.py`

 * *Files 9% similar despite different names*

```diff
@@ -10,28 +10,31 @@
 from cognite.client.data_classes import Datapoints, DatapointsArrayList, DatapointsList, TimeSeriesList
 from cognite.client.data_classes.datapoints import Aggregate
 from cognite.powerops.client._generated.v1.data_classes._price_area_afrr import _create_price_area_afrr_filter
 from ._core import DEFAULT_LIMIT_READ, INSTANCE_QUERY_LIMIT
 
 ColumnNames = Literal[
     "name",
+    "displayName",
+    "ordering",
+    "assetType",
     "timezone",
     "capacityPriceUp",
     "capacityPriceDown",
     "activationPriceUp",
     "activationPriceDown",
     "relativeActivation",
     "totalCapacityAllocationUp",
     "totalCapacityAllocationDown",
     "ownCapacityAllocationUp",
     "ownCapacityAllocationDown",
 ]
 
 
-class PriceAreaAFRRCapacityPriceDownQuery:
+class PriceAreaAFRRCapacityPriceUpQuery:
     def __init__(
         self,
         client: CogniteClient,
         view_id: dm.ViewId,
         timeseries_limit: int = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ):
@@ -48,15 +51,15 @@
         aggregates: Aggregate | list[Aggregate] | None = None,
         granularity: str | None = None,
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         limit: int | None = None,
         include_outside_points: bool = False,
     ) -> DatapointsList:
-        """`Retrieve datapoints for the `price_area_afrr.capacity_price_down` timeseries.
+        """`Retrieve datapoints for the `price_area_afrr.capacity_price_up` timeseries.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -73,19 +76,19 @@
 
         Returns:
             A ``DatapointsList`` with the requested datapoints.
 
         Examples:
 
             In this example,
-            we are using the time-ago format to get raw data for the 'my_capacity_price_down' from 2 weeks ago up until now::
+            we are using the time-ago format to get raw data for the 'my_capacity_price_up' from 2 weeks ago up until now::
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> price_area_afrr_datapoints = client.price_area_afrr.capacity_price_down(external_id="my_capacity_price_down").retrieve(start="2w-ago")
+                >>> price_area_afrr_datapoints = client.price_area_afrr.capacity_price_up(external_id="my_capacity_price_up").retrieve(start="2w-ago")
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra()
         if external_ids:
             return self._client.time_series.data.retrieve(
                 external_id=list(external_ids),
                 start=start,
                 end=end,
@@ -107,15 +110,15 @@
         aggregates: Aggregate | list[Aggregate] | None = None,
         granularity: str | None = None,
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         limit: int | None = None,
         include_outside_points: bool = False,
     ) -> DatapointsArrayList:
-        """`Retrieve numpy arrays for the `price_area_afrr.capacity_price_down` timeseries.
+        """`Retrieve numpy arrays for the `price_area_afrr.capacity_price_up` timeseries.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -132,19 +135,19 @@
 
         Returns:
             A ``DatapointsArrayList`` with the requested datapoints.
 
         Examples:
 
             In this example,
-            we are using the time-ago format to get raw data for the 'my_capacity_price_down' from 2 weeks ago up until now::
+            we are using the time-ago format to get raw data for the 'my_capacity_price_up' from 2 weeks ago up until now::
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> price_area_afrr_datapoints = client.price_area_afrr.capacity_price_down(external_id="my_capacity_price_down").retrieve_array(start="2w-ago")
+                >>> price_area_afrr_datapoints = client.price_area_afrr.capacity_price_up(external_id="my_capacity_price_up").retrieve_array(start="2w-ago")
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra()
         if external_ids:
             return self._client.time_series.data.retrieve_arrays(
                 external_id=list(external_ids),
                 start=start,
                 end=end,
@@ -168,17 +171,17 @@
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         limit: int | None = None,
         include_outside_points: bool = False,
         uniform_index: bool = False,
         include_aggregate_name: bool = True,
         include_granularity_name: bool = False,
-        column_names: ColumnNames | list[ColumnNames] = "capacityPriceDown",
+        column_names: ColumnNames | list[ColumnNames] = "capacityPriceUp",
     ) -> pd.DataFrame:
-        """`Retrieve DataFrames for the `price_area_afrr.capacity_price_down` timeseries.
+        """`Retrieve DataFrames for the `price_area_afrr.capacity_price_up` timeseries.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -191,28 +194,28 @@
             target_unit: The unit_external_id of the data points returned. If the time series does not have an unit_external_id that can be converted to the target_unit, an error will be returned. Cannot be used with target_unit_system.
             target_unit_system: The unit system of the data points returned. Cannot be used with target_unit.
             limit: Maximum number of datapoints to return for each time series. Default: None (no limit)
             include_outside_points: Whether to include outside points. Not allowed when fetching aggregates. Default: False
             uniform_index: If only querying aggregates AND a single granularity is used, AND no limit is used, specifying `uniform_index=True` will return a dataframe with an equidistant datetime index from the earliest `start` to the latest `end` (missing values will be NaNs). If these requirements are not met, a ValueError is raised. Default: False
             include_aggregate_name: Include 'aggregate' in the column name, e.g. `my-ts|average`. Ignored for raw time series. Default: True
             include_granularity_name: Include 'granularity' in the column name, e.g. `my-ts|12h`. Added after 'aggregate' when present. Ignored for raw time series. Default: False
-            column_names: Which property to use for column names. Defauts to capacityPriceDown
+            column_names: Which property to use for column names. Defauts to capacityPriceUp
 
 
         Returns:
             A ``DataFrame`` with the requested datapoints.
 
         Examples:
 
             In this example,
-            we are using the time-ago format to get raw data for the 'my_capacity_price_down' from 2 weeks ago up until now::
+            we are using the time-ago format to get raw data for the 'my_capacity_price_up' from 2 weeks ago up until now::
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> price_area_afrr_datapoints = client.price_area_afrr.capacity_price_down(external_id="my_capacity_price_down").retrieve_dataframe(start="2w-ago")
+                >>> price_area_afrr_datapoints = client.price_area_afrr.capacity_price_up(external_id="my_capacity_price_up").retrieve_dataframe(start="2w-ago")
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra(column_names)
         if external_ids:
             df = self._client.time_series.data.retrieve_dataframe(
                 external_id=list(external_ids),
                 start=start,
                 end=end,
@@ -245,17 +248,17 @@
         aggregates: Aggregate | Sequence[Aggregate] | None = None,
         granularity: str | None = None,
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         uniform_index: bool = False,
         include_aggregate_name: bool = True,
         include_granularity_name: bool = False,
-        column_names: ColumnNames | list[ColumnNames] = "capacityPriceDown",
+        column_names: ColumnNames | list[ColumnNames] = "capacityPriceUp",
     ) -> pd.DataFrame:
-        """Retrieve DataFrames for the `price_area_afrr.capacity_price_down` timeseries in Timezone.
+        """Retrieve DataFrames for the `price_area_afrr.capacity_price_up` timeseries in Timezone.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -268,30 +271,30 @@
             target_unit: The unit_external_id of the data points returned. If the time series does not have an unit_external_id that can be converted to the target_unit, an error will be returned. Cannot be used with target_unit_system.
             target_unit_system: The unit system of the data points returned. Cannot be used with target_unit.
             limit: Maximum number of datapoints to return for each time series. Default: None (no limit)
             include_outside_points: Whether to include outside points. Not allowed when fetching aggregates. Default: False
             uniform_index: If only querying aggregates AND a single granularity is used, AND no limit is used, specifying `uniform_index=True` will return a dataframe with an equidistant datetime index from the earliest `start` to the latest `end` (missing values will be NaNs). If these requirements are not met, a ValueError is raised. Default: False
             include_aggregate_name: Include 'aggregate' in the column name, e.g. `my-ts|average`. Ignored for raw time series. Default: True
             include_granularity_name: Include 'granularity' in the column name, e.g. `my-ts|12h`. Added after 'aggregate' when present. Ignored for raw time series. Default: False
-            column_names: Which property to use for column names. Defauts to capacityPriceDown
+            column_names: Which property to use for column names. Defauts to capacityPriceUp
 
 
         Returns:
             A ``DataFrame`` with the requested datapoints.
 
         Examples:
 
             In this example,
-            get weekly aggregates for the 'my_capacity_price_down' for the first month of 2023 in Oslo time:
+            get weekly aggregates for the 'my_capacity_price_up' for the first month of 2023 in Oslo time:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> from datetime import datetime, timezone
                 >>> client = PowerOpsModelsV1Client()
-                >>> price_area_afrr_datapoints = client.price_area_afrr.capacity_price_down(
-                ...     external_id="my_capacity_price_down").retrieve_dataframe_in_timezone(
+                >>> price_area_afrr_datapoints = client.price_area_afrr.capacity_price_up(
+                ...     external_id="my_capacity_price_up").retrieve_dataframe_in_timezone(
                 ...         datetime(2023, 1, 1, tzinfo=ZoneInfo("Europe/Oslo")),
                 ...         datetime(2023, 1, 2, tzinfo=ZoneInfo("Europe/Oslo")),
                 ...         aggregates="average",
                 ...         granularity="1week",
                 ...     )
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra(column_names)
@@ -329,17 +332,17 @@
                 external_id=list(external_ids),
                 before=before,
             )
         else:
             return None
 
     def _retrieve_timeseries_external_ids_with_extra(
-        self, extra_properties: ColumnNames | list[ColumnNames] = "capacityPriceDown"
+        self, extra_properties: ColumnNames | list[ColumnNames] = "capacityPriceUp"
     ) -> dict[str, list[str]]:
-        return _retrieve_timeseries_external_ids_with_extra_capacity_price_down(
+        return _retrieve_timeseries_external_ids_with_extra_capacity_price_up(
             self._client,
             self._view_id,
             self._filter,
             self._timeseries_limit,
             extra_properties,
         )
 
@@ -347,164 +350,200 @@
     def _rename_columns(
         external_ids: dict[str, list[str]],
         df: pd.DataFrame,
         column_names: ColumnNames | list[ColumnNames],
         include_aggregate_name: bool,
         include_granularity_name: bool,
     ) -> pd.DataFrame:
-        if isinstance(column_names, str) and column_names == "capacityPriceDown":
+        if isinstance(column_names, str) and column_names == "capacityPriceUp":
             return df
         splits = sum(included for included in [include_aggregate_name, include_granularity_name])
         if splits == 0:
             df.columns = ["-".join(external_ids[external_id]) for external_id in df.columns]
         else:
             column_parts = (col.rsplit("|", maxsplit=splits) for col in df.columns)
             df.columns = [
                 "-".join(external_ids[external_id]) + "|" + "|".join(parts) for external_id, *parts in column_parts
             ]
         return df
 
 
-class PriceAreaAFRRCapacityPriceDownAPI:
+class PriceAreaAFRRCapacityPriceUpAPI:
     def __init__(self, client: CogniteClient, view_id: dm.ViewId):
         self._client = client
         self._view_id = view_id
 
     def __call__(
         self,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
+        display_name: str | list[str] | None = None,
+        display_name_prefix: str | None = None,
+        min_ordering: int | None = None,
+        max_ordering: int | None = None,
+        asset_type: str | list[str] | None = None,
+        asset_type_prefix: str | None = None,
         timezone: str | list[str] | None = None,
         timezone_prefix: str | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
-    ) -> PriceAreaAFRRCapacityPriceDownQuery:
-        """Query timeseries `price_area_afrr.capacity_price_down`
+    ) -> PriceAreaAFRRCapacityPriceUpQuery:
+        """Query timeseries `price_area_afrr.capacity_price_up`
 
         Args:
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
+            display_name: The display name to filter on.
+            display_name_prefix: The prefix of the display name to filter on.
+            min_ordering: The minimum value of the ordering to filter on.
+            max_ordering: The maximum value of the ordering to filter on.
+            asset_type: The asset type to filter on.
+            asset_type_prefix: The prefix of the asset type to filter on.
             timezone: The timezone to filter on.
             timezone_prefix: The prefix of the timezone to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of price area afrrs to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
-            A query object that can be used to retrieve datapoins for the price_area_afrr.capacity_price_down timeseries
+            A query object that can be used to retrieve datapoins for the price_area_afrr.capacity_price_up timeseries
             selected in this method.
 
         Examples:
 
-            Retrieve all data for 5 price_area_afrr.capacity_price_down timeseries:
+            Retrieve all data for 5 price_area_afrr.capacity_price_up timeseries:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> price_area_afrrs = client.price_area_afrr.capacity_price_down(limit=5).retrieve()
+                >>> price_area_afrrs = client.price_area_afrr.capacity_price_up(limit=5).retrieve()
 
         """
         filter_ = _create_price_area_afrr_filter(
             self._view_id,
             name,
             name_prefix,
+            display_name,
+            display_name_prefix,
+            min_ordering,
+            max_ordering,
+            asset_type,
+            asset_type_prefix,
             timezone,
             timezone_prefix,
             external_id_prefix,
             space,
             filter,
         )
 
-        return PriceAreaAFRRCapacityPriceDownQuery(
+        return PriceAreaAFRRCapacityPriceUpQuery(
             client=self._client,
             view_id=self._view_id,
             timeseries_limit=limit,
             filter=filter_,
         )
 
     def list(
         self,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
+        display_name: str | list[str] | None = None,
+        display_name_prefix: str | None = None,
+        min_ordering: int | None = None,
+        max_ordering: int | None = None,
+        asset_type: str | list[str] | None = None,
+        asset_type_prefix: str | None = None,
         timezone: str | list[str] | None = None,
         timezone_prefix: str | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> TimeSeriesList:
-        """List timeseries `price_area_afrr.capacity_price_down`
+        """List timeseries `price_area_afrr.capacity_price_up`
 
         Args:
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
+            display_name: The display name to filter on.
+            display_name_prefix: The prefix of the display name to filter on.
+            min_ordering: The minimum value of the ordering to filter on.
+            max_ordering: The maximum value of the ordering to filter on.
+            asset_type: The asset type to filter on.
+            asset_type_prefix: The prefix of the asset type to filter on.
             timezone: The timezone to filter on.
             timezone_prefix: The prefix of the timezone to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of price area afrrs to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
-            List of Timeseries price_area_afrr.capacity_price_down.
+            List of Timeseries price_area_afrr.capacity_price_up.
 
         Examples:
 
-            List price_area_afrr.capacity_price_down and limit to 5:
+            List price_area_afrr.capacity_price_up and limit to 5:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> price_area_afrrs = client.price_area_afrr.capacity_price_down.list(limit=5)
+                >>> price_area_afrrs = client.price_area_afrr.capacity_price_up.list(limit=5)
 
         """
         filter_ = _create_price_area_afrr_filter(
             self._view_id,
             name,
             name_prefix,
+            display_name,
+            display_name_prefix,
+            min_ordering,
+            max_ordering,
+            asset_type,
+            asset_type_prefix,
             timezone,
             timezone_prefix,
             external_id_prefix,
             space,
             filter,
         )
-        external_ids = _retrieve_timeseries_external_ids_with_extra_capacity_price_down(
+        external_ids = _retrieve_timeseries_external_ids_with_extra_capacity_price_up(
             self._client, self._view_id, filter_, limit
         )
         if external_ids:
             return self._client.time_series.retrieve_multiple(external_ids=list(external_ids))
         else:
             return TimeSeriesList([])
 
 
-def _retrieve_timeseries_external_ids_with_extra_capacity_price_down(
+def _retrieve_timeseries_external_ids_with_extra_capacity_price_up(
     client: CogniteClient,
     view_id: dm.ViewId,
     filter_: dm.Filter | None,
     limit: int,
-    extra_properties: ColumnNames | list[ColumnNames] = "capacityPriceDown",
+    extra_properties: ColumnNames | list[ColumnNames] = "capacityPriceUp",
 ) -> dict[str, list[str]]:
     limit = float("inf") if limit is None or limit == -1 else limit
-    properties = ["capacityPriceDown"]
-    if extra_properties == "capacityPriceDown":
+    properties = ["capacityPriceUp"]
+    if extra_properties == "capacityPriceUp":
         ...
-    elif isinstance(extra_properties, str) and extra_properties != "capacityPriceDown":
+    elif isinstance(extra_properties, str) and extra_properties != "capacityPriceUp":
         properties.append(extra_properties)
     elif isinstance(extra_properties, list):
-        properties.extend([prop for prop in extra_properties if prop != "capacityPriceDown"])
+        properties.extend([prop for prop in extra_properties if prop != "capacityPriceUp"])
     else:
         raise ValueError(f"Invalid value for extra_properties: {extra_properties}")
 
     if isinstance(extra_properties, str):
         extra_list = [extra_properties]
     else:
         extra_list = extra_properties
     has_data = dm.filters.HasData(views=[view_id])
-    has_property = dm.filters.Exists(property=view_id.as_property_ref("capacityPriceDown"))
+    has_property = dm.filters.Exists(property=view_id.as_property_ref("capacityPriceUp"))
     filter_ = dm.filters.And(filter_, has_data, has_property) if filter_ else dm.filters.And(has_data, has_property)
 
     cursor = None
     external_ids: dict[str, list[str]] = {}
     total_retrieved = 0
     while True:
         query_limit = max(min(INSTANCE_QUERY_LIMIT, limit - total_retrieved), 0)
@@ -518,17 +557,15 @@
                     [dm.query.SourceSelector(view_id, properties)],
                 )
             },
             cursors={"nodes": cursor},
         )
         result = client.data_modeling.instances.query(query)
         batch_external_ids = {
-            node.properties[view_id]["capacityPriceDown"]: [
-                node.properties[view_id].get(prop, "") for prop in extra_list
-            ]
+            node.properties[view_id]["capacityPriceUp"]: [node.properties[view_id].get(prop, "") for prop in extra_list]
             for node in result.data["nodes"].data
         }
         total_retrieved += len(batch_external_ids)
         external_ids.update(batch_external_ids)
         cursor = result.cursors["nodes"]
         if total_retrieved >= limit or cursor is None:
             break
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/price_area_afrr_capacity_price_up.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/price_prod_case_price.py`

 * *Files 6% similar despite different names*

```diff
@@ -5,33 +5,21 @@
 from typing import Literal
 
 import pandas as pd
 from cognite.client import CogniteClient
 from cognite.client import data_modeling as dm
 from cognite.client.data_classes import Datapoints, DatapointsArrayList, DatapointsList, TimeSeriesList
 from cognite.client.data_classes.datapoints import Aggregate
-from cognite.powerops.client._generated.v1.data_classes._price_area_afrr import _create_price_area_afrr_filter
+from cognite.powerops.client._generated.v1.data_classes._price_prod_case import _create_price_prod_case_filter
 from ._core import DEFAULT_LIMIT_READ, INSTANCE_QUERY_LIMIT
 
-ColumnNames = Literal[
-    "name",
-    "timezone",
-    "capacityPriceUp",
-    "capacityPriceDown",
-    "activationPriceUp",
-    "activationPriceDown",
-    "relativeActivation",
-    "totalCapacityAllocationUp",
-    "totalCapacityAllocationDown",
-    "ownCapacityAllocationUp",
-    "ownCapacityAllocationDown",
-]
+ColumnNames = Literal["price", "production"]
 
 
-class PriceAreaAFRRCapacityPriceUpQuery:
+class PriceProdCasePriceQuery:
     def __init__(
         self,
         client: CogniteClient,
         view_id: dm.ViewId,
         timeseries_limit: int = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ):
@@ -48,15 +36,15 @@
         aggregates: Aggregate | list[Aggregate] | None = None,
         granularity: str | None = None,
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         limit: int | None = None,
         include_outside_points: bool = False,
     ) -> DatapointsList:
-        """`Retrieve datapoints for the `price_area_afrr.capacity_price_up` timeseries.
+        """`Retrieve datapoints for the `price_prod_case.price` timeseries.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -73,19 +61,19 @@
 
         Returns:
             A ``DatapointsList`` with the requested datapoints.
 
         Examples:
 
             In this example,
-            we are using the time-ago format to get raw data for the 'my_capacity_price_up' from 2 weeks ago up until now::
+            we are using the time-ago format to get raw data for the 'my_price' from 2 weeks ago up until now::
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> price_area_afrr_datapoints = client.price_area_afrr.capacity_price_up(external_id="my_capacity_price_up").retrieve(start="2w-ago")
+                >>> price_prod_case_datapoints = client.price_prod_case.price(external_id="my_price").retrieve(start="2w-ago")
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra()
         if external_ids:
             return self._client.time_series.data.retrieve(
                 external_id=list(external_ids),
                 start=start,
                 end=end,
@@ -107,15 +95,15 @@
         aggregates: Aggregate | list[Aggregate] | None = None,
         granularity: str | None = None,
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         limit: int | None = None,
         include_outside_points: bool = False,
     ) -> DatapointsArrayList:
-        """`Retrieve numpy arrays for the `price_area_afrr.capacity_price_up` timeseries.
+        """`Retrieve numpy arrays for the `price_prod_case.price` timeseries.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -132,19 +120,19 @@
 
         Returns:
             A ``DatapointsArrayList`` with the requested datapoints.
 
         Examples:
 
             In this example,
-            we are using the time-ago format to get raw data for the 'my_capacity_price_up' from 2 weeks ago up until now::
+            we are using the time-ago format to get raw data for the 'my_price' from 2 weeks ago up until now::
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> price_area_afrr_datapoints = client.price_area_afrr.capacity_price_up(external_id="my_capacity_price_up").retrieve_array(start="2w-ago")
+                >>> price_prod_case_datapoints = client.price_prod_case.price(external_id="my_price").retrieve_array(start="2w-ago")
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra()
         if external_ids:
             return self._client.time_series.data.retrieve_arrays(
                 external_id=list(external_ids),
                 start=start,
                 end=end,
@@ -168,17 +156,17 @@
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         limit: int | None = None,
         include_outside_points: bool = False,
         uniform_index: bool = False,
         include_aggregate_name: bool = True,
         include_granularity_name: bool = False,
-        column_names: ColumnNames | list[ColumnNames] = "capacityPriceUp",
+        column_names: ColumnNames | list[ColumnNames] = "price",
     ) -> pd.DataFrame:
-        """`Retrieve DataFrames for the `price_area_afrr.capacity_price_up` timeseries.
+        """`Retrieve DataFrames for the `price_prod_case.price` timeseries.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -191,28 +179,28 @@
             target_unit: The unit_external_id of the data points returned. If the time series does not have an unit_external_id that can be converted to the target_unit, an error will be returned. Cannot be used with target_unit_system.
             target_unit_system: The unit system of the data points returned. Cannot be used with target_unit.
             limit: Maximum number of datapoints to return for each time series. Default: None (no limit)
             include_outside_points: Whether to include outside points. Not allowed when fetching aggregates. Default: False
             uniform_index: If only querying aggregates AND a single granularity is used, AND no limit is used, specifying `uniform_index=True` will return a dataframe with an equidistant datetime index from the earliest `start` to the latest `end` (missing values will be NaNs). If these requirements are not met, a ValueError is raised. Default: False
             include_aggregate_name: Include 'aggregate' in the column name, e.g. `my-ts|average`. Ignored for raw time series. Default: True
             include_granularity_name: Include 'granularity' in the column name, e.g. `my-ts|12h`. Added after 'aggregate' when present. Ignored for raw time series. Default: False
-            column_names: Which property to use for column names. Defauts to capacityPriceUp
+            column_names: Which property to use for column names. Defauts to price
 
 
         Returns:
             A ``DataFrame`` with the requested datapoints.
 
         Examples:
 
             In this example,
-            we are using the time-ago format to get raw data for the 'my_capacity_price_up' from 2 weeks ago up until now::
+            we are using the time-ago format to get raw data for the 'my_price' from 2 weeks ago up until now::
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> price_area_afrr_datapoints = client.price_area_afrr.capacity_price_up(external_id="my_capacity_price_up").retrieve_dataframe(start="2w-ago")
+                >>> price_prod_case_datapoints = client.price_prod_case.price(external_id="my_price").retrieve_dataframe(start="2w-ago")
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra(column_names)
         if external_ids:
             df = self._client.time_series.data.retrieve_dataframe(
                 external_id=list(external_ids),
                 start=start,
                 end=end,
@@ -245,17 +233,17 @@
         aggregates: Aggregate | Sequence[Aggregate] | None = None,
         granularity: str | None = None,
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         uniform_index: bool = False,
         include_aggregate_name: bool = True,
         include_granularity_name: bool = False,
-        column_names: ColumnNames | list[ColumnNames] = "capacityPriceUp",
+        column_names: ColumnNames | list[ColumnNames] = "price",
     ) -> pd.DataFrame:
-        """Retrieve DataFrames for the `price_area_afrr.capacity_price_up` timeseries in Timezone.
+        """Retrieve DataFrames for the `price_prod_case.price` timeseries in Timezone.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -268,30 +256,30 @@
             target_unit: The unit_external_id of the data points returned. If the time series does not have an unit_external_id that can be converted to the target_unit, an error will be returned. Cannot be used with target_unit_system.
             target_unit_system: The unit system of the data points returned. Cannot be used with target_unit.
             limit: Maximum number of datapoints to return for each time series. Default: None (no limit)
             include_outside_points: Whether to include outside points. Not allowed when fetching aggregates. Default: False
             uniform_index: If only querying aggregates AND a single granularity is used, AND no limit is used, specifying `uniform_index=True` will return a dataframe with an equidistant datetime index from the earliest `start` to the latest `end` (missing values will be NaNs). If these requirements are not met, a ValueError is raised. Default: False
             include_aggregate_name: Include 'aggregate' in the column name, e.g. `my-ts|average`. Ignored for raw time series. Default: True
             include_granularity_name: Include 'granularity' in the column name, e.g. `my-ts|12h`. Added after 'aggregate' when present. Ignored for raw time series. Default: False
-            column_names: Which property to use for column names. Defauts to capacityPriceUp
+            column_names: Which property to use for column names. Defauts to price
 
 
         Returns:
             A ``DataFrame`` with the requested datapoints.
 
         Examples:
 
             In this example,
-            get weekly aggregates for the 'my_capacity_price_up' for the first month of 2023 in Oslo time:
+            get weekly aggregates for the 'my_price' for the first month of 2023 in Oslo time:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> from datetime import datetime, timezone
                 >>> client = PowerOpsModelsV1Client()
-                >>> price_area_afrr_datapoints = client.price_area_afrr.capacity_price_up(
-                ...     external_id="my_capacity_price_up").retrieve_dataframe_in_timezone(
+                >>> price_prod_case_datapoints = client.price_prod_case.price(
+                ...     external_id="my_price").retrieve_dataframe_in_timezone(
                 ...         datetime(2023, 1, 1, tzinfo=ZoneInfo("Europe/Oslo")),
                 ...         datetime(2023, 1, 2, tzinfo=ZoneInfo("Europe/Oslo")),
                 ...         aggregates="average",
                 ...         granularity="1week",
                 ...     )
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra(column_names)
@@ -329,17 +317,17 @@
                 external_id=list(external_ids),
                 before=before,
             )
         else:
             return None
 
     def _retrieve_timeseries_external_ids_with_extra(
-        self, extra_properties: ColumnNames | list[ColumnNames] = "capacityPriceUp"
+        self, extra_properties: ColumnNames | list[ColumnNames] = "price"
     ) -> dict[str, list[str]]:
-        return _retrieve_timeseries_external_ids_with_extra_capacity_price_up(
+        return _retrieve_timeseries_external_ids_with_extra_price(
             self._client,
             self._view_id,
             self._filter,
             self._timeseries_limit,
             extra_properties,
         )
 
@@ -347,164 +335,144 @@
     def _rename_columns(
         external_ids: dict[str, list[str]],
         df: pd.DataFrame,
         column_names: ColumnNames | list[ColumnNames],
         include_aggregate_name: bool,
         include_granularity_name: bool,
     ) -> pd.DataFrame:
-        if isinstance(column_names, str) and column_names == "capacityPriceUp":
+        if isinstance(column_names, str) and column_names == "price":
             return df
         splits = sum(included for included in [include_aggregate_name, include_granularity_name])
         if splits == 0:
             df.columns = ["-".join(external_ids[external_id]) for external_id in df.columns]
         else:
             column_parts = (col.rsplit("|", maxsplit=splits) for col in df.columns)
             df.columns = [
                 "-".join(external_ids[external_id]) + "|" + "|".join(parts) for external_id, *parts in column_parts
             ]
         return df
 
 
-class PriceAreaAFRRCapacityPriceUpAPI:
+class PriceProdCasePriceAPI:
     def __init__(self, client: CogniteClient, view_id: dm.ViewId):
         self._client = client
         self._view_id = view_id
 
     def __call__(
         self,
-        name: str | list[str] | None = None,
-        name_prefix: str | None = None,
-        timezone: str | list[str] | None = None,
-        timezone_prefix: str | None = None,
+        case: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
-    ) -> PriceAreaAFRRCapacityPriceUpQuery:
-        """Query timeseries `price_area_afrr.capacity_price_up`
+    ) -> PriceProdCasePriceQuery:
+        """Query timeseries `price_prod_case.price`
 
         Args:
-            name: The name to filter on.
-            name_prefix: The prefix of the name to filter on.
-            timezone: The timezone to filter on.
-            timezone_prefix: The prefix of the timezone to filter on.
+            case: The case to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of price area afrrs to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of price prod cases to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
-            A query object that can be used to retrieve datapoins for the price_area_afrr.capacity_price_up timeseries
+            A query object that can be used to retrieve datapoins for the price_prod_case.price timeseries
             selected in this method.
 
         Examples:
 
-            Retrieve all data for 5 price_area_afrr.capacity_price_up timeseries:
+            Retrieve all data for 5 price_prod_case.price timeseries:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> price_area_afrrs = client.price_area_afrr.capacity_price_up(limit=5).retrieve()
+                >>> price_prod_cases = client.price_prod_case.price(limit=5).retrieve()
 
         """
-        filter_ = _create_price_area_afrr_filter(
+        filter_ = _create_price_prod_case_filter(
             self._view_id,
-            name,
-            name_prefix,
-            timezone,
-            timezone_prefix,
+            case,
             external_id_prefix,
             space,
             filter,
         )
 
-        return PriceAreaAFRRCapacityPriceUpQuery(
+        return PriceProdCasePriceQuery(
             client=self._client,
             view_id=self._view_id,
             timeseries_limit=limit,
             filter=filter_,
         )
 
     def list(
         self,
-        name: str | list[str] | None = None,
-        name_prefix: str | None = None,
-        timezone: str | list[str] | None = None,
-        timezone_prefix: str | None = None,
+        case: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> TimeSeriesList:
-        """List timeseries `price_area_afrr.capacity_price_up`
+        """List timeseries `price_prod_case.price`
 
         Args:
-            name: The name to filter on.
-            name_prefix: The prefix of the name to filter on.
-            timezone: The timezone to filter on.
-            timezone_prefix: The prefix of the timezone to filter on.
+            case: The case to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of price area afrrs to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of price prod cases to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
-            List of Timeseries price_area_afrr.capacity_price_up.
+            List of Timeseries price_prod_case.price.
 
         Examples:
 
-            List price_area_afrr.capacity_price_up and limit to 5:
+            List price_prod_case.price and limit to 5:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> price_area_afrrs = client.price_area_afrr.capacity_price_up.list(limit=5)
+                >>> price_prod_cases = client.price_prod_case.price.list(limit=5)
 
         """
-        filter_ = _create_price_area_afrr_filter(
+        filter_ = _create_price_prod_case_filter(
             self._view_id,
-            name,
-            name_prefix,
-            timezone,
-            timezone_prefix,
+            case,
             external_id_prefix,
             space,
             filter,
         )
-        external_ids = _retrieve_timeseries_external_ids_with_extra_capacity_price_up(
-            self._client, self._view_id, filter_, limit
-        )
+        external_ids = _retrieve_timeseries_external_ids_with_extra_price(self._client, self._view_id, filter_, limit)
         if external_ids:
             return self._client.time_series.retrieve_multiple(external_ids=list(external_ids))
         else:
             return TimeSeriesList([])
 
 
-def _retrieve_timeseries_external_ids_with_extra_capacity_price_up(
+def _retrieve_timeseries_external_ids_with_extra_price(
     client: CogniteClient,
     view_id: dm.ViewId,
     filter_: dm.Filter | None,
     limit: int,
-    extra_properties: ColumnNames | list[ColumnNames] = "capacityPriceUp",
+    extra_properties: ColumnNames | list[ColumnNames] = "price",
 ) -> dict[str, list[str]]:
     limit = float("inf") if limit is None or limit == -1 else limit
-    properties = ["capacityPriceUp"]
-    if extra_properties == "capacityPriceUp":
+    properties = ["price"]
+    if extra_properties == "price":
         ...
-    elif isinstance(extra_properties, str) and extra_properties != "capacityPriceUp":
+    elif isinstance(extra_properties, str) and extra_properties != "price":
         properties.append(extra_properties)
     elif isinstance(extra_properties, list):
-        properties.extend([prop for prop in extra_properties if prop != "capacityPriceUp"])
+        properties.extend([prop for prop in extra_properties if prop != "price"])
     else:
         raise ValueError(f"Invalid value for extra_properties: {extra_properties}")
 
     if isinstance(extra_properties, str):
         extra_list = [extra_properties]
     else:
         extra_list = extra_properties
     has_data = dm.filters.HasData(views=[view_id])
-    has_property = dm.filters.Exists(property=view_id.as_property_ref("capacityPriceUp"))
+    has_property = dm.filters.Exists(property=view_id.as_property_ref("price"))
     filter_ = dm.filters.And(filter_, has_data, has_property) if filter_ else dm.filters.And(has_data, has_property)
 
     cursor = None
     external_ids: dict[str, list[str]] = {}
     total_retrieved = 0
     while True:
         query_limit = max(min(INSTANCE_QUERY_LIMIT, limit - total_retrieved), 0)
@@ -518,15 +486,15 @@
                     [dm.query.SourceSelector(view_id, properties)],
                 )
             },
             cursors={"nodes": cursor},
         )
         result = client.data_modeling.instances.query(query)
         batch_external_ids = {
-            node.properties[view_id]["capacityPriceUp"]: [node.properties[view_id].get(prop, "") for prop in extra_list]
+            node.properties[view_id]["price"]: [node.properties[view_id].get(prop, "") for prop in extra_list]
             for node in result.data["nodes"].data
         }
         total_retrieved += len(batch_external_ids)
         external_ids.update(batch_external_ids)
         cursor = result.cursors["nodes"]
         if total_retrieved >= limit or cursor is None:
             break
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/price_area_afrr_own_capacity_allocation_down.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/price_area_afrr_own_capacity_allocation_down.py`

 * *Files 5% similar despite different names*

```diff
@@ -10,14 +10,17 @@
 from cognite.client.data_classes import Datapoints, DatapointsArrayList, DatapointsList, TimeSeriesList
 from cognite.client.data_classes.datapoints import Aggregate
 from cognite.powerops.client._generated.v1.data_classes._price_area_afrr import _create_price_area_afrr_filter
 from ._core import DEFAULT_LIMIT_READ, INSTANCE_QUERY_LIMIT
 
 ColumnNames = Literal[
     "name",
+    "displayName",
+    "ordering",
+    "assetType",
     "timezone",
     "capacityPriceUp",
     "capacityPriceDown",
     "activationPriceUp",
     "activationPriceDown",
     "relativeActivation",
     "totalCapacityAllocationUp",
@@ -369,26 +372,38 @@
         self._client = client
         self._view_id = view_id
 
     def __call__(
         self,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
+        display_name: str | list[str] | None = None,
+        display_name_prefix: str | None = None,
+        min_ordering: int | None = None,
+        max_ordering: int | None = None,
+        asset_type: str | list[str] | None = None,
+        asset_type_prefix: str | None = None,
         timezone: str | list[str] | None = None,
         timezone_prefix: str | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> PriceAreaAFRROwnCapacityAllocationDownQuery:
         """Query timeseries `price_area_afrr.own_capacity_allocation_down`
 
         Args:
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
+            display_name: The display name to filter on.
+            display_name_prefix: The prefix of the display name to filter on.
+            min_ordering: The minimum value of the ordering to filter on.
+            max_ordering: The maximum value of the ordering to filter on.
+            asset_type: The asset type to filter on.
+            asset_type_prefix: The prefix of the asset type to filter on.
             timezone: The timezone to filter on.
             timezone_prefix: The prefix of the timezone to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of price area afrrs to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
@@ -405,14 +420,20 @@
                 >>> price_area_afrrs = client.price_area_afrr.own_capacity_allocation_down(limit=5).retrieve()
 
         """
         filter_ = _create_price_area_afrr_filter(
             self._view_id,
             name,
             name_prefix,
+            display_name,
+            display_name_prefix,
+            min_ordering,
+            max_ordering,
+            asset_type,
+            asset_type_prefix,
             timezone,
             timezone_prefix,
             external_id_prefix,
             space,
             filter,
         )
 
@@ -423,26 +444,38 @@
             filter=filter_,
         )
 
     def list(
         self,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
+        display_name: str | list[str] | None = None,
+        display_name_prefix: str | None = None,
+        min_ordering: int | None = None,
+        max_ordering: int | None = None,
+        asset_type: str | list[str] | None = None,
+        asset_type_prefix: str | None = None,
         timezone: str | list[str] | None = None,
         timezone_prefix: str | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> TimeSeriesList:
         """List timeseries `price_area_afrr.own_capacity_allocation_down`
 
         Args:
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
+            display_name: The display name to filter on.
+            display_name_prefix: The prefix of the display name to filter on.
+            min_ordering: The minimum value of the ordering to filter on.
+            max_ordering: The maximum value of the ordering to filter on.
+            asset_type: The asset type to filter on.
+            asset_type_prefix: The prefix of the asset type to filter on.
             timezone: The timezone to filter on.
             timezone_prefix: The prefix of the timezone to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of price area afrrs to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
@@ -458,14 +491,20 @@
                 >>> price_area_afrrs = client.price_area_afrr.own_capacity_allocation_down.list(limit=5)
 
         """
         filter_ = _create_price_area_afrr_filter(
             self._view_id,
             name,
             name_prefix,
+            display_name,
+            display_name_prefix,
+            min_ordering,
+            max_ordering,
+            asset_type,
+            asset_type_prefix,
             timezone,
             timezone_prefix,
             external_id_prefix,
             space,
             filter,
         )
         external_ids = _retrieve_timeseries_external_ids_with_extra_own_capacity_allocation_down(
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/price_area_afrr_own_capacity_allocation_up.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/price_area_afrr_total_capacity_allocation_up.py`

 * *Files 7% similar despite different names*

```diff
@@ -10,28 +10,31 @@
 from cognite.client.data_classes import Datapoints, DatapointsArrayList, DatapointsList, TimeSeriesList
 from cognite.client.data_classes.datapoints import Aggregate
 from cognite.powerops.client._generated.v1.data_classes._price_area_afrr import _create_price_area_afrr_filter
 from ._core import DEFAULT_LIMIT_READ, INSTANCE_QUERY_LIMIT
 
 ColumnNames = Literal[
     "name",
+    "displayName",
+    "ordering",
+    "assetType",
     "timezone",
     "capacityPriceUp",
     "capacityPriceDown",
     "activationPriceUp",
     "activationPriceDown",
     "relativeActivation",
     "totalCapacityAllocationUp",
     "totalCapacityAllocationDown",
     "ownCapacityAllocationUp",
     "ownCapacityAllocationDown",
 ]
 
 
-class PriceAreaAFRROwnCapacityAllocationUpQuery:
+class PriceAreaAFRRTotalCapacityAllocationUpQuery:
     def __init__(
         self,
         client: CogniteClient,
         view_id: dm.ViewId,
         timeseries_limit: int = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ):
@@ -48,15 +51,15 @@
         aggregates: Aggregate | list[Aggregate] | None = None,
         granularity: str | None = None,
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         limit: int | None = None,
         include_outside_points: bool = False,
     ) -> DatapointsList:
-        """`Retrieve datapoints for the `price_area_afrr.own_capacity_allocation_up` timeseries.
+        """`Retrieve datapoints for the `price_area_afrr.total_capacity_allocation_up` timeseries.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -73,19 +76,19 @@
 
         Returns:
             A ``DatapointsList`` with the requested datapoints.
 
         Examples:
 
             In this example,
-            we are using the time-ago format to get raw data for the 'my_own_capacity_allocation_up' from 2 weeks ago up until now::
+            we are using the time-ago format to get raw data for the 'my_total_capacity_allocation_up' from 2 weeks ago up until now::
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> price_area_afrr_datapoints = client.price_area_afrr.own_capacity_allocation_up(external_id="my_own_capacity_allocation_up").retrieve(start="2w-ago")
+                >>> price_area_afrr_datapoints = client.price_area_afrr.total_capacity_allocation_up(external_id="my_total_capacity_allocation_up").retrieve(start="2w-ago")
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra()
         if external_ids:
             return self._client.time_series.data.retrieve(
                 external_id=list(external_ids),
                 start=start,
                 end=end,
@@ -107,15 +110,15 @@
         aggregates: Aggregate | list[Aggregate] | None = None,
         granularity: str | None = None,
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         limit: int | None = None,
         include_outside_points: bool = False,
     ) -> DatapointsArrayList:
-        """`Retrieve numpy arrays for the `price_area_afrr.own_capacity_allocation_up` timeseries.
+        """`Retrieve numpy arrays for the `price_area_afrr.total_capacity_allocation_up` timeseries.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -132,19 +135,19 @@
 
         Returns:
             A ``DatapointsArrayList`` with the requested datapoints.
 
         Examples:
 
             In this example,
-            we are using the time-ago format to get raw data for the 'my_own_capacity_allocation_up' from 2 weeks ago up until now::
+            we are using the time-ago format to get raw data for the 'my_total_capacity_allocation_up' from 2 weeks ago up until now::
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> price_area_afrr_datapoints = client.price_area_afrr.own_capacity_allocation_up(external_id="my_own_capacity_allocation_up").retrieve_array(start="2w-ago")
+                >>> price_area_afrr_datapoints = client.price_area_afrr.total_capacity_allocation_up(external_id="my_total_capacity_allocation_up").retrieve_array(start="2w-ago")
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra()
         if external_ids:
             return self._client.time_series.data.retrieve_arrays(
                 external_id=list(external_ids),
                 start=start,
                 end=end,
@@ -168,17 +171,17 @@
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         limit: int | None = None,
         include_outside_points: bool = False,
         uniform_index: bool = False,
         include_aggregate_name: bool = True,
         include_granularity_name: bool = False,
-        column_names: ColumnNames | list[ColumnNames] = "ownCapacityAllocationUp",
+        column_names: ColumnNames | list[ColumnNames] = "totalCapacityAllocationUp",
     ) -> pd.DataFrame:
-        """`Retrieve DataFrames for the `price_area_afrr.own_capacity_allocation_up` timeseries.
+        """`Retrieve DataFrames for the `price_area_afrr.total_capacity_allocation_up` timeseries.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -191,28 +194,28 @@
             target_unit: The unit_external_id of the data points returned. If the time series does not have an unit_external_id that can be converted to the target_unit, an error will be returned. Cannot be used with target_unit_system.
             target_unit_system: The unit system of the data points returned. Cannot be used with target_unit.
             limit: Maximum number of datapoints to return for each time series. Default: None (no limit)
             include_outside_points: Whether to include outside points. Not allowed when fetching aggregates. Default: False
             uniform_index: If only querying aggregates AND a single granularity is used, AND no limit is used, specifying `uniform_index=True` will return a dataframe with an equidistant datetime index from the earliest `start` to the latest `end` (missing values will be NaNs). If these requirements are not met, a ValueError is raised. Default: False
             include_aggregate_name: Include 'aggregate' in the column name, e.g. `my-ts|average`. Ignored for raw time series. Default: True
             include_granularity_name: Include 'granularity' in the column name, e.g. `my-ts|12h`. Added after 'aggregate' when present. Ignored for raw time series. Default: False
-            column_names: Which property to use for column names. Defauts to ownCapacityAllocationUp
+            column_names: Which property to use for column names. Defauts to totalCapacityAllocationUp
 
 
         Returns:
             A ``DataFrame`` with the requested datapoints.
 
         Examples:
 
             In this example,
-            we are using the time-ago format to get raw data for the 'my_own_capacity_allocation_up' from 2 weeks ago up until now::
+            we are using the time-ago format to get raw data for the 'my_total_capacity_allocation_up' from 2 weeks ago up until now::
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> price_area_afrr_datapoints = client.price_area_afrr.own_capacity_allocation_up(external_id="my_own_capacity_allocation_up").retrieve_dataframe(start="2w-ago")
+                >>> price_area_afrr_datapoints = client.price_area_afrr.total_capacity_allocation_up(external_id="my_total_capacity_allocation_up").retrieve_dataframe(start="2w-ago")
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra(column_names)
         if external_ids:
             df = self._client.time_series.data.retrieve_dataframe(
                 external_id=list(external_ids),
                 start=start,
                 end=end,
@@ -245,17 +248,17 @@
         aggregates: Aggregate | Sequence[Aggregate] | None = None,
         granularity: str | None = None,
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         uniform_index: bool = False,
         include_aggregate_name: bool = True,
         include_granularity_name: bool = False,
-        column_names: ColumnNames | list[ColumnNames] = "ownCapacityAllocationUp",
+        column_names: ColumnNames | list[ColumnNames] = "totalCapacityAllocationUp",
     ) -> pd.DataFrame:
-        """Retrieve DataFrames for the `price_area_afrr.own_capacity_allocation_up` timeseries in Timezone.
+        """Retrieve DataFrames for the `price_area_afrr.total_capacity_allocation_up` timeseries in Timezone.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -268,30 +271,30 @@
             target_unit: The unit_external_id of the data points returned. If the time series does not have an unit_external_id that can be converted to the target_unit, an error will be returned. Cannot be used with target_unit_system.
             target_unit_system: The unit system of the data points returned. Cannot be used with target_unit.
             limit: Maximum number of datapoints to return for each time series. Default: None (no limit)
             include_outside_points: Whether to include outside points. Not allowed when fetching aggregates. Default: False
             uniform_index: If only querying aggregates AND a single granularity is used, AND no limit is used, specifying `uniform_index=True` will return a dataframe with an equidistant datetime index from the earliest `start` to the latest `end` (missing values will be NaNs). If these requirements are not met, a ValueError is raised. Default: False
             include_aggregate_name: Include 'aggregate' in the column name, e.g. `my-ts|average`. Ignored for raw time series. Default: True
             include_granularity_name: Include 'granularity' in the column name, e.g. `my-ts|12h`. Added after 'aggregate' when present. Ignored for raw time series. Default: False
-            column_names: Which property to use for column names. Defauts to ownCapacityAllocationUp
+            column_names: Which property to use for column names. Defauts to totalCapacityAllocationUp
 
 
         Returns:
             A ``DataFrame`` with the requested datapoints.
 
         Examples:
 
             In this example,
-            get weekly aggregates for the 'my_own_capacity_allocation_up' for the first month of 2023 in Oslo time:
+            get weekly aggregates for the 'my_total_capacity_allocation_up' for the first month of 2023 in Oslo time:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> from datetime import datetime, timezone
                 >>> client = PowerOpsModelsV1Client()
-                >>> price_area_afrr_datapoints = client.price_area_afrr.own_capacity_allocation_up(
-                ...     external_id="my_own_capacity_allocation_up").retrieve_dataframe_in_timezone(
+                >>> price_area_afrr_datapoints = client.price_area_afrr.total_capacity_allocation_up(
+                ...     external_id="my_total_capacity_allocation_up").retrieve_dataframe_in_timezone(
                 ...         datetime(2023, 1, 1, tzinfo=ZoneInfo("Europe/Oslo")),
                 ...         datetime(2023, 1, 2, tzinfo=ZoneInfo("Europe/Oslo")),
                 ...         aggregates="average",
                 ...         granularity="1week",
                 ...     )
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra(column_names)
@@ -329,17 +332,17 @@
                 external_id=list(external_ids),
                 before=before,
             )
         else:
             return None
 
     def _retrieve_timeseries_external_ids_with_extra(
-        self, extra_properties: ColumnNames | list[ColumnNames] = "ownCapacityAllocationUp"
+        self, extra_properties: ColumnNames | list[ColumnNames] = "totalCapacityAllocationUp"
     ) -> dict[str, list[str]]:
-        return _retrieve_timeseries_external_ids_with_extra_own_capacity_allocation_up(
+        return _retrieve_timeseries_external_ids_with_extra_total_capacity_allocation_up(
             self._client,
             self._view_id,
             self._filter,
             self._timeseries_limit,
             extra_properties,
         )
 
@@ -347,164 +350,200 @@
     def _rename_columns(
         external_ids: dict[str, list[str]],
         df: pd.DataFrame,
         column_names: ColumnNames | list[ColumnNames],
         include_aggregate_name: bool,
         include_granularity_name: bool,
     ) -> pd.DataFrame:
-        if isinstance(column_names, str) and column_names == "ownCapacityAllocationUp":
+        if isinstance(column_names, str) and column_names == "totalCapacityAllocationUp":
             return df
         splits = sum(included for included in [include_aggregate_name, include_granularity_name])
         if splits == 0:
             df.columns = ["-".join(external_ids[external_id]) for external_id in df.columns]
         else:
             column_parts = (col.rsplit("|", maxsplit=splits) for col in df.columns)
             df.columns = [
                 "-".join(external_ids[external_id]) + "|" + "|".join(parts) for external_id, *parts in column_parts
             ]
         return df
 
 
-class PriceAreaAFRROwnCapacityAllocationUpAPI:
+class PriceAreaAFRRTotalCapacityAllocationUpAPI:
     def __init__(self, client: CogniteClient, view_id: dm.ViewId):
         self._client = client
         self._view_id = view_id
 
     def __call__(
         self,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
+        display_name: str | list[str] | None = None,
+        display_name_prefix: str | None = None,
+        min_ordering: int | None = None,
+        max_ordering: int | None = None,
+        asset_type: str | list[str] | None = None,
+        asset_type_prefix: str | None = None,
         timezone: str | list[str] | None = None,
         timezone_prefix: str | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
-    ) -> PriceAreaAFRROwnCapacityAllocationUpQuery:
-        """Query timeseries `price_area_afrr.own_capacity_allocation_up`
+    ) -> PriceAreaAFRRTotalCapacityAllocationUpQuery:
+        """Query timeseries `price_area_afrr.total_capacity_allocation_up`
 
         Args:
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
+            display_name: The display name to filter on.
+            display_name_prefix: The prefix of the display name to filter on.
+            min_ordering: The minimum value of the ordering to filter on.
+            max_ordering: The maximum value of the ordering to filter on.
+            asset_type: The asset type to filter on.
+            asset_type_prefix: The prefix of the asset type to filter on.
             timezone: The timezone to filter on.
             timezone_prefix: The prefix of the timezone to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of price area afrrs to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
-            A query object that can be used to retrieve datapoins for the price_area_afrr.own_capacity_allocation_up timeseries
+            A query object that can be used to retrieve datapoins for the price_area_afrr.total_capacity_allocation_up timeseries
             selected in this method.
 
         Examples:
 
-            Retrieve all data for 5 price_area_afrr.own_capacity_allocation_up timeseries:
+            Retrieve all data for 5 price_area_afrr.total_capacity_allocation_up timeseries:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> price_area_afrrs = client.price_area_afrr.own_capacity_allocation_up(limit=5).retrieve()
+                >>> price_area_afrrs = client.price_area_afrr.total_capacity_allocation_up(limit=5).retrieve()
 
         """
         filter_ = _create_price_area_afrr_filter(
             self._view_id,
             name,
             name_prefix,
+            display_name,
+            display_name_prefix,
+            min_ordering,
+            max_ordering,
+            asset_type,
+            asset_type_prefix,
             timezone,
             timezone_prefix,
             external_id_prefix,
             space,
             filter,
         )
 
-        return PriceAreaAFRROwnCapacityAllocationUpQuery(
+        return PriceAreaAFRRTotalCapacityAllocationUpQuery(
             client=self._client,
             view_id=self._view_id,
             timeseries_limit=limit,
             filter=filter_,
         )
 
     def list(
         self,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
+        display_name: str | list[str] | None = None,
+        display_name_prefix: str | None = None,
+        min_ordering: int | None = None,
+        max_ordering: int | None = None,
+        asset_type: str | list[str] | None = None,
+        asset_type_prefix: str | None = None,
         timezone: str | list[str] | None = None,
         timezone_prefix: str | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> TimeSeriesList:
-        """List timeseries `price_area_afrr.own_capacity_allocation_up`
+        """List timeseries `price_area_afrr.total_capacity_allocation_up`
 
         Args:
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
+            display_name: The display name to filter on.
+            display_name_prefix: The prefix of the display name to filter on.
+            min_ordering: The minimum value of the ordering to filter on.
+            max_ordering: The maximum value of the ordering to filter on.
+            asset_type: The asset type to filter on.
+            asset_type_prefix: The prefix of the asset type to filter on.
             timezone: The timezone to filter on.
             timezone_prefix: The prefix of the timezone to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of price area afrrs to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
-            List of Timeseries price_area_afrr.own_capacity_allocation_up.
+            List of Timeseries price_area_afrr.total_capacity_allocation_up.
 
         Examples:
 
-            List price_area_afrr.own_capacity_allocation_up and limit to 5:
+            List price_area_afrr.total_capacity_allocation_up and limit to 5:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> price_area_afrrs = client.price_area_afrr.own_capacity_allocation_up.list(limit=5)
+                >>> price_area_afrrs = client.price_area_afrr.total_capacity_allocation_up.list(limit=5)
 
         """
         filter_ = _create_price_area_afrr_filter(
             self._view_id,
             name,
             name_prefix,
+            display_name,
+            display_name_prefix,
+            min_ordering,
+            max_ordering,
+            asset_type,
+            asset_type_prefix,
             timezone,
             timezone_prefix,
             external_id_prefix,
             space,
             filter,
         )
-        external_ids = _retrieve_timeseries_external_ids_with_extra_own_capacity_allocation_up(
+        external_ids = _retrieve_timeseries_external_ids_with_extra_total_capacity_allocation_up(
             self._client, self._view_id, filter_, limit
         )
         if external_ids:
             return self._client.time_series.retrieve_multiple(external_ids=list(external_ids))
         else:
             return TimeSeriesList([])
 
 
-def _retrieve_timeseries_external_ids_with_extra_own_capacity_allocation_up(
+def _retrieve_timeseries_external_ids_with_extra_total_capacity_allocation_up(
     client: CogniteClient,
     view_id: dm.ViewId,
     filter_: dm.Filter | None,
     limit: int,
-    extra_properties: ColumnNames | list[ColumnNames] = "ownCapacityAllocationUp",
+    extra_properties: ColumnNames | list[ColumnNames] = "totalCapacityAllocationUp",
 ) -> dict[str, list[str]]:
     limit = float("inf") if limit is None or limit == -1 else limit
-    properties = ["ownCapacityAllocationUp"]
-    if extra_properties == "ownCapacityAllocationUp":
+    properties = ["totalCapacityAllocationUp"]
+    if extra_properties == "totalCapacityAllocationUp":
         ...
-    elif isinstance(extra_properties, str) and extra_properties != "ownCapacityAllocationUp":
+    elif isinstance(extra_properties, str) and extra_properties != "totalCapacityAllocationUp":
         properties.append(extra_properties)
     elif isinstance(extra_properties, list):
-        properties.extend([prop for prop in extra_properties if prop != "ownCapacityAllocationUp"])
+        properties.extend([prop for prop in extra_properties if prop != "totalCapacityAllocationUp"])
     else:
         raise ValueError(f"Invalid value for extra_properties: {extra_properties}")
 
     if isinstance(extra_properties, str):
         extra_list = [extra_properties]
     else:
         extra_list = extra_properties
     has_data = dm.filters.HasData(views=[view_id])
-    has_property = dm.filters.Exists(property=view_id.as_property_ref("ownCapacityAllocationUp"))
+    has_property = dm.filters.Exists(property=view_id.as_property_ref("totalCapacityAllocationUp"))
     filter_ = dm.filters.And(filter_, has_data, has_property) if filter_ else dm.filters.And(has_data, has_property)
 
     cursor = None
     external_ids: dict[str, list[str]] = {}
     total_retrieved = 0
     while True:
         query_limit = max(min(INSTANCE_QUERY_LIMIT, limit - total_retrieved), 0)
@@ -518,15 +557,15 @@
                     [dm.query.SourceSelector(view_id, properties)],
                 )
             },
             cursors={"nodes": cursor},
         )
         result = client.data_modeling.instances.query(query)
         batch_external_ids = {
-            node.properties[view_id]["ownCapacityAllocationUp"]: [
+            node.properties[view_id]["totalCapacityAllocationUp"]: [
                 node.properties[view_id].get(prop, "") for prop in extra_list
             ]
             for node in result.data["nodes"].data
         }
         total_retrieved += len(batch_external_ids)
         external_ids.update(batch_external_ids)
         cursor = result.cursors["nodes"]
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/price_area_afrr_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/price_area_afrr_query.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/price_area_afrr_relative_activation.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/price_area_afrr_relative_activation.py`

 * *Files 5% similar despite different names*

```diff
@@ -10,14 +10,17 @@
 from cognite.client.data_classes import Datapoints, DatapointsArrayList, DatapointsList, TimeSeriesList
 from cognite.client.data_classes.datapoints import Aggregate
 from cognite.powerops.client._generated.v1.data_classes._price_area_afrr import _create_price_area_afrr_filter
 from ._core import DEFAULT_LIMIT_READ, INSTANCE_QUERY_LIMIT
 
 ColumnNames = Literal[
     "name",
+    "displayName",
+    "ordering",
+    "assetType",
     "timezone",
     "capacityPriceUp",
     "capacityPriceDown",
     "activationPriceUp",
     "activationPriceDown",
     "relativeActivation",
     "totalCapacityAllocationUp",
@@ -369,26 +372,38 @@
         self._client = client
         self._view_id = view_id
 
     def __call__(
         self,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
+        display_name: str | list[str] | None = None,
+        display_name_prefix: str | None = None,
+        min_ordering: int | None = None,
+        max_ordering: int | None = None,
+        asset_type: str | list[str] | None = None,
+        asset_type_prefix: str | None = None,
         timezone: str | list[str] | None = None,
         timezone_prefix: str | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> PriceAreaAFRRRelativeActivationQuery:
         """Query timeseries `price_area_afrr.relative_activation`
 
         Args:
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
+            display_name: The display name to filter on.
+            display_name_prefix: The prefix of the display name to filter on.
+            min_ordering: The minimum value of the ordering to filter on.
+            max_ordering: The maximum value of the ordering to filter on.
+            asset_type: The asset type to filter on.
+            asset_type_prefix: The prefix of the asset type to filter on.
             timezone: The timezone to filter on.
             timezone_prefix: The prefix of the timezone to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of price area afrrs to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
@@ -405,14 +420,20 @@
                 >>> price_area_afrrs = client.price_area_afrr.relative_activation(limit=5).retrieve()
 
         """
         filter_ = _create_price_area_afrr_filter(
             self._view_id,
             name,
             name_prefix,
+            display_name,
+            display_name_prefix,
+            min_ordering,
+            max_ordering,
+            asset_type,
+            asset_type_prefix,
             timezone,
             timezone_prefix,
             external_id_prefix,
             space,
             filter,
         )
 
@@ -423,26 +444,38 @@
             filter=filter_,
         )
 
     def list(
         self,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
+        display_name: str | list[str] | None = None,
+        display_name_prefix: str | None = None,
+        min_ordering: int | None = None,
+        max_ordering: int | None = None,
+        asset_type: str | list[str] | None = None,
+        asset_type_prefix: str | None = None,
         timezone: str | list[str] | None = None,
         timezone_prefix: str | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> TimeSeriesList:
         """List timeseries `price_area_afrr.relative_activation`
 
         Args:
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
+            display_name: The display name to filter on.
+            display_name_prefix: The prefix of the display name to filter on.
+            min_ordering: The minimum value of the ordering to filter on.
+            max_ordering: The maximum value of the ordering to filter on.
+            asset_type: The asset type to filter on.
+            asset_type_prefix: The prefix of the asset type to filter on.
             timezone: The timezone to filter on.
             timezone_prefix: The prefix of the timezone to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of price area afrrs to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
@@ -458,14 +491,20 @@
                 >>> price_area_afrrs = client.price_area_afrr.relative_activation.list(limit=5)
 
         """
         filter_ = _create_price_area_afrr_filter(
             self._view_id,
             name,
             name_prefix,
+            display_name,
+            display_name_prefix,
+            min_ordering,
+            max_ordering,
+            asset_type,
+            asset_type_prefix,
             timezone,
             timezone_prefix,
             external_id_prefix,
             space,
             filter,
         )
         external_ids = _retrieve_timeseries_external_ids_with_extra_relative_activation(
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/price_area_afrr_total_capacity_allocation_down.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/price_area_afrr_total_capacity_allocation_down.py`

 * *Files 4% similar despite different names*

```diff
@@ -10,14 +10,17 @@
 from cognite.client.data_classes import Datapoints, DatapointsArrayList, DatapointsList, TimeSeriesList
 from cognite.client.data_classes.datapoints import Aggregate
 from cognite.powerops.client._generated.v1.data_classes._price_area_afrr import _create_price_area_afrr_filter
 from ._core import DEFAULT_LIMIT_READ, INSTANCE_QUERY_LIMIT
 
 ColumnNames = Literal[
     "name",
+    "displayName",
+    "ordering",
+    "assetType",
     "timezone",
     "capacityPriceUp",
     "capacityPriceDown",
     "activationPriceUp",
     "activationPriceDown",
     "relativeActivation",
     "totalCapacityAllocationUp",
@@ -369,26 +372,38 @@
         self._client = client
         self._view_id = view_id
 
     def __call__(
         self,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
+        display_name: str | list[str] | None = None,
+        display_name_prefix: str | None = None,
+        min_ordering: int | None = None,
+        max_ordering: int | None = None,
+        asset_type: str | list[str] | None = None,
+        asset_type_prefix: str | None = None,
         timezone: str | list[str] | None = None,
         timezone_prefix: str | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> PriceAreaAFRRTotalCapacityAllocationDownQuery:
         """Query timeseries `price_area_afrr.total_capacity_allocation_down`
 
         Args:
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
+            display_name: The display name to filter on.
+            display_name_prefix: The prefix of the display name to filter on.
+            min_ordering: The minimum value of the ordering to filter on.
+            max_ordering: The maximum value of the ordering to filter on.
+            asset_type: The asset type to filter on.
+            asset_type_prefix: The prefix of the asset type to filter on.
             timezone: The timezone to filter on.
             timezone_prefix: The prefix of the timezone to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of price area afrrs to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
@@ -405,14 +420,20 @@
                 >>> price_area_afrrs = client.price_area_afrr.total_capacity_allocation_down(limit=5).retrieve()
 
         """
         filter_ = _create_price_area_afrr_filter(
             self._view_id,
             name,
             name_prefix,
+            display_name,
+            display_name_prefix,
+            min_ordering,
+            max_ordering,
+            asset_type,
+            asset_type_prefix,
             timezone,
             timezone_prefix,
             external_id_prefix,
             space,
             filter,
         )
 
@@ -423,26 +444,38 @@
             filter=filter_,
         )
 
     def list(
         self,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
+        display_name: str | list[str] | None = None,
+        display_name_prefix: str | None = None,
+        min_ordering: int | None = None,
+        max_ordering: int | None = None,
+        asset_type: str | list[str] | None = None,
+        asset_type_prefix: str | None = None,
         timezone: str | list[str] | None = None,
         timezone_prefix: str | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> TimeSeriesList:
         """List timeseries `price_area_afrr.total_capacity_allocation_down`
 
         Args:
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
+            display_name: The display name to filter on.
+            display_name_prefix: The prefix of the display name to filter on.
+            min_ordering: The minimum value of the ordering to filter on.
+            max_ordering: The maximum value of the ordering to filter on.
+            asset_type: The asset type to filter on.
+            asset_type_prefix: The prefix of the asset type to filter on.
             timezone: The timezone to filter on.
             timezone_prefix: The prefix of the timezone to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of price area afrrs to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
@@ -458,14 +491,20 @@
                 >>> price_area_afrrs = client.price_area_afrr.total_capacity_allocation_down.list(limit=5)
 
         """
         filter_ = _create_price_area_afrr_filter(
             self._view_id,
             name,
             name_prefix,
+            display_name,
+            display_name_prefix,
+            min_ordering,
+            max_ordering,
+            asset_type,
+            asset_type_prefix,
             timezone,
             timezone_prefix,
             external_id_prefix,
             space,
             filter,
         )
         external_ids = _retrieve_timeseries_external_ids_with_extra_total_capacity_allocation_down(
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/price_area_afrr_total_capacity_allocation_up.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/price_area_afrr_capacity_price_down.py`

 * *Files 11% similar despite different names*

```diff
@@ -10,28 +10,31 @@
 from cognite.client.data_classes import Datapoints, DatapointsArrayList, DatapointsList, TimeSeriesList
 from cognite.client.data_classes.datapoints import Aggregate
 from cognite.powerops.client._generated.v1.data_classes._price_area_afrr import _create_price_area_afrr_filter
 from ._core import DEFAULT_LIMIT_READ, INSTANCE_QUERY_LIMIT
 
 ColumnNames = Literal[
     "name",
+    "displayName",
+    "ordering",
+    "assetType",
     "timezone",
     "capacityPriceUp",
     "capacityPriceDown",
     "activationPriceUp",
     "activationPriceDown",
     "relativeActivation",
     "totalCapacityAllocationUp",
     "totalCapacityAllocationDown",
     "ownCapacityAllocationUp",
     "ownCapacityAllocationDown",
 ]
 
 
-class PriceAreaAFRRTotalCapacityAllocationUpQuery:
+class PriceAreaAFRRCapacityPriceDownQuery:
     def __init__(
         self,
         client: CogniteClient,
         view_id: dm.ViewId,
         timeseries_limit: int = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ):
@@ -48,15 +51,15 @@
         aggregates: Aggregate | list[Aggregate] | None = None,
         granularity: str | None = None,
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         limit: int | None = None,
         include_outside_points: bool = False,
     ) -> DatapointsList:
-        """`Retrieve datapoints for the `price_area_afrr.total_capacity_allocation_up` timeseries.
+        """`Retrieve datapoints for the `price_area_afrr.capacity_price_down` timeseries.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -73,19 +76,19 @@
 
         Returns:
             A ``DatapointsList`` with the requested datapoints.
 
         Examples:
 
             In this example,
-            we are using the time-ago format to get raw data for the 'my_total_capacity_allocation_up' from 2 weeks ago up until now::
+            we are using the time-ago format to get raw data for the 'my_capacity_price_down' from 2 weeks ago up until now::
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> price_area_afrr_datapoints = client.price_area_afrr.total_capacity_allocation_up(external_id="my_total_capacity_allocation_up").retrieve(start="2w-ago")
+                >>> price_area_afrr_datapoints = client.price_area_afrr.capacity_price_down(external_id="my_capacity_price_down").retrieve(start="2w-ago")
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra()
         if external_ids:
             return self._client.time_series.data.retrieve(
                 external_id=list(external_ids),
                 start=start,
                 end=end,
@@ -107,15 +110,15 @@
         aggregates: Aggregate | list[Aggregate] | None = None,
         granularity: str | None = None,
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         limit: int | None = None,
         include_outside_points: bool = False,
     ) -> DatapointsArrayList:
-        """`Retrieve numpy arrays for the `price_area_afrr.total_capacity_allocation_up` timeseries.
+        """`Retrieve numpy arrays for the `price_area_afrr.capacity_price_down` timeseries.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -132,19 +135,19 @@
 
         Returns:
             A ``DatapointsArrayList`` with the requested datapoints.
 
         Examples:
 
             In this example,
-            we are using the time-ago format to get raw data for the 'my_total_capacity_allocation_up' from 2 weeks ago up until now::
+            we are using the time-ago format to get raw data for the 'my_capacity_price_down' from 2 weeks ago up until now::
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> price_area_afrr_datapoints = client.price_area_afrr.total_capacity_allocation_up(external_id="my_total_capacity_allocation_up").retrieve_array(start="2w-ago")
+                >>> price_area_afrr_datapoints = client.price_area_afrr.capacity_price_down(external_id="my_capacity_price_down").retrieve_array(start="2w-ago")
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra()
         if external_ids:
             return self._client.time_series.data.retrieve_arrays(
                 external_id=list(external_ids),
                 start=start,
                 end=end,
@@ -168,17 +171,17 @@
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         limit: int | None = None,
         include_outside_points: bool = False,
         uniform_index: bool = False,
         include_aggregate_name: bool = True,
         include_granularity_name: bool = False,
-        column_names: ColumnNames | list[ColumnNames] = "totalCapacityAllocationUp",
+        column_names: ColumnNames | list[ColumnNames] = "capacityPriceDown",
     ) -> pd.DataFrame:
-        """`Retrieve DataFrames for the `price_area_afrr.total_capacity_allocation_up` timeseries.
+        """`Retrieve DataFrames for the `price_area_afrr.capacity_price_down` timeseries.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -191,28 +194,28 @@
             target_unit: The unit_external_id of the data points returned. If the time series does not have an unit_external_id that can be converted to the target_unit, an error will be returned. Cannot be used with target_unit_system.
             target_unit_system: The unit system of the data points returned. Cannot be used with target_unit.
             limit: Maximum number of datapoints to return for each time series. Default: None (no limit)
             include_outside_points: Whether to include outside points. Not allowed when fetching aggregates. Default: False
             uniform_index: If only querying aggregates AND a single granularity is used, AND no limit is used, specifying `uniform_index=True` will return a dataframe with an equidistant datetime index from the earliest `start` to the latest `end` (missing values will be NaNs). If these requirements are not met, a ValueError is raised. Default: False
             include_aggregate_name: Include 'aggregate' in the column name, e.g. `my-ts|average`. Ignored for raw time series. Default: True
             include_granularity_name: Include 'granularity' in the column name, e.g. `my-ts|12h`. Added after 'aggregate' when present. Ignored for raw time series. Default: False
-            column_names: Which property to use for column names. Defauts to totalCapacityAllocationUp
+            column_names: Which property to use for column names. Defauts to capacityPriceDown
 
 
         Returns:
             A ``DataFrame`` with the requested datapoints.
 
         Examples:
 
             In this example,
-            we are using the time-ago format to get raw data for the 'my_total_capacity_allocation_up' from 2 weeks ago up until now::
+            we are using the time-ago format to get raw data for the 'my_capacity_price_down' from 2 weeks ago up until now::
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> price_area_afrr_datapoints = client.price_area_afrr.total_capacity_allocation_up(external_id="my_total_capacity_allocation_up").retrieve_dataframe(start="2w-ago")
+                >>> price_area_afrr_datapoints = client.price_area_afrr.capacity_price_down(external_id="my_capacity_price_down").retrieve_dataframe(start="2w-ago")
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra(column_names)
         if external_ids:
             df = self._client.time_series.data.retrieve_dataframe(
                 external_id=list(external_ids),
                 start=start,
                 end=end,
@@ -245,17 +248,17 @@
         aggregates: Aggregate | Sequence[Aggregate] | None = None,
         granularity: str | None = None,
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         uniform_index: bool = False,
         include_aggregate_name: bool = True,
         include_granularity_name: bool = False,
-        column_names: ColumnNames | list[ColumnNames] = "totalCapacityAllocationUp",
+        column_names: ColumnNames | list[ColumnNames] = "capacityPriceDown",
     ) -> pd.DataFrame:
-        """Retrieve DataFrames for the `price_area_afrr.total_capacity_allocation_up` timeseries in Timezone.
+        """Retrieve DataFrames for the `price_area_afrr.capacity_price_down` timeseries in Timezone.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -268,30 +271,30 @@
             target_unit: The unit_external_id of the data points returned. If the time series does not have an unit_external_id that can be converted to the target_unit, an error will be returned. Cannot be used with target_unit_system.
             target_unit_system: The unit system of the data points returned. Cannot be used with target_unit.
             limit: Maximum number of datapoints to return for each time series. Default: None (no limit)
             include_outside_points: Whether to include outside points. Not allowed when fetching aggregates. Default: False
             uniform_index: If only querying aggregates AND a single granularity is used, AND no limit is used, specifying `uniform_index=True` will return a dataframe with an equidistant datetime index from the earliest `start` to the latest `end` (missing values will be NaNs). If these requirements are not met, a ValueError is raised. Default: False
             include_aggregate_name: Include 'aggregate' in the column name, e.g. `my-ts|average`. Ignored for raw time series. Default: True
             include_granularity_name: Include 'granularity' in the column name, e.g. `my-ts|12h`. Added after 'aggregate' when present. Ignored for raw time series. Default: False
-            column_names: Which property to use for column names. Defauts to totalCapacityAllocationUp
+            column_names: Which property to use for column names. Defauts to capacityPriceDown
 
 
         Returns:
             A ``DataFrame`` with the requested datapoints.
 
         Examples:
 
             In this example,
-            get weekly aggregates for the 'my_total_capacity_allocation_up' for the first month of 2023 in Oslo time:
+            get weekly aggregates for the 'my_capacity_price_down' for the first month of 2023 in Oslo time:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> from datetime import datetime, timezone
                 >>> client = PowerOpsModelsV1Client()
-                >>> price_area_afrr_datapoints = client.price_area_afrr.total_capacity_allocation_up(
-                ...     external_id="my_total_capacity_allocation_up").retrieve_dataframe_in_timezone(
+                >>> price_area_afrr_datapoints = client.price_area_afrr.capacity_price_down(
+                ...     external_id="my_capacity_price_down").retrieve_dataframe_in_timezone(
                 ...         datetime(2023, 1, 1, tzinfo=ZoneInfo("Europe/Oslo")),
                 ...         datetime(2023, 1, 2, tzinfo=ZoneInfo("Europe/Oslo")),
                 ...         aggregates="average",
                 ...         granularity="1week",
                 ...     )
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra(column_names)
@@ -329,17 +332,17 @@
                 external_id=list(external_ids),
                 before=before,
             )
         else:
             return None
 
     def _retrieve_timeseries_external_ids_with_extra(
-        self, extra_properties: ColumnNames | list[ColumnNames] = "totalCapacityAllocationUp"
+        self, extra_properties: ColumnNames | list[ColumnNames] = "capacityPriceDown"
     ) -> dict[str, list[str]]:
-        return _retrieve_timeseries_external_ids_with_extra_total_capacity_allocation_up(
+        return _retrieve_timeseries_external_ids_with_extra_capacity_price_down(
             self._client,
             self._view_id,
             self._filter,
             self._timeseries_limit,
             extra_properties,
         )
 
@@ -347,164 +350,200 @@
     def _rename_columns(
         external_ids: dict[str, list[str]],
         df: pd.DataFrame,
         column_names: ColumnNames | list[ColumnNames],
         include_aggregate_name: bool,
         include_granularity_name: bool,
     ) -> pd.DataFrame:
-        if isinstance(column_names, str) and column_names == "totalCapacityAllocationUp":
+        if isinstance(column_names, str) and column_names == "capacityPriceDown":
             return df
         splits = sum(included for included in [include_aggregate_name, include_granularity_name])
         if splits == 0:
             df.columns = ["-".join(external_ids[external_id]) for external_id in df.columns]
         else:
             column_parts = (col.rsplit("|", maxsplit=splits) for col in df.columns)
             df.columns = [
                 "-".join(external_ids[external_id]) + "|" + "|".join(parts) for external_id, *parts in column_parts
             ]
         return df
 
 
-class PriceAreaAFRRTotalCapacityAllocationUpAPI:
+class PriceAreaAFRRCapacityPriceDownAPI:
     def __init__(self, client: CogniteClient, view_id: dm.ViewId):
         self._client = client
         self._view_id = view_id
 
     def __call__(
         self,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
+        display_name: str | list[str] | None = None,
+        display_name_prefix: str | None = None,
+        min_ordering: int | None = None,
+        max_ordering: int | None = None,
+        asset_type: str | list[str] | None = None,
+        asset_type_prefix: str | None = None,
         timezone: str | list[str] | None = None,
         timezone_prefix: str | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
-    ) -> PriceAreaAFRRTotalCapacityAllocationUpQuery:
-        """Query timeseries `price_area_afrr.total_capacity_allocation_up`
+    ) -> PriceAreaAFRRCapacityPriceDownQuery:
+        """Query timeseries `price_area_afrr.capacity_price_down`
 
         Args:
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
+            display_name: The display name to filter on.
+            display_name_prefix: The prefix of the display name to filter on.
+            min_ordering: The minimum value of the ordering to filter on.
+            max_ordering: The maximum value of the ordering to filter on.
+            asset_type: The asset type to filter on.
+            asset_type_prefix: The prefix of the asset type to filter on.
             timezone: The timezone to filter on.
             timezone_prefix: The prefix of the timezone to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of price area afrrs to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
-            A query object that can be used to retrieve datapoins for the price_area_afrr.total_capacity_allocation_up timeseries
+            A query object that can be used to retrieve datapoins for the price_area_afrr.capacity_price_down timeseries
             selected in this method.
 
         Examples:
 
-            Retrieve all data for 5 price_area_afrr.total_capacity_allocation_up timeseries:
+            Retrieve all data for 5 price_area_afrr.capacity_price_down timeseries:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> price_area_afrrs = client.price_area_afrr.total_capacity_allocation_up(limit=5).retrieve()
+                >>> price_area_afrrs = client.price_area_afrr.capacity_price_down(limit=5).retrieve()
 
         """
         filter_ = _create_price_area_afrr_filter(
             self._view_id,
             name,
             name_prefix,
+            display_name,
+            display_name_prefix,
+            min_ordering,
+            max_ordering,
+            asset_type,
+            asset_type_prefix,
             timezone,
             timezone_prefix,
             external_id_prefix,
             space,
             filter,
         )
 
-        return PriceAreaAFRRTotalCapacityAllocationUpQuery(
+        return PriceAreaAFRRCapacityPriceDownQuery(
             client=self._client,
             view_id=self._view_id,
             timeseries_limit=limit,
             filter=filter_,
         )
 
     def list(
         self,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
+        display_name: str | list[str] | None = None,
+        display_name_prefix: str | None = None,
+        min_ordering: int | None = None,
+        max_ordering: int | None = None,
+        asset_type: str | list[str] | None = None,
+        asset_type_prefix: str | None = None,
         timezone: str | list[str] | None = None,
         timezone_prefix: str | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> TimeSeriesList:
-        """List timeseries `price_area_afrr.total_capacity_allocation_up`
+        """List timeseries `price_area_afrr.capacity_price_down`
 
         Args:
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
+            display_name: The display name to filter on.
+            display_name_prefix: The prefix of the display name to filter on.
+            min_ordering: The minimum value of the ordering to filter on.
+            max_ordering: The maximum value of the ordering to filter on.
+            asset_type: The asset type to filter on.
+            asset_type_prefix: The prefix of the asset type to filter on.
             timezone: The timezone to filter on.
             timezone_prefix: The prefix of the timezone to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of price area afrrs to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
-            List of Timeseries price_area_afrr.total_capacity_allocation_up.
+            List of Timeseries price_area_afrr.capacity_price_down.
 
         Examples:
 
-            List price_area_afrr.total_capacity_allocation_up and limit to 5:
+            List price_area_afrr.capacity_price_down and limit to 5:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> price_area_afrrs = client.price_area_afrr.total_capacity_allocation_up.list(limit=5)
+                >>> price_area_afrrs = client.price_area_afrr.capacity_price_down.list(limit=5)
 
         """
         filter_ = _create_price_area_afrr_filter(
             self._view_id,
             name,
             name_prefix,
+            display_name,
+            display_name_prefix,
+            min_ordering,
+            max_ordering,
+            asset_type,
+            asset_type_prefix,
             timezone,
             timezone_prefix,
             external_id_prefix,
             space,
             filter,
         )
-        external_ids = _retrieve_timeseries_external_ids_with_extra_total_capacity_allocation_up(
+        external_ids = _retrieve_timeseries_external_ids_with_extra_capacity_price_down(
             self._client, self._view_id, filter_, limit
         )
         if external_ids:
             return self._client.time_series.retrieve_multiple(external_ids=list(external_ids))
         else:
             return TimeSeriesList([])
 
 
-def _retrieve_timeseries_external_ids_with_extra_total_capacity_allocation_up(
+def _retrieve_timeseries_external_ids_with_extra_capacity_price_down(
     client: CogniteClient,
     view_id: dm.ViewId,
     filter_: dm.Filter | None,
     limit: int,
-    extra_properties: ColumnNames | list[ColumnNames] = "totalCapacityAllocationUp",
+    extra_properties: ColumnNames | list[ColumnNames] = "capacityPriceDown",
 ) -> dict[str, list[str]]:
     limit = float("inf") if limit is None or limit == -1 else limit
-    properties = ["totalCapacityAllocationUp"]
-    if extra_properties == "totalCapacityAllocationUp":
+    properties = ["capacityPriceDown"]
+    if extra_properties == "capacityPriceDown":
         ...
-    elif isinstance(extra_properties, str) and extra_properties != "totalCapacityAllocationUp":
+    elif isinstance(extra_properties, str) and extra_properties != "capacityPriceDown":
         properties.append(extra_properties)
     elif isinstance(extra_properties, list):
-        properties.extend([prop for prop in extra_properties if prop != "totalCapacityAllocationUp"])
+        properties.extend([prop for prop in extra_properties if prop != "capacityPriceDown"])
     else:
         raise ValueError(f"Invalid value for extra_properties: {extra_properties}")
 
     if isinstance(extra_properties, str):
         extra_list = [extra_properties]
     else:
         extra_list = extra_properties
     has_data = dm.filters.HasData(views=[view_id])
-    has_property = dm.filters.Exists(property=view_id.as_property_ref("totalCapacityAllocationUp"))
+    has_property = dm.filters.Exists(property=view_id.as_property_ref("capacityPriceDown"))
     filter_ = dm.filters.And(filter_, has_data, has_property) if filter_ else dm.filters.And(has_data, has_property)
 
     cursor = None
     external_ids: dict[str, list[str]] = {}
     total_retrieved = 0
     while True:
         query_limit = max(min(INSTANCE_QUERY_LIMIT, limit - total_retrieved), 0)
@@ -518,15 +557,15 @@
                     [dm.query.SourceSelector(view_id, properties)],
                 )
             },
             cursors={"nodes": cursor},
         )
         result = client.data_modeling.instances.query(query)
         batch_external_ids = {
-            node.properties[view_id]["totalCapacityAllocationUp"]: [
+            node.properties[view_id]["capacityPriceDown"]: [
                 node.properties[view_id].get(prop, "") for prop in extra_list
             ]
             for node in result.data["nodes"].data
         }
         total_retrieved += len(batch_external_ids)
         external_ids.update(batch_external_ids)
         cursor = result.cursors["nodes"]
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/price_area_asset.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/bid_configuration.py`

 * *Files 18% similar despite different names*

```diff
@@ -9,287 +9,280 @@
 from cognite.client.data_classes.data_modeling.instances import InstanceAggregationResultList
 
 from cognite.powerops.client._generated.v1.data_classes._core import DEFAULT_INSTANCE_SPACE
 from cognite.powerops.client._generated.v1.data_classes import (
     DomainModelCore,
     DomainModelWrite,
     ResourcesWriteResult,
-    PriceAreaAsset,
-    PriceAreaAssetWrite,
-    PriceAreaAssetFields,
-    PriceAreaAssetList,
-    PriceAreaAssetWriteList,
-    PriceAreaAssetTextFields,
+    BidConfiguration,
+    BidConfigurationWrite,
+    BidConfigurationFields,
+    BidConfigurationList,
+    BidConfigurationWriteList,
+    BidConfigurationTextFields,
 )
-from cognite.powerops.client._generated.v1.data_classes._price_area_asset import (
-    _PRICEAREAASSET_PROPERTIES_BY_FIELD,
-    _create_price_area_asset_filter,
+from cognite.powerops.client._generated.v1.data_classes._bid_configuration import (
+    _BIDCONFIGURATION_PROPERTIES_BY_FIELD,
+    _create_bid_configuration_filter,
 )
 from ._core import (
     DEFAULT_LIMIT_READ,
     DEFAULT_QUERY_LIMIT,
     Aggregations,
     NodeAPI,
     SequenceNotStr,
     QueryStep,
     QueryBuilder,
 )
-from .price_area_asset_plants import PriceAreaAssetPlantsAPI
-from .price_area_asset_watercourses import PriceAreaAssetWatercoursesAPI
-from .price_area_asset_query import PriceAreaAssetQueryAPI
+from .bid_configuration_partials import BidConfigurationPartialsAPI
+from .bid_configuration_query import BidConfigurationQueryAPI
 
 
-class PriceAreaAssetAPI(NodeAPI[PriceAreaAsset, PriceAreaAssetWrite, PriceAreaAssetList]):
+class BidConfigurationAPI(NodeAPI[BidConfiguration, BidConfigurationWrite, BidConfigurationList]):
     def __init__(self, client: CogniteClient, view_by_read_class: dict[type[DomainModelCore], dm.ViewId]):
-        view_id = view_by_read_class[PriceAreaAsset]
+        view_id = view_by_read_class[BidConfiguration]
         super().__init__(
             client=client,
             sources=view_id,
-            class_type=PriceAreaAsset,
-            class_list=PriceAreaAssetList,
-            class_write_list=PriceAreaAssetWriteList,
+            class_type=BidConfiguration,
+            class_list=BidConfigurationList,
+            class_write_list=BidConfigurationWriteList,
             view_by_read_class=view_by_read_class,
         )
         self._view_id = view_id
-        self.plants_edge = PriceAreaAssetPlantsAPI(client)
-        self.watercourses_edge = PriceAreaAssetWatercoursesAPI(client)
+        self.partials_edge = BidConfigurationPartialsAPI(client)
 
     def __call__(
         self,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
-        timezone: str | list[str] | None = None,
-        timezone_prefix: str | None = None,
+        market_configuration: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        price_area: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_QUERY_LIMIT,
         filter: dm.Filter | None = None,
-    ) -> PriceAreaAssetQueryAPI[PriceAreaAssetList]:
-        """Query starting at price area assets.
+    ) -> BidConfigurationQueryAPI[BidConfigurationList]:
+        """Query starting at bid configurations.
 
         Args:
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
-            timezone: The timezone to filter on.
-            timezone_prefix: The prefix of the timezone to filter on.
+            market_configuration: The market configuration to filter on.
+            price_area: The price area to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of price area assets to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of bid configurations to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
-            A query API for price area assets.
+            A query API for bid configurations.
 
         """
         has_data = dm.filters.HasData(views=[self._view_id])
-        filter_ = _create_price_area_asset_filter(
+        filter_ = _create_bid_configuration_filter(
             self._view_id,
             name,
             name_prefix,
-            timezone,
-            timezone_prefix,
+            market_configuration,
+            price_area,
             external_id_prefix,
             space,
             (filter and dm.filters.And(filter, has_data)) or has_data,
         )
-        builder = QueryBuilder(PriceAreaAssetList)
-        return PriceAreaAssetQueryAPI(self._client, builder, self._view_by_read_class, filter_, limit)
+        builder = QueryBuilder(BidConfigurationList)
+        return BidConfigurationQueryAPI(self._client, builder, self._view_by_read_class, filter_, limit)
 
     def apply(
         self,
-        price_area_asset: PriceAreaAssetWrite | Sequence[PriceAreaAssetWrite],
+        bid_configuration: BidConfigurationWrite | Sequence[BidConfigurationWrite],
         replace: bool = False,
         write_none: bool = False,
     ) -> ResourcesWriteResult:
-        """Add or update (upsert) price area assets.
+        """Add or update (upsert) bid configurations.
 
-        Note: This method iterates through all nodes and timeseries linked to price_area_asset and creates them including the edges
-        between the nodes. For example, if any of `plants` or `watercourses` are set, then these
+        Note: This method iterates through all nodes and timeseries linked to bid_configuration and creates them including the edges
+        between the nodes. For example, if any of `partials` are set, then these
         nodes as well as any nodes linked to them, and all the edges linking these nodes will be created.
 
         Args:
-            price_area_asset: Price area asset or sequence of price area assets to upsert.
+            bid_configuration: Bid configuration or sequence of bid configurations to upsert.
             replace (bool): How do we behave when a property value exists? Do we replace all matching and existing values with the supplied values (true)?
                 Or should we merge in new values for properties together with the existing values (false)? Note: This setting applies for all nodes or edges specified in the ingestion call.
             write_none (bool): This method, will by default, skip properties that are set to None. However, if you want to set properties to None,
                 you can set this parameter to True. Note this only applies to properties that are nullable.
         Returns:
             Created instance(s), i.e., nodes, edges, and time series.
 
         Examples:
 
-            Create a new price_area_asset:
+            Create a new bid_configuration:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
-                >>> from cognite.powerops.client._generated.v1.data_classes import PriceAreaAssetWrite
+                >>> from cognite.powerops.client._generated.v1.data_classes import BidConfigurationWrite
                 >>> client = PowerOpsModelsV1Client()
-                >>> price_area_asset = PriceAreaAssetWrite(external_id="my_price_area_asset", ...)
-                >>> result = client.price_area_asset.apply(price_area_asset)
+                >>> bid_configuration = BidConfigurationWrite(external_id="my_bid_configuration", ...)
+                >>> result = client.bid_configuration.apply(bid_configuration)
 
         """
         warnings.warn(
             "The .apply method is deprecated and will be removed in v1.0. "
             "Please use the .upsert method on the client instead. This means instead of "
-            "`my_client.price_area_asset.apply(my_items)` please use `my_client.upsert(my_items)`."
+            "`my_client.bid_configuration.apply(my_items)` please use `my_client.upsert(my_items)`."
             "The motivation is that all apply methods are the same, and having one apply method per API "
             " class encourages users to create items in small batches, which is inefficient."
             "In addition, .upsert method is more descriptive of what the method does.",
             UserWarning,
             stacklevel=2,
         )
-        return self._apply(price_area_asset, replace, write_none)
+        return self._apply(bid_configuration, replace, write_none)
 
     def delete(
         self, external_id: str | SequenceNotStr[str], space: str = DEFAULT_INSTANCE_SPACE
     ) -> dm.InstancesDeleteResult:
-        """Delete one or more price area asset.
+        """Delete one or more bid configuration.
 
         Args:
-            external_id: External id of the price area asset to delete.
-            space: The space where all the price area asset are located.
+            external_id: External id of the bid configuration to delete.
+            space: The space where all the bid configuration are located.
 
         Returns:
             The instance(s), i.e., nodes and edges which has been deleted. Empty list if nothing was deleted.
 
         Examples:
 
-            Delete price_area_asset by id:
+            Delete bid_configuration by id:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> client.price_area_asset.delete("my_price_area_asset")
+                >>> client.bid_configuration.delete("my_bid_configuration")
         """
         warnings.warn(
             "The .delete method is deprecated and will be removed in v1.0. "
             "Please use the .delete method on the client instead. This means instead of "
-            "`my_client.price_area_asset.delete(my_ids)` please use `my_client.delete(my_ids)`."
+            "`my_client.bid_configuration.delete(my_ids)` please use `my_client.delete(my_ids)`."
             "The motivation is that all delete methods are the same, and having one delete method per API "
             " class encourages users to delete items in small batches, which is inefficient.",
             UserWarning,
             stacklevel=2,
         )
         return self._delete(external_id, space)
 
     @overload
-    def retrieve(self, external_id: str, space: str = DEFAULT_INSTANCE_SPACE) -> PriceAreaAsset | None: ...
+    def retrieve(self, external_id: str, space: str = DEFAULT_INSTANCE_SPACE) -> BidConfiguration | None: ...
 
     @overload
-    def retrieve(self, external_id: SequenceNotStr[str], space: str = DEFAULT_INSTANCE_SPACE) -> PriceAreaAssetList: ...
+    def retrieve(
+        self, external_id: SequenceNotStr[str], space: str = DEFAULT_INSTANCE_SPACE
+    ) -> BidConfigurationList: ...
 
     def retrieve(
         self, external_id: str | SequenceNotStr[str], space: str = DEFAULT_INSTANCE_SPACE
-    ) -> PriceAreaAsset | PriceAreaAssetList | None:
-        """Retrieve one or more price area assets by id(s).
+    ) -> BidConfiguration | BidConfigurationList | None:
+        """Retrieve one or more bid configurations by id(s).
 
         Args:
-            external_id: External id or list of external ids of the price area assets.
-            space: The space where all the price area assets are located.
+            external_id: External id or list of external ids of the bid configurations.
+            space: The space where all the bid configurations are located.
 
         Returns:
-            The requested price area assets.
+            The requested bid configurations.
 
         Examples:
 
-            Retrieve price_area_asset by id:
+            Retrieve bid_configuration by id:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> price_area_asset = client.price_area_asset.retrieve("my_price_area_asset")
+                >>> bid_configuration = client.bid_configuration.retrieve("my_bid_configuration")
 
         """
         return self._retrieve(
             external_id,
             space,
             retrieve_edges=True,
             edge_api_name_type_direction_view_id_penta=[
                 (
-                    self.plants_edge,
-                    "plants",
-                    dm.DirectRelationReference("sp_powerops_types", "isPlantOf"),
-                    "outwards",
-                    dm.ViewId("sp_powerops_models", "Plant", "1"),
-                ),
-                (
-                    self.watercourses_edge,
-                    "watercourses",
-                    dm.DirectRelationReference("sp_powerops_types", "isWatercourseOf"),
+                    self.partials_edge,
+                    "partials",
+                    dm.DirectRelationReference("sp_powerops_types_temp", "BidConfiguration.partials"),
                     "outwards",
-                    dm.ViewId("sp_powerops_models", "Watercourse", "1"),
+                    dm.ViewId("sp_powerops_models_temp", "PartialBidConfiguration", "1"),
                 ),
             ],
         )
 
     def search(
         self,
         query: str,
-        properties: PriceAreaAssetTextFields | Sequence[PriceAreaAssetTextFields] | None = None,
+        properties: BidConfigurationTextFields | Sequence[BidConfigurationTextFields] | None = None,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
-        timezone: str | list[str] | None = None,
-        timezone_prefix: str | None = None,
+        market_configuration: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        price_area: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
-    ) -> PriceAreaAssetList:
-        """Search price area assets
+    ) -> BidConfigurationList:
+        """Search bid configurations
 
         Args:
             query: The search query,
             properties: The property to search, if nothing is passed all text fields will be searched.
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
-            timezone: The timezone to filter on.
-            timezone_prefix: The prefix of the timezone to filter on.
+            market_configuration: The market configuration to filter on.
+            price_area: The price area to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of price area assets to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of bid configurations to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
-            Search results price area assets matching the query.
+            Search results bid configurations matching the query.
 
         Examples:
 
-           Search for 'my_price_area_asset' in all text properties:
+           Search for 'my_bid_configuration' in all text properties:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> price_area_assets = client.price_area_asset.search('my_price_area_asset')
+                >>> bid_configurations = client.bid_configuration.search('my_bid_configuration')
 
         """
-        filter_ = _create_price_area_asset_filter(
+        filter_ = _create_bid_configuration_filter(
             self._view_id,
             name,
             name_prefix,
-            timezone,
-            timezone_prefix,
+            market_configuration,
+            price_area,
             external_id_prefix,
             space,
             filter,
         )
-        return self._search(self._view_id, query, _PRICEAREAASSET_PROPERTIES_BY_FIELD, properties, filter_, limit)
+        return self._search(self._view_id, query, _BIDCONFIGURATION_PROPERTIES_BY_FIELD, properties, filter_, limit)
 
     @overload
     def aggregate(
         self,
         aggregations: (
             Aggregations
             | dm.aggregations.MetricAggregation
             | Sequence[Aggregations]
             | Sequence[dm.aggregations.MetricAggregation]
         ),
-        property: PriceAreaAssetFields | Sequence[PriceAreaAssetFields] | None = None,
+        property: BidConfigurationFields | Sequence[BidConfigurationFields] | None = None,
         group_by: None = None,
         query: str | None = None,
-        search_properties: PriceAreaAssetTextFields | Sequence[PriceAreaAssetTextFields] | None = None,
+        search_properties: BidConfigurationTextFields | Sequence[BidConfigurationTextFields] | None = None,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
-        timezone: str | list[str] | None = None,
-        timezone_prefix: str | None = None,
+        market_configuration: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        price_area: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> list[dm.aggregations.AggregatedNumberedValue]: ...
 
     @overload
@@ -297,219 +290,212 @@
         self,
         aggregations: (
             Aggregations
             | dm.aggregations.MetricAggregation
             | Sequence[Aggregations]
             | Sequence[dm.aggregations.MetricAggregation]
         ),
-        property: PriceAreaAssetFields | Sequence[PriceAreaAssetFields] | None = None,
-        group_by: PriceAreaAssetFields | Sequence[PriceAreaAssetFields] = None,
+        property: BidConfigurationFields | Sequence[BidConfigurationFields] | None = None,
+        group_by: BidConfigurationFields | Sequence[BidConfigurationFields] = None,
         query: str | None = None,
-        search_properties: PriceAreaAssetTextFields | Sequence[PriceAreaAssetTextFields] | None = None,
+        search_properties: BidConfigurationTextFields | Sequence[BidConfigurationTextFields] | None = None,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
-        timezone: str | list[str] | None = None,
-        timezone_prefix: str | None = None,
+        market_configuration: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        price_area: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> InstanceAggregationResultList: ...
 
     def aggregate(
         self,
         aggregate: (
             Aggregations
             | dm.aggregations.MetricAggregation
             | Sequence[Aggregations]
             | Sequence[dm.aggregations.MetricAggregation]
         ),
-        property: PriceAreaAssetFields | Sequence[PriceAreaAssetFields] | None = None,
-        group_by: PriceAreaAssetFields | Sequence[PriceAreaAssetFields] | None = None,
+        property: BidConfigurationFields | Sequence[BidConfigurationFields] | None = None,
+        group_by: BidConfigurationFields | Sequence[BidConfigurationFields] | None = None,
         query: str | None = None,
-        search_property: PriceAreaAssetTextFields | Sequence[PriceAreaAssetTextFields] | None = None,
+        search_property: BidConfigurationTextFields | Sequence[BidConfigurationTextFields] | None = None,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
-        timezone: str | list[str] | None = None,
-        timezone_prefix: str | None = None,
+        market_configuration: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        price_area: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> list[dm.aggregations.AggregatedNumberedValue] | InstanceAggregationResultList:
-        """Aggregate data across price area assets
+        """Aggregate data across bid configurations
 
         Args:
             aggregate: The aggregation to perform.
             property: The property to perform aggregation on.
             group_by: The property to group by when doing the aggregation.
             query: The query to search for in the text field.
             search_property: The text field to search in.
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
-            timezone: The timezone to filter on.
-            timezone_prefix: The prefix of the timezone to filter on.
+            market_configuration: The market configuration to filter on.
+            price_area: The price area to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of price area assets to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of bid configurations to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
             Aggregation results.
 
         Examples:
 
-            Count price area assets in space `my_space`:
+            Count bid configurations in space `my_space`:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> result = client.price_area_asset.aggregate("count", space="my_space")
+                >>> result = client.bid_configuration.aggregate("count", space="my_space")
 
         """
 
-        filter_ = _create_price_area_asset_filter(
+        filter_ = _create_bid_configuration_filter(
             self._view_id,
             name,
             name_prefix,
-            timezone,
-            timezone_prefix,
+            market_configuration,
+            price_area,
             external_id_prefix,
             space,
             filter,
         )
         return self._aggregate(
             self._view_id,
             aggregate,
-            _PRICEAREAASSET_PROPERTIES_BY_FIELD,
+            _BIDCONFIGURATION_PROPERTIES_BY_FIELD,
             property,
             group_by,
             query,
             search_property,
             limit,
             filter_,
         )
 
     def histogram(
         self,
-        property: PriceAreaAssetFields,
+        property: BidConfigurationFields,
         interval: float,
         query: str | None = None,
-        search_property: PriceAreaAssetTextFields | Sequence[PriceAreaAssetTextFields] | None = None,
+        search_property: BidConfigurationTextFields | Sequence[BidConfigurationTextFields] | None = None,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
-        timezone: str | list[str] | None = None,
-        timezone_prefix: str | None = None,
+        market_configuration: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        price_area: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> dm.aggregations.HistogramValue:
-        """Produces histograms for price area assets
+        """Produces histograms for bid configurations
 
         Args:
             property: The property to use as the value in the histogram.
             interval: The interval to use for the histogram bins.
             query: The query to search for in the text field.
             search_property: The text field to search in.
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
-            timezone: The timezone to filter on.
-            timezone_prefix: The prefix of the timezone to filter on.
+            market_configuration: The market configuration to filter on.
+            price_area: The price area to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of price area assets to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of bid configurations to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
             Bucketed histogram results.
 
         """
-        filter_ = _create_price_area_asset_filter(
+        filter_ = _create_bid_configuration_filter(
             self._view_id,
             name,
             name_prefix,
-            timezone,
-            timezone_prefix,
+            market_configuration,
+            price_area,
             external_id_prefix,
             space,
             filter,
         )
         return self._histogram(
             self._view_id,
             property,
             interval,
-            _PRICEAREAASSET_PROPERTIES_BY_FIELD,
+            _BIDCONFIGURATION_PROPERTIES_BY_FIELD,
             query,
             search_property,
             limit,
             filter_,
         )
 
     def list(
         self,
         name: str | list[str] | None = None,
         name_prefix: str | None = None,
-        timezone: str | list[str] | None = None,
-        timezone_prefix: str | None = None,
+        market_configuration: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        price_area: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
         retrieve_edges: bool = True,
-    ) -> PriceAreaAssetList:
-        """List/filter price area assets
+    ) -> BidConfigurationList:
+        """List/filter bid configurations
 
         Args:
             name: The name to filter on.
             name_prefix: The prefix of the name to filter on.
-            timezone: The timezone to filter on.
-            timezone_prefix: The prefix of the timezone to filter on.
+            market_configuration: The market configuration to filter on.
+            price_area: The price area to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of price area assets to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of bid configurations to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
-            retrieve_edges: Whether to retrieve `plants` or `watercourses` external ids for the price area assets. Defaults to True.
+            retrieve_edges: Whether to retrieve `partials` external ids for the bid configurations. Defaults to True.
 
         Returns:
-            List of requested price area assets
+            List of requested bid configurations
 
         Examples:
 
-            List price area assets and limit to 5:
+            List bid configurations and limit to 5:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> price_area_assets = client.price_area_asset.list(limit=5)
+                >>> bid_configurations = client.bid_configuration.list(limit=5)
 
         """
-        filter_ = _create_price_area_asset_filter(
+        filter_ = _create_bid_configuration_filter(
             self._view_id,
             name,
             name_prefix,
-            timezone,
-            timezone_prefix,
+            market_configuration,
+            price_area,
             external_id_prefix,
             space,
             filter,
         )
 
         return self._list(
             limit=limit,
             filter=filter_,
             retrieve_edges=retrieve_edges,
             edge_api_name_type_direction_view_id_penta=[
                 (
-                    self.plants_edge,
-                    "plants",
-                    dm.DirectRelationReference("sp_powerops_types", "isPlantOf"),
-                    "outwards",
-                    dm.ViewId("sp_powerops_models", "Plant", "1"),
-                ),
-                (
-                    self.watercourses_edge,
-                    "watercourses",
-                    dm.DirectRelationReference("sp_powerops_types", "isWatercourseOf"),
+                    self.partials_edge,
+                    "partials",
+                    dm.DirectRelationReference("sp_powerops_types_temp", "BidConfiguration.partials"),
                     "outwards",
-                    dm.ViewId("sp_powerops_models", "Watercourse", "1"),
+                    dm.ViewId("sp_powerops_models_temp", "PartialBidConfiguration", "1"),
                 ),
             ],
         )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/price_area_asset_plants.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/price_area_asset_plants.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/price_area_asset_watercourses.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/price_area_asset_watercourses.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/price_area_day_ahead.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/price_area_day_ahead.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/price_area_day_ahead_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/price_area_day_ahead_query.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/price_area_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/price_area_query.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/price_prod_case.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/price_prod_case.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/price_prod_case_price.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/price_production_price.py`

 * *Files 5% similar despite different names*

```diff
@@ -5,21 +5,21 @@
 from typing import Literal
 
 import pandas as pd
 from cognite.client import CogniteClient
 from cognite.client import data_modeling as dm
 from cognite.client.data_classes import Datapoints, DatapointsArrayList, DatapointsList, TimeSeriesList
 from cognite.client.data_classes.datapoints import Aggregate
-from cognite.powerops.client._generated.v1.data_classes._price_prod_case import _create_price_prod_case_filter
+from cognite.powerops.client._generated.v1.data_classes._price_production import _create_price_production_filter
 from ._core import DEFAULT_LIMIT_READ, INSTANCE_QUERY_LIMIT
 
 ColumnNames = Literal["price", "production"]
 
 
-class PriceProdCasePriceQuery:
+class PriceProductionPriceQuery:
     def __init__(
         self,
         client: CogniteClient,
         view_id: dm.ViewId,
         timeseries_limit: int = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ):
@@ -36,15 +36,15 @@
         aggregates: Aggregate | list[Aggregate] | None = None,
         granularity: str | None = None,
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         limit: int | None = None,
         include_outside_points: bool = False,
     ) -> DatapointsList:
-        """`Retrieve datapoints for the `price_prod_case.price` timeseries.
+        """`Retrieve datapoints for the `price_production.price` timeseries.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -65,15 +65,15 @@
         Examples:
 
             In this example,
             we are using the time-ago format to get raw data for the 'my_price' from 2 weeks ago up until now::
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> price_prod_case_datapoints = client.price_prod_case.price(external_id="my_price").retrieve(start="2w-ago")
+                >>> price_production_datapoints = client.price_production.price(external_id="my_price").retrieve(start="2w-ago")
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra()
         if external_ids:
             return self._client.time_series.data.retrieve(
                 external_id=list(external_ids),
                 start=start,
                 end=end,
@@ -95,15 +95,15 @@
         aggregates: Aggregate | list[Aggregate] | None = None,
         granularity: str | None = None,
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         limit: int | None = None,
         include_outside_points: bool = False,
     ) -> DatapointsArrayList:
-        """`Retrieve numpy arrays for the `price_prod_case.price` timeseries.
+        """`Retrieve numpy arrays for the `price_production.price` timeseries.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -124,15 +124,15 @@
         Examples:
 
             In this example,
             we are using the time-ago format to get raw data for the 'my_price' from 2 weeks ago up until now::
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> price_prod_case_datapoints = client.price_prod_case.price(external_id="my_price").retrieve_array(start="2w-ago")
+                >>> price_production_datapoints = client.price_production.price(external_id="my_price").retrieve_array(start="2w-ago")
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra()
         if external_ids:
             return self._client.time_series.data.retrieve_arrays(
                 external_id=list(external_ids),
                 start=start,
                 end=end,
@@ -158,15 +158,15 @@
         limit: int | None = None,
         include_outside_points: bool = False,
         uniform_index: bool = False,
         include_aggregate_name: bool = True,
         include_granularity_name: bool = False,
         column_names: ColumnNames | list[ColumnNames] = "price",
     ) -> pd.DataFrame:
-        """`Retrieve DataFrames for the `price_prod_case.price` timeseries.
+        """`Retrieve DataFrames for the `price_production.price` timeseries.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -192,15 +192,15 @@
         Examples:
 
             In this example,
             we are using the time-ago format to get raw data for the 'my_price' from 2 weeks ago up until now::
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> price_prod_case_datapoints = client.price_prod_case.price(external_id="my_price").retrieve_dataframe(start="2w-ago")
+                >>> price_production_datapoints = client.price_production.price(external_id="my_price").retrieve_dataframe(start="2w-ago")
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra(column_names)
         if external_ids:
             df = self._client.time_series.data.retrieve_dataframe(
                 external_id=list(external_ids),
                 start=start,
                 end=end,
@@ -235,15 +235,15 @@
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         uniform_index: bool = False,
         include_aggregate_name: bool = True,
         include_granularity_name: bool = False,
         column_names: ColumnNames | list[ColumnNames] = "price",
     ) -> pd.DataFrame:
-        """Retrieve DataFrames for the `price_prod_case.price` timeseries in Timezone.
+        """Retrieve DataFrames for the `price_production.price` timeseries in Timezone.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -270,15 +270,15 @@
 
             In this example,
             get weekly aggregates for the 'my_price' for the first month of 2023 in Oslo time:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> from datetime import datetime, timezone
                 >>> client = PowerOpsModelsV1Client()
-                >>> price_prod_case_datapoints = client.price_prod_case.price(
+                >>> price_production_datapoints = client.price_production.price(
                 ...     external_id="my_price").retrieve_dataframe_in_timezone(
                 ...         datetime(2023, 1, 1, tzinfo=ZoneInfo("Europe/Oslo")),
                 ...         datetime(2023, 1, 2, tzinfo=ZoneInfo("Europe/Oslo")),
                 ...         aggregates="average",
                 ...         granularity="1week",
                 ...     )
         """
@@ -348,96 +348,96 @@
             column_parts = (col.rsplit("|", maxsplit=splits) for col in df.columns)
             df.columns = [
                 "-".join(external_ids[external_id]) + "|" + "|".join(parts) for external_id, *parts in column_parts
             ]
         return df
 
 
-class PriceProdCasePriceAPI:
+class PriceProductionPriceAPI:
     def __init__(self, client: CogniteClient, view_id: dm.ViewId):
         self._client = client
         self._view_id = view_id
 
     def __call__(
         self,
-        case: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        shop_result: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
-    ) -> PriceProdCasePriceQuery:
-        """Query timeseries `price_prod_case.price`
+    ) -> PriceProductionPriceQuery:
+        """Query timeseries `price_production.price`
 
         Args:
-            case: The case to filter on.
+            shop_result: The shop result to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of price prod cases to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of price productions to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
-            A query object that can be used to retrieve datapoins for the price_prod_case.price timeseries
+            A query object that can be used to retrieve datapoins for the price_production.price timeseries
             selected in this method.
 
         Examples:
 
-            Retrieve all data for 5 price_prod_case.price timeseries:
+            Retrieve all data for 5 price_production.price timeseries:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> price_prod_cases = client.price_prod_case.price(limit=5).retrieve()
+                >>> price_productions = client.price_production.price(limit=5).retrieve()
 
         """
-        filter_ = _create_price_prod_case_filter(
+        filter_ = _create_price_production_filter(
             self._view_id,
-            case,
+            shop_result,
             external_id_prefix,
             space,
             filter,
         )
 
-        return PriceProdCasePriceQuery(
+        return PriceProductionPriceQuery(
             client=self._client,
             view_id=self._view_id,
             timeseries_limit=limit,
             filter=filter_,
         )
 
     def list(
         self,
-        case: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        shop_result: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> TimeSeriesList:
-        """List timeseries `price_prod_case.price`
+        """List timeseries `price_production.price`
 
         Args:
-            case: The case to filter on.
+            shop_result: The shop result to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of price prod cases to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of price productions to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
-            List of Timeseries price_prod_case.price.
+            List of Timeseries price_production.price.
 
         Examples:
 
-            List price_prod_case.price and limit to 5:
+            List price_production.price and limit to 5:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> price_prod_cases = client.price_prod_case.price.list(limit=5)
+                >>> price_productions = client.price_production.price.list(limit=5)
 
         """
-        filter_ = _create_price_prod_case_filter(
+        filter_ = _create_price_production_filter(
             self._view_id,
-            case,
+            shop_result,
             external_id_prefix,
             space,
             filter,
         )
         external_ids = _retrieve_timeseries_external_ids_with_extra_price(self._client, self._view_id, filter_, limit)
         if external_ids:
             return self._client.time_series.retrieve_multiple(external_ids=list(external_ids))
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/price_prod_case_production.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/price_prod_case_production.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/price_prod_case_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/water_value_based_partial_bid_configuration_query.py`

 * *Files 13% similar despite different names*

```diff
@@ -3,71 +3,74 @@
 import datetime
 from typing import TYPE_CHECKING
 
 from cognite.client import data_modeling as dm, CogniteClient
 
 from cognite.powerops.client._generated.v1.data_classes import (
     DomainModelCore,
-    PriceProdCase,
-    Case,
+    WaterValueBasedPartialBidConfiguration,
+    Plant,
 )
 from ._core import DEFAULT_QUERY_LIMIT, QueryBuilder, QueryStep, QueryAPI, T_DomainModelList, _create_edge_filter
 
 
-class PriceProdCaseQueryAPI(QueryAPI[T_DomainModelList]):
+class WaterValueBasedPartialBidConfigurationQueryAPI(QueryAPI[T_DomainModelList]):
     def __init__(
         self,
         client: CogniteClient,
         builder: QueryBuilder[T_DomainModelList],
         view_by_read_class: dict[type[DomainModelCore], dm.ViewId],
         filter_: dm.filters.Filter | None = None,
         limit: int = DEFAULT_QUERY_LIMIT,
     ):
         super().__init__(client, builder, view_by_read_class)
 
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("price_prod_case"),
+                name=self._builder.next_name("water_value_based_partial_bid_configuration"),
                 expression=dm.query.NodeResultSetExpression(
                     from_=self._builder[-1].name if self._builder else None,
                     filter=filter_,
                 ),
-                select=dm.query.Select([dm.query.SourceSelector(self._view_by_read_class[PriceProdCase], ["*"])]),
-                result_cls=PriceProdCase,
+                select=dm.query.Select(
+                    [dm.query.SourceSelector(self._view_by_read_class[WaterValueBasedPartialBidConfiguration], ["*"])]
+                ),
+                result_cls=WaterValueBasedPartialBidConfiguration,
                 max_retrieve_limit=limit,
             )
         )
 
     def query(
         self,
-        retrieve_case: bool = False,
+        retrieve_plant: bool = False,
     ) -> T_DomainModelList:
         """Execute query and return the result.
 
         Args:
-            retrieve_case: Whether to retrieve the case for each price prod case or not.
+            retrieve_plant: Whether to retrieve the plant for each water value based partial bid configuration or not.
 
         Returns:
             The list of the source nodes of the query.
 
         """
         from_ = self._builder[-1].name
-        if retrieve_case:
-            self._query_append_case(from_)
+        if retrieve_plant:
+            self._query_append_plant(from_)
         return self._query()
 
-    def _query_append_case(self, from_: str) -> None:
-        view_id = self._view_by_read_class[Case]
+    def _query_append_plant(self, from_: str) -> None:
+        view_id = self._view_by_read_class[Plant]
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("case"),
+                name=self._builder.next_name("plant"),
                 expression=dm.query.NodeResultSetExpression(
                     filter=dm.filters.HasData(views=[view_id]),
                     from_=from_,
-                    through=self._view_by_read_class[PriceProdCase].as_property_ref("case"),
+                    through=self._view_by_read_class[WaterValueBasedPartialBidConfiguration].as_property_ref("plant"),
                     direction="outwards",
                 ),
                 select=dm.query.Select([dm.query.SourceSelector(view_id, ["*"])]),
                 max_retrieve_limit=-1,
-                result_cls=Case,
+                result_cls=Plant,
+                is_single_direct_relation=True,
             ),
         )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/price_scenario.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/price_scenario.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/price_scenario_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/price_scenario_query.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/price_scenario_timeseries.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/price_scenario_timeseries.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/reservoir.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/reservoir.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/reservoir_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/reservoir_query.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/scenario.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/function_output.py`

 * *Files 17% similar despite different names*

```diff
@@ -9,292 +9,306 @@
 from cognite.client.data_classes.data_modeling.instances import InstanceAggregationResultList
 
 from cognite.powerops.client._generated.v1.data_classes._core import DEFAULT_INSTANCE_SPACE
 from cognite.powerops.client._generated.v1.data_classes import (
     DomainModelCore,
     DomainModelWrite,
     ResourcesWriteResult,
-    Scenario,
-    ScenarioWrite,
-    ScenarioFields,
-    ScenarioList,
-    ScenarioWriteList,
-    ScenarioTextFields,
+    FunctionOutput,
+    FunctionOutputWrite,
+    FunctionOutputFields,
+    FunctionOutputList,
+    FunctionOutputWriteList,
+    FunctionOutputTextFields,
 )
-from cognite.powerops.client._generated.v1.data_classes._scenario import (
-    _SCENARIO_PROPERTIES_BY_FIELD,
-    _create_scenario_filter,
+from cognite.powerops.client._generated.v1.data_classes._function_output import (
+    _FUNCTIONOUTPUT_PROPERTIES_BY_FIELD,
+    _create_function_output_filter,
 )
 from ._core import (
     DEFAULT_LIMIT_READ,
     DEFAULT_QUERY_LIMIT,
     Aggregations,
     NodeAPI,
     SequenceNotStr,
     QueryStep,
     QueryBuilder,
 )
-from .scenario_mappings_override import ScenarioMappingsOverrideAPI
-from .scenario_query import ScenarioQueryAPI
+from .function_output_alerts import FunctionOutputAlertsAPI
+from .function_output_query import FunctionOutputQueryAPI
 
 
-class ScenarioAPI(NodeAPI[Scenario, ScenarioWrite, ScenarioList]):
+class FunctionOutputAPI(NodeAPI[FunctionOutput, FunctionOutputWrite, FunctionOutputList]):
     def __init__(self, client: CogniteClient, view_by_read_class: dict[type[DomainModelCore], dm.ViewId]):
-        view_id = view_by_read_class[Scenario]
+        view_id = view_by_read_class[FunctionOutput]
         super().__init__(
             client=client,
             sources=view_id,
-            class_type=Scenario,
-            class_list=ScenarioList,
-            class_write_list=ScenarioWriteList,
+            class_type=FunctionOutput,
+            class_list=FunctionOutputList,
+            class_write_list=FunctionOutputWriteList,
             view_by_read_class=view_by_read_class,
         )
         self._view_id = view_id
-        self.mappings_override_edge = ScenarioMappingsOverrideAPI(client)
+        self.alerts_edge = FunctionOutputAlertsAPI(client)
 
     def __call__(
         self,
-        name: str | list[str] | None = None,
-        name_prefix: str | None = None,
-        model_template: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
-        commands: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
-        source: str | list[str] | None = None,
-        source_prefix: str | None = None,
+        process_id: str | list[str] | None = None,
+        process_id_prefix: str | None = None,
+        min_process_step: int | None = None,
+        max_process_step: int | None = None,
+        function_name: str | list[str] | None = None,
+        function_name_prefix: str | None = None,
+        function_call_id: str | list[str] | None = None,
+        function_call_id_prefix: str | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_QUERY_LIMIT,
         filter: dm.Filter | None = None,
-    ) -> ScenarioQueryAPI[ScenarioList]:
-        """Query starting at scenarios.
+    ) -> FunctionOutputQueryAPI[FunctionOutputList]:
+        """Query starting at function outputs.
 
         Args:
-            name: The name to filter on.
-            name_prefix: The prefix of the name to filter on.
-            model_template: The model template to filter on.
-            commands: The command to filter on.
-            source: The source to filter on.
-            source_prefix: The prefix of the source to filter on.
+            process_id: The process id to filter on.
+            process_id_prefix: The prefix of the process id to filter on.
+            min_process_step: The minimum value of the process step to filter on.
+            max_process_step: The maximum value of the process step to filter on.
+            function_name: The function name to filter on.
+            function_name_prefix: The prefix of the function name to filter on.
+            function_call_id: The function call id to filter on.
+            function_call_id_prefix: The prefix of the function call id to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of scenarios to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of function outputs to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
-            A query API for scenarios.
+            A query API for function outputs.
 
         """
         has_data = dm.filters.HasData(views=[self._view_id])
-        filter_ = _create_scenario_filter(
+        filter_ = _create_function_output_filter(
             self._view_id,
-            name,
-            name_prefix,
-            model_template,
-            commands,
-            source,
-            source_prefix,
+            process_id,
+            process_id_prefix,
+            min_process_step,
+            max_process_step,
+            function_name,
+            function_name_prefix,
+            function_call_id,
+            function_call_id_prefix,
             external_id_prefix,
             space,
             (filter and dm.filters.And(filter, has_data)) or has_data,
         )
-        builder = QueryBuilder(ScenarioList)
-        return ScenarioQueryAPI(self._client, builder, self._view_by_read_class, filter_, limit)
+        builder = QueryBuilder(FunctionOutputList)
+        return FunctionOutputQueryAPI(self._client, builder, self._view_by_read_class, filter_, limit)
 
     def apply(
         self,
-        scenario: ScenarioWrite | Sequence[ScenarioWrite],
+        function_output: FunctionOutputWrite | Sequence[FunctionOutputWrite],
         replace: bool = False,
         write_none: bool = False,
     ) -> ResourcesWriteResult:
-        """Add or update (upsert) scenarios.
+        """Add or update (upsert) function outputs.
 
-        Note: This method iterates through all nodes and timeseries linked to scenario and creates them including the edges
-        between the nodes. For example, if any of `mappings_override` are set, then these
+        Note: This method iterates through all nodes and timeseries linked to function_output and creates them including the edges
+        between the nodes. For example, if any of `alerts` are set, then these
         nodes as well as any nodes linked to them, and all the edges linking these nodes will be created.
 
         Args:
-            scenario: Scenario or sequence of scenarios to upsert.
+            function_output: Function output or sequence of function outputs to upsert.
             replace (bool): How do we behave when a property value exists? Do we replace all matching and existing values with the supplied values (true)?
                 Or should we merge in new values for properties together with the existing values (false)? Note: This setting applies for all nodes or edges specified in the ingestion call.
             write_none (bool): This method, will by default, skip properties that are set to None. However, if you want to set properties to None,
                 you can set this parameter to True. Note this only applies to properties that are nullable.
         Returns:
             Created instance(s), i.e., nodes, edges, and time series.
 
         Examples:
 
-            Create a new scenario:
+            Create a new function_output:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
-                >>> from cognite.powerops.client._generated.v1.data_classes import ScenarioWrite
+                >>> from cognite.powerops.client._generated.v1.data_classes import FunctionOutputWrite
                 >>> client = PowerOpsModelsV1Client()
-                >>> scenario = ScenarioWrite(external_id="my_scenario", ...)
-                >>> result = client.scenario.apply(scenario)
+                >>> function_output = FunctionOutputWrite(external_id="my_function_output", ...)
+                >>> result = client.function_output.apply(function_output)
 
         """
         warnings.warn(
             "The .apply method is deprecated and will be removed in v1.0. "
             "Please use the .upsert method on the client instead. This means instead of "
-            "`my_client.scenario.apply(my_items)` please use `my_client.upsert(my_items)`."
+            "`my_client.function_output.apply(my_items)` please use `my_client.upsert(my_items)`."
             "The motivation is that all apply methods are the same, and having one apply method per API "
             " class encourages users to create items in small batches, which is inefficient."
             "In addition, .upsert method is more descriptive of what the method does.",
             UserWarning,
             stacklevel=2,
         )
-        return self._apply(scenario, replace, write_none)
+        return self._apply(function_output, replace, write_none)
 
     def delete(
         self, external_id: str | SequenceNotStr[str], space: str = DEFAULT_INSTANCE_SPACE
     ) -> dm.InstancesDeleteResult:
-        """Delete one or more scenario.
+        """Delete one or more function output.
 
         Args:
-            external_id: External id of the scenario to delete.
-            space: The space where all the scenario are located.
+            external_id: External id of the function output to delete.
+            space: The space where all the function output are located.
 
         Returns:
             The instance(s), i.e., nodes and edges which has been deleted. Empty list if nothing was deleted.
 
         Examples:
 
-            Delete scenario by id:
+            Delete function_output by id:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> client.scenario.delete("my_scenario")
+                >>> client.function_output.delete("my_function_output")
         """
         warnings.warn(
             "The .delete method is deprecated and will be removed in v1.0. "
             "Please use the .delete method on the client instead. This means instead of "
-            "`my_client.scenario.delete(my_ids)` please use `my_client.delete(my_ids)`."
+            "`my_client.function_output.delete(my_ids)` please use `my_client.delete(my_ids)`."
             "The motivation is that all delete methods are the same, and having one delete method per API "
             " class encourages users to delete items in small batches, which is inefficient.",
             UserWarning,
             stacklevel=2,
         )
         return self._delete(external_id, space)
 
     @overload
-    def retrieve(self, external_id: str, space: str = DEFAULT_INSTANCE_SPACE) -> Scenario | None: ...
+    def retrieve(self, external_id: str, space: str = DEFAULT_INSTANCE_SPACE) -> FunctionOutput | None: ...
 
     @overload
-    def retrieve(self, external_id: SequenceNotStr[str], space: str = DEFAULT_INSTANCE_SPACE) -> ScenarioList: ...
+    def retrieve(self, external_id: SequenceNotStr[str], space: str = DEFAULT_INSTANCE_SPACE) -> FunctionOutputList: ...
 
     def retrieve(
         self, external_id: str | SequenceNotStr[str], space: str = DEFAULT_INSTANCE_SPACE
-    ) -> Scenario | ScenarioList | None:
-        """Retrieve one or more scenarios by id(s).
+    ) -> FunctionOutput | FunctionOutputList | None:
+        """Retrieve one or more function outputs by id(s).
 
         Args:
-            external_id: External id or list of external ids of the scenarios.
-            space: The space where all the scenarios are located.
+            external_id: External id or list of external ids of the function outputs.
+            space: The space where all the function outputs are located.
 
         Returns:
-            The requested scenarios.
+            The requested function outputs.
 
         Examples:
 
-            Retrieve scenario by id:
+            Retrieve function_output by id:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> scenario = client.scenario.retrieve("my_scenario")
+                >>> function_output = client.function_output.retrieve("my_function_output")
 
         """
         return self._retrieve(
             external_id,
             space,
             retrieve_edges=True,
             edge_api_name_type_direction_view_id_penta=[
                 (
-                    self.mappings_override_edge,
-                    "mappings_override",
-                    dm.DirectRelationReference("sp_powerops_types", "Mapping"),
+                    self.alerts_edge,
+                    "alerts",
+                    dm.DirectRelationReference("sp_powerops_types_temp", "calculationIssue"),
                     "outwards",
-                    dm.ViewId("sp_powerops_models", "Mapping", "1"),
+                    dm.ViewId("sp_powerops_models_temp", "Alert", "1"),
                 ),
             ],
         )
 
     def search(
         self,
         query: str,
-        properties: ScenarioTextFields | Sequence[ScenarioTextFields] | None = None,
-        name: str | list[str] | None = None,
-        name_prefix: str | None = None,
-        model_template: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
-        commands: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
-        source: str | list[str] | None = None,
-        source_prefix: str | None = None,
+        properties: FunctionOutputTextFields | Sequence[FunctionOutputTextFields] | None = None,
+        process_id: str | list[str] | None = None,
+        process_id_prefix: str | None = None,
+        min_process_step: int | None = None,
+        max_process_step: int | None = None,
+        function_name: str | list[str] | None = None,
+        function_name_prefix: str | None = None,
+        function_call_id: str | list[str] | None = None,
+        function_call_id_prefix: str | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
-    ) -> ScenarioList:
-        """Search scenarios
+    ) -> FunctionOutputList:
+        """Search function outputs
 
         Args:
             query: The search query,
             properties: The property to search, if nothing is passed all text fields will be searched.
-            name: The name to filter on.
-            name_prefix: The prefix of the name to filter on.
-            model_template: The model template to filter on.
-            commands: The command to filter on.
-            source: The source to filter on.
-            source_prefix: The prefix of the source to filter on.
+            process_id: The process id to filter on.
+            process_id_prefix: The prefix of the process id to filter on.
+            min_process_step: The minimum value of the process step to filter on.
+            max_process_step: The maximum value of the process step to filter on.
+            function_name: The function name to filter on.
+            function_name_prefix: The prefix of the function name to filter on.
+            function_call_id: The function call id to filter on.
+            function_call_id_prefix: The prefix of the function call id to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of scenarios to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of function outputs to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
-            Search results scenarios matching the query.
+            Search results function outputs matching the query.
 
         Examples:
 
-           Search for 'my_scenario' in all text properties:
+           Search for 'my_function_output' in all text properties:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> scenarios = client.scenario.search('my_scenario')
+                >>> function_outputs = client.function_output.search('my_function_output')
 
         """
-        filter_ = _create_scenario_filter(
+        filter_ = _create_function_output_filter(
             self._view_id,
-            name,
-            name_prefix,
-            model_template,
-            commands,
-            source,
-            source_prefix,
+            process_id,
+            process_id_prefix,
+            min_process_step,
+            max_process_step,
+            function_name,
+            function_name_prefix,
+            function_call_id,
+            function_call_id_prefix,
             external_id_prefix,
             space,
             filter,
         )
-        return self._search(self._view_id, query, _SCENARIO_PROPERTIES_BY_FIELD, properties, filter_, limit)
+        return self._search(self._view_id, query, _FUNCTIONOUTPUT_PROPERTIES_BY_FIELD, properties, filter_, limit)
 
     @overload
     def aggregate(
         self,
         aggregations: (
             Aggregations
             | dm.aggregations.MetricAggregation
             | Sequence[Aggregations]
             | Sequence[dm.aggregations.MetricAggregation]
         ),
-        property: ScenarioFields | Sequence[ScenarioFields] | None = None,
+        property: FunctionOutputFields | Sequence[FunctionOutputFields] | None = None,
         group_by: None = None,
         query: str | None = None,
-        search_properties: ScenarioTextFields | Sequence[ScenarioTextFields] | None = None,
-        name: str | list[str] | None = None,
-        name_prefix: str | None = None,
-        model_template: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
-        commands: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
-        source: str | list[str] | None = None,
-        source_prefix: str | None = None,
+        search_properties: FunctionOutputTextFields | Sequence[FunctionOutputTextFields] | None = None,
+        process_id: str | list[str] | None = None,
+        process_id_prefix: str | None = None,
+        min_process_step: int | None = None,
+        max_process_step: int | None = None,
+        function_name: str | list[str] | None = None,
+        function_name_prefix: str | None = None,
+        function_call_id: str | list[str] | None = None,
+        function_call_id_prefix: str | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> list[dm.aggregations.AggregatedNumberedValue]: ...
 
     @overload
@@ -302,232 +316,252 @@
         self,
         aggregations: (
             Aggregations
             | dm.aggregations.MetricAggregation
             | Sequence[Aggregations]
             | Sequence[dm.aggregations.MetricAggregation]
         ),
-        property: ScenarioFields | Sequence[ScenarioFields] | None = None,
-        group_by: ScenarioFields | Sequence[ScenarioFields] = None,
+        property: FunctionOutputFields | Sequence[FunctionOutputFields] | None = None,
+        group_by: FunctionOutputFields | Sequence[FunctionOutputFields] = None,
         query: str | None = None,
-        search_properties: ScenarioTextFields | Sequence[ScenarioTextFields] | None = None,
-        name: str | list[str] | None = None,
-        name_prefix: str | None = None,
-        model_template: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
-        commands: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
-        source: str | list[str] | None = None,
-        source_prefix: str | None = None,
+        search_properties: FunctionOutputTextFields | Sequence[FunctionOutputTextFields] | None = None,
+        process_id: str | list[str] | None = None,
+        process_id_prefix: str | None = None,
+        min_process_step: int | None = None,
+        max_process_step: int | None = None,
+        function_name: str | list[str] | None = None,
+        function_name_prefix: str | None = None,
+        function_call_id: str | list[str] | None = None,
+        function_call_id_prefix: str | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> InstanceAggregationResultList: ...
 
     def aggregate(
         self,
         aggregate: (
             Aggregations
             | dm.aggregations.MetricAggregation
             | Sequence[Aggregations]
             | Sequence[dm.aggregations.MetricAggregation]
         ),
-        property: ScenarioFields | Sequence[ScenarioFields] | None = None,
-        group_by: ScenarioFields | Sequence[ScenarioFields] | None = None,
+        property: FunctionOutputFields | Sequence[FunctionOutputFields] | None = None,
+        group_by: FunctionOutputFields | Sequence[FunctionOutputFields] | None = None,
         query: str | None = None,
-        search_property: ScenarioTextFields | Sequence[ScenarioTextFields] | None = None,
-        name: str | list[str] | None = None,
-        name_prefix: str | None = None,
-        model_template: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
-        commands: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
-        source: str | list[str] | None = None,
-        source_prefix: str | None = None,
+        search_property: FunctionOutputTextFields | Sequence[FunctionOutputTextFields] | None = None,
+        process_id: str | list[str] | None = None,
+        process_id_prefix: str | None = None,
+        min_process_step: int | None = None,
+        max_process_step: int | None = None,
+        function_name: str | list[str] | None = None,
+        function_name_prefix: str | None = None,
+        function_call_id: str | list[str] | None = None,
+        function_call_id_prefix: str | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> list[dm.aggregations.AggregatedNumberedValue] | InstanceAggregationResultList:
-        """Aggregate data across scenarios
+        """Aggregate data across function outputs
 
         Args:
             aggregate: The aggregation to perform.
             property: The property to perform aggregation on.
             group_by: The property to group by when doing the aggregation.
             query: The query to search for in the text field.
             search_property: The text field to search in.
-            name: The name to filter on.
-            name_prefix: The prefix of the name to filter on.
-            model_template: The model template to filter on.
-            commands: The command to filter on.
-            source: The source to filter on.
-            source_prefix: The prefix of the source to filter on.
+            process_id: The process id to filter on.
+            process_id_prefix: The prefix of the process id to filter on.
+            min_process_step: The minimum value of the process step to filter on.
+            max_process_step: The maximum value of the process step to filter on.
+            function_name: The function name to filter on.
+            function_name_prefix: The prefix of the function name to filter on.
+            function_call_id: The function call id to filter on.
+            function_call_id_prefix: The prefix of the function call id to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of scenarios to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of function outputs to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
             Aggregation results.
 
         Examples:
 
-            Count scenarios in space `my_space`:
+            Count function outputs in space `my_space`:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> result = client.scenario.aggregate("count", space="my_space")
+                >>> result = client.function_output.aggregate("count", space="my_space")
 
         """
 
-        filter_ = _create_scenario_filter(
+        filter_ = _create_function_output_filter(
             self._view_id,
-            name,
-            name_prefix,
-            model_template,
-            commands,
-            source,
-            source_prefix,
+            process_id,
+            process_id_prefix,
+            min_process_step,
+            max_process_step,
+            function_name,
+            function_name_prefix,
+            function_call_id,
+            function_call_id_prefix,
             external_id_prefix,
             space,
             filter,
         )
         return self._aggregate(
             self._view_id,
             aggregate,
-            _SCENARIO_PROPERTIES_BY_FIELD,
+            _FUNCTIONOUTPUT_PROPERTIES_BY_FIELD,
             property,
             group_by,
             query,
             search_property,
             limit,
             filter_,
         )
 
     def histogram(
         self,
-        property: ScenarioFields,
+        property: FunctionOutputFields,
         interval: float,
         query: str | None = None,
-        search_property: ScenarioTextFields | Sequence[ScenarioTextFields] | None = None,
-        name: str | list[str] | None = None,
-        name_prefix: str | None = None,
-        model_template: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
-        commands: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
-        source: str | list[str] | None = None,
-        source_prefix: str | None = None,
+        search_property: FunctionOutputTextFields | Sequence[FunctionOutputTextFields] | None = None,
+        process_id: str | list[str] | None = None,
+        process_id_prefix: str | None = None,
+        min_process_step: int | None = None,
+        max_process_step: int | None = None,
+        function_name: str | list[str] | None = None,
+        function_name_prefix: str | None = None,
+        function_call_id: str | list[str] | None = None,
+        function_call_id_prefix: str | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> dm.aggregations.HistogramValue:
-        """Produces histograms for scenarios
+        """Produces histograms for function outputs
 
         Args:
             property: The property to use as the value in the histogram.
             interval: The interval to use for the histogram bins.
             query: The query to search for in the text field.
             search_property: The text field to search in.
-            name: The name to filter on.
-            name_prefix: The prefix of the name to filter on.
-            model_template: The model template to filter on.
-            commands: The command to filter on.
-            source: The source to filter on.
-            source_prefix: The prefix of the source to filter on.
+            process_id: The process id to filter on.
+            process_id_prefix: The prefix of the process id to filter on.
+            min_process_step: The minimum value of the process step to filter on.
+            max_process_step: The maximum value of the process step to filter on.
+            function_name: The function name to filter on.
+            function_name_prefix: The prefix of the function name to filter on.
+            function_call_id: The function call id to filter on.
+            function_call_id_prefix: The prefix of the function call id to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of scenarios to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of function outputs to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
             Bucketed histogram results.
 
         """
-        filter_ = _create_scenario_filter(
+        filter_ = _create_function_output_filter(
             self._view_id,
-            name,
-            name_prefix,
-            model_template,
-            commands,
-            source,
-            source_prefix,
+            process_id,
+            process_id_prefix,
+            min_process_step,
+            max_process_step,
+            function_name,
+            function_name_prefix,
+            function_call_id,
+            function_call_id_prefix,
             external_id_prefix,
             space,
             filter,
         )
         return self._histogram(
             self._view_id,
             property,
             interval,
-            _SCENARIO_PROPERTIES_BY_FIELD,
+            _FUNCTIONOUTPUT_PROPERTIES_BY_FIELD,
             query,
             search_property,
             limit,
             filter_,
         )
 
     def list(
         self,
-        name: str | list[str] | None = None,
-        name_prefix: str | None = None,
-        model_template: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
-        commands: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
-        source: str | list[str] | None = None,
-        source_prefix: str | None = None,
+        process_id: str | list[str] | None = None,
+        process_id_prefix: str | None = None,
+        min_process_step: int | None = None,
+        max_process_step: int | None = None,
+        function_name: str | list[str] | None = None,
+        function_name_prefix: str | None = None,
+        function_call_id: str | list[str] | None = None,
+        function_call_id_prefix: str | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
         retrieve_edges: bool = True,
-    ) -> ScenarioList:
-        """List/filter scenarios
+    ) -> FunctionOutputList:
+        """List/filter function outputs
 
         Args:
-            name: The name to filter on.
-            name_prefix: The prefix of the name to filter on.
-            model_template: The model template to filter on.
-            commands: The command to filter on.
-            source: The source to filter on.
-            source_prefix: The prefix of the source to filter on.
+            process_id: The process id to filter on.
+            process_id_prefix: The prefix of the process id to filter on.
+            min_process_step: The minimum value of the process step to filter on.
+            max_process_step: The maximum value of the process step to filter on.
+            function_name: The function name to filter on.
+            function_name_prefix: The prefix of the function name to filter on.
+            function_call_id: The function call id to filter on.
+            function_call_id_prefix: The prefix of the function call id to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of scenarios to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of function outputs to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
-            retrieve_edges: Whether to retrieve `mappings_override` external ids for the scenarios. Defaults to True.
+            retrieve_edges: Whether to retrieve `alerts` external ids for the function outputs. Defaults to True.
 
         Returns:
-            List of requested scenarios
+            List of requested function outputs
 
         Examples:
 
-            List scenarios and limit to 5:
+            List function outputs and limit to 5:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> scenarios = client.scenario.list(limit=5)
+                >>> function_outputs = client.function_output.list(limit=5)
 
         """
-        filter_ = _create_scenario_filter(
+        filter_ = _create_function_output_filter(
             self._view_id,
-            name,
-            name_prefix,
-            model_template,
-            commands,
-            source,
-            source_prefix,
+            process_id,
+            process_id_prefix,
+            min_process_step,
+            max_process_step,
+            function_name,
+            function_name_prefix,
+            function_call_id,
+            function_call_id_prefix,
             external_id_prefix,
             space,
             filter,
         )
 
         return self._list(
             limit=limit,
             filter=filter_,
             retrieve_edges=retrieve_edges,
             edge_api_name_type_direction_view_id_penta=[
                 (
-                    self.mappings_override_edge,
-                    "mappings_override",
-                    dm.DirectRelationReference("sp_powerops_types", "Mapping"),
+                    self.alerts_edge,
+                    "alerts",
+                    dm.DirectRelationReference("sp_powerops_types_temp", "calculationIssue"),
                     "outwards",
-                    dm.ViewId("sp_powerops_models", "Mapping", "1"),
+                    dm.ViewId("sp_powerops_models_temp", "Alert", "1"),
                 ),
             ],
         )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/scenario_mappings_override.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/scenario_mappings_override.py`

 * *Files 1% similar despite different names*

```diff
@@ -39,15 +39,15 @@
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
                 >>> scenario = client.scenario.mappings_override_edge.list("my_scenario", limit=5)
 
         """
         filter_ = _create_edge_filter(
-            dm.DirectRelationReference("sp_powerops_types", "Mapping"),
+            dm.DirectRelationReference("sp_powerops_types_temp", "Mapping"),
             from_scenario,
             from_scenario_space,
             to_mapping,
             to_mapping_space,
             external_id_prefix,
             space,
         )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/scenario_raw.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/scenario_raw.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/scenario_raw_mappings_override.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/scenario_raw_mappings_override.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/shop_partial_bid_calculation_input.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/shop_partial_bid_calculation_input.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/shop_partial_bid_calculation_input_alerts.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/shop_partial_bid_calculation_input_alerts.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/shop_partial_bid_calculation_input_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/water_value_based_partial_bid_matrix_calculation_input_query.py`

 * *Files 27% similar despite different names*

```diff
@@ -3,148 +3,105 @@
 import datetime
 from typing import TYPE_CHECKING
 
 from cognite.client import data_modeling as dm, CogniteClient
 
 from cognite.powerops.client._generated.v1.data_classes import (
     DomainModelCore,
-    ShopPartialBidCalculationInput,
-    PlantShop,
-    MarketConfiguration,
+    WaterValueBasedPartialBidMatrixCalculationInput,
+    BidConfiguration,
+    WaterValueBasedPartialBidConfiguration,
 )
 from ._core import DEFAULT_QUERY_LIMIT, QueryBuilder, QueryStep, QueryAPI, T_DomainModelList, _create_edge_filter
 
-if TYPE_CHECKING:
-    from .shop_result_price_prod_query import SHOPResultPriceProdQueryAPI
 
-
-class ShopPartialBidCalculationInputQueryAPI(QueryAPI[T_DomainModelList]):
+class WaterValueBasedPartialBidMatrixCalculationInputQueryAPI(QueryAPI[T_DomainModelList]):
     def __init__(
         self,
         client: CogniteClient,
         builder: QueryBuilder[T_DomainModelList],
         view_by_read_class: dict[type[DomainModelCore], dm.ViewId],
         filter_: dm.filters.Filter | None = None,
         limit: int = DEFAULT_QUERY_LIMIT,
     ):
         super().__init__(client, builder, view_by_read_class)
 
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("shop_partial_bid_calculation_input"),
+                name=self._builder.next_name("water_value_based_partial_bid_matrix_calculation_input"),
                 expression=dm.query.NodeResultSetExpression(
                     from_=self._builder[-1].name if self._builder else None,
                     filter=filter_,
                 ),
                 select=dm.query.Select(
-                    [dm.query.SourceSelector(self._view_by_read_class[ShopPartialBidCalculationInput], ["*"])]
+                    [
+                        dm.query.SourceSelector(
+                            self._view_by_read_class[WaterValueBasedPartialBidMatrixCalculationInput], ["*"]
+                        )
+                    ]
                 ),
-                result_cls=ShopPartialBidCalculationInput,
+                result_cls=WaterValueBasedPartialBidMatrixCalculationInput,
                 max_retrieve_limit=limit,
             )
         )
 
-    def shop_result_price_prod(
-        self,
-        external_id_prefix: str | None = None,
-        space: str | list[str] | None = None,
-        limit: int | None = DEFAULT_QUERY_LIMIT,
-        retrieve_plant: bool = False,
-        retrieve_market_configuration: bool = False,
-    ) -> SHOPResultPriceProdQueryAPI[T_DomainModelList]:
-        """Query along the shop result price prod edges of the shop partial bid calculation input.
-
-        Args:
-            external_id_prefix: The prefix of the external ID to filter on.
-            space: The space to filter on.
-            limit: Maximum number of shop result price prod edges to return. Defaults to 25. Set to -1, float("inf") or None
-                to return all items.
-            retrieve_plant: Whether to retrieve the plant for each shop partial bid calculation input or not.
-            retrieve_market_configuration: Whether to retrieve the market configuration for each shop partial bid calculation input or not.
-
-        Returns:
-            SHOPResultPriceProdQueryAPI: The query API for the shop result price prod.
-        """
-        from .shop_result_price_prod_query import SHOPResultPriceProdQueryAPI
-
-        from_ = self._builder[-1].name
-
-        edge_filter = _create_edge_filter(
-            dm.DirectRelationReference("sp_powerops_types", "SHOPResultPriceProd"),
-            external_id_prefix=external_id_prefix,
-            space=space,
-        )
-        self._builder.append(
-            QueryStep(
-                name=self._builder.next_name("shop_result_price_prod"),
-                expression=dm.query.EdgeResultSetExpression(
-                    filter=edge_filter,
-                    from_=from_,
-                    direction="outwards",
-                ),
-                select=dm.query.Select(),
-                max_retrieve_limit=limit,
-            )
-        )
-        if retrieve_plant:
-            self._query_append_plant(from_)
-        if retrieve_market_configuration:
-            self._query_append_market_configuration(from_)
-        return SHOPResultPriceProdQueryAPI(self._client, self._builder, self._view_by_read_class, None, limit)
-
     def query(
         self,
-        retrieve_plant: bool = False,
-        retrieve_market_configuration: bool = False,
+        retrieve_bid_configuration: bool = False,
+        retrieve_partial_bid_configuration: bool = False,
     ) -> T_DomainModelList:
         """Execute query and return the result.
 
         Args:
-            retrieve_plant: Whether to retrieve the plant for each shop partial bid calculation input or not.
-            retrieve_market_configuration: Whether to retrieve the market configuration for each shop partial bid calculation input or not.
+            retrieve_bid_configuration: Whether to retrieve the bid configuration for each water value based partial bid matrix calculation input or not.
+            retrieve_partial_bid_configuration: Whether to retrieve the partial bid configuration for each water value based partial bid matrix calculation input or not.
 
         Returns:
             The list of the source nodes of the query.
 
         """
         from_ = self._builder[-1].name
-        if retrieve_plant:
-            self._query_append_plant(from_)
-        if retrieve_market_configuration:
-            self._query_append_market_configuration(from_)
+        if retrieve_bid_configuration:
+            self._query_append_bid_configuration(from_)
+        if retrieve_partial_bid_configuration:
+            self._query_append_partial_bid_configuration(from_)
         return self._query()
 
-    def _query_append_plant(self, from_: str) -> None:
-        view_id = self._view_by_read_class[PlantShop]
+    def _query_append_bid_configuration(self, from_: str) -> None:
+        view_id = self._view_by_read_class[BidConfiguration]
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("plant"),
+                name=self._builder.next_name("bid_configuration"),
                 expression=dm.query.NodeResultSetExpression(
                     filter=dm.filters.HasData(views=[view_id]),
                     from_=from_,
-                    through=self._view_by_read_class[ShopPartialBidCalculationInput].as_property_ref("plant"),
+                    through=self._view_by_read_class[WaterValueBasedPartialBidMatrixCalculationInput].as_property_ref(
+                        "bidConfiguration"
+                    ),
                     direction="outwards",
                 ),
                 select=dm.query.Select([dm.query.SourceSelector(view_id, ["*"])]),
                 max_retrieve_limit=-1,
-                result_cls=PlantShop,
+                result_cls=BidConfiguration,
+                is_single_direct_relation=True,
             ),
         )
 
-    def _query_append_market_configuration(self, from_: str) -> None:
-        view_id = self._view_by_read_class[MarketConfiguration]
+    def _query_append_partial_bid_configuration(self, from_: str) -> None:
+        view_id = self._view_by_read_class[WaterValueBasedPartialBidConfiguration]
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("market_configuration"),
+                name=self._builder.next_name("partial_bid_configuration"),
                 expression=dm.query.NodeResultSetExpression(
                     filter=dm.filters.HasData(views=[view_id]),
                     from_=from_,
-                    through=self._view_by_read_class[ShopPartialBidCalculationInput].as_property_ref(
-                        "marketConfiguration"
+                    through=self._view_by_read_class[WaterValueBasedPartialBidMatrixCalculationInput].as_property_ref(
+                        "partialBidConfiguration"
                     ),
                     direction="outwards",
                 ),
                 select=dm.query.Select([dm.query.SourceSelector(view_id, ["*"])]),
                 max_retrieve_limit=-1,
-                result_cls=MarketConfiguration,
+                result_cls=WaterValueBasedPartialBidConfiguration,
+                is_single_direct_relation=True,
             ),
         )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/shop_partial_bid_calculation_input_shop_result_price_prod.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/shop_partial_bid_calculation_input_shop_result_price_prod.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/shop_partial_bid_calculation_input_shop_results.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/shop_partial_bid_calculation_input_shop_results.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/shop_partial_bid_calculation_output.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/shop_partial_bid_calculation_output.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/shop_partial_bid_calculation_output_alerts.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/shop_partial_bid_calculation_output_alerts.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/shop_result.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/shop_result.py`

 * *Files 5% similar despite different names*

```diff
@@ -46,15 +46,15 @@
             class_type=SHOPResult,
             class_list=SHOPResultList,
             class_write_list=SHOPResultWriteList,
             view_by_read_class=view_by_read_class,
         )
         self._view_id = view_id
         self.alerts_edge = SHOPResultAlertsAPI(client)
-        self.output_timeseries = SHOPResultOutputTimeseriesAPI(client, view_id)
+        self.output_timeseries_edge = SHOPResultOutputTimeseriesAPI(client)
 
     def __call__(
         self,
         case: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_QUERY_LIMIT,
@@ -89,15 +89,15 @@
         shop_result: SHOPResultWrite | Sequence[SHOPResultWrite],
         replace: bool = False,
         write_none: bool = False,
     ) -> ResourcesWriteResult:
         """Add or update (upsert) shop results.
 
         Note: This method iterates through all nodes and timeseries linked to shop_result and creates them including the edges
-        between the nodes. For example, if any of `alerts` are set, then these
+        between the nodes. For example, if any of `alerts` or `output_timeseries` are set, then these
         nodes as well as any nodes linked to them, and all the edges linking these nodes will be created.
 
         Args:
             shop_result: Shop result or sequence of shop results to upsert.
             replace (bool): How do we behave when a property value exists? Do we replace all matching and existing values with the supplied values (true)?
                 Or should we merge in new values for properties together with the existing values (false)? Note: This setting applies for all nodes or edges specified in the ingestion call.
             write_none (bool): This method, will by default, skip properties that are set to None. However, if you want to set properties to None,
@@ -190,17 +190,24 @@
             external_id,
             space,
             retrieve_edges=True,
             edge_api_name_type_direction_view_id_penta=[
                 (
                     self.alerts_edge,
                     "alerts",
-                    dm.DirectRelationReference("sp_powerops_types", "calculationIssue"),
+                    dm.DirectRelationReference("sp_powerops_types_temp", "calculationIssue"),
                     "outwards",
-                    dm.ViewId("sp_powerops_models", "Alert", "1"),
+                    dm.ViewId("sp_powerops_models_temp", "Alert", "1"),
+                ),
+                (
+                    self.output_timeseries_edge,
+                    "output_timeseries",
+                    dm.DirectRelationReference("sp_powerops_types_temp", "SHOPResult.outputTimeseries"),
+                    "outwards",
+                    dm.ViewId("sp_powerops_models_temp", "SHOPTimeSeries", "1"),
                 ),
             ],
         )
 
     @overload
     def aggregate(
         self,
@@ -353,15 +360,15 @@
 
         Args:
             case: The case to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of shop results to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
-            retrieve_edges: Whether to retrieve `alerts` external ids for the shop results. Defaults to True.
+            retrieve_edges: Whether to retrieve `alerts` or `output_timeseries` external ids for the shop results. Defaults to True.
 
         Returns:
             List of requested shop results
 
         Examples:
 
             List shop results and limit to 5:
@@ -383,13 +390,20 @@
             limit=limit,
             filter=filter_,
             retrieve_edges=retrieve_edges,
             edge_api_name_type_direction_view_id_penta=[
                 (
                     self.alerts_edge,
                     "alerts",
-                    dm.DirectRelationReference("sp_powerops_types", "calculationIssue"),
+                    dm.DirectRelationReference("sp_powerops_types_temp", "calculationIssue"),
+                    "outwards",
+                    dm.ViewId("sp_powerops_models_temp", "Alert", "1"),
+                ),
+                (
+                    self.output_timeseries_edge,
+                    "output_timeseries",
+                    dm.DirectRelationReference("sp_powerops_types_temp", "SHOPResult.outputTimeseries"),
                     "outwards",
-                    dm.ViewId("sp_powerops_models", "Alert", "1"),
+                    dm.ViewId("sp_powerops_models_temp", "SHOPTimeSeries", "1"),
                 ),
             ],
         )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/shop_result_alerts.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/shop_result_alerts.py`

 * *Files 4% similar despite different names*

```diff
@@ -39,15 +39,15 @@
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
                 >>> shop_result = client.shop_result.alerts_edge.list("my_shop_result", limit=5)
 
         """
         filter_ = _create_edge_filter(
-            dm.DirectRelationReference("sp_powerops_types", "calculationIssue"),
+            dm.DirectRelationReference("sp_powerops_types_temp", "calculationIssue"),
             from_shop_result,
             from_shop_result_space,
             to_alert,
             to_alert_space,
             external_id_prefix,
             space,
         )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/shop_result_output_timeseries.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/shop_result_price_prod_output_timeseries.py`

 * *Files 3% similar despite different names*

```diff
@@ -5,21 +5,23 @@
 from typing import Literal
 
 import pandas as pd
 from cognite.client import CogniteClient
 from cognite.client import data_modeling as dm
 from cognite.client.data_classes import Datapoints, DatapointsArrayList, DatapointsList, TimeSeriesList
 from cognite.client.data_classes.datapoints import Aggregate
-from cognite.powerops.client._generated.v1.data_classes._shop_result import _create_shop_result_filter
+from cognite.powerops.client._generated.v1.data_classes._shop_result_price_prod import (
+    _create_shop_result_price_prod_filter,
+)
 from ._core import DEFAULT_LIMIT_READ, INSTANCE_QUERY_LIMIT
 
 ColumnNames = Literal["outputTimeseries", "objectiveSequence", "preRun", "postRun", "shopMessages", "cplexLogs"]
 
 
-class SHOPResultOutputTimeseriesQuery:
+class SHOPResultPriceProdOutputTimeseriesQuery:
     def __init__(
         self,
         client: CogniteClient,
         view_id: dm.ViewId,
         timeseries_limit: int = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ):
@@ -36,15 +38,15 @@
         aggregates: Aggregate | list[Aggregate] | None = None,
         granularity: str | None = None,
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         limit: int | None = None,
         include_outside_points: bool = False,
     ) -> DatapointsList:
-        """`Retrieve datapoints for the `shop_result.output_timeseries` timeseries.
+        """`Retrieve datapoints for the `shop_result_price_prod.output_timeseries` timeseries.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -65,15 +67,15 @@
         Examples:
 
             In this example,
             we are using the time-ago format to get raw data for the 'my_output_timeseries' from 2 weeks ago up until now::
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> shop_result_datapoints = client.shop_result.output_timeseries(external_id="my_output_timeseries").retrieve(start="2w-ago")
+                >>> shop_result_price_prod_datapoints = client.shop_result_price_prod.output_timeseries(external_id="my_output_timeseries").retrieve(start="2w-ago")
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra()
         if external_ids:
             return self._client.time_series.data.retrieve(
                 external_id=list(external_ids),
                 start=start,
                 end=end,
@@ -95,15 +97,15 @@
         aggregates: Aggregate | list[Aggregate] | None = None,
         granularity: str | None = None,
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         limit: int | None = None,
         include_outside_points: bool = False,
     ) -> DatapointsArrayList:
-        """`Retrieve numpy arrays for the `shop_result.output_timeseries` timeseries.
+        """`Retrieve numpy arrays for the `shop_result_price_prod.output_timeseries` timeseries.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -124,15 +126,15 @@
         Examples:
 
             In this example,
             we are using the time-ago format to get raw data for the 'my_output_timeseries' from 2 weeks ago up until now::
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> shop_result_datapoints = client.shop_result.output_timeseries(external_id="my_output_timeseries").retrieve_array(start="2w-ago")
+                >>> shop_result_price_prod_datapoints = client.shop_result_price_prod.output_timeseries(external_id="my_output_timeseries").retrieve_array(start="2w-ago")
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra()
         if external_ids:
             return self._client.time_series.data.retrieve_arrays(
                 external_id=list(external_ids),
                 start=start,
                 end=end,
@@ -158,15 +160,15 @@
         limit: int | None = None,
         include_outside_points: bool = False,
         uniform_index: bool = False,
         include_aggregate_name: bool = True,
         include_granularity_name: bool = False,
         column_names: ColumnNames | list[ColumnNames] = "outputTimeseries",
     ) -> pd.DataFrame:
-        """`Retrieve DataFrames for the `shop_result.output_timeseries` timeseries.
+        """`Retrieve DataFrames for the `shop_result_price_prod.output_timeseries` timeseries.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -192,15 +194,15 @@
         Examples:
 
             In this example,
             we are using the time-ago format to get raw data for the 'my_output_timeseries' from 2 weeks ago up until now::
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> shop_result_datapoints = client.shop_result.output_timeseries(external_id="my_output_timeseries").retrieve_dataframe(start="2w-ago")
+                >>> shop_result_price_prod_datapoints = client.shop_result_price_prod.output_timeseries(external_id="my_output_timeseries").retrieve_dataframe(start="2w-ago")
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra(column_names)
         if external_ids:
             df = self._client.time_series.data.retrieve_dataframe(
                 external_id=list(external_ids),
                 start=start,
                 end=end,
@@ -235,15 +237,15 @@
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         uniform_index: bool = False,
         include_aggregate_name: bool = True,
         include_granularity_name: bool = False,
         column_names: ColumnNames | list[ColumnNames] = "outputTimeseries",
     ) -> pd.DataFrame:
-        """Retrieve DataFrames for the `shop_result.output_timeseries` timeseries in Timezone.
+        """Retrieve DataFrames for the `shop_result_price_prod.output_timeseries` timeseries in Timezone.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -270,15 +272,15 @@
 
             In this example,
             get weekly aggregates for the 'my_output_timeseries' for the first month of 2023 in Oslo time:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> from datetime import datetime, timezone
                 >>> client = PowerOpsModelsV1Client()
-                >>> shop_result_datapoints = client.shop_result.output_timeseries(
+                >>> shop_result_price_prod_datapoints = client.shop_result_price_prod.output_timeseries(
                 ...     external_id="my_output_timeseries").retrieve_dataframe_in_timezone(
                 ...         datetime(2023, 1, 1, tzinfo=ZoneInfo("Europe/Oslo")),
                 ...         datetime(2023, 1, 2, tzinfo=ZoneInfo("Europe/Oslo")),
                 ...         aggregates="average",
                 ...         granularity="1week",
                 ...     )
         """
@@ -348,96 +350,102 @@
             column_parts = (col.rsplit("|", maxsplit=splits) for col in df.columns)
             df.columns = [
                 "-".join(external_ids[external_id]) + "|" + "|".join(parts) for external_id, *parts in column_parts
             ]
         return df
 
 
-class SHOPResultOutputTimeseriesAPI:
+class SHOPResultPriceProdOutputTimeseriesAPI:
     def __init__(self, client: CogniteClient, view_id: dm.ViewId):
         self._client = client
         self._view_id = view_id
 
     def __call__(
         self,
         case: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        price_timeseries: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
-    ) -> SHOPResultOutputTimeseriesQuery:
-        """Query timeseries `shop_result.output_timeseries`
+    ) -> SHOPResultPriceProdOutputTimeseriesQuery:
+        """Query timeseries `shop_result_price_prod.output_timeseries`
 
         Args:
             case: The case to filter on.
+            price_timeseries: The price timesery to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of shop results to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of shop result price prods to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
-            A query object that can be used to retrieve datapoins for the shop_result.output_timeseries timeseries
+            A query object that can be used to retrieve datapoins for the shop_result_price_prod.output_timeseries timeseries
             selected in this method.
 
         Examples:
 
-            Retrieve all data for 5 shop_result.output_timeseries timeseries:
+            Retrieve all data for 5 shop_result_price_prod.output_timeseries timeseries:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> shop_results = client.shop_result.output_timeseries(limit=5).retrieve()
+                >>> shop_result_price_prods = client.shop_result_price_prod.output_timeseries(limit=5).retrieve()
 
         """
-        filter_ = _create_shop_result_filter(
+        filter_ = _create_shop_result_price_prod_filter(
             self._view_id,
             case,
+            price_timeseries,
             external_id_prefix,
             space,
             filter,
         )
 
-        return SHOPResultOutputTimeseriesQuery(
+        return SHOPResultPriceProdOutputTimeseriesQuery(
             client=self._client,
             view_id=self._view_id,
             timeseries_limit=limit,
             filter=filter_,
         )
 
     def list(
         self,
         case: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        price_timeseries: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> TimeSeriesList:
-        """List timeseries `shop_result.output_timeseries`
+        """List timeseries `shop_result_price_prod.output_timeseries`
 
         Args:
             case: The case to filter on.
+            price_timeseries: The price timesery to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of shop results to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of shop result price prods to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
-            List of Timeseries shop_result.output_timeseries.
+            List of Timeseries shop_result_price_prod.output_timeseries.
 
         Examples:
 
-            List shop_result.output_timeseries and limit to 5:
+            List shop_result_price_prod.output_timeseries and limit to 5:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> shop_results = client.shop_result.output_timeseries.list(limit=5)
+                >>> shop_result_price_prods = client.shop_result_price_prod.output_timeseries.list(limit=5)
 
         """
-        filter_ = _create_shop_result_filter(
+        filter_ = _create_shop_result_price_prod_filter(
             self._view_id,
             case,
+            price_timeseries,
             external_id_prefix,
             space,
             filter,
         )
         external_ids = _retrieve_timeseries_external_ids_with_extra_output_timesery(
             self._client, self._view_id, filter_, limit
         )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/shop_result_price.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/shop_result_price.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/shop_result_price_prod.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/shop_result_price_prod.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/shop_result_price_prod_alerts.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/shop_result_price_prod_alerts.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/shop_result_price_prod_output_timeseries.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/price_production_production.py`

 * *Files 8% similar despite different names*

```diff
@@ -5,23 +5,21 @@
 from typing import Literal
 
 import pandas as pd
 from cognite.client import CogniteClient
 from cognite.client import data_modeling as dm
 from cognite.client.data_classes import Datapoints, DatapointsArrayList, DatapointsList, TimeSeriesList
 from cognite.client.data_classes.datapoints import Aggregate
-from cognite.powerops.client._generated.v1.data_classes._shop_result_price_prod import (
-    _create_shop_result_price_prod_filter,
-)
+from cognite.powerops.client._generated.v1.data_classes._price_production import _create_price_production_filter
 from ._core import DEFAULT_LIMIT_READ, INSTANCE_QUERY_LIMIT
 
-ColumnNames = Literal["outputTimeseries", "objectiveSequence", "preRun", "postRun", "shopMessages", "cplexLogs"]
+ColumnNames = Literal["price", "production"]
 
 
-class SHOPResultPriceProdOutputTimeseriesQuery:
+class PriceProductionProductionQuery:
     def __init__(
         self,
         client: CogniteClient,
         view_id: dm.ViewId,
         timeseries_limit: int = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ):
@@ -38,15 +36,15 @@
         aggregates: Aggregate | list[Aggregate] | None = None,
         granularity: str | None = None,
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         limit: int | None = None,
         include_outside_points: bool = False,
     ) -> DatapointsList:
-        """`Retrieve datapoints for the `shop_result_price_prod.output_timeseries` timeseries.
+        """`Retrieve datapoints for the `price_production.production` timeseries.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -63,19 +61,19 @@
 
         Returns:
             A ``DatapointsList`` with the requested datapoints.
 
         Examples:
 
             In this example,
-            we are using the time-ago format to get raw data for the 'my_output_timeseries' from 2 weeks ago up until now::
+            we are using the time-ago format to get raw data for the 'my_production' from 2 weeks ago up until now::
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> shop_result_price_prod_datapoints = client.shop_result_price_prod.output_timeseries(external_id="my_output_timeseries").retrieve(start="2w-ago")
+                >>> price_production_datapoints = client.price_production.production(external_id="my_production").retrieve(start="2w-ago")
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra()
         if external_ids:
             return self._client.time_series.data.retrieve(
                 external_id=list(external_ids),
                 start=start,
                 end=end,
@@ -97,15 +95,15 @@
         aggregates: Aggregate | list[Aggregate] | None = None,
         granularity: str | None = None,
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         limit: int | None = None,
         include_outside_points: bool = False,
     ) -> DatapointsArrayList:
-        """`Retrieve numpy arrays for the `shop_result_price_prod.output_timeseries` timeseries.
+        """`Retrieve numpy arrays for the `price_production.production` timeseries.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -122,19 +120,19 @@
 
         Returns:
             A ``DatapointsArrayList`` with the requested datapoints.
 
         Examples:
 
             In this example,
-            we are using the time-ago format to get raw data for the 'my_output_timeseries' from 2 weeks ago up until now::
+            we are using the time-ago format to get raw data for the 'my_production' from 2 weeks ago up until now::
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> shop_result_price_prod_datapoints = client.shop_result_price_prod.output_timeseries(external_id="my_output_timeseries").retrieve_array(start="2w-ago")
+                >>> price_production_datapoints = client.price_production.production(external_id="my_production").retrieve_array(start="2w-ago")
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra()
         if external_ids:
             return self._client.time_series.data.retrieve_arrays(
                 external_id=list(external_ids),
                 start=start,
                 end=end,
@@ -158,17 +156,17 @@
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         limit: int | None = None,
         include_outside_points: bool = False,
         uniform_index: bool = False,
         include_aggregate_name: bool = True,
         include_granularity_name: bool = False,
-        column_names: ColumnNames | list[ColumnNames] = "outputTimeseries",
+        column_names: ColumnNames | list[ColumnNames] = "production",
     ) -> pd.DataFrame:
-        """`Retrieve DataFrames for the `shop_result_price_prod.output_timeseries` timeseries.
+        """`Retrieve DataFrames for the `price_production.production` timeseries.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -181,28 +179,28 @@
             target_unit: The unit_external_id of the data points returned. If the time series does not have an unit_external_id that can be converted to the target_unit, an error will be returned. Cannot be used with target_unit_system.
             target_unit_system: The unit system of the data points returned. Cannot be used with target_unit.
             limit: Maximum number of datapoints to return for each time series. Default: None (no limit)
             include_outside_points: Whether to include outside points. Not allowed when fetching aggregates. Default: False
             uniform_index: If only querying aggregates AND a single granularity is used, AND no limit is used, specifying `uniform_index=True` will return a dataframe with an equidistant datetime index from the earliest `start` to the latest `end` (missing values will be NaNs). If these requirements are not met, a ValueError is raised. Default: False
             include_aggregate_name: Include 'aggregate' in the column name, e.g. `my-ts|average`. Ignored for raw time series. Default: True
             include_granularity_name: Include 'granularity' in the column name, e.g. `my-ts|12h`. Added after 'aggregate' when present. Ignored for raw time series. Default: False
-            column_names: Which property to use for column names. Defauts to outputTimeseries
+            column_names: Which property to use for column names. Defauts to production
 
 
         Returns:
             A ``DataFrame`` with the requested datapoints.
 
         Examples:
 
             In this example,
-            we are using the time-ago format to get raw data for the 'my_output_timeseries' from 2 weeks ago up until now::
+            we are using the time-ago format to get raw data for the 'my_production' from 2 weeks ago up until now::
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> shop_result_price_prod_datapoints = client.shop_result_price_prod.output_timeseries(external_id="my_output_timeseries").retrieve_dataframe(start="2w-ago")
+                >>> price_production_datapoints = client.price_production.production(external_id="my_production").retrieve_dataframe(start="2w-ago")
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra(column_names)
         if external_ids:
             df = self._client.time_series.data.retrieve_dataframe(
                 external_id=list(external_ids),
                 start=start,
                 end=end,
@@ -235,17 +233,17 @@
         aggregates: Aggregate | Sequence[Aggregate] | None = None,
         granularity: str | None = None,
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         uniform_index: bool = False,
         include_aggregate_name: bool = True,
         include_granularity_name: bool = False,
-        column_names: ColumnNames | list[ColumnNames] = "outputTimeseries",
+        column_names: ColumnNames | list[ColumnNames] = "production",
     ) -> pd.DataFrame:
-        """Retrieve DataFrames for the `shop_result_price_prod.output_timeseries` timeseries in Timezone.
+        """Retrieve DataFrames for the `price_production.production` timeseries in Timezone.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -258,30 +256,30 @@
             target_unit: The unit_external_id of the data points returned. If the time series does not have an unit_external_id that can be converted to the target_unit, an error will be returned. Cannot be used with target_unit_system.
             target_unit_system: The unit system of the data points returned. Cannot be used with target_unit.
             limit: Maximum number of datapoints to return for each time series. Default: None (no limit)
             include_outside_points: Whether to include outside points. Not allowed when fetching aggregates. Default: False
             uniform_index: If only querying aggregates AND a single granularity is used, AND no limit is used, specifying `uniform_index=True` will return a dataframe with an equidistant datetime index from the earliest `start` to the latest `end` (missing values will be NaNs). If these requirements are not met, a ValueError is raised. Default: False
             include_aggregate_name: Include 'aggregate' in the column name, e.g. `my-ts|average`. Ignored for raw time series. Default: True
             include_granularity_name: Include 'granularity' in the column name, e.g. `my-ts|12h`. Added after 'aggregate' when present. Ignored for raw time series. Default: False
-            column_names: Which property to use for column names. Defauts to outputTimeseries
+            column_names: Which property to use for column names. Defauts to production
 
 
         Returns:
             A ``DataFrame`` with the requested datapoints.
 
         Examples:
 
             In this example,
-            get weekly aggregates for the 'my_output_timeseries' for the first month of 2023 in Oslo time:
+            get weekly aggregates for the 'my_production' for the first month of 2023 in Oslo time:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> from datetime import datetime, timezone
                 >>> client = PowerOpsModelsV1Client()
-                >>> shop_result_price_prod_datapoints = client.shop_result_price_prod.output_timeseries(
-                ...     external_id="my_output_timeseries").retrieve_dataframe_in_timezone(
+                >>> price_production_datapoints = client.price_production.production(
+                ...     external_id="my_production").retrieve_dataframe_in_timezone(
                 ...         datetime(2023, 1, 1, tzinfo=ZoneInfo("Europe/Oslo")),
                 ...         datetime(2023, 1, 2, tzinfo=ZoneInfo("Europe/Oslo")),
                 ...         aggregates="average",
                 ...         granularity="1week",
                 ...     )
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra(column_names)
@@ -319,17 +317,17 @@
                 external_id=list(external_ids),
                 before=before,
             )
         else:
             return None
 
     def _retrieve_timeseries_external_ids_with_extra(
-        self, extra_properties: ColumnNames | list[ColumnNames] = "outputTimeseries"
+        self, extra_properties: ColumnNames | list[ColumnNames] = "production"
     ) -> dict[str, list[str]]:
-        return _retrieve_timeseries_external_ids_with_extra_output_timesery(
+        return _retrieve_timeseries_external_ids_with_extra_production(
             self._client,
             self._view_id,
             self._filter,
             self._timeseries_limit,
             extra_properties,
         )
 
@@ -337,152 +335,146 @@
     def _rename_columns(
         external_ids: dict[str, list[str]],
         df: pd.DataFrame,
         column_names: ColumnNames | list[ColumnNames],
         include_aggregate_name: bool,
         include_granularity_name: bool,
     ) -> pd.DataFrame:
-        if isinstance(column_names, str) and column_names == "outputTimeseries":
+        if isinstance(column_names, str) and column_names == "production":
             return df
         splits = sum(included for included in [include_aggregate_name, include_granularity_name])
         if splits == 0:
             df.columns = ["-".join(external_ids[external_id]) for external_id in df.columns]
         else:
             column_parts = (col.rsplit("|", maxsplit=splits) for col in df.columns)
             df.columns = [
                 "-".join(external_ids[external_id]) + "|" + "|".join(parts) for external_id, *parts in column_parts
             ]
         return df
 
 
-class SHOPResultPriceProdOutputTimeseriesAPI:
+class PriceProductionProductionAPI:
     def __init__(self, client: CogniteClient, view_id: dm.ViewId):
         self._client = client
         self._view_id = view_id
 
     def __call__(
         self,
-        case: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
-        price_timeseries: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        shop_result: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
-    ) -> SHOPResultPriceProdOutputTimeseriesQuery:
-        """Query timeseries `shop_result_price_prod.output_timeseries`
+    ) -> PriceProductionProductionQuery:
+        """Query timeseries `price_production.production`
 
         Args:
-            case: The case to filter on.
-            price_timeseries: The price timesery to filter on.
+            shop_result: The shop result to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of shop result price prods to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of price productions to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
-            A query object that can be used to retrieve datapoins for the shop_result_price_prod.output_timeseries timeseries
+            A query object that can be used to retrieve datapoins for the price_production.production timeseries
             selected in this method.
 
         Examples:
 
-            Retrieve all data for 5 shop_result_price_prod.output_timeseries timeseries:
+            Retrieve all data for 5 price_production.production timeseries:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> shop_result_price_prods = client.shop_result_price_prod.output_timeseries(limit=5).retrieve()
+                >>> price_productions = client.price_production.production(limit=5).retrieve()
 
         """
-        filter_ = _create_shop_result_price_prod_filter(
+        filter_ = _create_price_production_filter(
             self._view_id,
-            case,
-            price_timeseries,
+            shop_result,
             external_id_prefix,
             space,
             filter,
         )
 
-        return SHOPResultPriceProdOutputTimeseriesQuery(
+        return PriceProductionProductionQuery(
             client=self._client,
             view_id=self._view_id,
             timeseries_limit=limit,
             filter=filter_,
         )
 
     def list(
         self,
-        case: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
-        price_timeseries: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        shop_result: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> TimeSeriesList:
-        """List timeseries `shop_result_price_prod.output_timeseries`
+        """List timeseries `price_production.production`
 
         Args:
-            case: The case to filter on.
-            price_timeseries: The price timesery to filter on.
+            shop_result: The shop result to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of shop result price prods to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of price productions to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
-            List of Timeseries shop_result_price_prod.output_timeseries.
+            List of Timeseries price_production.production.
 
         Examples:
 
-            List shop_result_price_prod.output_timeseries and limit to 5:
+            List price_production.production and limit to 5:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> shop_result_price_prods = client.shop_result_price_prod.output_timeseries.list(limit=5)
+                >>> price_productions = client.price_production.production.list(limit=5)
 
         """
-        filter_ = _create_shop_result_price_prod_filter(
+        filter_ = _create_price_production_filter(
             self._view_id,
-            case,
-            price_timeseries,
+            shop_result,
             external_id_prefix,
             space,
             filter,
         )
-        external_ids = _retrieve_timeseries_external_ids_with_extra_output_timesery(
+        external_ids = _retrieve_timeseries_external_ids_with_extra_production(
             self._client, self._view_id, filter_, limit
         )
         if external_ids:
             return self._client.time_series.retrieve_multiple(external_ids=list(external_ids))
         else:
             return TimeSeriesList([])
 
 
-def _retrieve_timeseries_external_ids_with_extra_output_timesery(
+def _retrieve_timeseries_external_ids_with_extra_production(
     client: CogniteClient,
     view_id: dm.ViewId,
     filter_: dm.Filter | None,
     limit: int,
-    extra_properties: ColumnNames | list[ColumnNames] = "outputTimeseries",
+    extra_properties: ColumnNames | list[ColumnNames] = "production",
 ) -> dict[str, list[str]]:
     limit = float("inf") if limit is None or limit == -1 else limit
-    properties = ["outputTimeseries"]
-    if extra_properties == "outputTimeseries":
+    properties = ["production"]
+    if extra_properties == "production":
         ...
-    elif isinstance(extra_properties, str) and extra_properties != "outputTimeseries":
+    elif isinstance(extra_properties, str) and extra_properties != "production":
         properties.append(extra_properties)
     elif isinstance(extra_properties, list):
-        properties.extend([prop for prop in extra_properties if prop != "outputTimeseries"])
+        properties.extend([prop for prop in extra_properties if prop != "production"])
     else:
         raise ValueError(f"Invalid value for extra_properties: {extra_properties}")
 
     if isinstance(extra_properties, str):
         extra_list = [extra_properties]
     else:
         extra_list = extra_properties
     has_data = dm.filters.HasData(views=[view_id])
-    has_property = dm.filters.Exists(property=view_id.as_property_ref("outputTimeseries"))
+    has_property = dm.filters.Exists(property=view_id.as_property_ref("production"))
     filter_ = dm.filters.And(filter_, has_data, has_property) if filter_ else dm.filters.And(has_data, has_property)
 
     cursor = None
     external_ids: dict[str, list[str]] = {}
     total_retrieved = 0
     while True:
         query_limit = max(min(INSTANCE_QUERY_LIMIT, limit - total_retrieved), 0)
@@ -496,17 +488,15 @@
                     [dm.query.SourceSelector(view_id, properties)],
                 )
             },
             cursors={"nodes": cursor},
         )
         result = client.data_modeling.instances.query(query)
         batch_external_ids = {
-            node.properties[view_id]["outputTimeseries"]: [
-                node.properties[view_id].get(prop, "") for prop in extra_list
-            ]
+            node.properties[view_id]["production"]: [node.properties[view_id].get(prop, "") for prop in extra_list]
             for node in result.data["nodes"].data
         }
         total_retrieved += len(batch_external_ids)
         external_ids.update(batch_external_ids)
         cursor = result.cursors["nodes"]
         if total_retrieved >= limit or cursor is None:
             break
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/shop_result_price_prod_production_timeseries.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/shop_result_price_prod_production_timeseries.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/shop_result_price_prod_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/shop_result_price_prod_query.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/shop_result_production.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/shop_result_production.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/shop_result_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/water_partial_bid_calculation_input_query.py`

 * *Files 24% similar despite different names*

```diff
@@ -3,118 +3,75 @@
 import datetime
 from typing import TYPE_CHECKING
 
 from cognite.client import data_modeling as dm, CogniteClient
 
 from cognite.powerops.client._generated.v1.data_classes import (
     DomainModelCore,
-    SHOPResult,
-    Case,
+    WaterPartialBidCalculationInput,
+    BidCalculationTask,
 )
 from ._core import DEFAULT_QUERY_LIMIT, QueryBuilder, QueryStep, QueryAPI, T_DomainModelList, _create_edge_filter
 
-if TYPE_CHECKING:
-    from .alert_query import AlertQueryAPI
 
-
-class SHOPResultQueryAPI(QueryAPI[T_DomainModelList]):
+class WaterPartialBidCalculationInputQueryAPI(QueryAPI[T_DomainModelList]):
     def __init__(
         self,
         client: CogniteClient,
         builder: QueryBuilder[T_DomainModelList],
         view_by_read_class: dict[type[DomainModelCore], dm.ViewId],
         filter_: dm.filters.Filter | None = None,
         limit: int = DEFAULT_QUERY_LIMIT,
     ):
         super().__init__(client, builder, view_by_read_class)
 
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("shop_result"),
+                name=self._builder.next_name("water_partial_bid_calculation_input"),
                 expression=dm.query.NodeResultSetExpression(
                     from_=self._builder[-1].name if self._builder else None,
                     filter=filter_,
                 ),
-                select=dm.query.Select([dm.query.SourceSelector(self._view_by_read_class[SHOPResult], ["*"])]),
-                result_cls=SHOPResult,
-                max_retrieve_limit=limit,
-            )
-        )
-
-    def alerts(
-        self,
-        external_id_prefix: str | None = None,
-        space: str | list[str] | None = None,
-        limit: int | None = DEFAULT_QUERY_LIMIT,
-        retrieve_case: bool = False,
-    ) -> AlertQueryAPI[T_DomainModelList]:
-        """Query along the alert edges of the shop result.
-
-        Args:
-            external_id_prefix: The prefix of the external ID to filter on.
-            space: The space to filter on.
-            limit: Maximum number of alert edges to return. Defaults to 25. Set to -1, float("inf") or None
-                to return all items.
-            retrieve_case: Whether to retrieve the case for each shop result or not.
-
-        Returns:
-            AlertQueryAPI: The query API for the alert.
-        """
-        from .alert_query import AlertQueryAPI
-
-        from_ = self._builder[-1].name
-
-        edge_filter = _create_edge_filter(
-            dm.DirectRelationReference("sp_powerops_types", "calculationIssue"),
-            external_id_prefix=external_id_prefix,
-            space=space,
-        )
-        self._builder.append(
-            QueryStep(
-                name=self._builder.next_name("alerts"),
-                expression=dm.query.EdgeResultSetExpression(
-                    filter=edge_filter,
-                    from_=from_,
-                    direction="outwards",
+                select=dm.query.Select(
+                    [dm.query.SourceSelector(self._view_by_read_class[WaterPartialBidCalculationInput], ["*"])]
                 ),
-                select=dm.query.Select(),
+                result_cls=WaterPartialBidCalculationInput,
                 max_retrieve_limit=limit,
             )
         )
-        if retrieve_case:
-            self._query_append_case(from_)
-        return AlertQueryAPI(self._client, self._builder, self._view_by_read_class, None, limit)
 
     def query(
         self,
-        retrieve_case: bool = False,
+        retrieve_calculation_task: bool = False,
     ) -> T_DomainModelList:
         """Execute query and return the result.
 
         Args:
-            retrieve_case: Whether to retrieve the case for each shop result or not.
+            retrieve_calculation_task: Whether to retrieve the calculation task for each water partial bid calculation input or not.
 
         Returns:
             The list of the source nodes of the query.
 
         """
         from_ = self._builder[-1].name
-        if retrieve_case:
-            self._query_append_case(from_)
+        if retrieve_calculation_task:
+            self._query_append_calculation_task(from_)
         return self._query()
 
-    def _query_append_case(self, from_: str) -> None:
-        view_id = self._view_by_read_class[Case]
+    def _query_append_calculation_task(self, from_: str) -> None:
+        view_id = self._view_by_read_class[BidCalculationTask]
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("case"),
+                name=self._builder.next_name("calculation_task"),
                 expression=dm.query.NodeResultSetExpression(
                     filter=dm.filters.HasData(views=[view_id]),
                     from_=from_,
-                    through=self._view_by_read_class[SHOPResult].as_property_ref("case"),
+                    through=self._view_by_read_class[WaterPartialBidCalculationInput].as_property_ref(
+                        "calculationTask"
+                    ),
                     direction="outwards",
                 ),
                 select=dm.query.Select([dm.query.SourceSelector(view_id, ["*"])]),
                 max_retrieve_limit=-1,
-                result_cls=Case,
+                result_cls=BidCalculationTask,
             ),
         )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/shop_time_series.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/shop_time_series.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/shop_time_series_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/shop_time_series_query.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/shop_time_series_timeseries.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/price_area_afrr_own_capacity_allocation_up.py`

 * *Files 10% similar despite different names*

```diff
@@ -5,21 +5,36 @@
 from typing import Literal
 
 import pandas as pd
 from cognite.client import CogniteClient
 from cognite.client import data_modeling as dm
 from cognite.client.data_classes import Datapoints, DatapointsArrayList, DatapointsList, TimeSeriesList
 from cognite.client.data_classes.datapoints import Aggregate
-from cognite.powerops.client._generated.v1.data_classes._shop_time_series import _create_shop_time_series_filter
+from cognite.powerops.client._generated.v1.data_classes._price_area_afrr import _create_price_area_afrr_filter
 from ._core import DEFAULT_LIMIT_READ, INSTANCE_QUERY_LIMIT
 
-ColumnNames = Literal["objectType", "objectName", "attributeName", "timeseries"]
+ColumnNames = Literal[
+    "name",
+    "displayName",
+    "ordering",
+    "assetType",
+    "timezone",
+    "capacityPriceUp",
+    "capacityPriceDown",
+    "activationPriceUp",
+    "activationPriceDown",
+    "relativeActivation",
+    "totalCapacityAllocationUp",
+    "totalCapacityAllocationDown",
+    "ownCapacityAllocationUp",
+    "ownCapacityAllocationDown",
+]
 
 
-class SHOPTimeSeriesTimeseriesQuery:
+class PriceAreaAFRROwnCapacityAllocationUpQuery:
     def __init__(
         self,
         client: CogniteClient,
         view_id: dm.ViewId,
         timeseries_limit: int = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ):
@@ -36,15 +51,15 @@
         aggregates: Aggregate | list[Aggregate] | None = None,
         granularity: str | None = None,
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         limit: int | None = None,
         include_outside_points: bool = False,
     ) -> DatapointsList:
-        """`Retrieve datapoints for the `shop_time_series.timeseries` timeseries.
+        """`Retrieve datapoints for the `price_area_afrr.own_capacity_allocation_up` timeseries.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -61,19 +76,19 @@
 
         Returns:
             A ``DatapointsList`` with the requested datapoints.
 
         Examples:
 
             In this example,
-            we are using the time-ago format to get raw data for the 'my_timeseries' from 2 weeks ago up until now::
+            we are using the time-ago format to get raw data for the 'my_own_capacity_allocation_up' from 2 weeks ago up until now::
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> shop_time_series_datapoints = client.shop_time_series.timeseries(external_id="my_timeseries").retrieve(start="2w-ago")
+                >>> price_area_afrr_datapoints = client.price_area_afrr.own_capacity_allocation_up(external_id="my_own_capacity_allocation_up").retrieve(start="2w-ago")
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra()
         if external_ids:
             return self._client.time_series.data.retrieve(
                 external_id=list(external_ids),
                 start=start,
                 end=end,
@@ -95,15 +110,15 @@
         aggregates: Aggregate | list[Aggregate] | None = None,
         granularity: str | None = None,
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         limit: int | None = None,
         include_outside_points: bool = False,
     ) -> DatapointsArrayList:
-        """`Retrieve numpy arrays for the `shop_time_series.timeseries` timeseries.
+        """`Retrieve numpy arrays for the `price_area_afrr.own_capacity_allocation_up` timeseries.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -120,19 +135,19 @@
 
         Returns:
             A ``DatapointsArrayList`` with the requested datapoints.
 
         Examples:
 
             In this example,
-            we are using the time-ago format to get raw data for the 'my_timeseries' from 2 weeks ago up until now::
+            we are using the time-ago format to get raw data for the 'my_own_capacity_allocation_up' from 2 weeks ago up until now::
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> shop_time_series_datapoints = client.shop_time_series.timeseries(external_id="my_timeseries").retrieve_array(start="2w-ago")
+                >>> price_area_afrr_datapoints = client.price_area_afrr.own_capacity_allocation_up(external_id="my_own_capacity_allocation_up").retrieve_array(start="2w-ago")
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra()
         if external_ids:
             return self._client.time_series.data.retrieve_arrays(
                 external_id=list(external_ids),
                 start=start,
                 end=end,
@@ -156,17 +171,17 @@
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         limit: int | None = None,
         include_outside_points: bool = False,
         uniform_index: bool = False,
         include_aggregate_name: bool = True,
         include_granularity_name: bool = False,
-        column_names: ColumnNames | list[ColumnNames] = "timeseries",
+        column_names: ColumnNames | list[ColumnNames] = "ownCapacityAllocationUp",
     ) -> pd.DataFrame:
-        """`Retrieve DataFrames for the `shop_time_series.timeseries` timeseries.
+        """`Retrieve DataFrames for the `price_area_afrr.own_capacity_allocation_up` timeseries.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -179,28 +194,28 @@
             target_unit: The unit_external_id of the data points returned. If the time series does not have an unit_external_id that can be converted to the target_unit, an error will be returned. Cannot be used with target_unit_system.
             target_unit_system: The unit system of the data points returned. Cannot be used with target_unit.
             limit: Maximum number of datapoints to return for each time series. Default: None (no limit)
             include_outside_points: Whether to include outside points. Not allowed when fetching aggregates. Default: False
             uniform_index: If only querying aggregates AND a single granularity is used, AND no limit is used, specifying `uniform_index=True` will return a dataframe with an equidistant datetime index from the earliest `start` to the latest `end` (missing values will be NaNs). If these requirements are not met, a ValueError is raised. Default: False
             include_aggregate_name: Include 'aggregate' in the column name, e.g. `my-ts|average`. Ignored for raw time series. Default: True
             include_granularity_name: Include 'granularity' in the column name, e.g. `my-ts|12h`. Added after 'aggregate' when present. Ignored for raw time series. Default: False
-            column_names: Which property to use for column names. Defauts to timeseries
+            column_names: Which property to use for column names. Defauts to ownCapacityAllocationUp
 
 
         Returns:
             A ``DataFrame`` with the requested datapoints.
 
         Examples:
 
             In this example,
-            we are using the time-ago format to get raw data for the 'my_timeseries' from 2 weeks ago up until now::
+            we are using the time-ago format to get raw data for the 'my_own_capacity_allocation_up' from 2 weeks ago up until now::
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> shop_time_series_datapoints = client.shop_time_series.timeseries(external_id="my_timeseries").retrieve_dataframe(start="2w-ago")
+                >>> price_area_afrr_datapoints = client.price_area_afrr.own_capacity_allocation_up(external_id="my_own_capacity_allocation_up").retrieve_dataframe(start="2w-ago")
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra(column_names)
         if external_ids:
             df = self._client.time_series.data.retrieve_dataframe(
                 external_id=list(external_ids),
                 start=start,
                 end=end,
@@ -233,17 +248,17 @@
         aggregates: Aggregate | Sequence[Aggregate] | None = None,
         granularity: str | None = None,
         target_unit: str | None = None,
         target_unit_system: str | None = None,
         uniform_index: bool = False,
         include_aggregate_name: bool = True,
         include_granularity_name: bool = False,
-        column_names: ColumnNames | list[ColumnNames] = "timeseries",
+        column_names: ColumnNames | list[ColumnNames] = "ownCapacityAllocationUp",
     ) -> pd.DataFrame:
-        """Retrieve DataFrames for the `shop_time_series.timeseries` timeseries in Timezone.
+        """Retrieve DataFrames for the `price_area_afrr.own_capacity_allocation_up` timeseries in Timezone.
 
         **Performance guide**:
             In order to retrieve millions of datapoints as efficiently as possible, here are a few guidelines:
 
             1. For the best speed, and significantly lower memory usage, consider using ``retrieve_arrays(...)`` which uses ``numpy.ndarrays`` for data storage.
             2. Only unlimited queries with (``limit=None``) are fetched in parallel, so specifying a large finite ``limit`` like 1 million, comes with severe performance penalty as data is fetched serially.
             3. Try to avoid specifying `start` and `end` to be very far from the actual data: If you had data from 2000 to 2015, don't set start=0 (1970).
@@ -256,30 +271,30 @@
             target_unit: The unit_external_id of the data points returned. If the time series does not have an unit_external_id that can be converted to the target_unit, an error will be returned. Cannot be used with target_unit_system.
             target_unit_system: The unit system of the data points returned. Cannot be used with target_unit.
             limit: Maximum number of datapoints to return for each time series. Default: None (no limit)
             include_outside_points: Whether to include outside points. Not allowed when fetching aggregates. Default: False
             uniform_index: If only querying aggregates AND a single granularity is used, AND no limit is used, specifying `uniform_index=True` will return a dataframe with an equidistant datetime index from the earliest `start` to the latest `end` (missing values will be NaNs). If these requirements are not met, a ValueError is raised. Default: False
             include_aggregate_name: Include 'aggregate' in the column name, e.g. `my-ts|average`. Ignored for raw time series. Default: True
             include_granularity_name: Include 'granularity' in the column name, e.g. `my-ts|12h`. Added after 'aggregate' when present. Ignored for raw time series. Default: False
-            column_names: Which property to use for column names. Defauts to timeseries
+            column_names: Which property to use for column names. Defauts to ownCapacityAllocationUp
 
 
         Returns:
             A ``DataFrame`` with the requested datapoints.
 
         Examples:
 
             In this example,
-            get weekly aggregates for the 'my_timeseries' for the first month of 2023 in Oslo time:
+            get weekly aggregates for the 'my_own_capacity_allocation_up' for the first month of 2023 in Oslo time:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> from datetime import datetime, timezone
                 >>> client = PowerOpsModelsV1Client()
-                >>> shop_time_series_datapoints = client.shop_time_series.timeseries(
-                ...     external_id="my_timeseries").retrieve_dataframe_in_timezone(
+                >>> price_area_afrr_datapoints = client.price_area_afrr.own_capacity_allocation_up(
+                ...     external_id="my_own_capacity_allocation_up").retrieve_dataframe_in_timezone(
                 ...         datetime(2023, 1, 1, tzinfo=ZoneInfo("Europe/Oslo")),
                 ...         datetime(2023, 1, 2, tzinfo=ZoneInfo("Europe/Oslo")),
                 ...         aggregates="average",
                 ...         granularity="1week",
                 ...     )
         """
         external_ids = self._retrieve_timeseries_external_ids_with_extra(column_names)
@@ -317,17 +332,17 @@
                 external_id=list(external_ids),
                 before=before,
             )
         else:
             return None
 
     def _retrieve_timeseries_external_ids_with_extra(
-        self, extra_properties: ColumnNames | list[ColumnNames] = "timeseries"
+        self, extra_properties: ColumnNames | list[ColumnNames] = "ownCapacityAllocationUp"
     ) -> dict[str, list[str]]:
-        return _retrieve_timeseries_external_ids_with_extra_timesery(
+        return _retrieve_timeseries_external_ids_with_extra_own_capacity_allocation_up(
             self._client,
             self._view_id,
             self._filter,
             self._timeseries_limit,
             extra_properties,
         )
 
@@ -335,176 +350,200 @@
     def _rename_columns(
         external_ids: dict[str, list[str]],
         df: pd.DataFrame,
         column_names: ColumnNames | list[ColumnNames],
         include_aggregate_name: bool,
         include_granularity_name: bool,
     ) -> pd.DataFrame:
-        if isinstance(column_names, str) and column_names == "timeseries":
+        if isinstance(column_names, str) and column_names == "ownCapacityAllocationUp":
             return df
         splits = sum(included for included in [include_aggregate_name, include_granularity_name])
         if splits == 0:
             df.columns = ["-".join(external_ids[external_id]) for external_id in df.columns]
         else:
             column_parts = (col.rsplit("|", maxsplit=splits) for col in df.columns)
             df.columns = [
                 "-".join(external_ids[external_id]) + "|" + "|".join(parts) for external_id, *parts in column_parts
             ]
         return df
 
 
-class SHOPTimeSeriesTimeseriesAPI:
+class PriceAreaAFRROwnCapacityAllocationUpAPI:
     def __init__(self, client: CogniteClient, view_id: dm.ViewId):
         self._client = client
         self._view_id = view_id
 
     def __call__(
         self,
-        object_type: str | list[str] | None = None,
-        object_type_prefix: str | None = None,
-        object_name: str | list[str] | None = None,
-        object_name_prefix: str | None = None,
-        attribute_name: str | list[str] | None = None,
-        attribute_name_prefix: str | None = None,
+        name: str | list[str] | None = None,
+        name_prefix: str | None = None,
+        display_name: str | list[str] | None = None,
+        display_name_prefix: str | None = None,
+        min_ordering: int | None = None,
+        max_ordering: int | None = None,
+        asset_type: str | list[str] | None = None,
+        asset_type_prefix: str | None = None,
+        timezone: str | list[str] | None = None,
+        timezone_prefix: str | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
-    ) -> SHOPTimeSeriesTimeseriesQuery:
-        """Query timeseries `shop_time_series.timeseries`
+    ) -> PriceAreaAFRROwnCapacityAllocationUpQuery:
+        """Query timeseries `price_area_afrr.own_capacity_allocation_up`
 
         Args:
-            object_type: The object type to filter on.
-            object_type_prefix: The prefix of the object type to filter on.
-            object_name: The object name to filter on.
-            object_name_prefix: The prefix of the object name to filter on.
-            attribute_name: The attribute name to filter on.
-            attribute_name_prefix: The prefix of the attribute name to filter on.
+            name: The name to filter on.
+            name_prefix: The prefix of the name to filter on.
+            display_name: The display name to filter on.
+            display_name_prefix: The prefix of the display name to filter on.
+            min_ordering: The minimum value of the ordering to filter on.
+            max_ordering: The maximum value of the ordering to filter on.
+            asset_type: The asset type to filter on.
+            asset_type_prefix: The prefix of the asset type to filter on.
+            timezone: The timezone to filter on.
+            timezone_prefix: The prefix of the timezone to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of shop time series to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of price area afrrs to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
-            A query object that can be used to retrieve datapoins for the shop_time_series.timeseries timeseries
+            A query object that can be used to retrieve datapoins for the price_area_afrr.own_capacity_allocation_up timeseries
             selected in this method.
 
         Examples:
 
-            Retrieve all data for 5 shop_time_series.timeseries timeseries:
+            Retrieve all data for 5 price_area_afrr.own_capacity_allocation_up timeseries:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> shop_time_series_list = client.shop_time_series.timeseries(limit=5).retrieve()
+                >>> price_area_afrrs = client.price_area_afrr.own_capacity_allocation_up(limit=5).retrieve()
 
         """
-        filter_ = _create_shop_time_series_filter(
+        filter_ = _create_price_area_afrr_filter(
             self._view_id,
-            object_type,
-            object_type_prefix,
-            object_name,
-            object_name_prefix,
-            attribute_name,
-            attribute_name_prefix,
+            name,
+            name_prefix,
+            display_name,
+            display_name_prefix,
+            min_ordering,
+            max_ordering,
+            asset_type,
+            asset_type_prefix,
+            timezone,
+            timezone_prefix,
             external_id_prefix,
             space,
             filter,
         )
 
-        return SHOPTimeSeriesTimeseriesQuery(
+        return PriceAreaAFRROwnCapacityAllocationUpQuery(
             client=self._client,
             view_id=self._view_id,
             timeseries_limit=limit,
             filter=filter_,
         )
 
     def list(
         self,
-        object_type: str | list[str] | None = None,
-        object_type_prefix: str | None = None,
-        object_name: str | list[str] | None = None,
-        object_name_prefix: str | None = None,
-        attribute_name: str | list[str] | None = None,
-        attribute_name_prefix: str | None = None,
+        name: str | list[str] | None = None,
+        name_prefix: str | None = None,
+        display_name: str | list[str] | None = None,
+        display_name_prefix: str | None = None,
+        min_ordering: int | None = None,
+        max_ordering: int | None = None,
+        asset_type: str | list[str] | None = None,
+        asset_type_prefix: str | None = None,
+        timezone: str | list[str] | None = None,
+        timezone_prefix: str | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> TimeSeriesList:
-        """List timeseries `shop_time_series.timeseries`
+        """List timeseries `price_area_afrr.own_capacity_allocation_up`
 
         Args:
-            object_type: The object type to filter on.
-            object_type_prefix: The prefix of the object type to filter on.
-            object_name: The object name to filter on.
-            object_name_prefix: The prefix of the object name to filter on.
-            attribute_name: The attribute name to filter on.
-            attribute_name_prefix: The prefix of the attribute name to filter on.
+            name: The name to filter on.
+            name_prefix: The prefix of the name to filter on.
+            display_name: The display name to filter on.
+            display_name_prefix: The prefix of the display name to filter on.
+            min_ordering: The minimum value of the ordering to filter on.
+            max_ordering: The maximum value of the ordering to filter on.
+            asset_type: The asset type to filter on.
+            asset_type_prefix: The prefix of the asset type to filter on.
+            timezone: The timezone to filter on.
+            timezone_prefix: The prefix of the timezone to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of shop time series to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of price area afrrs to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
-            List of Timeseries shop_time_series.timeseries.
+            List of Timeseries price_area_afrr.own_capacity_allocation_up.
 
         Examples:
 
-            List shop_time_series.timeseries and limit to 5:
+            List price_area_afrr.own_capacity_allocation_up and limit to 5:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> shop_time_series_list = client.shop_time_series.timeseries.list(limit=5)
+                >>> price_area_afrrs = client.price_area_afrr.own_capacity_allocation_up.list(limit=5)
 
         """
-        filter_ = _create_shop_time_series_filter(
+        filter_ = _create_price_area_afrr_filter(
             self._view_id,
-            object_type,
-            object_type_prefix,
-            object_name,
-            object_name_prefix,
-            attribute_name,
-            attribute_name_prefix,
+            name,
+            name_prefix,
+            display_name,
+            display_name_prefix,
+            min_ordering,
+            max_ordering,
+            asset_type,
+            asset_type_prefix,
+            timezone,
+            timezone_prefix,
             external_id_prefix,
             space,
             filter,
         )
-        external_ids = _retrieve_timeseries_external_ids_with_extra_timesery(
+        external_ids = _retrieve_timeseries_external_ids_with_extra_own_capacity_allocation_up(
             self._client, self._view_id, filter_, limit
         )
         if external_ids:
             return self._client.time_series.retrieve_multiple(external_ids=list(external_ids))
         else:
             return TimeSeriesList([])
 
 
-def _retrieve_timeseries_external_ids_with_extra_timesery(
+def _retrieve_timeseries_external_ids_with_extra_own_capacity_allocation_up(
     client: CogniteClient,
     view_id: dm.ViewId,
     filter_: dm.Filter | None,
     limit: int,
-    extra_properties: ColumnNames | list[ColumnNames] = "timeseries",
+    extra_properties: ColumnNames | list[ColumnNames] = "ownCapacityAllocationUp",
 ) -> dict[str, list[str]]:
     limit = float("inf") if limit is None or limit == -1 else limit
-    properties = ["timeseries"]
-    if extra_properties == "timeseries":
+    properties = ["ownCapacityAllocationUp"]
+    if extra_properties == "ownCapacityAllocationUp":
         ...
-    elif isinstance(extra_properties, str) and extra_properties != "timeseries":
+    elif isinstance(extra_properties, str) and extra_properties != "ownCapacityAllocationUp":
         properties.append(extra_properties)
     elif isinstance(extra_properties, list):
-        properties.extend([prop for prop in extra_properties if prop != "timeseries"])
+        properties.extend([prop for prop in extra_properties if prop != "ownCapacityAllocationUp"])
     else:
         raise ValueError(f"Invalid value for extra_properties: {extra_properties}")
 
     if isinstance(extra_properties, str):
         extra_list = [extra_properties]
     else:
         extra_list = extra_properties
     has_data = dm.filters.HasData(views=[view_id])
-    has_property = dm.filters.Exists(property=view_id.as_property_ref("timeseries"))
+    has_property = dm.filters.Exists(property=view_id.as_property_ref("ownCapacityAllocationUp"))
     filter_ = dm.filters.And(filter_, has_data, has_property) if filter_ else dm.filters.And(has_data, has_property)
 
     cursor = None
     external_ids: dict[str, list[str]] = {}
     total_retrieved = 0
     while True:
         query_limit = max(min(INSTANCE_QUERY_LIMIT, limit - total_retrieved), 0)
@@ -518,15 +557,17 @@
                     [dm.query.SourceSelector(view_id, properties)],
                 )
             },
             cursors={"nodes": cursor},
         )
         result = client.data_modeling.instances.query(query)
         batch_external_ids = {
-            node.properties[view_id]["timeseries"]: [node.properties[view_id].get(prop, "") for prop in extra_list]
+            node.properties[view_id]["ownCapacityAllocationUp"]: [
+                node.properties[view_id].get(prop, "") for prop in extra_list
+            ]
             for node in result.data["nodes"].data
         }
         total_retrieved += len(batch_external_ids)
         external_ids.update(batch_external_ids)
         cursor = result.cursors["nodes"]
         if total_retrieved >= limit or cursor is None:
             break
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/shop_trigger_input.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/shop_trigger_input.py`

 * *Files 4% similar despite different names*

```diff
@@ -57,15 +57,16 @@
         max_process_step: int | None = None,
         function_name: str | list[str] | None = None,
         function_name_prefix: str | None = None,
         function_call_id: str | list[str] | None = None,
         function_call_id_prefix: str | None = None,
         cog_shop_tag: str | list[str] | None = None,
         cog_shop_tag_prefix: str | None = None,
-        scenario: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        case: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        pre_processor_input: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_QUERY_LIMIT,
         filter: dm.Filter | None = None,
     ) -> SHOPTriggerInputQueryAPI[SHOPTriggerInputList]:
         """Query starting at shop trigger inputs.
 
@@ -76,15 +77,16 @@
             max_process_step: The maximum value of the process step to filter on.
             function_name: The function name to filter on.
             function_name_prefix: The prefix of the function name to filter on.
             function_call_id: The function call id to filter on.
             function_call_id_prefix: The prefix of the function call id to filter on.
             cog_shop_tag: The cog shop tag to filter on.
             cog_shop_tag_prefix: The prefix of the cog shop tag to filter on.
-            scenario: The scenario to filter on.
+            case: The case to filter on.
+            pre_processor_input: The pre processor input to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of shop trigger inputs to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
             A query API for shop trigger inputs.
@@ -99,15 +101,16 @@
             max_process_step,
             function_name,
             function_name_prefix,
             function_call_id,
             function_call_id_prefix,
             cog_shop_tag,
             cog_shop_tag_prefix,
-            scenario,
+            case,
+            pre_processor_input,
             external_id_prefix,
             space,
             (filter and dm.filters.And(filter, has_data)) or has_data,
         )
         builder = QueryBuilder(SHOPTriggerInputList)
         return SHOPTriggerInputQueryAPI(self._client, builder, self._view_by_read_class, filter_, limit)
 
@@ -223,15 +226,16 @@
         max_process_step: int | None = None,
         function_name: str | list[str] | None = None,
         function_name_prefix: str | None = None,
         function_call_id: str | list[str] | None = None,
         function_call_id_prefix: str | None = None,
         cog_shop_tag: str | list[str] | None = None,
         cog_shop_tag_prefix: str | None = None,
-        scenario: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        case: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        pre_processor_input: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> SHOPTriggerInputList:
         """Search shop trigger inputs
 
@@ -244,15 +248,16 @@
             max_process_step: The maximum value of the process step to filter on.
             function_name: The function name to filter on.
             function_name_prefix: The prefix of the function name to filter on.
             function_call_id: The function call id to filter on.
             function_call_id_prefix: The prefix of the function call id to filter on.
             cog_shop_tag: The cog shop tag to filter on.
             cog_shop_tag_prefix: The prefix of the cog shop tag to filter on.
-            scenario: The scenario to filter on.
+            case: The case to filter on.
+            pre_processor_input: The pre processor input to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of shop trigger inputs to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
             Search results shop trigger inputs matching the query.
@@ -274,15 +279,16 @@
             max_process_step,
             function_name,
             function_name_prefix,
             function_call_id,
             function_call_id_prefix,
             cog_shop_tag,
             cog_shop_tag_prefix,
-            scenario,
+            case,
+            pre_processor_input,
             external_id_prefix,
             space,
             filter,
         )
         return self._search(self._view_id, query, _SHOPTRIGGERINPUT_PROPERTIES_BY_FIELD, properties, filter_, limit)
 
     @overload
@@ -304,15 +310,16 @@
         max_process_step: int | None = None,
         function_name: str | list[str] | None = None,
         function_name_prefix: str | None = None,
         function_call_id: str | list[str] | None = None,
         function_call_id_prefix: str | None = None,
         cog_shop_tag: str | list[str] | None = None,
         cog_shop_tag_prefix: str | None = None,
-        scenario: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        case: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        pre_processor_input: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> list[dm.aggregations.AggregatedNumberedValue]: ...
 
     @overload
@@ -334,15 +341,16 @@
         max_process_step: int | None = None,
         function_name: str | list[str] | None = None,
         function_name_prefix: str | None = None,
         function_call_id: str | list[str] | None = None,
         function_call_id_prefix: str | None = None,
         cog_shop_tag: str | list[str] | None = None,
         cog_shop_tag_prefix: str | None = None,
-        scenario: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        case: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        pre_processor_input: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> InstanceAggregationResultList: ...
 
     def aggregate(
@@ -363,15 +371,16 @@
         max_process_step: int | None = None,
         function_name: str | list[str] | None = None,
         function_name_prefix: str | None = None,
         function_call_id: str | list[str] | None = None,
         function_call_id_prefix: str | None = None,
         cog_shop_tag: str | list[str] | None = None,
         cog_shop_tag_prefix: str | None = None,
-        scenario: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        case: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        pre_processor_input: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> list[dm.aggregations.AggregatedNumberedValue] | InstanceAggregationResultList:
         """Aggregate data across shop trigger inputs
 
@@ -387,15 +396,16 @@
             max_process_step: The maximum value of the process step to filter on.
             function_name: The function name to filter on.
             function_name_prefix: The prefix of the function name to filter on.
             function_call_id: The function call id to filter on.
             function_call_id_prefix: The prefix of the function call id to filter on.
             cog_shop_tag: The cog shop tag to filter on.
             cog_shop_tag_prefix: The prefix of the cog shop tag to filter on.
-            scenario: The scenario to filter on.
+            case: The case to filter on.
+            pre_processor_input: The pre processor input to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of shop trigger inputs to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
             Aggregation results.
@@ -418,15 +428,16 @@
             max_process_step,
             function_name,
             function_name_prefix,
             function_call_id,
             function_call_id_prefix,
             cog_shop_tag,
             cog_shop_tag_prefix,
-            scenario,
+            case,
+            pre_processor_input,
             external_id_prefix,
             space,
             filter,
         )
         return self._aggregate(
             self._view_id,
             aggregate,
@@ -451,15 +462,16 @@
         max_process_step: int | None = None,
         function_name: str | list[str] | None = None,
         function_name_prefix: str | None = None,
         function_call_id: str | list[str] | None = None,
         function_call_id_prefix: str | None = None,
         cog_shop_tag: str | list[str] | None = None,
         cog_shop_tag_prefix: str | None = None,
-        scenario: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        case: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        pre_processor_input: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> dm.aggregations.HistogramValue:
         """Produces histograms for shop trigger inputs
 
@@ -474,15 +486,16 @@
             max_process_step: The maximum value of the process step to filter on.
             function_name: The function name to filter on.
             function_name_prefix: The prefix of the function name to filter on.
             function_call_id: The function call id to filter on.
             function_call_id_prefix: The prefix of the function call id to filter on.
             cog_shop_tag: The cog shop tag to filter on.
             cog_shop_tag_prefix: The prefix of the cog shop tag to filter on.
-            scenario: The scenario to filter on.
+            case: The case to filter on.
+            pre_processor_input: The pre processor input to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of shop trigger inputs to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
             Bucketed histogram results.
@@ -496,15 +509,16 @@
             max_process_step,
             function_name,
             function_name_prefix,
             function_call_id,
             function_call_id_prefix,
             cog_shop_tag,
             cog_shop_tag_prefix,
-            scenario,
+            case,
+            pre_processor_input,
             external_id_prefix,
             space,
             filter,
         )
         return self._histogram(
             self._view_id,
             property,
@@ -524,15 +538,16 @@
         max_process_step: int | None = None,
         function_name: str | list[str] | None = None,
         function_name_prefix: str | None = None,
         function_call_id: str | list[str] | None = None,
         function_call_id_prefix: str | None = None,
         cog_shop_tag: str | list[str] | None = None,
         cog_shop_tag_prefix: str | None = None,
-        scenario: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        case: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        pre_processor_input: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> SHOPTriggerInputList:
         """List/filter shop trigger inputs
 
@@ -543,15 +558,16 @@
             max_process_step: The maximum value of the process step to filter on.
             function_name: The function name to filter on.
             function_name_prefix: The prefix of the function name to filter on.
             function_call_id: The function call id to filter on.
             function_call_id_prefix: The prefix of the function call id to filter on.
             cog_shop_tag: The cog shop tag to filter on.
             cog_shop_tag_prefix: The prefix of the cog shop tag to filter on.
-            scenario: The scenario to filter on.
+            case: The case to filter on.
+            pre_processor_input: The pre processor input to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
             limit: Maximum number of shop trigger inputs to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
             List of requested shop trigger inputs
@@ -573,13 +589,14 @@
             max_process_step,
             function_name,
             function_name_prefix,
             function_call_id,
             function_call_id_prefix,
             cog_shop_tag,
             cog_shop_tag_prefix,
-            scenario,
+            case,
+            pre_processor_input,
             external_id_prefix,
             space,
             filter,
         )
         return self._list(limit=limit, filter=filter_)
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/shop_trigger_input_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/power_asset_query.py`

 * *Files 22% similar despite different names*

```diff
@@ -3,71 +3,46 @@
 import datetime
 from typing import TYPE_CHECKING
 
 from cognite.client import data_modeling as dm, CogniteClient
 
 from cognite.powerops.client._generated.v1.data_classes import (
     DomainModelCore,
-    SHOPTriggerInput,
-    Scenario,
+    PowerAsset,
 )
 from ._core import DEFAULT_QUERY_LIMIT, QueryBuilder, QueryStep, QueryAPI, T_DomainModelList, _create_edge_filter
 
 
-class SHOPTriggerInputQueryAPI(QueryAPI[T_DomainModelList]):
+class PowerAssetQueryAPI(QueryAPI[T_DomainModelList]):
     def __init__(
         self,
         client: CogniteClient,
         builder: QueryBuilder[T_DomainModelList],
         view_by_read_class: dict[type[DomainModelCore], dm.ViewId],
         filter_: dm.filters.Filter | None = None,
         limit: int = DEFAULT_QUERY_LIMIT,
     ):
         super().__init__(client, builder, view_by_read_class)
 
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("shop_trigger_input"),
+                name=self._builder.next_name("power_asset"),
                 expression=dm.query.NodeResultSetExpression(
                     from_=self._builder[-1].name if self._builder else None,
                     filter=filter_,
                 ),
-                select=dm.query.Select([dm.query.SourceSelector(self._view_by_read_class[SHOPTriggerInput], ["*"])]),
-                result_cls=SHOPTriggerInput,
+                select=dm.query.Select([dm.query.SourceSelector(self._view_by_read_class[PowerAsset], ["*"])]),
+                result_cls=PowerAsset,
                 max_retrieve_limit=limit,
             )
         )
 
     def query(
         self,
-        retrieve_scenario: bool = False,
     ) -> T_DomainModelList:
         """Execute query and return the result.
 
-        Args:
-            retrieve_scenario: Whether to retrieve the scenario for each shop trigger input or not.
-
         Returns:
             The list of the source nodes of the query.
 
         """
-        from_ = self._builder[-1].name
-        if retrieve_scenario:
-            self._query_append_scenario(from_)
         return self._query()
-
-    def _query_append_scenario(self, from_: str) -> None:
-        view_id = self._view_by_read_class[Scenario]
-        self._builder.append(
-            QueryStep(
-                name=self._builder.next_name("scenario"),
-                expression=dm.query.NodeResultSetExpression(
-                    filter=dm.filters.HasData(views=[view_id]),
-                    from_=from_,
-                    through=self._view_by_read_class[SHOPTriggerInput].as_property_ref("scenario"),
-                    direction="outwards",
-                ),
-                select=dm.query.Select([dm.query.SourceSelector(view_id, ["*"])]),
-                max_retrieve_limit=-1,
-                result_cls=Scenario,
-            ),
-        )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/shop_trigger_output.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/shop_trigger_output.py`

 * *Files 2% similar despite different names*

```diff
@@ -218,17 +218,17 @@
             external_id,
             space,
             retrieve_edges=True,
             edge_api_name_type_direction_view_id_penta=[
                 (
                     self.alerts_edge,
                     "alerts",
-                    dm.DirectRelationReference("sp_powerops_types", "calculationIssue"),
+                    dm.DirectRelationReference("sp_powerops_types_temp", "calculationIssue"),
                     "outwards",
-                    dm.ViewId("sp_powerops_models", "Alert", "1"),
+                    dm.ViewId("sp_powerops_models_temp", "Alert", "1"),
                 ),
             ],
         )
 
     def search(
         self,
         query: str,
@@ -591,13 +591,13 @@
             limit=limit,
             filter=filter_,
             retrieve_edges=retrieve_edges,
             edge_api_name_type_direction_view_id_penta=[
                 (
                     self.alerts_edge,
                     "alerts",
-                    dm.DirectRelationReference("sp_powerops_types", "calculationIssue"),
+                    dm.DirectRelationReference("sp_powerops_types_temp", "calculationIssue"),
                     "outwards",
-                    dm.ViewId("sp_powerops_models", "Alert", "1"),
+                    dm.ViewId("sp_powerops_models_temp", "Alert", "1"),
                 ),
             ],
         )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/shop_trigger_output_alerts.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/shop_trigger_output_alerts.py`

 * *Files 1% similar despite different names*

```diff
@@ -39,15 +39,15 @@
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
                 >>> shop_trigger_output = client.shop_trigger_output.alerts_edge.list("my_shop_trigger_output", limit=5)
 
         """
         filter_ = _create_edge_filter(
-            dm.DirectRelationReference("sp_powerops_types", "calculationIssue"),
+            dm.DirectRelationReference("sp_powerops_types_temp", "calculationIssue"),
             from_shop_trigger_output,
             from_shop_trigger_output_space,
             to_alert,
             to_alert_space,
             external_id_prefix,
             space,
         )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/shop_trigger_output_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/total_bid_matrix_calculation_input_query.py`

 * *Files 24% similar despite different names*

```diff
@@ -3,144 +3,153 @@
 import datetime
 from typing import TYPE_CHECKING
 
 from cognite.client import data_modeling as dm, CogniteClient
 
 from cognite.powerops.client._generated.v1.data_classes import (
     DomainModelCore,
-    SHOPTriggerOutput,
-    SHOPResult,
-    SHOPTriggerInput,
+    TotalBidMatrixCalculationInput,
+    BidConfiguration,
+)
+from cognite.powerops.client._generated.v1.data_classes._bid_matrix import (
+    BidMatrix,
+    _create_bid_matrix_filter,
 )
 from ._core import DEFAULT_QUERY_LIMIT, QueryBuilder, QueryStep, QueryAPI, T_DomainModelList, _create_edge_filter
 
 if TYPE_CHECKING:
-    from .alert_query import AlertQueryAPI
+    from .bid_matrix_query import BidMatrixQueryAPI
 
 
-class SHOPTriggerOutputQueryAPI(QueryAPI[T_DomainModelList]):
+class TotalBidMatrixCalculationInputQueryAPI(QueryAPI[T_DomainModelList]):
     def __init__(
         self,
         client: CogniteClient,
         builder: QueryBuilder[T_DomainModelList],
         view_by_read_class: dict[type[DomainModelCore], dm.ViewId],
         filter_: dm.filters.Filter | None = None,
         limit: int = DEFAULT_QUERY_LIMIT,
     ):
         super().__init__(client, builder, view_by_read_class)
 
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("shop_trigger_output"),
+                name=self._builder.next_name("total_bid_matrix_calculation_input"),
                 expression=dm.query.NodeResultSetExpression(
                     from_=self._builder[-1].name if self._builder else None,
                     filter=filter_,
                 ),
-                select=dm.query.Select([dm.query.SourceSelector(self._view_by_read_class[SHOPTriggerOutput], ["*"])]),
-                result_cls=SHOPTriggerOutput,
+                select=dm.query.Select(
+                    [dm.query.SourceSelector(self._view_by_read_class[TotalBidMatrixCalculationInput], ["*"])]
+                ),
+                result_cls=TotalBidMatrixCalculationInput,
                 max_retrieve_limit=limit,
             )
         )
 
-    def alerts(
+    def partial_bid_matrices(
         self,
+        power_asset: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        state: str | list[str] | None = None,
+        state_prefix: str | None = None,
+        partial_bid_configuration: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
+        external_id_prefix_edge: str | None = None,
+        space_edge: str | list[str] | None = None,
+        filter: dm.Filter | None = None,
         limit: int | None = DEFAULT_QUERY_LIMIT,
-        retrieve_shop_result: bool = False,
-        retrieve_input_: bool = False,
-    ) -> AlertQueryAPI[T_DomainModelList]:
-        """Query along the alert edges of the shop trigger output.
+        retrieve_bid_configuration: bool = False,
+    ) -> BidMatrixQueryAPI[T_DomainModelList]:
+        """Query along the partial bid matrice edges of the total bid matrix calculation input.
 
         Args:
+            power_asset: The power asset to filter on.
+            state: The state to filter on.
+            state_prefix: The prefix of the state to filter on.
+            partial_bid_configuration: The partial bid configuration to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of alert edges to return. Defaults to 25. Set to -1, float("inf") or None
+            external_id_prefix_edge: The prefix of the external ID to filter on.
+            space_edge: The space to filter on.
+            filter: (Advanced) Filter applied to node. If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
+            limit: Maximum number of partial bid matrice edges to return. Defaults to 3. Set to -1, float("inf") or None
                 to return all items.
-            retrieve_shop_result: Whether to retrieve the shop result for each shop trigger output or not.
-            retrieve_input_: Whether to retrieve the input for each shop trigger output or not.
+            retrieve_bid_configuration: Whether to retrieve the bid configuration for each total bid matrix calculation input or not.
 
         Returns:
-            AlertQueryAPI: The query API for the alert.
+            BidMatrixQueryAPI: The query API for the bid matrix.
         """
-        from .alert_query import AlertQueryAPI
+        from .bid_matrix_query import BidMatrixQueryAPI
 
         from_ = self._builder[-1].name
-
         edge_filter = _create_edge_filter(
-            dm.DirectRelationReference("sp_powerops_types", "calculationIssue"),
-            external_id_prefix=external_id_prefix,
-            space=space,
+            dm.DirectRelationReference("sp_powerops_types_temp", "BidMatrix"),
+            external_id_prefix=external_id_prefix_edge,
+            space=space_edge,
         )
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("alerts"),
+                name=self._builder.next_name("partial_bid_matrices"),
                 expression=dm.query.EdgeResultSetExpression(
                     filter=edge_filter,
                     from_=from_,
                     direction="outwards",
                 ),
                 select=dm.query.Select(),
                 max_retrieve_limit=limit,
             )
         )
-        if retrieve_shop_result:
-            self._query_append_shop_result(from_)
-        if retrieve_input_:
-            self._query_append_input_(from_)
-        return AlertQueryAPI(self._client, self._builder, self._view_by_read_class, None, limit)
+
+        view_id = self._view_by_read_class[BidMatrix]
+        has_data = dm.filters.HasData(views=[view_id])
+        node_filer = _create_bid_matrix_filter(
+            view_id,
+            power_asset,
+            state,
+            state_prefix,
+            partial_bid_configuration,
+            external_id_prefix,
+            space,
+            (filter and dm.filters.And(filter, has_data)) or has_data,
+        )
+        if retrieve_bid_configuration:
+            self._query_append_bid_configuration(from_)
+        return BidMatrixQueryAPI(self._client, self._builder, self._view_by_read_class, node_filer, limit)
 
     def query(
         self,
-        retrieve_shop_result: bool = False,
-        retrieve_input_: bool = False,
+        retrieve_bid_configuration: bool = False,
     ) -> T_DomainModelList:
         """Execute query and return the result.
 
         Args:
-            retrieve_shop_result: Whether to retrieve the shop result for each shop trigger output or not.
-            retrieve_input_: Whether to retrieve the input for each shop trigger output or not.
+            retrieve_bid_configuration: Whether to retrieve the bid configuration for each total bid matrix calculation input or not.
 
         Returns:
             The list of the source nodes of the query.
 
         """
         from_ = self._builder[-1].name
-        if retrieve_shop_result:
-            self._query_append_shop_result(from_)
-        if retrieve_input_:
-            self._query_append_input_(from_)
+        if retrieve_bid_configuration:
+            self._query_append_bid_configuration(from_)
         return self._query()
 
-    def _query_append_shop_result(self, from_: str) -> None:
-        view_id = self._view_by_read_class[SHOPResult]
-        self._builder.append(
-            QueryStep(
-                name=self._builder.next_name("shop_result"),
-                expression=dm.query.NodeResultSetExpression(
-                    filter=dm.filters.HasData(views=[view_id]),
-                    from_=from_,
-                    through=self._view_by_read_class[SHOPTriggerOutput].as_property_ref("shopResult"),
-                    direction="outwards",
-                ),
-                select=dm.query.Select([dm.query.SourceSelector(view_id, ["*"])]),
-                max_retrieve_limit=-1,
-                result_cls=SHOPResult,
-            ),
-        )
-
-    def _query_append_input_(self, from_: str) -> None:
-        view_id = self._view_by_read_class[SHOPTriggerInput]
+    def _query_append_bid_configuration(self, from_: str) -> None:
+        view_id = self._view_by_read_class[BidConfiguration]
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("input_"),
+                name=self._builder.next_name("bid_configuration"),
                 expression=dm.query.NodeResultSetExpression(
                     filter=dm.filters.HasData(views=[view_id]),
                     from_=from_,
-                    through=self._view_by_read_class[SHOPTriggerOutput].as_property_ref("input"),
+                    through=self._view_by_read_class[TotalBidMatrixCalculationInput].as_property_ref(
+                        "bidConfiguration"
+                    ),
                     direction="outwards",
                 ),
                 select=dm.query.Select([dm.query.SourceSelector(view_id, ["*"])]),
                 max_retrieve_limit=-1,
-                result_cls=SHOPTriggerInput,
+                result_cls=BidConfiguration,
+                is_single_direct_relation=True,
             ),
         )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/task_dispatcher_shop_input.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/task_dispatcher_shop_input.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/task_dispatcher_shop_input_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/task_dispatcher_input_query.py`

 * *Files 13% similar despite different names*

```diff
@@ -3,73 +3,72 @@
 import datetime
 from typing import TYPE_CHECKING
 
 from cognite.client import data_modeling as dm, CogniteClient
 
 from cognite.powerops.client._generated.v1.data_classes import (
     DomainModelCore,
-    TaskDispatcherShopInput,
-    BidConfigurationShop,
+    TaskDispatcherInput,
+    BidConfiguration,
 )
 from ._core import DEFAULT_QUERY_LIMIT, QueryBuilder, QueryStep, QueryAPI, T_DomainModelList, _create_edge_filter
 
 
-class TaskDispatcherShopInputQueryAPI(QueryAPI[T_DomainModelList]):
+class TaskDispatcherInputQueryAPI(QueryAPI[T_DomainModelList]):
     def __init__(
         self,
         client: CogniteClient,
         builder: QueryBuilder[T_DomainModelList],
         view_by_read_class: dict[type[DomainModelCore], dm.ViewId],
         filter_: dm.filters.Filter | None = None,
         limit: int = DEFAULT_QUERY_LIMIT,
     ):
         super().__init__(client, builder, view_by_read_class)
 
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("task_dispatcher_shop_input"),
+                name=self._builder.next_name("task_dispatcher_input"),
                 expression=dm.query.NodeResultSetExpression(
                     from_=self._builder[-1].name if self._builder else None,
                     filter=filter_,
                 ),
-                select=dm.query.Select(
-                    [dm.query.SourceSelector(self._view_by_read_class[TaskDispatcherShopInput], ["*"])]
-                ),
-                result_cls=TaskDispatcherShopInput,
+                select=dm.query.Select([dm.query.SourceSelector(self._view_by_read_class[TaskDispatcherInput], ["*"])]),
+                result_cls=TaskDispatcherInput,
                 max_retrieve_limit=limit,
             )
         )
 
     def query(
         self,
         retrieve_bid_configuration: bool = False,
     ) -> T_DomainModelList:
         """Execute query and return the result.
 
         Args:
-            retrieve_bid_configuration: Whether to retrieve the bid configuration for each task dispatcher shop input or not.
+            retrieve_bid_configuration: Whether to retrieve the bid configuration for each task dispatcher input or not.
 
         Returns:
             The list of the source nodes of the query.
 
         """
         from_ = self._builder[-1].name
         if retrieve_bid_configuration:
             self._query_append_bid_configuration(from_)
         return self._query()
 
     def _query_append_bid_configuration(self, from_: str) -> None:
-        view_id = self._view_by_read_class[BidConfigurationShop]
+        view_id = self._view_by_read_class[BidConfiguration]
         self._builder.append(
             QueryStep(
                 name=self._builder.next_name("bid_configuration"),
                 expression=dm.query.NodeResultSetExpression(
                     filter=dm.filters.HasData(views=[view_id]),
                     from_=from_,
-                    through=self._view_by_read_class[TaskDispatcherShopInput].as_property_ref("bidConfiguration"),
+                    through=self._view_by_read_class[TaskDispatcherInput].as_property_ref("bidConfiguration"),
                     direction="outwards",
                 ),
                 select=dm.query.Select([dm.query.SourceSelector(view_id, ["*"])]),
                 max_retrieve_limit=-1,
-                result_cls=BidConfigurationShop,
+                result_cls=BidConfiguration,
+                is_single_direct_relation=True,
             ),
         )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/task_dispatcher_shop_output.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/task_dispatcher_shop_output.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/task_dispatcher_shop_output_alerts.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/task_dispatcher_shop_output_alerts.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/task_dispatcher_shop_output_partial_bid_calculations.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/task_dispatcher_shop_output_partial_bid_calculations.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/task_dispatcher_shop_output_preprocessor_calculations.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/task_dispatcher_shop_output_preprocessor_calculations.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/task_dispatcher_shop_output_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/afrr_bid/_api/bid_row_query.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,214 +1,208 @@
 from __future__ import annotations
 
 import datetime
 from typing import TYPE_CHECKING
 
 from cognite.client import data_modeling as dm, CogniteClient
 
-from cognite.powerops.client._generated.v1.data_classes import (
+from cognite.powerops.client._generated.afrr_bid.data_classes import (
     DomainModelCore,
-    TaskDispatcherShopOutput,
-    TaskDispatcherShopInput,
+    BidRow,
+    BidRow,
+    BidMethod,
+)
+from cognite.powerops.client._generated.afrr_bid.data_classes._alert import (
+    Alert,
+    _create_alert_filter,
 )
 from ._core import DEFAULT_QUERY_LIMIT, QueryBuilder, QueryStep, QueryAPI, T_DomainModelList, _create_edge_filter
 
 if TYPE_CHECKING:
     from .alert_query import AlertQueryAPI
-    from .shop_partial_bid_calculation_input_query import ShopPartialBidCalculationInputQueryAPI
-    from .preprocessor_input_query import PreprocessorInputQueryAPI
 
 
-class TaskDispatcherShopOutputQueryAPI(QueryAPI[T_DomainModelList]):
+class BidRowQueryAPI(QueryAPI[T_DomainModelList]):
     def __init__(
         self,
         client: CogniteClient,
         builder: QueryBuilder[T_DomainModelList],
         view_by_read_class: dict[type[DomainModelCore], dm.ViewId],
         filter_: dm.filters.Filter | None = None,
         limit: int = DEFAULT_QUERY_LIMIT,
     ):
         super().__init__(client, builder, view_by_read_class)
 
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("task_dispatcher_shop_output"),
+                name=self._builder.next_name("bid_row"),
                 expression=dm.query.NodeResultSetExpression(
                     from_=self._builder[-1].name if self._builder else None,
                     filter=filter_,
                 ),
-                select=dm.query.Select(
-                    [dm.query.SourceSelector(self._view_by_read_class[TaskDispatcherShopOutput], ["*"])]
-                ),
-                result_cls=TaskDispatcherShopOutput,
+                select=dm.query.Select([dm.query.SourceSelector(self._view_by_read_class[BidRow], ["*"])]),
+                result_cls=BidRow,
                 max_retrieve_limit=limit,
             )
         )
 
     def alerts(
         self,
+        min_time: datetime.datetime | None = None,
+        max_time: datetime.datetime | None = None,
+        title: str | list[str] | None = None,
+        title_prefix: str | None = None,
+        description: str | list[str] | None = None,
+        description_prefix: str | None = None,
+        severity: str | list[str] | None = None,
+        severity_prefix: str | None = None,
+        alert_type: str | list[str] | None = None,
+        alert_type_prefix: str | None = None,
+        min_status_code: int | None = None,
+        max_status_code: int | None = None,
+        calculation_run: str | list[str] | None = None,
+        calculation_run_prefix: str | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
+        external_id_prefix_edge: str | None = None,
+        space_edge: str | list[str] | None = None,
+        filter: dm.Filter | None = None,
         limit: int | None = DEFAULT_QUERY_LIMIT,
-        retrieve_input_: bool = False,
+        retrieve_linked_bid: bool = False,
+        retrieve_method: bool = False,
     ) -> AlertQueryAPI[T_DomainModelList]:
-        """Query along the alert edges of the task dispatcher shop output.
+        """Query along the alert edges of the bid row.
 
         Args:
+            min_time: The minimum value of the time to filter on.
+            max_time: The maximum value of the time to filter on.
+            title: The title to filter on.
+            title_prefix: The prefix of the title to filter on.
+            description: The description to filter on.
+            description_prefix: The prefix of the description to filter on.
+            severity: The severity to filter on.
+            severity_prefix: The prefix of the severity to filter on.
+            alert_type: The alert type to filter on.
+            alert_type_prefix: The prefix of the alert type to filter on.
+            min_status_code: The minimum value of the status code to filter on.
+            max_status_code: The maximum value of the status code to filter on.
+            calculation_run: The calculation run to filter on.
+            calculation_run_prefix: The prefix of the calculation run to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of alert edges to return. Defaults to 25. Set to -1, float("inf") or None
+            external_id_prefix_edge: The prefix of the external ID to filter on.
+            space_edge: The space to filter on.
+            filter: (Advanced) Filter applied to node. If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
+            limit: Maximum number of alert edges to return. Defaults to 3. Set to -1, float("inf") or None
                 to return all items.
-            retrieve_input_: Whether to retrieve the input for each task dispatcher shop output or not.
+            retrieve_linked_bid: Whether to retrieve the linked bid for each bid row or not.
+            retrieve_method: Whether to retrieve the method for each bid row or not.
 
         Returns:
             AlertQueryAPI: The query API for the alert.
         """
         from .alert_query import AlertQueryAPI
 
         from_ = self._builder[-1].name
-
         edge_filter = _create_edge_filter(
-            dm.DirectRelationReference("sp_powerops_types", "calculationIssue"),
-            external_id_prefix=external_id_prefix,
-            space=space,
+            dm.DirectRelationReference("power-ops-types", "calculationIssue"),
+            external_id_prefix=external_id_prefix_edge,
+            space=space_edge,
         )
         self._builder.append(
             QueryStep(
                 name=self._builder.next_name("alerts"),
                 expression=dm.query.EdgeResultSetExpression(
                     filter=edge_filter,
                     from_=from_,
                     direction="outwards",
                 ),
                 select=dm.query.Select(),
                 max_retrieve_limit=limit,
             )
         )
-        if retrieve_input_:
-            self._query_append_input_(from_)
-        return AlertQueryAPI(self._client, self._builder, self._view_by_read_class, None, limit)
-
-    def partial_bid_calculations(
-        self,
-        external_id_prefix: str | None = None,
-        space: str | list[str] | None = None,
-        limit: int | None = DEFAULT_QUERY_LIMIT,
-        retrieve_input_: bool = False,
-    ) -> ShopPartialBidCalculationInputQueryAPI[T_DomainModelList]:
-        """Query along the partial bid calculation edges of the task dispatcher shop output.
-
-        Args:
-            external_id_prefix: The prefix of the external ID to filter on.
-            space: The space to filter on.
-            limit: Maximum number of partial bid calculation edges to return. Defaults to 25. Set to -1, float("inf") or None
-                to return all items.
-            retrieve_input_: Whether to retrieve the input for each task dispatcher shop output or not.
-
-        Returns:
-            ShopPartialBidCalculationInputQueryAPI: The query API for the shop partial bid calculation input.
-        """
-        from .shop_partial_bid_calculation_input_query import ShopPartialBidCalculationInputQueryAPI
-
-        from_ = self._builder[-1].name
 
-        edge_filter = _create_edge_filter(
-            dm.DirectRelationReference("sp_powerops_types", "ShopPartialBidCalculationInput"),
-            external_id_prefix=external_id_prefix,
-            space=space,
-        )
-        self._builder.append(
-            QueryStep(
-                name=self._builder.next_name("partial_bid_calculations"),
-                expression=dm.query.EdgeResultSetExpression(
-                    filter=edge_filter,
-                    from_=from_,
-                    direction="outwards",
-                ),
-                select=dm.query.Select(),
-                max_retrieve_limit=limit,
-            )
-        )
-        if retrieve_input_:
-            self._query_append_input_(from_)
-        return ShopPartialBidCalculationInputQueryAPI(
-            self._client, self._builder, self._view_by_read_class, None, limit
-        )
+        view_id = self._view_by_read_class[Alert]
+        has_data = dm.filters.HasData(views=[view_id])
+        node_filer = _create_alert_filter(
+            view_id,
+            min_time,
+            max_time,
+            title,
+            title_prefix,
+            description,
+            description_prefix,
+            severity,
+            severity_prefix,
+            alert_type,
+            alert_type_prefix,
+            min_status_code,
+            max_status_code,
+            calculation_run,
+            calculation_run_prefix,
+            external_id_prefix,
+            space,
+            (filter and dm.filters.And(filter, has_data)) or has_data,
+        )
+        if retrieve_linked_bid:
+            self._query_append_linked_bid(from_)
+        if retrieve_method:
+            self._query_append_method(from_)
+        return AlertQueryAPI(self._client, self._builder, self._view_by_read_class, node_filer, limit)
 
-    def preprocessor_calculations(
+    def query(
         self,
-        external_id_prefix: str | None = None,
-        space: str | list[str] | None = None,
-        limit: int | None = DEFAULT_QUERY_LIMIT,
-        retrieve_input_: bool = False,
-    ) -> PreprocessorInputQueryAPI[T_DomainModelList]:
-        """Query along the preprocessor calculation edges of the task dispatcher shop output.
+        retrieve_linked_bid: bool = False,
+        retrieve_method: bool = False,
+    ) -> T_DomainModelList:
+        """Execute query and return the result.
 
         Args:
-            external_id_prefix: The prefix of the external ID to filter on.
-            space: The space to filter on.
-            limit: Maximum number of preprocessor calculation edges to return. Defaults to 25. Set to -1, float("inf") or None
-                to return all items.
-            retrieve_input_: Whether to retrieve the input for each task dispatcher shop output or not.
+            retrieve_linked_bid: Whether to retrieve the linked bid for each bid row or not.
+            retrieve_method: Whether to retrieve the method for each bid row or not.
 
         Returns:
-            PreprocessorInputQueryAPI: The query API for the preprocessor input.
-        """
-        from .preprocessor_input_query import PreprocessorInputQueryAPI
+            The list of the source nodes of the query.
 
+        """
         from_ = self._builder[-1].name
+        if retrieve_linked_bid:
+            self._query_append_linked_bid(from_)
+        if retrieve_method:
+            self._query_append_method(from_)
+        return self._query()
 
-        edge_filter = _create_edge_filter(
-            dm.DirectRelationReference("sp_powerops_types", "PreprocessorInput"),
-            external_id_prefix=external_id_prefix,
-            space=space,
-        )
+    def _query_append_linked_bid(self, from_: str) -> None:
+        view_id = self._view_by_read_class[BidRow]
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("preprocessor_calculations"),
-                expression=dm.query.EdgeResultSetExpression(
-                    filter=edge_filter,
+                name=self._builder.next_name("linked_bid"),
+                expression=dm.query.NodeResultSetExpression(
+                    filter=dm.filters.HasData(views=[view_id]),
                     from_=from_,
+                    through=self._view_by_read_class[BidRow].as_property_ref("linkedBid"),
                     direction="outwards",
                 ),
-                select=dm.query.Select(),
-                max_retrieve_limit=limit,
-            )
+                select=dm.query.Select([dm.query.SourceSelector(view_id, ["*"])]),
+                max_retrieve_limit=-1,
+                result_cls=BidRow,
+                is_single_direct_relation=True,
+            ),
         )
-        if retrieve_input_:
-            self._query_append_input_(from_)
-        return PreprocessorInputQueryAPI(self._client, self._builder, self._view_by_read_class, None, limit)
-
-    def query(
-        self,
-        retrieve_input_: bool = False,
-    ) -> T_DomainModelList:
-        """Execute query and return the result.
-
-        Args:
-            retrieve_input_: Whether to retrieve the input for each task dispatcher shop output or not.
-
-        Returns:
-            The list of the source nodes of the query.
-
-        """
-        from_ = self._builder[-1].name
-        if retrieve_input_:
-            self._query_append_input_(from_)
-        return self._query()
 
-    def _query_append_input_(self, from_: str) -> None:
-        view_id = self._view_by_read_class[TaskDispatcherShopInput]
+    def _query_append_method(self, from_: str) -> None:
+        view_id = self._view_by_read_class[BidMethod]
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("input_"),
+                name=self._builder.next_name("method"),
                 expression=dm.query.NodeResultSetExpression(
                     filter=dm.filters.HasData(views=[view_id]),
                     from_=from_,
-                    through=self._view_by_read_class[TaskDispatcherShopOutput].as_property_ref("input"),
+                    through=self._view_by_read_class[BidRow].as_property_ref("method"),
                     direction="outwards",
                 ),
                 select=dm.query.Select([dm.query.SourceSelector(view_id, ["*"])]),
                 max_retrieve_limit=-1,
-                result_cls=TaskDispatcherShopInput,
+                result_cls=BidMethod,
+                is_single_direct_relation=True,
             ),
         )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/task_dispatcher_water_input.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/task_dispatcher_water_input.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/task_dispatcher_water_input_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/task_dispatcher_water_input_query.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/task_dispatcher_water_output.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/task_dispatcher_water_output.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/task_dispatcher_water_output_alerts.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/task_dispatcher_water_output_alerts.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/task_dispatcher_water_output_bid_calculation_tasks.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/task_dispatcher_water_output_bid_calculation_tasks.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/task_dispatcher_water_output_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/_api/bid_matrix_query.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,169 +1,181 @@
 from __future__ import annotations
 
 import datetime
 from typing import TYPE_CHECKING
 
 from cognite.client import data_modeling as dm, CogniteClient
 
-from cognite.powerops.client._generated.v1.data_classes import (
+from cognite.powerops.client._generated.day_ahead_bid.data_classes import (
     DomainModelCore,
-    TaskDispatcherWaterOutput,
-    TaskDispatcherWaterInput,
+    BidMatrix,
+    BidMethod,
+)
+from cognite.powerops.client._generated.day_ahead_bid.data_classes._alert import (
+    Alert,
+    _create_alert_filter,
 )
 from ._core import DEFAULT_QUERY_LIMIT, QueryBuilder, QueryStep, QueryAPI, T_DomainModelList, _create_edge_filter
 
 if TYPE_CHECKING:
     from .alert_query import AlertQueryAPI
-    from .water_partial_bid_calculation_input_query import WaterPartialBidCalculationInputQueryAPI
 
 
-class TaskDispatcherWaterOutputQueryAPI(QueryAPI[T_DomainModelList]):
+class BidMatrixQueryAPI(QueryAPI[T_DomainModelList]):
     def __init__(
         self,
         client: CogniteClient,
         builder: QueryBuilder[T_DomainModelList],
         view_by_read_class: dict[type[DomainModelCore], dm.ViewId],
         filter_: dm.filters.Filter | None = None,
         limit: int = DEFAULT_QUERY_LIMIT,
     ):
         super().__init__(client, builder, view_by_read_class)
 
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("task_dispatcher_water_output"),
+                name=self._builder.next_name("bid_matrix"),
                 expression=dm.query.NodeResultSetExpression(
                     from_=self._builder[-1].name if self._builder else None,
                     filter=filter_,
                 ),
-                select=dm.query.Select(
-                    [dm.query.SourceSelector(self._view_by_read_class[TaskDispatcherWaterOutput], ["*"])]
-                ),
-                result_cls=TaskDispatcherWaterOutput,
+                select=dm.query.Select([dm.query.SourceSelector(self._view_by_read_class[BidMatrix], ["*"])]),
+                result_cls=BidMatrix,
                 max_retrieve_limit=limit,
             )
         )
 
     def alerts(
         self,
+        min_time: datetime.datetime | None = None,
+        max_time: datetime.datetime | None = None,
+        title: str | list[str] | None = None,
+        title_prefix: str | None = None,
+        description: str | list[str] | None = None,
+        description_prefix: str | None = None,
+        severity: str | list[str] | None = None,
+        severity_prefix: str | None = None,
+        alert_type: str | list[str] | None = None,
+        alert_type_prefix: str | None = None,
+        min_status_code: int | None = None,
+        max_status_code: int | None = None,
+        calculation_run: str | list[str] | None = None,
+        calculation_run_prefix: str | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
+        external_id_prefix_edge: str | None = None,
+        space_edge: str | list[str] | None = None,
+        filter: dm.Filter | None = None,
         limit: int | None = DEFAULT_QUERY_LIMIT,
-        retrieve_input_: bool = False,
+        retrieve_method: bool = False,
     ) -> AlertQueryAPI[T_DomainModelList]:
-        """Query along the alert edges of the task dispatcher water output.
+        """Query along the alert edges of the bid matrix.
 
         Args:
+            min_time: The minimum value of the time to filter on.
+            max_time: The maximum value of the time to filter on.
+            title: The title to filter on.
+            title_prefix: The prefix of the title to filter on.
+            description: The description to filter on.
+            description_prefix: The prefix of the description to filter on.
+            severity: The severity to filter on.
+            severity_prefix: The prefix of the severity to filter on.
+            alert_type: The alert type to filter on.
+            alert_type_prefix: The prefix of the alert type to filter on.
+            min_status_code: The minimum value of the status code to filter on.
+            max_status_code: The maximum value of the status code to filter on.
+            calculation_run: The calculation run to filter on.
+            calculation_run_prefix: The prefix of the calculation run to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of alert edges to return. Defaults to 25. Set to -1, float("inf") or None
+            external_id_prefix_edge: The prefix of the external ID to filter on.
+            space_edge: The space to filter on.
+            filter: (Advanced) Filter applied to node. If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
+            limit: Maximum number of alert edges to return. Defaults to 3. Set to -1, float("inf") or None
                 to return all items.
-            retrieve_input_: Whether to retrieve the input for each task dispatcher water output or not.
+            retrieve_method: Whether to retrieve the method for each bid matrix or not.
 
         Returns:
             AlertQueryAPI: The query API for the alert.
         """
         from .alert_query import AlertQueryAPI
 
         from_ = self._builder[-1].name
-
         edge_filter = _create_edge_filter(
-            dm.DirectRelationReference("sp_powerops_types", "calculationIssue"),
-            external_id_prefix=external_id_prefix,
-            space=space,
+            dm.DirectRelationReference("power-ops-types", "calculationIssue"),
+            external_id_prefix=external_id_prefix_edge,
+            space=space_edge,
         )
         self._builder.append(
             QueryStep(
                 name=self._builder.next_name("alerts"),
                 expression=dm.query.EdgeResultSetExpression(
                     filter=edge_filter,
                     from_=from_,
                     direction="outwards",
                 ),
                 select=dm.query.Select(),
                 max_retrieve_limit=limit,
             )
         )
-        if retrieve_input_:
-            self._query_append_input_(from_)
-        return AlertQueryAPI(self._client, self._builder, self._view_by_read_class, None, limit)
-
-    def bid_calculation_tasks(
-        self,
-        external_id_prefix: str | None = None,
-        space: str | list[str] | None = None,
-        limit: int | None = DEFAULT_QUERY_LIMIT,
-        retrieve_input_: bool = False,
-    ) -> WaterPartialBidCalculationInputQueryAPI[T_DomainModelList]:
-        """Query along the bid calculation task edges of the task dispatcher water output.
-
-        Args:
-            external_id_prefix: The prefix of the external ID to filter on.
-            space: The space to filter on.
-            limit: Maximum number of bid calculation task edges to return. Defaults to 25. Set to -1, float("inf") or None
-                to return all items.
-            retrieve_input_: Whether to retrieve the input for each task dispatcher water output or not.
-
-        Returns:
-            WaterPartialBidCalculationInputQueryAPI: The query API for the water partial bid calculation input.
-        """
-        from .water_partial_bid_calculation_input_query import WaterPartialBidCalculationInputQueryAPI
 
-        from_ = self._builder[-1].name
-
-        edge_filter = _create_edge_filter(
-            dm.DirectRelationReference("sp_powerops_types", "Water.partialBidCalculations"),
-            external_id_prefix=external_id_prefix,
-            space=space,
-        )
-        self._builder.append(
-            QueryStep(
-                name=self._builder.next_name("bid_calculation_tasks"),
-                expression=dm.query.EdgeResultSetExpression(
-                    filter=edge_filter,
-                    from_=from_,
-                    direction="outwards",
-                ),
-                select=dm.query.Select(),
-                max_retrieve_limit=limit,
-            )
-        )
-        if retrieve_input_:
-            self._query_append_input_(from_)
-        return WaterPartialBidCalculationInputQueryAPI(
-            self._client, self._builder, self._view_by_read_class, None, limit
-        )
+        view_id = self._view_by_read_class[Alert]
+        has_data = dm.filters.HasData(views=[view_id])
+        node_filer = _create_alert_filter(
+            view_id,
+            min_time,
+            max_time,
+            title,
+            title_prefix,
+            description,
+            description_prefix,
+            severity,
+            severity_prefix,
+            alert_type,
+            alert_type_prefix,
+            min_status_code,
+            max_status_code,
+            calculation_run,
+            calculation_run_prefix,
+            external_id_prefix,
+            space,
+            (filter and dm.filters.And(filter, has_data)) or has_data,
+        )
+        if retrieve_method:
+            self._query_append_method(from_)
+        return AlertQueryAPI(self._client, self._builder, self._view_by_read_class, node_filer, limit)
 
     def query(
         self,
-        retrieve_input_: bool = False,
+        retrieve_method: bool = False,
     ) -> T_DomainModelList:
         """Execute query and return the result.
 
         Args:
-            retrieve_input_: Whether to retrieve the input for each task dispatcher water output or not.
+            retrieve_method: Whether to retrieve the method for each bid matrix or not.
 
         Returns:
             The list of the source nodes of the query.
 
         """
         from_ = self._builder[-1].name
-        if retrieve_input_:
-            self._query_append_input_(from_)
+        if retrieve_method:
+            self._query_append_method(from_)
         return self._query()
 
-    def _query_append_input_(self, from_: str) -> None:
-        view_id = self._view_by_read_class[TaskDispatcherWaterInput]
+    def _query_append_method(self, from_: str) -> None:
+        view_id = self._view_by_read_class[BidMethod]
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("input_"),
+                name=self._builder.next_name("method"),
                 expression=dm.query.NodeResultSetExpression(
                     filter=dm.filters.HasData(views=[view_id]),
                     from_=from_,
-                    through=self._view_by_read_class[TaskDispatcherWaterOutput].as_property_ref("input"),
+                    through=self._view_by_read_class[BidMatrix].as_property_ref("method"),
                     direction="outwards",
                 ),
                 select=dm.query.Select([dm.query.SourceSelector(view_id, ["*"])]),
                 max_retrieve_limit=-1,
-                result_cls=TaskDispatcherWaterInput,
+                result_cls=BidMethod,
+                is_single_direct_relation=True,
             ),
         )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/total_bid_matrix_calculation_input.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/total_bid_matrix_calculation_output.py`

 * *Files 10% similar despite different names*

```diff
@@ -9,320 +9,334 @@
 from cognite.client.data_classes.data_modeling.instances import InstanceAggregationResultList
 
 from cognite.powerops.client._generated.v1.data_classes._core import DEFAULT_INSTANCE_SPACE
 from cognite.powerops.client._generated.v1.data_classes import (
     DomainModelCore,
     DomainModelWrite,
     ResourcesWriteResult,
-    TotalBidMatrixCalculationInput,
-    TotalBidMatrixCalculationInputWrite,
-    TotalBidMatrixCalculationInputFields,
-    TotalBidMatrixCalculationInputList,
-    TotalBidMatrixCalculationInputWriteList,
-    TotalBidMatrixCalculationInputTextFields,
+    TotalBidMatrixCalculationOutput,
+    TotalBidMatrixCalculationOutputWrite,
+    TotalBidMatrixCalculationOutputFields,
+    TotalBidMatrixCalculationOutputList,
+    TotalBidMatrixCalculationOutputWriteList,
+    TotalBidMatrixCalculationOutputTextFields,
 )
-from cognite.powerops.client._generated.v1.data_classes._total_bid_matrix_calculation_input import (
-    _TOTALBIDMATRIXCALCULATIONINPUT_PROPERTIES_BY_FIELD,
-    _create_total_bid_matrix_calculation_input_filter,
+from cognite.powerops.client._generated.v1.data_classes._total_bid_matrix_calculation_output import (
+    _TOTALBIDMATRIXCALCULATIONOUTPUT_PROPERTIES_BY_FIELD,
+    _create_total_bid_matrix_calculation_output_filter,
 )
 from ._core import (
     DEFAULT_LIMIT_READ,
     DEFAULT_QUERY_LIMIT,
     Aggregations,
     NodeAPI,
     SequenceNotStr,
     QueryStep,
     QueryBuilder,
 )
-from .total_bid_matrix_calculation_input_partial_bid_matrices import TotalBidMatrixCalculationInputPartialBidMatricesAPI
-from .total_bid_matrix_calculation_input_query import TotalBidMatrixCalculationInputQueryAPI
+from .total_bid_matrix_calculation_output_alerts import TotalBidMatrixCalculationOutputAlertsAPI
+from .total_bid_matrix_calculation_output_query import TotalBidMatrixCalculationOutputQueryAPI
 
 
-class TotalBidMatrixCalculationInputAPI(
-    NodeAPI[TotalBidMatrixCalculationInput, TotalBidMatrixCalculationInputWrite, TotalBidMatrixCalculationInputList]
+class TotalBidMatrixCalculationOutputAPI(
+    NodeAPI[TotalBidMatrixCalculationOutput, TotalBidMatrixCalculationOutputWrite, TotalBidMatrixCalculationOutputList]
 ):
     def __init__(self, client: CogniteClient, view_by_read_class: dict[type[DomainModelCore], dm.ViewId]):
-        view_id = view_by_read_class[TotalBidMatrixCalculationInput]
+        view_id = view_by_read_class[TotalBidMatrixCalculationOutput]
         super().__init__(
             client=client,
             sources=view_id,
-            class_type=TotalBidMatrixCalculationInput,
-            class_list=TotalBidMatrixCalculationInputList,
-            class_write_list=TotalBidMatrixCalculationInputWriteList,
+            class_type=TotalBidMatrixCalculationOutput,
+            class_list=TotalBidMatrixCalculationOutputList,
+            class_write_list=TotalBidMatrixCalculationOutputWriteList,
             view_by_read_class=view_by_read_class,
         )
         self._view_id = view_id
-        self.partial_bid_matrices_edge = TotalBidMatrixCalculationInputPartialBidMatricesAPI(client)
+        self.alerts_edge = TotalBidMatrixCalculationOutputAlertsAPI(client)
 
     def __call__(
         self,
         process_id: str | list[str] | None = None,
         process_id_prefix: str | None = None,
         min_process_step: int | None = None,
         max_process_step: int | None = None,
         function_name: str | list[str] | None = None,
         function_name_prefix: str | None = None,
         function_call_id: str | list[str] | None = None,
         function_call_id_prefix: str | None = None,
+        bid_document: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        input_: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_QUERY_LIMIT,
         filter: dm.Filter | None = None,
-    ) -> TotalBidMatrixCalculationInputQueryAPI[TotalBidMatrixCalculationInputList]:
-        """Query starting at total bid matrix calculation inputs.
+    ) -> TotalBidMatrixCalculationOutputQueryAPI[TotalBidMatrixCalculationOutputList]:
+        """Query starting at total bid matrix calculation outputs.
 
         Args:
             process_id: The process id to filter on.
             process_id_prefix: The prefix of the process id to filter on.
             min_process_step: The minimum value of the process step to filter on.
             max_process_step: The maximum value of the process step to filter on.
             function_name: The function name to filter on.
             function_name_prefix: The prefix of the function name to filter on.
             function_call_id: The function call id to filter on.
             function_call_id_prefix: The prefix of the function call id to filter on.
+            bid_document: The bid document to filter on.
+            input_: The input to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of total bid matrix calculation inputs to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of total bid matrix calculation outputs to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
-            A query API for total bid matrix calculation inputs.
+            A query API for total bid matrix calculation outputs.
 
         """
         has_data = dm.filters.HasData(views=[self._view_id])
-        filter_ = _create_total_bid_matrix_calculation_input_filter(
+        filter_ = _create_total_bid_matrix_calculation_output_filter(
             self._view_id,
             process_id,
             process_id_prefix,
             min_process_step,
             max_process_step,
             function_name,
             function_name_prefix,
             function_call_id,
             function_call_id_prefix,
+            bid_document,
+            input_,
             external_id_prefix,
             space,
             (filter and dm.filters.And(filter, has_data)) or has_data,
         )
-        builder = QueryBuilder(TotalBidMatrixCalculationInputList)
-        return TotalBidMatrixCalculationInputQueryAPI(self._client, builder, self._view_by_read_class, filter_, limit)
+        builder = QueryBuilder(TotalBidMatrixCalculationOutputList)
+        return TotalBidMatrixCalculationOutputQueryAPI(self._client, builder, self._view_by_read_class, filter_, limit)
 
     def apply(
         self,
-        total_bid_matrix_calculation_input: (
-            TotalBidMatrixCalculationInputWrite | Sequence[TotalBidMatrixCalculationInputWrite]
+        total_bid_matrix_calculation_output: (
+            TotalBidMatrixCalculationOutputWrite | Sequence[TotalBidMatrixCalculationOutputWrite]
         ),
         replace: bool = False,
         write_none: bool = False,
     ) -> ResourcesWriteResult:
-        """Add or update (upsert) total bid matrix calculation inputs.
+        """Add or update (upsert) total bid matrix calculation outputs.
 
-        Note: This method iterates through all nodes and timeseries linked to total_bid_matrix_calculation_input and creates them including the edges
-        between the nodes. For example, if any of `partial_bid_matrices` are set, then these
+        Note: This method iterates through all nodes and timeseries linked to total_bid_matrix_calculation_output and creates them including the edges
+        between the nodes. For example, if any of `alerts` are set, then these
         nodes as well as any nodes linked to them, and all the edges linking these nodes will be created.
 
         Args:
-            total_bid_matrix_calculation_input: Total bid matrix calculation input or sequence of total bid matrix calculation inputs to upsert.
+            total_bid_matrix_calculation_output: Total bid matrix calculation output or sequence of total bid matrix calculation outputs to upsert.
             replace (bool): How do we behave when a property value exists? Do we replace all matching and existing values with the supplied values (true)?
                 Or should we merge in new values for properties together with the existing values (false)? Note: This setting applies for all nodes or edges specified in the ingestion call.
             write_none (bool): This method, will by default, skip properties that are set to None. However, if you want to set properties to None,
                 you can set this parameter to True. Note this only applies to properties that are nullable.
         Returns:
             Created instance(s), i.e., nodes, edges, and time series.
 
         Examples:
 
-            Create a new total_bid_matrix_calculation_input:
+            Create a new total_bid_matrix_calculation_output:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
-                >>> from cognite.powerops.client._generated.v1.data_classes import TotalBidMatrixCalculationInputWrite
+                >>> from cognite.powerops.client._generated.v1.data_classes import TotalBidMatrixCalculationOutputWrite
                 >>> client = PowerOpsModelsV1Client()
-                >>> total_bid_matrix_calculation_input = TotalBidMatrixCalculationInputWrite(external_id="my_total_bid_matrix_calculation_input", ...)
-                >>> result = client.total_bid_matrix_calculation_input.apply(total_bid_matrix_calculation_input)
+                >>> total_bid_matrix_calculation_output = TotalBidMatrixCalculationOutputWrite(external_id="my_total_bid_matrix_calculation_output", ...)
+                >>> result = client.total_bid_matrix_calculation_output.apply(total_bid_matrix_calculation_output)
 
         """
         warnings.warn(
             "The .apply method is deprecated and will be removed in v1.0. "
             "Please use the .upsert method on the client instead. This means instead of "
-            "`my_client.total_bid_matrix_calculation_input.apply(my_items)` please use `my_client.upsert(my_items)`."
+            "`my_client.total_bid_matrix_calculation_output.apply(my_items)` please use `my_client.upsert(my_items)`."
             "The motivation is that all apply methods are the same, and having one apply method per API "
             " class encourages users to create items in small batches, which is inefficient."
             "In addition, .upsert method is more descriptive of what the method does.",
             UserWarning,
             stacklevel=2,
         )
-        return self._apply(total_bid_matrix_calculation_input, replace, write_none)
+        return self._apply(total_bid_matrix_calculation_output, replace, write_none)
 
     def delete(
         self, external_id: str | SequenceNotStr[str], space: str = DEFAULT_INSTANCE_SPACE
     ) -> dm.InstancesDeleteResult:
-        """Delete one or more total bid matrix calculation input.
+        """Delete one or more total bid matrix calculation output.
 
         Args:
-            external_id: External id of the total bid matrix calculation input to delete.
-            space: The space where all the total bid matrix calculation input are located.
+            external_id: External id of the total bid matrix calculation output to delete.
+            space: The space where all the total bid matrix calculation output are located.
 
         Returns:
             The instance(s), i.e., nodes and edges which has been deleted. Empty list if nothing was deleted.
 
         Examples:
 
-            Delete total_bid_matrix_calculation_input by id:
+            Delete total_bid_matrix_calculation_output by id:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> client.total_bid_matrix_calculation_input.delete("my_total_bid_matrix_calculation_input")
+                >>> client.total_bid_matrix_calculation_output.delete("my_total_bid_matrix_calculation_output")
         """
         warnings.warn(
             "The .delete method is deprecated and will be removed in v1.0. "
             "Please use the .delete method on the client instead. This means instead of "
-            "`my_client.total_bid_matrix_calculation_input.delete(my_ids)` please use `my_client.delete(my_ids)`."
+            "`my_client.total_bid_matrix_calculation_output.delete(my_ids)` please use `my_client.delete(my_ids)`."
             "The motivation is that all delete methods are the same, and having one delete method per API "
             " class encourages users to delete items in small batches, which is inefficient.",
             UserWarning,
             stacklevel=2,
         )
         return self._delete(external_id, space)
 
     @overload
     def retrieve(
         self, external_id: str, space: str = DEFAULT_INSTANCE_SPACE
-    ) -> TotalBidMatrixCalculationInput | None: ...
+    ) -> TotalBidMatrixCalculationOutput | None: ...
 
     @overload
     def retrieve(
         self, external_id: SequenceNotStr[str], space: str = DEFAULT_INSTANCE_SPACE
-    ) -> TotalBidMatrixCalculationInputList: ...
+    ) -> TotalBidMatrixCalculationOutputList: ...
 
     def retrieve(
         self, external_id: str | SequenceNotStr[str], space: str = DEFAULT_INSTANCE_SPACE
-    ) -> TotalBidMatrixCalculationInput | TotalBidMatrixCalculationInputList | None:
-        """Retrieve one or more total bid matrix calculation inputs by id(s).
+    ) -> TotalBidMatrixCalculationOutput | TotalBidMatrixCalculationOutputList | None:
+        """Retrieve one or more total bid matrix calculation outputs by id(s).
 
         Args:
-            external_id: External id or list of external ids of the total bid matrix calculation inputs.
-            space: The space where all the total bid matrix calculation inputs are located.
+            external_id: External id or list of external ids of the total bid matrix calculation outputs.
+            space: The space where all the total bid matrix calculation outputs are located.
 
         Returns:
-            The requested total bid matrix calculation inputs.
+            The requested total bid matrix calculation outputs.
 
         Examples:
 
-            Retrieve total_bid_matrix_calculation_input by id:
+            Retrieve total_bid_matrix_calculation_output by id:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> total_bid_matrix_calculation_input = client.total_bid_matrix_calculation_input.retrieve("my_total_bid_matrix_calculation_input")
+                >>> total_bid_matrix_calculation_output = client.total_bid_matrix_calculation_output.retrieve("my_total_bid_matrix_calculation_output")
 
         """
         return self._retrieve(
             external_id,
             space,
             retrieve_edges=True,
             edge_api_name_type_direction_view_id_penta=[
                 (
-                    self.partial_bid_matrices_edge,
-                    "partial_bid_matrices",
-                    dm.DirectRelationReference("sp_powerops_types", "BidMatrix"),
+                    self.alerts_edge,
+                    "alerts",
+                    dm.DirectRelationReference("sp_powerops_types_temp", "calculationIssue"),
                     "outwards",
-                    dm.ViewId("sp_powerops_models", "BidMatrix", "1"),
+                    dm.ViewId("sp_powerops_models_temp", "Alert", "1"),
                 ),
             ],
         )
 
     def search(
         self,
         query: str,
         properties: (
-            TotalBidMatrixCalculationInputTextFields | Sequence[TotalBidMatrixCalculationInputTextFields] | None
+            TotalBidMatrixCalculationOutputTextFields | Sequence[TotalBidMatrixCalculationOutputTextFields] | None
         ) = None,
         process_id: str | list[str] | None = None,
         process_id_prefix: str | None = None,
         min_process_step: int | None = None,
         max_process_step: int | None = None,
         function_name: str | list[str] | None = None,
         function_name_prefix: str | None = None,
         function_call_id: str | list[str] | None = None,
         function_call_id_prefix: str | None = None,
+        bid_document: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        input_: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
-    ) -> TotalBidMatrixCalculationInputList:
-        """Search total bid matrix calculation inputs
+    ) -> TotalBidMatrixCalculationOutputList:
+        """Search total bid matrix calculation outputs
 
         Args:
             query: The search query,
             properties: The property to search, if nothing is passed all text fields will be searched.
             process_id: The process id to filter on.
             process_id_prefix: The prefix of the process id to filter on.
             min_process_step: The minimum value of the process step to filter on.
             max_process_step: The maximum value of the process step to filter on.
             function_name: The function name to filter on.
             function_name_prefix: The prefix of the function name to filter on.
             function_call_id: The function call id to filter on.
             function_call_id_prefix: The prefix of the function call id to filter on.
+            bid_document: The bid document to filter on.
+            input_: The input to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of total bid matrix calculation inputs to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of total bid matrix calculation outputs to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
-            Search results total bid matrix calculation inputs matching the query.
+            Search results total bid matrix calculation outputs matching the query.
 
         Examples:
 
-           Search for 'my_total_bid_matrix_calculation_input' in all text properties:
+           Search for 'my_total_bid_matrix_calculation_output' in all text properties:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> total_bid_matrix_calculation_inputs = client.total_bid_matrix_calculation_input.search('my_total_bid_matrix_calculation_input')
+                >>> total_bid_matrix_calculation_outputs = client.total_bid_matrix_calculation_output.search('my_total_bid_matrix_calculation_output')
 
         """
-        filter_ = _create_total_bid_matrix_calculation_input_filter(
+        filter_ = _create_total_bid_matrix_calculation_output_filter(
             self._view_id,
             process_id,
             process_id_prefix,
             min_process_step,
             max_process_step,
             function_name,
             function_name_prefix,
             function_call_id,
             function_call_id_prefix,
+            bid_document,
+            input_,
             external_id_prefix,
             space,
             filter,
         )
         return self._search(
-            self._view_id, query, _TOTALBIDMATRIXCALCULATIONINPUT_PROPERTIES_BY_FIELD, properties, filter_, limit
+            self._view_id, query, _TOTALBIDMATRIXCALCULATIONOUTPUT_PROPERTIES_BY_FIELD, properties, filter_, limit
         )
 
     @overload
     def aggregate(
         self,
         aggregations: (
             Aggregations
             | dm.aggregations.MetricAggregation
             | Sequence[Aggregations]
             | Sequence[dm.aggregations.MetricAggregation]
         ),
-        property: TotalBidMatrixCalculationInputFields | Sequence[TotalBidMatrixCalculationInputFields] | None = None,
+        property: TotalBidMatrixCalculationOutputFields | Sequence[TotalBidMatrixCalculationOutputFields] | None = None,
         group_by: None = None,
         query: str | None = None,
         search_properties: (
-            TotalBidMatrixCalculationInputTextFields | Sequence[TotalBidMatrixCalculationInputTextFields] | None
+            TotalBidMatrixCalculationOutputTextFields | Sequence[TotalBidMatrixCalculationOutputTextFields] | None
         ) = None,
         process_id: str | list[str] | None = None,
         process_id_prefix: str | None = None,
         min_process_step: int | None = None,
         max_process_step: int | None = None,
         function_name: str | list[str] | None = None,
         function_name_prefix: str | None = None,
         function_call_id: str | list[str] | None = None,
         function_call_id_prefix: str | None = None,
+        bid_document: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        input_: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> list[dm.aggregations.AggregatedNumberedValue]: ...
 
     @overload
@@ -330,62 +344,66 @@
         self,
         aggregations: (
             Aggregations
             | dm.aggregations.MetricAggregation
             | Sequence[Aggregations]
             | Sequence[dm.aggregations.MetricAggregation]
         ),
-        property: TotalBidMatrixCalculationInputFields | Sequence[TotalBidMatrixCalculationInputFields] | None = None,
-        group_by: TotalBidMatrixCalculationInputFields | Sequence[TotalBidMatrixCalculationInputFields] = None,
+        property: TotalBidMatrixCalculationOutputFields | Sequence[TotalBidMatrixCalculationOutputFields] | None = None,
+        group_by: TotalBidMatrixCalculationOutputFields | Sequence[TotalBidMatrixCalculationOutputFields] = None,
         query: str | None = None,
         search_properties: (
-            TotalBidMatrixCalculationInputTextFields | Sequence[TotalBidMatrixCalculationInputTextFields] | None
+            TotalBidMatrixCalculationOutputTextFields | Sequence[TotalBidMatrixCalculationOutputTextFields] | None
         ) = None,
         process_id: str | list[str] | None = None,
         process_id_prefix: str | None = None,
         min_process_step: int | None = None,
         max_process_step: int | None = None,
         function_name: str | list[str] | None = None,
         function_name_prefix: str | None = None,
         function_call_id: str | list[str] | None = None,
         function_call_id_prefix: str | None = None,
+        bid_document: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        input_: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> InstanceAggregationResultList: ...
 
     def aggregate(
         self,
         aggregate: (
             Aggregations
             | dm.aggregations.MetricAggregation
             | Sequence[Aggregations]
             | Sequence[dm.aggregations.MetricAggregation]
         ),
-        property: TotalBidMatrixCalculationInputFields | Sequence[TotalBidMatrixCalculationInputFields] | None = None,
-        group_by: TotalBidMatrixCalculationInputFields | Sequence[TotalBidMatrixCalculationInputFields] | None = None,
+        property: TotalBidMatrixCalculationOutputFields | Sequence[TotalBidMatrixCalculationOutputFields] | None = None,
+        group_by: TotalBidMatrixCalculationOutputFields | Sequence[TotalBidMatrixCalculationOutputFields] | None = None,
         query: str | None = None,
         search_property: (
-            TotalBidMatrixCalculationInputTextFields | Sequence[TotalBidMatrixCalculationInputTextFields] | None
+            TotalBidMatrixCalculationOutputTextFields | Sequence[TotalBidMatrixCalculationOutputTextFields] | None
         ) = None,
         process_id: str | list[str] | None = None,
         process_id_prefix: str | None = None,
         min_process_step: int | None = None,
         max_process_step: int | None = None,
         function_name: str | list[str] | None = None,
         function_name_prefix: str | None = None,
         function_call_id: str | list[str] | None = None,
         function_call_id_prefix: str | None = None,
+        bid_document: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        input_: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> list[dm.aggregations.AggregatedNumberedValue] | InstanceAggregationResultList:
-        """Aggregate data across total bid matrix calculation inputs
+        """Aggregate data across total bid matrix calculation outputs
 
         Args:
             aggregate: The aggregation to perform.
             property: The property to perform aggregation on.
             group_by: The property to group by when doing the aggregation.
             query: The query to search for in the text field.
             search_property: The text field to search in.
@@ -393,122 +411,132 @@
             process_id_prefix: The prefix of the process id to filter on.
             min_process_step: The minimum value of the process step to filter on.
             max_process_step: The maximum value of the process step to filter on.
             function_name: The function name to filter on.
             function_name_prefix: The prefix of the function name to filter on.
             function_call_id: The function call id to filter on.
             function_call_id_prefix: The prefix of the function call id to filter on.
+            bid_document: The bid document to filter on.
+            input_: The input to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of total bid matrix calculation inputs to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of total bid matrix calculation outputs to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
             Aggregation results.
 
         Examples:
 
-            Count total bid matrix calculation inputs in space `my_space`:
+            Count total bid matrix calculation outputs in space `my_space`:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> result = client.total_bid_matrix_calculation_input.aggregate("count", space="my_space")
+                >>> result = client.total_bid_matrix_calculation_output.aggregate("count", space="my_space")
 
         """
 
-        filter_ = _create_total_bid_matrix_calculation_input_filter(
+        filter_ = _create_total_bid_matrix_calculation_output_filter(
             self._view_id,
             process_id,
             process_id_prefix,
             min_process_step,
             max_process_step,
             function_name,
             function_name_prefix,
             function_call_id,
             function_call_id_prefix,
+            bid_document,
+            input_,
             external_id_prefix,
             space,
             filter,
         )
         return self._aggregate(
             self._view_id,
             aggregate,
-            _TOTALBIDMATRIXCALCULATIONINPUT_PROPERTIES_BY_FIELD,
+            _TOTALBIDMATRIXCALCULATIONOUTPUT_PROPERTIES_BY_FIELD,
             property,
             group_by,
             query,
             search_property,
             limit,
             filter_,
         )
 
     def histogram(
         self,
-        property: TotalBidMatrixCalculationInputFields,
+        property: TotalBidMatrixCalculationOutputFields,
         interval: float,
         query: str | None = None,
         search_property: (
-            TotalBidMatrixCalculationInputTextFields | Sequence[TotalBidMatrixCalculationInputTextFields] | None
+            TotalBidMatrixCalculationOutputTextFields | Sequence[TotalBidMatrixCalculationOutputTextFields] | None
         ) = None,
         process_id: str | list[str] | None = None,
         process_id_prefix: str | None = None,
         min_process_step: int | None = None,
         max_process_step: int | None = None,
         function_name: str | list[str] | None = None,
         function_name_prefix: str | None = None,
         function_call_id: str | list[str] | None = None,
         function_call_id_prefix: str | None = None,
+        bid_document: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        input_: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> dm.aggregations.HistogramValue:
-        """Produces histograms for total bid matrix calculation inputs
+        """Produces histograms for total bid matrix calculation outputs
 
         Args:
             property: The property to use as the value in the histogram.
             interval: The interval to use for the histogram bins.
             query: The query to search for in the text field.
             search_property: The text field to search in.
             process_id: The process id to filter on.
             process_id_prefix: The prefix of the process id to filter on.
             min_process_step: The minimum value of the process step to filter on.
             max_process_step: The maximum value of the process step to filter on.
             function_name: The function name to filter on.
             function_name_prefix: The prefix of the function name to filter on.
             function_call_id: The function call id to filter on.
             function_call_id_prefix: The prefix of the function call id to filter on.
+            bid_document: The bid document to filter on.
+            input_: The input to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of total bid matrix calculation inputs to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of total bid matrix calculation outputs to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
             Bucketed histogram results.
 
         """
-        filter_ = _create_total_bid_matrix_calculation_input_filter(
+        filter_ = _create_total_bid_matrix_calculation_output_filter(
             self._view_id,
             process_id,
             process_id_prefix,
             min_process_step,
             max_process_step,
             function_name,
             function_name_prefix,
             function_call_id,
             function_call_id_prefix,
+            bid_document,
+            input_,
             external_id_prefix,
             space,
             filter,
         )
         return self._histogram(
             self._view_id,
             property,
             interval,
-            _TOTALBIDMATRIXCALCULATIONINPUT_PROPERTIES_BY_FIELD,
+            _TOTALBIDMATRIXCALCULATIONOUTPUT_PROPERTIES_BY_FIELD,
             query,
             search_property,
             limit,
             filter_,
         )
 
     def list(
@@ -517,71 +545,77 @@
         process_id_prefix: str | None = None,
         min_process_step: int | None = None,
         max_process_step: int | None = None,
         function_name: str | list[str] | None = None,
         function_name_prefix: str | None = None,
         function_call_id: str | list[str] | None = None,
         function_call_id_prefix: str | None = None,
+        bid_document: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        input_: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
         retrieve_edges: bool = True,
-    ) -> TotalBidMatrixCalculationInputList:
-        """List/filter total bid matrix calculation inputs
+    ) -> TotalBidMatrixCalculationOutputList:
+        """List/filter total bid matrix calculation outputs
 
         Args:
             process_id: The process id to filter on.
             process_id_prefix: The prefix of the process id to filter on.
             min_process_step: The minimum value of the process step to filter on.
             max_process_step: The maximum value of the process step to filter on.
             function_name: The function name to filter on.
             function_name_prefix: The prefix of the function name to filter on.
             function_call_id: The function call id to filter on.
             function_call_id_prefix: The prefix of the function call id to filter on.
+            bid_document: The bid document to filter on.
+            input_: The input to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of total bid matrix calculation inputs to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of total bid matrix calculation outputs to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
-            retrieve_edges: Whether to retrieve `partial_bid_matrices` external ids for the total bid matrix calculation inputs. Defaults to True.
+            retrieve_edges: Whether to retrieve `alerts` external ids for the total bid matrix calculation outputs. Defaults to True.
 
         Returns:
-            List of requested total bid matrix calculation inputs
+            List of requested total bid matrix calculation outputs
 
         Examples:
 
-            List total bid matrix calculation inputs and limit to 5:
+            List total bid matrix calculation outputs and limit to 5:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> total_bid_matrix_calculation_inputs = client.total_bid_matrix_calculation_input.list(limit=5)
+                >>> total_bid_matrix_calculation_outputs = client.total_bid_matrix_calculation_output.list(limit=5)
 
         """
-        filter_ = _create_total_bid_matrix_calculation_input_filter(
+        filter_ = _create_total_bid_matrix_calculation_output_filter(
             self._view_id,
             process_id,
             process_id_prefix,
             min_process_step,
             max_process_step,
             function_name,
             function_name_prefix,
             function_call_id,
             function_call_id_prefix,
+            bid_document,
+            input_,
             external_id_prefix,
             space,
             filter,
         )
 
         return self._list(
             limit=limit,
             filter=filter_,
             retrieve_edges=retrieve_edges,
             edge_api_name_type_direction_view_id_penta=[
                 (
-                    self.partial_bid_matrices_edge,
-                    "partial_bid_matrices",
-                    dm.DirectRelationReference("sp_powerops_types", "BidMatrix"),
+                    self.alerts_edge,
+                    "alerts",
+                    dm.DirectRelationReference("sp_powerops_types_temp", "calculationIssue"),
                     "outwards",
-                    dm.ViewId("sp_powerops_models", "BidMatrix", "1"),
+                    dm.ViewId("sp_powerops_models_temp", "Alert", "1"),
                 ),
             ],
         )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/total_bid_matrix_calculation_input_partial_bid_matrices.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/total_bid_matrix_calculation_input_partial_bid_matrices.py`

 * *Files 1% similar despite different names*

```diff
@@ -39,15 +39,15 @@
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
                 >>> total_bid_matrix_calculation_input = client.total_bid_matrix_calculation_input.partial_bid_matrices_edge.list("my_total_bid_matrix_calculation_input", limit=5)
 
         """
         filter_ = _create_edge_filter(
-            dm.DirectRelationReference("sp_powerops_types", "BidMatrix"),
+            dm.DirectRelationReference("sp_powerops_types_temp", "BidMatrix"),
             from_total_bid_matrix_calculation_input,
             from_total_bid_matrix_calculation_input_space,
             to_bid_matrix,
             to_bid_matrix_space,
             external_id_prefix,
             space,
         )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/total_bid_matrix_calculation_input_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/partial_bid_matrix_calculation_input_query.py`

 * *Files 18% similar despite different names*

```diff
@@ -3,91 +3,76 @@
 import datetime
 from typing import TYPE_CHECKING
 
 from cognite.client import data_modeling as dm, CogniteClient
 
 from cognite.powerops.client._generated.v1.data_classes import (
     DomainModelCore,
-    TotalBidMatrixCalculationInput,
+    PartialBidMatrixCalculationInput,
+    BidConfiguration,
 )
 from ._core import DEFAULT_QUERY_LIMIT, QueryBuilder, QueryStep, QueryAPI, T_DomainModelList, _create_edge_filter
 
-if TYPE_CHECKING:
-    from .bid_matrix_query import BidMatrixQueryAPI
 
-
-class TotalBidMatrixCalculationInputQueryAPI(QueryAPI[T_DomainModelList]):
+class PartialBidMatrixCalculationInputQueryAPI(QueryAPI[T_DomainModelList]):
     def __init__(
         self,
         client: CogniteClient,
         builder: QueryBuilder[T_DomainModelList],
         view_by_read_class: dict[type[DomainModelCore], dm.ViewId],
         filter_: dm.filters.Filter | None = None,
         limit: int = DEFAULT_QUERY_LIMIT,
     ):
         super().__init__(client, builder, view_by_read_class)
 
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("total_bid_matrix_calculation_input"),
+                name=self._builder.next_name("partial_bid_matrix_calculation_input"),
                 expression=dm.query.NodeResultSetExpression(
                     from_=self._builder[-1].name if self._builder else None,
                     filter=filter_,
                 ),
                 select=dm.query.Select(
-                    [dm.query.SourceSelector(self._view_by_read_class[TotalBidMatrixCalculationInput], ["*"])]
+                    [dm.query.SourceSelector(self._view_by_read_class[PartialBidMatrixCalculationInput], ["*"])]
                 ),
-                result_cls=TotalBidMatrixCalculationInput,
+                result_cls=PartialBidMatrixCalculationInput,
                 max_retrieve_limit=limit,
             )
         )
 
-    def partial_bid_matrices(
+    def query(
         self,
-        external_id_prefix: str | None = None,
-        space: str | list[str] | None = None,
-        limit: int | None = DEFAULT_QUERY_LIMIT,
-    ) -> BidMatrixQueryAPI[T_DomainModelList]:
-        """Query along the partial bid matrice edges of the total bid matrix calculation input.
+        retrieve_bid_configuration: bool = False,
+    ) -> T_DomainModelList:
+        """Execute query and return the result.
 
         Args:
-            external_id_prefix: The prefix of the external ID to filter on.
-            space: The space to filter on.
-            limit: Maximum number of partial bid matrice edges to return. Defaults to 25. Set to -1, float("inf") or None
-                to return all items.
+            retrieve_bid_configuration: Whether to retrieve the bid configuration for each partial bid matrix calculation input or not.
 
         Returns:
-            BidMatrixQueryAPI: The query API for the bid matrix.
-        """
-        from .bid_matrix_query import BidMatrixQueryAPI
+            The list of the source nodes of the query.
 
+        """
         from_ = self._builder[-1].name
+        if retrieve_bid_configuration:
+            self._query_append_bid_configuration(from_)
+        return self._query()
 
-        edge_filter = _create_edge_filter(
-            dm.DirectRelationReference("sp_powerops_types", "BidMatrix"),
-            external_id_prefix=external_id_prefix,
-            space=space,
-        )
+    def _query_append_bid_configuration(self, from_: str) -> None:
+        view_id = self._view_by_read_class[BidConfiguration]
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("partial_bid_matrices"),
-                expression=dm.query.EdgeResultSetExpression(
-                    filter=edge_filter,
+                name=self._builder.next_name("bid_configuration"),
+                expression=dm.query.NodeResultSetExpression(
+                    filter=dm.filters.HasData(views=[view_id]),
                     from_=from_,
+                    through=self._view_by_read_class[PartialBidMatrixCalculationInput].as_property_ref(
+                        "bidConfiguration"
+                    ),
                     direction="outwards",
                 ),
-                select=dm.query.Select(),
-                max_retrieve_limit=limit,
-            )
+                select=dm.query.Select([dm.query.SourceSelector(view_id, ["*"])]),
+                max_retrieve_limit=-1,
+                result_cls=BidConfiguration,
+                is_single_direct_relation=True,
+            ),
         )
-        return BidMatrixQueryAPI(self._client, self._builder, self._view_by_read_class, None, limit)
-
-    def query(
-        self,
-    ) -> T_DomainModelList:
-        """Execute query and return the result.
-
-        Returns:
-            The list of the source nodes of the query.
-
-        """
-        return self._query()
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/total_bid_matrix_calculation_output.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/partial_bid_matrix_calculation_output.py`

 * *Files 12% similar despite different names*

```diff
@@ -9,333 +9,346 @@
 from cognite.client.data_classes.data_modeling.instances import InstanceAggregationResultList
 
 from cognite.powerops.client._generated.v1.data_classes._core import DEFAULT_INSTANCE_SPACE
 from cognite.powerops.client._generated.v1.data_classes import (
     DomainModelCore,
     DomainModelWrite,
     ResourcesWriteResult,
-    TotalBidMatrixCalculationOutput,
-    TotalBidMatrixCalculationOutputWrite,
-    TotalBidMatrixCalculationOutputFields,
-    TotalBidMatrixCalculationOutputList,
-    TotalBidMatrixCalculationOutputWriteList,
-    TotalBidMatrixCalculationOutputTextFields,
+    PartialBidMatrixCalculationOutput,
+    PartialBidMatrixCalculationOutputWrite,
+    PartialBidMatrixCalculationOutputFields,
+    PartialBidMatrixCalculationOutputList,
+    PartialBidMatrixCalculationOutputWriteList,
+    PartialBidMatrixCalculationOutputTextFields,
 )
-from cognite.powerops.client._generated.v1.data_classes._total_bid_matrix_calculation_output import (
-    _TOTALBIDMATRIXCALCULATIONOUTPUT_PROPERTIES_BY_FIELD,
-    _create_total_bid_matrix_calculation_output_filter,
+from cognite.powerops.client._generated.v1.data_classes._partial_bid_matrix_calculation_output import (
+    _PARTIALBIDMATRIXCALCULATIONOUTPUT_PROPERTIES_BY_FIELD,
+    _create_partial_bid_matrix_calculation_output_filter,
 )
 from ._core import (
     DEFAULT_LIMIT_READ,
     DEFAULT_QUERY_LIMIT,
     Aggregations,
     NodeAPI,
     SequenceNotStr,
     QueryStep,
     QueryBuilder,
 )
-from .total_bid_matrix_calculation_output_alerts import TotalBidMatrixCalculationOutputAlertsAPI
-from .total_bid_matrix_calculation_output_query import TotalBidMatrixCalculationOutputQueryAPI
+from .partial_bid_matrix_calculation_output_alerts import PartialBidMatrixCalculationOutputAlertsAPI
+from .partial_bid_matrix_calculation_output_query import PartialBidMatrixCalculationOutputQueryAPI
 
 
-class TotalBidMatrixCalculationOutputAPI(
-    NodeAPI[TotalBidMatrixCalculationOutput, TotalBidMatrixCalculationOutputWrite, TotalBidMatrixCalculationOutputList]
+class PartialBidMatrixCalculationOutputAPI(
+    NodeAPI[
+        PartialBidMatrixCalculationOutput, PartialBidMatrixCalculationOutputWrite, PartialBidMatrixCalculationOutputList
+    ]
 ):
     def __init__(self, client: CogniteClient, view_by_read_class: dict[type[DomainModelCore], dm.ViewId]):
-        view_id = view_by_read_class[TotalBidMatrixCalculationOutput]
+        view_id = view_by_read_class[PartialBidMatrixCalculationOutput]
         super().__init__(
             client=client,
             sources=view_id,
-            class_type=TotalBidMatrixCalculationOutput,
-            class_list=TotalBidMatrixCalculationOutputList,
-            class_write_list=TotalBidMatrixCalculationOutputWriteList,
+            class_type=PartialBidMatrixCalculationOutput,
+            class_list=PartialBidMatrixCalculationOutputList,
+            class_write_list=PartialBidMatrixCalculationOutputWriteList,
             view_by_read_class=view_by_read_class,
         )
         self._view_id = view_id
-        self.alerts_edge = TotalBidMatrixCalculationOutputAlertsAPI(client)
+        self.alerts_edge = PartialBidMatrixCalculationOutputAlertsAPI(client)
 
     def __call__(
         self,
         process_id: str | list[str] | None = None,
         process_id_prefix: str | None = None,
         min_process_step: int | None = None,
         max_process_step: int | None = None,
         function_name: str | list[str] | None = None,
         function_name_prefix: str | None = None,
         function_call_id: str | list[str] | None = None,
         function_call_id_prefix: str | None = None,
-        bid_document: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        partial_matrix: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        bid_configuration: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         input_: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_QUERY_LIMIT,
         filter: dm.Filter | None = None,
-    ) -> TotalBidMatrixCalculationOutputQueryAPI[TotalBidMatrixCalculationOutputList]:
-        """Query starting at total bid matrix calculation outputs.
+    ) -> PartialBidMatrixCalculationOutputQueryAPI[PartialBidMatrixCalculationOutputList]:
+        """Query starting at partial bid matrix calculation outputs.
 
         Args:
             process_id: The process id to filter on.
             process_id_prefix: The prefix of the process id to filter on.
             min_process_step: The minimum value of the process step to filter on.
             max_process_step: The maximum value of the process step to filter on.
             function_name: The function name to filter on.
             function_name_prefix: The prefix of the function name to filter on.
             function_call_id: The function call id to filter on.
             function_call_id_prefix: The prefix of the function call id to filter on.
-            bid_document: The bid document to filter on.
+            partial_matrix: The partial matrix to filter on.
+            bid_configuration: The bid configuration to filter on.
             input_: The input to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of total bid matrix calculation outputs to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of partial bid matrix calculation outputs to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
-            A query API for total bid matrix calculation outputs.
+            A query API for partial bid matrix calculation outputs.
 
         """
         has_data = dm.filters.HasData(views=[self._view_id])
-        filter_ = _create_total_bid_matrix_calculation_output_filter(
+        filter_ = _create_partial_bid_matrix_calculation_output_filter(
             self._view_id,
             process_id,
             process_id_prefix,
             min_process_step,
             max_process_step,
             function_name,
             function_name_prefix,
             function_call_id,
             function_call_id_prefix,
-            bid_document,
+            partial_matrix,
+            bid_configuration,
             input_,
             external_id_prefix,
             space,
             (filter and dm.filters.And(filter, has_data)) or has_data,
         )
-        builder = QueryBuilder(TotalBidMatrixCalculationOutputList)
-        return TotalBidMatrixCalculationOutputQueryAPI(self._client, builder, self._view_by_read_class, filter_, limit)
+        builder = QueryBuilder(PartialBidMatrixCalculationOutputList)
+        return PartialBidMatrixCalculationOutputQueryAPI(
+            self._client, builder, self._view_by_read_class, filter_, limit
+        )
 
     def apply(
         self,
-        total_bid_matrix_calculation_output: (
-            TotalBidMatrixCalculationOutputWrite | Sequence[TotalBidMatrixCalculationOutputWrite]
+        partial_bid_matrix_calculation_output: (
+            PartialBidMatrixCalculationOutputWrite | Sequence[PartialBidMatrixCalculationOutputWrite]
         ),
         replace: bool = False,
         write_none: bool = False,
     ) -> ResourcesWriteResult:
-        """Add or update (upsert) total bid matrix calculation outputs.
+        """Add or update (upsert) partial bid matrix calculation outputs.
 
-        Note: This method iterates through all nodes and timeseries linked to total_bid_matrix_calculation_output and creates them including the edges
+        Note: This method iterates through all nodes and timeseries linked to partial_bid_matrix_calculation_output and creates them including the edges
         between the nodes. For example, if any of `alerts` are set, then these
         nodes as well as any nodes linked to them, and all the edges linking these nodes will be created.
 
         Args:
-            total_bid_matrix_calculation_output: Total bid matrix calculation output or sequence of total bid matrix calculation outputs to upsert.
+            partial_bid_matrix_calculation_output: Partial bid matrix calculation output or sequence of partial bid matrix calculation outputs to upsert.
             replace (bool): How do we behave when a property value exists? Do we replace all matching and existing values with the supplied values (true)?
                 Or should we merge in new values for properties together with the existing values (false)? Note: This setting applies for all nodes or edges specified in the ingestion call.
             write_none (bool): This method, will by default, skip properties that are set to None. However, if you want to set properties to None,
                 you can set this parameter to True. Note this only applies to properties that are nullable.
         Returns:
             Created instance(s), i.e., nodes, edges, and time series.
 
         Examples:
 
-            Create a new total_bid_matrix_calculation_output:
+            Create a new partial_bid_matrix_calculation_output:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
-                >>> from cognite.powerops.client._generated.v1.data_classes import TotalBidMatrixCalculationOutputWrite
+                >>> from cognite.powerops.client._generated.v1.data_classes import PartialBidMatrixCalculationOutputWrite
                 >>> client = PowerOpsModelsV1Client()
-                >>> total_bid_matrix_calculation_output = TotalBidMatrixCalculationOutputWrite(external_id="my_total_bid_matrix_calculation_output", ...)
-                >>> result = client.total_bid_matrix_calculation_output.apply(total_bid_matrix_calculation_output)
+                >>> partial_bid_matrix_calculation_output = PartialBidMatrixCalculationOutputWrite(external_id="my_partial_bid_matrix_calculation_output", ...)
+                >>> result = client.partial_bid_matrix_calculation_output.apply(partial_bid_matrix_calculation_output)
 
         """
         warnings.warn(
             "The .apply method is deprecated and will be removed in v1.0. "
             "Please use the .upsert method on the client instead. This means instead of "
-            "`my_client.total_bid_matrix_calculation_output.apply(my_items)` please use `my_client.upsert(my_items)`."
+            "`my_client.partial_bid_matrix_calculation_output.apply(my_items)` please use `my_client.upsert(my_items)`."
             "The motivation is that all apply methods are the same, and having one apply method per API "
             " class encourages users to create items in small batches, which is inefficient."
             "In addition, .upsert method is more descriptive of what the method does.",
             UserWarning,
             stacklevel=2,
         )
-        return self._apply(total_bid_matrix_calculation_output, replace, write_none)
+        return self._apply(partial_bid_matrix_calculation_output, replace, write_none)
 
     def delete(
         self, external_id: str | SequenceNotStr[str], space: str = DEFAULT_INSTANCE_SPACE
     ) -> dm.InstancesDeleteResult:
-        """Delete one or more total bid matrix calculation output.
+        """Delete one or more partial bid matrix calculation output.
 
         Args:
-            external_id: External id of the total bid matrix calculation output to delete.
-            space: The space where all the total bid matrix calculation output are located.
+            external_id: External id of the partial bid matrix calculation output to delete.
+            space: The space where all the partial bid matrix calculation output are located.
 
         Returns:
             The instance(s), i.e., nodes and edges which has been deleted. Empty list if nothing was deleted.
 
         Examples:
 
-            Delete total_bid_matrix_calculation_output by id:
+            Delete partial_bid_matrix_calculation_output by id:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> client.total_bid_matrix_calculation_output.delete("my_total_bid_matrix_calculation_output")
+                >>> client.partial_bid_matrix_calculation_output.delete("my_partial_bid_matrix_calculation_output")
         """
         warnings.warn(
             "The .delete method is deprecated and will be removed in v1.0. "
             "Please use the .delete method on the client instead. This means instead of "
-            "`my_client.total_bid_matrix_calculation_output.delete(my_ids)` please use `my_client.delete(my_ids)`."
+            "`my_client.partial_bid_matrix_calculation_output.delete(my_ids)` please use `my_client.delete(my_ids)`."
             "The motivation is that all delete methods are the same, and having one delete method per API "
             " class encourages users to delete items in small batches, which is inefficient.",
             UserWarning,
             stacklevel=2,
         )
         return self._delete(external_id, space)
 
     @overload
     def retrieve(
         self, external_id: str, space: str = DEFAULT_INSTANCE_SPACE
-    ) -> TotalBidMatrixCalculationOutput | None: ...
+    ) -> PartialBidMatrixCalculationOutput | None: ...
 
     @overload
     def retrieve(
         self, external_id: SequenceNotStr[str], space: str = DEFAULT_INSTANCE_SPACE
-    ) -> TotalBidMatrixCalculationOutputList: ...
+    ) -> PartialBidMatrixCalculationOutputList: ...
 
     def retrieve(
         self, external_id: str | SequenceNotStr[str], space: str = DEFAULT_INSTANCE_SPACE
-    ) -> TotalBidMatrixCalculationOutput | TotalBidMatrixCalculationOutputList | None:
-        """Retrieve one or more total bid matrix calculation outputs by id(s).
+    ) -> PartialBidMatrixCalculationOutput | PartialBidMatrixCalculationOutputList | None:
+        """Retrieve one or more partial bid matrix calculation outputs by id(s).
 
         Args:
-            external_id: External id or list of external ids of the total bid matrix calculation outputs.
-            space: The space where all the total bid matrix calculation outputs are located.
+            external_id: External id or list of external ids of the partial bid matrix calculation outputs.
+            space: The space where all the partial bid matrix calculation outputs are located.
 
         Returns:
-            The requested total bid matrix calculation outputs.
+            The requested partial bid matrix calculation outputs.
 
         Examples:
 
-            Retrieve total_bid_matrix_calculation_output by id:
+            Retrieve partial_bid_matrix_calculation_output by id:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> total_bid_matrix_calculation_output = client.total_bid_matrix_calculation_output.retrieve("my_total_bid_matrix_calculation_output")
+                >>> partial_bid_matrix_calculation_output = client.partial_bid_matrix_calculation_output.retrieve("my_partial_bid_matrix_calculation_output")
 
         """
         return self._retrieve(
             external_id,
             space,
             retrieve_edges=True,
             edge_api_name_type_direction_view_id_penta=[
                 (
                     self.alerts_edge,
                     "alerts",
-                    dm.DirectRelationReference("sp_powerops_types", "calculationIssue"),
+                    dm.DirectRelationReference("sp_powerops_types_temp", "calculationIssue"),
                     "outwards",
-                    dm.ViewId("sp_powerops_models", "Alert", "1"),
+                    dm.ViewId("sp_powerops_models_temp", "Alert", "1"),
                 ),
             ],
         )
 
     def search(
         self,
         query: str,
         properties: (
-            TotalBidMatrixCalculationOutputTextFields | Sequence[TotalBidMatrixCalculationOutputTextFields] | None
+            PartialBidMatrixCalculationOutputTextFields | Sequence[PartialBidMatrixCalculationOutputTextFields] | None
         ) = None,
         process_id: str | list[str] | None = None,
         process_id_prefix: str | None = None,
         min_process_step: int | None = None,
         max_process_step: int | None = None,
         function_name: str | list[str] | None = None,
         function_name_prefix: str | None = None,
         function_call_id: str | list[str] | None = None,
         function_call_id_prefix: str | None = None,
-        bid_document: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        partial_matrix: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        bid_configuration: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         input_: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
-    ) -> TotalBidMatrixCalculationOutputList:
-        """Search total bid matrix calculation outputs
+    ) -> PartialBidMatrixCalculationOutputList:
+        """Search partial bid matrix calculation outputs
 
         Args:
             query: The search query,
             properties: The property to search, if nothing is passed all text fields will be searched.
             process_id: The process id to filter on.
             process_id_prefix: The prefix of the process id to filter on.
             min_process_step: The minimum value of the process step to filter on.
             max_process_step: The maximum value of the process step to filter on.
             function_name: The function name to filter on.
             function_name_prefix: The prefix of the function name to filter on.
             function_call_id: The function call id to filter on.
             function_call_id_prefix: The prefix of the function call id to filter on.
-            bid_document: The bid document to filter on.
+            partial_matrix: The partial matrix to filter on.
+            bid_configuration: The bid configuration to filter on.
             input_: The input to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of total bid matrix calculation outputs to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of partial bid matrix calculation outputs to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
-            Search results total bid matrix calculation outputs matching the query.
+            Search results partial bid matrix calculation outputs matching the query.
 
         Examples:
 
-           Search for 'my_total_bid_matrix_calculation_output' in all text properties:
+           Search for 'my_partial_bid_matrix_calculation_output' in all text properties:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> total_bid_matrix_calculation_outputs = client.total_bid_matrix_calculation_output.search('my_total_bid_matrix_calculation_output')
+                >>> partial_bid_matrix_calculation_outputs = client.partial_bid_matrix_calculation_output.search('my_partial_bid_matrix_calculation_output')
 
         """
-        filter_ = _create_total_bid_matrix_calculation_output_filter(
+        filter_ = _create_partial_bid_matrix_calculation_output_filter(
             self._view_id,
             process_id,
             process_id_prefix,
             min_process_step,
             max_process_step,
             function_name,
             function_name_prefix,
             function_call_id,
             function_call_id_prefix,
-            bid_document,
+            partial_matrix,
+            bid_configuration,
             input_,
             external_id_prefix,
             space,
             filter,
         )
         return self._search(
-            self._view_id, query, _TOTALBIDMATRIXCALCULATIONOUTPUT_PROPERTIES_BY_FIELD, properties, filter_, limit
+            self._view_id, query, _PARTIALBIDMATRIXCALCULATIONOUTPUT_PROPERTIES_BY_FIELD, properties, filter_, limit
         )
 
     @overload
     def aggregate(
         self,
         aggregations: (
             Aggregations
             | dm.aggregations.MetricAggregation
             | Sequence[Aggregations]
             | Sequence[dm.aggregations.MetricAggregation]
         ),
-        property: TotalBidMatrixCalculationOutputFields | Sequence[TotalBidMatrixCalculationOutputFields] | None = None,
+        property: (
+            PartialBidMatrixCalculationOutputFields | Sequence[PartialBidMatrixCalculationOutputFields] | None
+        ) = None,
         group_by: None = None,
         query: str | None = None,
         search_properties: (
-            TotalBidMatrixCalculationOutputTextFields | Sequence[TotalBidMatrixCalculationOutputTextFields] | None
+            PartialBidMatrixCalculationOutputTextFields | Sequence[PartialBidMatrixCalculationOutputTextFields] | None
         ) = None,
         process_id: str | list[str] | None = None,
         process_id_prefix: str | None = None,
         min_process_step: int | None = None,
         max_process_step: int | None = None,
         function_name: str | list[str] | None = None,
         function_name_prefix: str | None = None,
         function_call_id: str | list[str] | None = None,
         function_call_id_prefix: str | None = None,
-        bid_document: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        partial_matrix: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        bid_configuration: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         input_: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> list[dm.aggregations.AggregatedNumberedValue]: ...
 
@@ -344,29 +357,32 @@
         self,
         aggregations: (
             Aggregations
             | dm.aggregations.MetricAggregation
             | Sequence[Aggregations]
             | Sequence[dm.aggregations.MetricAggregation]
         ),
-        property: TotalBidMatrixCalculationOutputFields | Sequence[TotalBidMatrixCalculationOutputFields] | None = None,
-        group_by: TotalBidMatrixCalculationOutputFields | Sequence[TotalBidMatrixCalculationOutputFields] = None,
+        property: (
+            PartialBidMatrixCalculationOutputFields | Sequence[PartialBidMatrixCalculationOutputFields] | None
+        ) = None,
+        group_by: PartialBidMatrixCalculationOutputFields | Sequence[PartialBidMatrixCalculationOutputFields] = None,
         query: str | None = None,
         search_properties: (
-            TotalBidMatrixCalculationOutputTextFields | Sequence[TotalBidMatrixCalculationOutputTextFields] | None
+            PartialBidMatrixCalculationOutputTextFields | Sequence[PartialBidMatrixCalculationOutputTextFields] | None
         ) = None,
         process_id: str | list[str] | None = None,
         process_id_prefix: str | None = None,
         min_process_step: int | None = None,
         max_process_step: int | None = None,
         function_name: str | list[str] | None = None,
         function_name_prefix: str | None = None,
         function_call_id: str | list[str] | None = None,
         function_call_id_prefix: str | None = None,
-        bid_document: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        partial_matrix: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        bid_configuration: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         input_: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> InstanceAggregationResultList: ...
 
@@ -374,36 +390,41 @@
         self,
         aggregate: (
             Aggregations
             | dm.aggregations.MetricAggregation
             | Sequence[Aggregations]
             | Sequence[dm.aggregations.MetricAggregation]
         ),
-        property: TotalBidMatrixCalculationOutputFields | Sequence[TotalBidMatrixCalculationOutputFields] | None = None,
-        group_by: TotalBidMatrixCalculationOutputFields | Sequence[TotalBidMatrixCalculationOutputFields] | None = None,
+        property: (
+            PartialBidMatrixCalculationOutputFields | Sequence[PartialBidMatrixCalculationOutputFields] | None
+        ) = None,
+        group_by: (
+            PartialBidMatrixCalculationOutputFields | Sequence[PartialBidMatrixCalculationOutputFields] | None
+        ) = None,
         query: str | None = None,
         search_property: (
-            TotalBidMatrixCalculationOutputTextFields | Sequence[TotalBidMatrixCalculationOutputTextFields] | None
+            PartialBidMatrixCalculationOutputTextFields | Sequence[PartialBidMatrixCalculationOutputTextFields] | None
         ) = None,
         process_id: str | list[str] | None = None,
         process_id_prefix: str | None = None,
         min_process_step: int | None = None,
         max_process_step: int | None = None,
         function_name: str | list[str] | None = None,
         function_name_prefix: str | None = None,
         function_call_id: str | list[str] | None = None,
         function_call_id_prefix: str | None = None,
-        bid_document: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        partial_matrix: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        bid_configuration: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         input_: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> list[dm.aggregations.AggregatedNumberedValue] | InstanceAggregationResultList:
-        """Aggregate data across total bid matrix calculation outputs
+        """Aggregate data across partial bid matrix calculation outputs
 
         Args:
             aggregate: The aggregation to perform.
             property: The property to perform aggregation on.
             group_by: The property to group by when doing the aggregation.
             query: The query to search for in the text field.
             search_property: The text field to search in.
@@ -411,132 +432,137 @@
             process_id_prefix: The prefix of the process id to filter on.
             min_process_step: The minimum value of the process step to filter on.
             max_process_step: The maximum value of the process step to filter on.
             function_name: The function name to filter on.
             function_name_prefix: The prefix of the function name to filter on.
             function_call_id: The function call id to filter on.
             function_call_id_prefix: The prefix of the function call id to filter on.
-            bid_document: The bid document to filter on.
+            partial_matrix: The partial matrix to filter on.
+            bid_configuration: The bid configuration to filter on.
             input_: The input to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of total bid matrix calculation outputs to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of partial bid matrix calculation outputs to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
             Aggregation results.
 
         Examples:
 
-            Count total bid matrix calculation outputs in space `my_space`:
+            Count partial bid matrix calculation outputs in space `my_space`:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> result = client.total_bid_matrix_calculation_output.aggregate("count", space="my_space")
+                >>> result = client.partial_bid_matrix_calculation_output.aggregate("count", space="my_space")
 
         """
 
-        filter_ = _create_total_bid_matrix_calculation_output_filter(
+        filter_ = _create_partial_bid_matrix_calculation_output_filter(
             self._view_id,
             process_id,
             process_id_prefix,
             min_process_step,
             max_process_step,
             function_name,
             function_name_prefix,
             function_call_id,
             function_call_id_prefix,
-            bid_document,
+            partial_matrix,
+            bid_configuration,
             input_,
             external_id_prefix,
             space,
             filter,
         )
         return self._aggregate(
             self._view_id,
             aggregate,
-            _TOTALBIDMATRIXCALCULATIONOUTPUT_PROPERTIES_BY_FIELD,
+            _PARTIALBIDMATRIXCALCULATIONOUTPUT_PROPERTIES_BY_FIELD,
             property,
             group_by,
             query,
             search_property,
             limit,
             filter_,
         )
 
     def histogram(
         self,
-        property: TotalBidMatrixCalculationOutputFields,
+        property: PartialBidMatrixCalculationOutputFields,
         interval: float,
         query: str | None = None,
         search_property: (
-            TotalBidMatrixCalculationOutputTextFields | Sequence[TotalBidMatrixCalculationOutputTextFields] | None
+            PartialBidMatrixCalculationOutputTextFields | Sequence[PartialBidMatrixCalculationOutputTextFields] | None
         ) = None,
         process_id: str | list[str] | None = None,
         process_id_prefix: str | None = None,
         min_process_step: int | None = None,
         max_process_step: int | None = None,
         function_name: str | list[str] | None = None,
         function_name_prefix: str | None = None,
         function_call_id: str | list[str] | None = None,
         function_call_id_prefix: str | None = None,
-        bid_document: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        partial_matrix: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        bid_configuration: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         input_: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
     ) -> dm.aggregations.HistogramValue:
-        """Produces histograms for total bid matrix calculation outputs
+        """Produces histograms for partial bid matrix calculation outputs
 
         Args:
             property: The property to use as the value in the histogram.
             interval: The interval to use for the histogram bins.
             query: The query to search for in the text field.
             search_property: The text field to search in.
             process_id: The process id to filter on.
             process_id_prefix: The prefix of the process id to filter on.
             min_process_step: The minimum value of the process step to filter on.
             max_process_step: The maximum value of the process step to filter on.
             function_name: The function name to filter on.
             function_name_prefix: The prefix of the function name to filter on.
             function_call_id: The function call id to filter on.
             function_call_id_prefix: The prefix of the function call id to filter on.
-            bid_document: The bid document to filter on.
+            partial_matrix: The partial matrix to filter on.
+            bid_configuration: The bid configuration to filter on.
             input_: The input to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of total bid matrix calculation outputs to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of partial bid matrix calculation outputs to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
 
         Returns:
             Bucketed histogram results.
 
         """
-        filter_ = _create_total_bid_matrix_calculation_output_filter(
+        filter_ = _create_partial_bid_matrix_calculation_output_filter(
             self._view_id,
             process_id,
             process_id_prefix,
             min_process_step,
             max_process_step,
             function_name,
             function_name_prefix,
             function_call_id,
             function_call_id_prefix,
-            bid_document,
+            partial_matrix,
+            bid_configuration,
             input_,
             external_id_prefix,
             space,
             filter,
         )
         return self._histogram(
             self._view_id,
             property,
             interval,
-            _TOTALBIDMATRIXCALCULATIONOUTPUT_PROPERTIES_BY_FIELD,
+            _PARTIALBIDMATRIXCALCULATIONOUTPUT_PROPERTIES_BY_FIELD,
             query,
             search_property,
             limit,
             filter_,
         )
 
     def list(
@@ -545,77 +571,80 @@
         process_id_prefix: str | None = None,
         min_process_step: int | None = None,
         max_process_step: int | None = None,
         function_name: str | list[str] | None = None,
         function_name_prefix: str | None = None,
         function_call_id: str | list[str] | None = None,
         function_call_id_prefix: str | None = None,
-        bid_document: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        partial_matrix: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+        bid_configuration: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         input_: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
         limit: int | None = DEFAULT_LIMIT_READ,
         filter: dm.Filter | None = None,
         retrieve_edges: bool = True,
-    ) -> TotalBidMatrixCalculationOutputList:
-        """List/filter total bid matrix calculation outputs
+    ) -> PartialBidMatrixCalculationOutputList:
+        """List/filter partial bid matrix calculation outputs
 
         Args:
             process_id: The process id to filter on.
             process_id_prefix: The prefix of the process id to filter on.
             min_process_step: The minimum value of the process step to filter on.
             max_process_step: The maximum value of the process step to filter on.
             function_name: The function name to filter on.
             function_name_prefix: The prefix of the function name to filter on.
             function_call_id: The function call id to filter on.
             function_call_id_prefix: The prefix of the function call id to filter on.
-            bid_document: The bid document to filter on.
+            partial_matrix: The partial matrix to filter on.
+            bid_configuration: The bid configuration to filter on.
             input_: The input to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of total bid matrix calculation outputs to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
+            limit: Maximum number of partial bid matrix calculation outputs to return. Defaults to 25. Set to -1, float("inf") or None to return all items.
             filter: (Advanced) If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
-            retrieve_edges: Whether to retrieve `alerts` external ids for the total bid matrix calculation outputs. Defaults to True.
+            retrieve_edges: Whether to retrieve `alerts` external ids for the partial bid matrix calculation outputs. Defaults to True.
 
         Returns:
-            List of requested total bid matrix calculation outputs
+            List of requested partial bid matrix calculation outputs
 
         Examples:
 
-            List total bid matrix calculation outputs and limit to 5:
+            List partial bid matrix calculation outputs and limit to 5:
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
-                >>> total_bid_matrix_calculation_outputs = client.total_bid_matrix_calculation_output.list(limit=5)
+                >>> partial_bid_matrix_calculation_outputs = client.partial_bid_matrix_calculation_output.list(limit=5)
 
         """
-        filter_ = _create_total_bid_matrix_calculation_output_filter(
+        filter_ = _create_partial_bid_matrix_calculation_output_filter(
             self._view_id,
             process_id,
             process_id_prefix,
             min_process_step,
             max_process_step,
             function_name,
             function_name_prefix,
             function_call_id,
             function_call_id_prefix,
-            bid_document,
+            partial_matrix,
+            bid_configuration,
             input_,
             external_id_prefix,
             space,
             filter,
         )
 
         return self._list(
             limit=limit,
             filter=filter_,
             retrieve_edges=retrieve_edges,
             edge_api_name_type_direction_view_id_penta=[
                 (
                     self.alerts_edge,
                     "alerts",
-                    dm.DirectRelationReference("sp_powerops_types", "calculationIssue"),
+                    dm.DirectRelationReference("sp_powerops_types_temp", "calculationIssue"),
                     "outwards",
-                    dm.ViewId("sp_powerops_models", "Alert", "1"),
+                    dm.ViewId("sp_powerops_models_temp", "Alert", "1"),
                 ),
             ],
         )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/total_bid_matrix_calculation_output_alerts.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/total_bid_matrix_calculation_output_alerts.py`

 * *Files 2% similar despite different names*

```diff
@@ -39,15 +39,15 @@
 
                 >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
                 >>> client = PowerOpsModelsV1Client()
                 >>> total_bid_matrix_calculation_output = client.total_bid_matrix_calculation_output.alerts_edge.list("my_total_bid_matrix_calculation_output", limit=5)
 
         """
         filter_ = _create_edge_filter(
-            dm.DirectRelationReference("sp_powerops_types", "calculationIssue"),
+            dm.DirectRelationReference("sp_powerops_types_temp", "calculationIssue"),
             from_total_bid_matrix_calculation_output,
             from_total_bid_matrix_calculation_output_space,
             to_alert,
             to_alert_space,
             external_id_prefix,
             space,
         )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/turbine_efficiency_curve.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/turbine_efficiency_curve.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/turbine_efficiency_curve_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/turbine_efficiency_curve_query.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/water_partial_bid_calculation_input.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/water_partial_bid_calculation_input.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/water_partial_bid_calculation_output.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/water_partial_bid_calculation_output.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/water_partial_bid_calculation_output_alerts.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/water_partial_bid_calculation_output_alerts.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/water_partial_bid_calculation_output_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/shop_partial_bid_matrix_calculation_input_query.py`

 * *Files 24% similar despite different names*

```diff
@@ -3,148 +3,173 @@
 import datetime
 from typing import TYPE_CHECKING
 
 from cognite.client import data_modeling as dm, CogniteClient
 
 from cognite.powerops.client._generated.v1.data_classes import (
     DomainModelCore,
-    WaterPartialBidCalculationOutput,
-    BidMatrixRaw,
-    WaterPartialBidCalculationInput,
+    ShopPartialBidMatrixCalculationInput,
+    BidConfiguration,
+    ShopBasedPartialBidConfiguration,
+)
+from cognite.powerops.client._generated.v1.data_classes._price_production import (
+    PriceProduction,
+    _create_price_production_filter,
 )
 from ._core import DEFAULT_QUERY_LIMIT, QueryBuilder, QueryStep, QueryAPI, T_DomainModelList, _create_edge_filter
 
 if TYPE_CHECKING:
-    from .alert_query import AlertQueryAPI
+    from .price_production_query import PriceProductionQueryAPI
 
 
-class WaterPartialBidCalculationOutputQueryAPI(QueryAPI[T_DomainModelList]):
+class ShopPartialBidMatrixCalculationInputQueryAPI(QueryAPI[T_DomainModelList]):
     def __init__(
         self,
         client: CogniteClient,
         builder: QueryBuilder[T_DomainModelList],
         view_by_read_class: dict[type[DomainModelCore], dm.ViewId],
         filter_: dm.filters.Filter | None = None,
         limit: int = DEFAULT_QUERY_LIMIT,
     ):
         super().__init__(client, builder, view_by_read_class)
 
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("water_partial_bid_calculation_output"),
+                name=self._builder.next_name("shop_partial_bid_matrix_calculation_input"),
                 expression=dm.query.NodeResultSetExpression(
                     from_=self._builder[-1].name if self._builder else None,
                     filter=filter_,
                 ),
                 select=dm.query.Select(
-                    [dm.query.SourceSelector(self._view_by_read_class[WaterPartialBidCalculationOutput], ["*"])]
+                    [dm.query.SourceSelector(self._view_by_read_class[ShopPartialBidMatrixCalculationInput], ["*"])]
                 ),
-                result_cls=WaterPartialBidCalculationOutput,
+                result_cls=ShopPartialBidMatrixCalculationInput,
                 max_retrieve_limit=limit,
             )
         )
 
-    def alerts(
+    def price_production(
         self,
+        shop_result: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
         external_id_prefix: str | None = None,
         space: str | list[str] | None = None,
+        external_id_prefix_edge: str | None = None,
+        space_edge: str | list[str] | None = None,
+        filter: dm.Filter | None = None,
         limit: int | None = DEFAULT_QUERY_LIMIT,
-        retrieve_raw_partial_matrix: bool = False,
-        retrieve_input_: bool = False,
-    ) -> AlertQueryAPI[T_DomainModelList]:
-        """Query along the alert edges of the water partial bid calculation output.
+        retrieve_bid_configuration: bool = False,
+        retrieve_partial_bid_configuration: bool = False,
+    ) -> PriceProductionQueryAPI[T_DomainModelList]:
+        """Query along the price production edges of the shop partial bid matrix calculation input.
 
         Args:
+            shop_result: The shop result to filter on.
             external_id_prefix: The prefix of the external ID to filter on.
             space: The space to filter on.
-            limit: Maximum number of alert edges to return. Defaults to 25. Set to -1, float("inf") or None
+            external_id_prefix_edge: The prefix of the external ID to filter on.
+            space_edge: The space to filter on.
+            filter: (Advanced) Filter applied to node. If the filtering available in the above is not sufficient, you can write your own filtering which will be ANDed with the filter above.
+            limit: Maximum number of price production edges to return. Defaults to 3. Set to -1, float("inf") or None
                 to return all items.
-            retrieve_raw_partial_matrix: Whether to retrieve the raw partial matrix for each water partial bid calculation output or not.
-            retrieve_input_: Whether to retrieve the input for each water partial bid calculation output or not.
+            retrieve_bid_configuration: Whether to retrieve the bid configuration for each shop partial bid matrix calculation input or not.
+            retrieve_partial_bid_configuration: Whether to retrieve the partial bid configuration for each shop partial bid matrix calculation input or not.
 
         Returns:
-            AlertQueryAPI: The query API for the alert.
+            PriceProductionQueryAPI: The query API for the price production.
         """
-        from .alert_query import AlertQueryAPI
+        from .price_production_query import PriceProductionQueryAPI
 
         from_ = self._builder[-1].name
-
         edge_filter = _create_edge_filter(
-            dm.DirectRelationReference("sp_powerops_types", "calculationIssue"),
-            external_id_prefix=external_id_prefix,
-            space=space,
+            dm.DirectRelationReference("sp_powerops_types_temp", "PriceProduction"),
+            external_id_prefix=external_id_prefix_edge,
+            space=space_edge,
         )
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("alerts"),
+                name=self._builder.next_name("price_production"),
                 expression=dm.query.EdgeResultSetExpression(
                     filter=edge_filter,
                     from_=from_,
                     direction="outwards",
                 ),
                 select=dm.query.Select(),
                 max_retrieve_limit=limit,
             )
         )
-        if retrieve_raw_partial_matrix:
-            self._query_append_raw_partial_matrix(from_)
-        if retrieve_input_:
-            self._query_append_input_(from_)
-        return AlertQueryAPI(self._client, self._builder, self._view_by_read_class, None, limit)
+
+        view_id = self._view_by_read_class[PriceProduction]
+        has_data = dm.filters.HasData(views=[view_id])
+        node_filer = _create_price_production_filter(
+            view_id,
+            shop_result,
+            external_id_prefix,
+            space,
+            (filter and dm.filters.And(filter, has_data)) or has_data,
+        )
+        if retrieve_bid_configuration:
+            self._query_append_bid_configuration(from_)
+        if retrieve_partial_bid_configuration:
+            self._query_append_partial_bid_configuration(from_)
+        return PriceProductionQueryAPI(self._client, self._builder, self._view_by_read_class, node_filer, limit)
 
     def query(
         self,
-        retrieve_raw_partial_matrix: bool = False,
-        retrieve_input_: bool = False,
+        retrieve_bid_configuration: bool = False,
+        retrieve_partial_bid_configuration: bool = False,
     ) -> T_DomainModelList:
         """Execute query and return the result.
 
         Args:
-            retrieve_raw_partial_matrix: Whether to retrieve the raw partial matrix for each water partial bid calculation output or not.
-            retrieve_input_: Whether to retrieve the input for each water partial bid calculation output or not.
+            retrieve_bid_configuration: Whether to retrieve the bid configuration for each shop partial bid matrix calculation input or not.
+            retrieve_partial_bid_configuration: Whether to retrieve the partial bid configuration for each shop partial bid matrix calculation input or not.
 
         Returns:
             The list of the source nodes of the query.
 
         """
         from_ = self._builder[-1].name
-        if retrieve_raw_partial_matrix:
-            self._query_append_raw_partial_matrix(from_)
-        if retrieve_input_:
-            self._query_append_input_(from_)
+        if retrieve_bid_configuration:
+            self._query_append_bid_configuration(from_)
+        if retrieve_partial_bid_configuration:
+            self._query_append_partial_bid_configuration(from_)
         return self._query()
 
-    def _query_append_raw_partial_matrix(self, from_: str) -> None:
-        view_id = self._view_by_read_class[BidMatrixRaw]
+    def _query_append_bid_configuration(self, from_: str) -> None:
+        view_id = self._view_by_read_class[BidConfiguration]
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("raw_partial_matrix"),
+                name=self._builder.next_name("bid_configuration"),
                 expression=dm.query.NodeResultSetExpression(
                     filter=dm.filters.HasData(views=[view_id]),
                     from_=from_,
-                    through=self._view_by_read_class[WaterPartialBidCalculationOutput].as_property_ref(
-                        "rawPartialMatrix"
+                    through=self._view_by_read_class[ShopPartialBidMatrixCalculationInput].as_property_ref(
+                        "bidConfiguration"
                     ),
                     direction="outwards",
                 ),
                 select=dm.query.Select([dm.query.SourceSelector(view_id, ["*"])]),
                 max_retrieve_limit=-1,
-                result_cls=BidMatrixRaw,
+                result_cls=BidConfiguration,
+                is_single_direct_relation=True,
             ),
         )
 
-    def _query_append_input_(self, from_: str) -> None:
-        view_id = self._view_by_read_class[WaterPartialBidCalculationInput]
+    def _query_append_partial_bid_configuration(self, from_: str) -> None:
+        view_id = self._view_by_read_class[ShopBasedPartialBidConfiguration]
         self._builder.append(
             QueryStep(
-                name=self._builder.next_name("input_"),
+                name=self._builder.next_name("partial_bid_configuration"),
                 expression=dm.query.NodeResultSetExpression(
                     filter=dm.filters.HasData(views=[view_id]),
                     from_=from_,
-                    through=self._view_by_read_class[WaterPartialBidCalculationOutput].as_property_ref("input"),
+                    through=self._view_by_read_class[ShopPartialBidMatrixCalculationInput].as_property_ref(
+                        "partialBidConfiguration"
+                    ),
                     direction="outwards",
                 ),
                 select=dm.query.Select([dm.query.SourceSelector(view_id, ["*"])]),
                 max_retrieve_limit=-1,
-                result_cls=WaterPartialBidCalculationInput,
+                result_cls=ShopBasedPartialBidConfiguration,
+                is_single_direct_relation=True,
             ),
         )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/watercourse.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/watercourse.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/watercourse_plants.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/watercourse_plants.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/watercourse_production_obligation.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/watercourse_production_obligation.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/watercourse_shop.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/watercourse_shop.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api/watercourse_shop_query.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api/watercourse_shop_query.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/_api_client.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/_api_client.py`

 * *Files 10% similar despite different names*

```diff
@@ -5,574 +5,572 @@
 from typing import Sequence
 
 from cognite.client import ClientConfig, CogniteClient, data_modeling as dm
 from cognite.client.data_classes import TimeSeriesList
 from cognite.client.credentials import OAuthClientCredentials
 
 from ._api.alert import AlertAPI
-from ._api.basic_bid_matrix import BasicBidMatrixAPI
-from ._api.bid_calculation_task import BidCalculationTaskAPI
 from ._api.bid_configuration import BidConfigurationAPI
-from ._api.bid_configuration_shop import BidConfigurationShopAPI
-from ._api.bid_configuration_water import BidConfigurationWaterAPI
+from ._api.bid_document import BidDocumentAPI
 from ._api.bid_document_afrr import BidDocumentAFRRAPI
 from ._api.bid_document_day_ahead import BidDocumentDayAheadAPI
 from ._api.bid_matrix import BidMatrixAPI
-from ._api.bid_matrix_raw import BidMatrixRawAPI
-from ._api.bid_method import BidMethodAPI
-from ._api.bid_method_afrr import BidMethodAFRRAPI
-from ._api.bid_method_custom import BidMethodCustomAPI
-from ._api.bid_method_day_ahead import BidMethodDayAheadAPI
-from ._api.bid_method_shop_multi_scenario import BidMethodSHOPMultiScenarioAPI
-from ._api.bid_method_water_value import BidMethodWaterValueAPI
 from ._api.bid_row import BidRowAPI
 from ._api.case import CaseAPI
 from ._api.commands import CommandsAPI
-from ._api.custom_bid_matrix import CustomBidMatrixAPI
+from ._api.function_input import FunctionInputAPI
+from ._api.function_output import FunctionOutputAPI
 from ._api.generator import GeneratorAPI
 from ._api.generator_efficiency_curve import GeneratorEfficiencyCurveAPI
 from ._api.mapping import MappingAPI
 from ._api.market_configuration import MarketConfigurationAPI
 from ._api.model_template import ModelTemplateAPI
-from ._api.multi_scenario_matrix import MultiScenarioMatrixAPI
-from ._api.multi_scenario_matrix_raw import MultiScenarioMatrixRawAPI
-from ._api.partial_post_processing_input import PartialPostProcessingInputAPI
-from ._api.partial_post_processing_output import PartialPostProcessingOutputAPI
+from ._api.partial_bid_configuration import PartialBidConfigurationAPI
+from ._api.partial_bid_matrix_calculation_input import PartialBidMatrixCalculationInputAPI
+from ._api.partial_bid_matrix_calculation_output import PartialBidMatrixCalculationOutputAPI
 from ._api.plant import PlantAPI
-from ._api.plant_shop import PlantShopAPI
+from ._api.power_asset import PowerAssetAPI
 from ._api.preprocessor_input import PreprocessorInputAPI
 from ._api.preprocessor_output import PreprocessorOutputAPI
 from ._api.price_area import PriceAreaAPI
 from ._api.price_area_afrr import PriceAreaAFRRAPI
-from ._api.price_area_asset import PriceAreaAssetAPI
-from ._api.price_prod_case import PriceProdCaseAPI
-from ._api.reservoir import ReservoirAPI
+from ._api.price_production import PriceProductionAPI
 from ._api.shop_result import SHOPResultAPI
-from ._api.shop_result_price_prod import SHOPResultPriceProdAPI
 from ._api.shop_time_series import SHOPTimeSeriesAPI
 from ._api.shop_trigger_input import SHOPTriggerInputAPI
 from ._api.shop_trigger_output import SHOPTriggerOutputAPI
 from ._api.scenario import ScenarioAPI
-from ._api.shop_partial_bid_calculation_input import ShopPartialBidCalculationInputAPI
-from ._api.shop_partial_bid_calculation_output import ShopPartialBidCalculationOutputAPI
-from ._api.task_dispatcher_shop_input import TaskDispatcherShopInputAPI
-from ._api.task_dispatcher_shop_output import TaskDispatcherShopOutputAPI
-from ._api.task_dispatcher_water_input import TaskDispatcherWaterInputAPI
-from ._api.task_dispatcher_water_output import TaskDispatcherWaterOutputAPI
+from ._api.scenario_set import ScenarioSetAPI
+from ._api.shop_based_partial_bid_configuration import ShopBasedPartialBidConfigurationAPI
+from ._api.shop_partial_bid_matrix_calculation_input import ShopPartialBidMatrixCalculationInputAPI
+from ._api.task_dispatcher_input import TaskDispatcherInputAPI
+from ._api.task_dispatcher_output import TaskDispatcherOutputAPI
 from ._api.total_bid_matrix_calculation_input import TotalBidMatrixCalculationInputAPI
 from ._api.total_bid_matrix_calculation_output import TotalBidMatrixCalculationOutputAPI
 from ._api.turbine_efficiency_curve import TurbineEfficiencyCurveAPI
-from ._api.water_partial_bid_calculation_input import WaterPartialBidCalculationInputAPI
-from ._api.water_partial_bid_calculation_output import WaterPartialBidCalculationOutputAPI
-from ._api.watercourse import WatercourseAPI
-from ._api.watercourse_shop import WatercourseShopAPI
+from ._api.water_value_based_partial_bid_configuration import WaterValueBasedPartialBidConfigurationAPI
+from ._api.water_value_based_partial_bid_matrix_calculation_input import (
+    WaterValueBasedPartialBidMatrixCalculationInputAPI,
+)
 from ._api._core import SequenceNotStr, GraphQLQueryResponse
 from .data_classes._core import DEFAULT_INSTANCE_SPACE, GraphQLList
 from . import data_classes
 
 
 class SHOPBasedDayAheadBidProcesAPIs:
     """
     SHOPBasedDayAheadBidProcesAPIs
 
     Data Model:
-        space: sp_powerops_models
+        space: sp_powerops_models_temp
         externalId: compute_SHOPBasedDayAhead
         version: 1
 
     """
 
     def __init__(self, client: CogniteClient):
         view_by_read_class = {
-            data_classes.Alert: dm.ViewId("sp_powerops_models", "Alert", "1"),
-            data_classes.BidConfigurationShop: dm.ViewId("sp_powerops_models", "BidConfigurationShop", "1"),
-            data_classes.BidMatrixRaw: dm.ViewId("sp_powerops_models", "BidMatrixRaw", "1"),
-            data_classes.BidMethodSHOPMultiScenario: dm.ViewId("sp_powerops_models", "BidMethodSHOPMultiScenario", "1"),
-            data_classes.Case: dm.ViewId("sp_powerops_models", "Case", "1"),
-            data_classes.Commands: dm.ViewId("sp_powerops_models", "Commands", "1"),
-            data_classes.Mapping: dm.ViewId("sp_powerops_models", "Mapping", "1"),
-            data_classes.MarketConfiguration: dm.ViewId("sp_powerops_models", "MarketConfiguration", "1"),
-            data_classes.ModelTemplate: dm.ViewId("sp_powerops_models", "ModelTemplate", "1"),
-            data_classes.MultiScenarioMatrixRaw: dm.ViewId("sp_powerops_models", "MultiScenarioMatrixRaw", "1"),
-            data_classes.PlantShop: dm.ViewId("sp_powerops_models", "PlantShop", "1"),
-            data_classes.PreprocessorInput: dm.ViewId("sp_powerops_models", "PreprocessorInput", "1"),
-            data_classes.PreprocessorOutput: dm.ViewId("sp_powerops_models", "PreprocessorOutput", "1"),
-            data_classes.PriceArea: dm.ViewId("sp_powerops_models", "PriceArea", "1"),
-            data_classes.PriceProdCase: dm.ViewId("sp_powerops_models", "PriceProdCase", "1"),
-            data_classes.SHOPResult: dm.ViewId("sp_powerops_models", "SHOPResult", "1"),
-            data_classes.SHOPResultPriceProd: dm.ViewId("sp_powerops_models", "SHOPResultPriceProd", "1"),
-            data_classes.SHOPTimeSeries: dm.ViewId("sp_powerops_models", "SHOPTimeSeries", "1"),
-            data_classes.SHOPTriggerInput: dm.ViewId("sp_powerops_models", "SHOPTriggerInput", "1"),
-            data_classes.SHOPTriggerOutput: dm.ViewId("sp_powerops_models", "SHOPTriggerOutput", "1"),
-            data_classes.Scenario: dm.ViewId("sp_powerops_models", "Scenario", "1"),
-            data_classes.ShopPartialBidCalculationInput: dm.ViewId(
-                "sp_powerops_models", "ShopPartialBidCalculationInput", "1"
-            ),
-            data_classes.ShopPartialBidCalculationOutput: dm.ViewId(
-                "sp_powerops_models", "ShopPartialBidCalculationOutput", "1"
-            ),
-            data_classes.TaskDispatcherShopInput: dm.ViewId("sp_powerops_models", "TaskDispatcherShopInput", "1"),
-            data_classes.TaskDispatcherShopOutput: dm.ViewId("sp_powerops_models", "TaskDispatcherShopOutput", "1"),
-            data_classes.WatercourseShop: dm.ViewId("sp_powerops_models", "WatercourseShop", "1"),
+            data_classes.Alert: dm.ViewId("sp_powerops_models_temp", "Alert", "1"),
+            data_classes.BidConfiguration: dm.ViewId("sp_powerops_models_temp", "BidConfiguration", "1"),
+            data_classes.BidMatrix: dm.ViewId("sp_powerops_models_temp", "BidMatrix", "1"),
+            data_classes.Case: dm.ViewId("sp_powerops_models_temp", "Case", "1"),
+            data_classes.Commands: dm.ViewId("sp_powerops_models_temp", "Commands", "1"),
+            data_classes.FunctionInput: dm.ViewId("sp_powerops_models_temp", "FunctionInput", "1"),
+            data_classes.FunctionOutput: dm.ViewId("sp_powerops_models_temp", "FunctionOutput", "1"),
+            data_classes.Mapping: dm.ViewId("sp_powerops_models_temp", "Mapping", "1"),
+            data_classes.MarketConfiguration: dm.ViewId("sp_powerops_models_temp", "MarketConfiguration", "1"),
+            data_classes.ModelTemplate: dm.ViewId("sp_powerops_models_temp", "ModelTemplate", "1"),
+            data_classes.PartialBidConfiguration: dm.ViewId("sp_powerops_models_temp", "PartialBidConfiguration", "1"),
+            data_classes.PartialBidMatrixCalculationInput: dm.ViewId(
+                "sp_powerops_models_temp", "PartialBidMatrixCalculationInput", "1"
+            ),
+            data_classes.PartialBidMatrixCalculationOutput: dm.ViewId(
+                "sp_powerops_models_temp", "PartialBidMatrixCalculationOutput", "1"
+            ),
+            data_classes.PowerAsset: dm.ViewId("sp_powerops_models_temp", "PowerAsset", "1"),
+            data_classes.PreprocessorInput: dm.ViewId("sp_powerops_models_temp", "PreprocessorInput", "1"),
+            data_classes.PreprocessorOutput: dm.ViewId("sp_powerops_models_temp", "PreprocessorOutput", "1"),
+            data_classes.PriceArea: dm.ViewId("sp_powerops_models_temp", "PriceArea", "1"),
+            data_classes.PriceProduction: dm.ViewId("sp_powerops_models_temp", "PriceProduction", "1"),
+            data_classes.SHOPResult: dm.ViewId("sp_powerops_models_temp", "SHOPResult", "1"),
+            data_classes.SHOPTimeSeries: dm.ViewId("sp_powerops_models_temp", "SHOPTimeSeries", "1"),
+            data_classes.SHOPTriggerInput: dm.ViewId("sp_powerops_models_temp", "SHOPTriggerInput", "1"),
+            data_classes.SHOPTriggerOutput: dm.ViewId("sp_powerops_models_temp", "SHOPTriggerOutput", "1"),
+            data_classes.Scenario: dm.ViewId("sp_powerops_models_temp", "Scenario", "1"),
+            data_classes.ScenarioSet: dm.ViewId("sp_powerops_models_temp", "ScenarioSet", "1"),
+            data_classes.ShopBasedPartialBidConfiguration: dm.ViewId(
+                "sp_powerops_models_temp", "ShopBasedPartialBidConfiguration", "1"
+            ),
+            data_classes.ShopPartialBidMatrixCalculationInput: dm.ViewId(
+                "sp_powerops_models_temp", "ShopPartialBidMatrixCalculationInput", "1"
+            ),
+            data_classes.TaskDispatcherInput: dm.ViewId("sp_powerops_models_temp", "TaskDispatcherInput", "1"),
+            data_classes.TaskDispatcherOutput: dm.ViewId("sp_powerops_models_temp", "TaskDispatcherOutput", "1"),
         }
         self._view_by_read_class = view_by_read_class
         self._client = client
 
         self.alert = AlertAPI(client, view_by_read_class)
-        self.bid_configuration_shop = BidConfigurationShopAPI(client, view_by_read_class)
-        self.bid_matrix_raw = BidMatrixRawAPI(client, view_by_read_class)
-        self.bid_method_shop_multi_scenario = BidMethodSHOPMultiScenarioAPI(client, view_by_read_class)
+        self.bid_configuration = BidConfigurationAPI(client, view_by_read_class)
+        self.bid_matrix = BidMatrixAPI(client, view_by_read_class)
         self.case = CaseAPI(client, view_by_read_class)
         self.commands = CommandsAPI(client, view_by_read_class)
+        self.function_input = FunctionInputAPI(client, view_by_read_class)
+        self.function_output = FunctionOutputAPI(client, view_by_read_class)
         self.mapping = MappingAPI(client, view_by_read_class)
         self.market_configuration = MarketConfigurationAPI(client, view_by_read_class)
         self.model_template = ModelTemplateAPI(client, view_by_read_class)
-        self.multi_scenario_matrix_raw = MultiScenarioMatrixRawAPI(client, view_by_read_class)
-        self.plant_shop = PlantShopAPI(client, view_by_read_class)
+        self.partial_bid_configuration = PartialBidConfigurationAPI(client, view_by_read_class)
+        self.partial_bid_matrix_calculation_input = PartialBidMatrixCalculationInputAPI(client, view_by_read_class)
+        self.partial_bid_matrix_calculation_output = PartialBidMatrixCalculationOutputAPI(client, view_by_read_class)
+        self.power_asset = PowerAssetAPI(client, view_by_read_class)
         self.preprocessor_input = PreprocessorInputAPI(client, view_by_read_class)
         self.preprocessor_output = PreprocessorOutputAPI(client, view_by_read_class)
         self.price_area = PriceAreaAPI(client, view_by_read_class)
-        self.price_prod_case = PriceProdCaseAPI(client, view_by_read_class)
+        self.price_production = PriceProductionAPI(client, view_by_read_class)
         self.shop_result = SHOPResultAPI(client, view_by_read_class)
-        self.shop_result_price_prod = SHOPResultPriceProdAPI(client, view_by_read_class)
         self.shop_time_series = SHOPTimeSeriesAPI(client, view_by_read_class)
         self.shop_trigger_input = SHOPTriggerInputAPI(client, view_by_read_class)
         self.shop_trigger_output = SHOPTriggerOutputAPI(client, view_by_read_class)
         self.scenario = ScenarioAPI(client, view_by_read_class)
-        self.shop_partial_bid_calculation_input = ShopPartialBidCalculationInputAPI(client, view_by_read_class)
-        self.shop_partial_bid_calculation_output = ShopPartialBidCalculationOutputAPI(client, view_by_read_class)
-        self.task_dispatcher_shop_input = TaskDispatcherShopInputAPI(client, view_by_read_class)
-        self.task_dispatcher_shop_output = TaskDispatcherShopOutputAPI(client, view_by_read_class)
-        self.watercourse_shop = WatercourseShopAPI(client, view_by_read_class)
+        self.scenario_set = ScenarioSetAPI(client, view_by_read_class)
+        self.shop_based_partial_bid_configuration = ShopBasedPartialBidConfigurationAPI(client, view_by_read_class)
+        self.shop_partial_bid_matrix_calculation_input = ShopPartialBidMatrixCalculationInputAPI(
+            client, view_by_read_class
+        )
+        self.task_dispatcher_input = TaskDispatcherInputAPI(client, view_by_read_class)
+        self.task_dispatcher_output = TaskDispatcherOutputAPI(client, view_by_read_class)
 
     def graphql_query(self, query: str, variables: dict[str, Any] | None = None) -> GraphQLList:
         """Execute a GraphQl query against the compute_SHOPBasedDayAhead data model.
 
         Args:
             query (str): The GraphQL query to issue.
             variables (dict[str, Any] | None): An optional dict of variables to pass to the query.
         """
-        data_model_id = dm.DataModelId("sp_powerops_models", "compute_SHOPBasedDayAhead", "1")
+        data_model_id = dm.DataModelId("sp_powerops_models_temp", "compute_SHOPBasedDayAhead", "1")
         result = self._client.data_modeling.graphql.query(data_model_id, query, variables)
         return GraphQLQueryResponse(data_model_id).parse(result)
 
 
-class TotalBidCalculationAPIs:
+class TotalBidMatrixCalculationAPIs:
     """
-    TotalBidCalculationAPIs
+    TotalBidMatrixCalculationAPIs
 
     Data Model:
-        space: sp_powerops_models
-        externalId: compute_TotalBidCalculation
+        space: sp_powerops_models_temp
+        externalId: compute_TotalBidMatrixCalculation
         version: 1
 
     """
 
     def __init__(self, client: CogniteClient):
         view_by_read_class = {
-            data_classes.Alert: dm.ViewId("sp_powerops_models", "Alert", "1"),
-            data_classes.BidDocumentDayAhead: dm.ViewId("sp_powerops_models", "BidDocumentDayAhead", "1"),
-            data_classes.BidMatrix: dm.ViewId("sp_powerops_models", "BidMatrix", "1"),
-            data_classes.BidMatrixRaw: dm.ViewId("sp_powerops_models", "BidMatrixRaw", "1"),
-            data_classes.BidMethodDayAhead: dm.ViewId("sp_powerops_models", "BidMethodDayAhead", "1"),
-            data_classes.BidMethodSHOPMultiScenario: dm.ViewId("sp_powerops_models", "BidMethodSHOPMultiScenario", "1"),
-            data_classes.BidMethodWaterValue: dm.ViewId("sp_powerops_models", "BidMethodWaterValue", "1"),
-            data_classes.Case: dm.ViewId("sp_powerops_models", "Case", "1"),
-            data_classes.Commands: dm.ViewId("sp_powerops_models", "Commands", "1"),
-            data_classes.Mapping: dm.ViewId("sp_powerops_models", "Mapping", "1"),
-            data_classes.MarketConfiguration: dm.ViewId("sp_powerops_models", "MarketConfiguration", "1"),
-            data_classes.ModelTemplate: dm.ViewId("sp_powerops_models", "ModelTemplate", "1"),
-            data_classes.MultiScenarioMatrix: dm.ViewId("sp_powerops_models", "MultiScenarioMatrix", "1"),
-            data_classes.MultiScenarioMatrixRaw: dm.ViewId("sp_powerops_models", "MultiScenarioMatrixRaw", "1"),
-            data_classes.PartialPostProcessingInput: dm.ViewId("sp_powerops_models", "PartialPostProcessingInput", "1"),
-            data_classes.PartialPostProcessingOutput: dm.ViewId(
-                "sp_powerops_models", "PartialPostProcessingOutput", "1"
-            ),
-            data_classes.PriceArea: dm.ViewId("sp_powerops_models", "PriceArea", "1"),
-            data_classes.PriceProdCase: dm.ViewId("sp_powerops_models", "PriceProdCase", "1"),
-            data_classes.SHOPResult: dm.ViewId("sp_powerops_models", "SHOPResult", "1"),
-            data_classes.SHOPResultPriceProd: dm.ViewId("sp_powerops_models", "SHOPResultPriceProd", "1"),
-            data_classes.SHOPTimeSeries: dm.ViewId("sp_powerops_models", "SHOPTimeSeries", "1"),
-            data_classes.Scenario: dm.ViewId("sp_powerops_models", "Scenario", "1"),
+            data_classes.Alert: dm.ViewId("sp_powerops_models_temp", "Alert", "1"),
+            data_classes.BidConfiguration: dm.ViewId("sp_powerops_models_temp", "BidConfiguration", "1"),
+            data_classes.BidDocument: dm.ViewId("sp_powerops_models_temp", "BidDocument", "1"),
+            data_classes.BidDocumentDayAhead: dm.ViewId("sp_powerops_models_temp", "BidDocumentDayAhead", "1"),
+            data_classes.BidMatrix: dm.ViewId("sp_powerops_models_temp", "BidMatrix", "1"),
+            data_classes.Case: dm.ViewId("sp_powerops_models_temp", "Case", "1"),
+            data_classes.Commands: dm.ViewId("sp_powerops_models_temp", "Commands", "1"),
+            data_classes.FunctionInput: dm.ViewId("sp_powerops_models_temp", "FunctionInput", "1"),
+            data_classes.FunctionOutput: dm.ViewId("sp_powerops_models_temp", "FunctionOutput", "1"),
+            data_classes.Mapping: dm.ViewId("sp_powerops_models_temp", "Mapping", "1"),
+            data_classes.MarketConfiguration: dm.ViewId("sp_powerops_models_temp", "MarketConfiguration", "1"),
+            data_classes.ModelTemplate: dm.ViewId("sp_powerops_models_temp", "ModelTemplate", "1"),
+            data_classes.PartialBidConfiguration: dm.ViewId("sp_powerops_models_temp", "PartialBidConfiguration", "1"),
+            data_classes.PowerAsset: dm.ViewId("sp_powerops_models_temp", "PowerAsset", "1"),
+            data_classes.PriceArea: dm.ViewId("sp_powerops_models_temp", "PriceArea", "1"),
+            data_classes.PriceProduction: dm.ViewId("sp_powerops_models_temp", "PriceProduction", "1"),
+            data_classes.SHOPResult: dm.ViewId("sp_powerops_models_temp", "SHOPResult", "1"),
+            data_classes.SHOPTimeSeries: dm.ViewId("sp_powerops_models_temp", "SHOPTimeSeries", "1"),
+            data_classes.Scenario: dm.ViewId("sp_powerops_models_temp", "Scenario", "1"),
             data_classes.TotalBidMatrixCalculationInput: dm.ViewId(
-                "sp_powerops_models", "TotalBidMatrixCalculationInput", "1"
+                "sp_powerops_models_temp", "TotalBidMatrixCalculationInput", "1"
             ),
             data_classes.TotalBidMatrixCalculationOutput: dm.ViewId(
-                "sp_powerops_models", "TotalBidMatrixCalculationOutput", "1"
+                "sp_powerops_models_temp", "TotalBidMatrixCalculationOutput", "1"
             ),
-            data_classes.WatercourseShop: dm.ViewId("sp_powerops_models", "WatercourseShop", "1"),
         }
         self._view_by_read_class = view_by_read_class
         self._client = client
 
         self.alert = AlertAPI(client, view_by_read_class)
+        self.bid_configuration = BidConfigurationAPI(client, view_by_read_class)
+        self.bid_document = BidDocumentAPI(client, view_by_read_class)
         self.bid_document_day_ahead = BidDocumentDayAheadAPI(client, view_by_read_class)
         self.bid_matrix = BidMatrixAPI(client, view_by_read_class)
-        self.bid_matrix_raw = BidMatrixRawAPI(client, view_by_read_class)
-        self.bid_method_day_ahead = BidMethodDayAheadAPI(client, view_by_read_class)
-        self.bid_method_shop_multi_scenario = BidMethodSHOPMultiScenarioAPI(client, view_by_read_class)
-        self.bid_method_water_value = BidMethodWaterValueAPI(client, view_by_read_class)
         self.case = CaseAPI(client, view_by_read_class)
         self.commands = CommandsAPI(client, view_by_read_class)
+        self.function_input = FunctionInputAPI(client, view_by_read_class)
+        self.function_output = FunctionOutputAPI(client, view_by_read_class)
         self.mapping = MappingAPI(client, view_by_read_class)
         self.market_configuration = MarketConfigurationAPI(client, view_by_read_class)
         self.model_template = ModelTemplateAPI(client, view_by_read_class)
-        self.multi_scenario_matrix = MultiScenarioMatrixAPI(client, view_by_read_class)
-        self.multi_scenario_matrix_raw = MultiScenarioMatrixRawAPI(client, view_by_read_class)
-        self.partial_post_processing_input = PartialPostProcessingInputAPI(client, view_by_read_class)
-        self.partial_post_processing_output = PartialPostProcessingOutputAPI(client, view_by_read_class)
+        self.partial_bid_configuration = PartialBidConfigurationAPI(client, view_by_read_class)
+        self.power_asset = PowerAssetAPI(client, view_by_read_class)
         self.price_area = PriceAreaAPI(client, view_by_read_class)
-        self.price_prod_case = PriceProdCaseAPI(client, view_by_read_class)
+        self.price_production = PriceProductionAPI(client, view_by_read_class)
         self.shop_result = SHOPResultAPI(client, view_by_read_class)
-        self.shop_result_price_prod = SHOPResultPriceProdAPI(client, view_by_read_class)
         self.shop_time_series = SHOPTimeSeriesAPI(client, view_by_read_class)
         self.scenario = ScenarioAPI(client, view_by_read_class)
         self.total_bid_matrix_calculation_input = TotalBidMatrixCalculationInputAPI(client, view_by_read_class)
         self.total_bid_matrix_calculation_output = TotalBidMatrixCalculationOutputAPI(client, view_by_read_class)
-        self.watercourse_shop = WatercourseShopAPI(client, view_by_read_class)
 
     def graphql_query(self, query: str, variables: dict[str, Any] | None = None) -> GraphQLList:
-        """Execute a GraphQl query against the compute_TotalBidCalculation data model.
+        """Execute a GraphQl query against the compute_TotalBidMatrixCalculation data model.
 
         Args:
             query (str): The GraphQL query to issue.
             variables (dict[str, Any] | None): An optional dict of variables to pass to the query.
         """
-        data_model_id = dm.DataModelId("sp_powerops_models", "compute_TotalBidCalculation", "1")
+        data_model_id = dm.DataModelId("sp_powerops_models_temp", "compute_TotalBidMatrixCalculation", "1")
         result = self._client.data_modeling.graphql.query(data_model_id, query, variables)
         return GraphQLQueryResponse(data_model_id).parse(result)
 
 
 class WaterValueBasedDayAheadBidProcesAPIs:
     """
     WaterValueBasedDayAheadBidProcesAPIs
 
     Data Model:
-        space: sp_powerops_models
+        space: sp_powerops_models_temp
         externalId: compute_WaterValueBasedDayAheadBid
         version: 1
 
     """
 
     def __init__(self, client: CogniteClient):
         view_by_read_class = {
-            data_classes.Alert: dm.ViewId("sp_powerops_models", "Alert", "1"),
-            data_classes.BidCalculationTask: dm.ViewId("sp_powerops_models", "BidCalculationTask", "1"),
-            data_classes.BidConfigurationWater: dm.ViewId("sp_powerops_models", "BidConfigurationWater", "1"),
-            data_classes.BidMatrixRaw: dm.ViewId("sp_powerops_models", "BidMatrixRaw", "1"),
-            data_classes.BidMethodWaterValue: dm.ViewId("sp_powerops_models", "BidMethodWaterValue", "1"),
-            data_classes.Generator: dm.ViewId("sp_powerops_models", "Generator", "1"),
-            data_classes.GeneratorEfficiencyCurve: dm.ViewId("sp_powerops_models", "GeneratorEfficiencyCurve", "1"),
-            data_classes.MarketConfiguration: dm.ViewId("sp_powerops_models", "MarketConfiguration", "1"),
-            data_classes.Plant: dm.ViewId("sp_powerops_models", "Plant", "1"),
-            data_classes.PriceArea: dm.ViewId("sp_powerops_models", "PriceArea", "1"),
-            data_classes.Reservoir: dm.ViewId("sp_powerops_models", "Reservoir", "1"),
-            data_classes.TaskDispatcherWaterInput: dm.ViewId("sp_powerops_models", "TaskDispatcherWaterInput", "1"),
-            data_classes.TaskDispatcherWaterOutput: dm.ViewId("sp_powerops_models", "TaskDispatcherWaterOutput", "1"),
-            data_classes.TurbineEfficiencyCurve: dm.ViewId("sp_powerops_models", "TurbineEfficiencyCurve", "1"),
-            data_classes.WaterPartialBidCalculationInput: dm.ViewId(
-                "sp_powerops_models", "WaterPartialBidCalculationInput", "1"
+            data_classes.Alert: dm.ViewId("sp_powerops_models_temp", "Alert", "1"),
+            data_classes.BidConfiguration: dm.ViewId("sp_powerops_models_temp", "BidConfiguration", "1"),
+            data_classes.BidMatrix: dm.ViewId("sp_powerops_models_temp", "BidMatrix", "1"),
+            data_classes.FunctionInput: dm.ViewId("sp_powerops_models_temp", "FunctionInput", "1"),
+            data_classes.FunctionOutput: dm.ViewId("sp_powerops_models_temp", "FunctionOutput", "1"),
+            data_classes.Generator: dm.ViewId("sp_powerops_models_temp", "Generator", "1"),
+            data_classes.GeneratorEfficiencyCurve: dm.ViewId(
+                "sp_powerops_models_temp", "GeneratorEfficiencyCurve", "1"
+            ),
+            data_classes.MarketConfiguration: dm.ViewId("sp_powerops_models_temp", "MarketConfiguration", "1"),
+            data_classes.PartialBidConfiguration: dm.ViewId("sp_powerops_models_temp", "PartialBidConfiguration", "1"),
+            data_classes.PartialBidMatrixCalculationInput: dm.ViewId(
+                "sp_powerops_models_temp", "PartialBidMatrixCalculationInput", "1"
+            ),
+            data_classes.PartialBidMatrixCalculationOutput: dm.ViewId(
+                "sp_powerops_models_temp", "PartialBidMatrixCalculationOutput", "1"
             ),
-            data_classes.WaterPartialBidCalculationOutput: dm.ViewId(
-                "sp_powerops_models", "WaterPartialBidCalculationOutput", "1"
+            data_classes.Plant: dm.ViewId("sp_powerops_models_temp", "Plant", "1"),
+            data_classes.PowerAsset: dm.ViewId("sp_powerops_models_temp", "PowerAsset", "1"),
+            data_classes.PriceArea: dm.ViewId("sp_powerops_models_temp", "PriceArea", "1"),
+            data_classes.TaskDispatcherInput: dm.ViewId("sp_powerops_models_temp", "TaskDispatcherInput", "1"),
+            data_classes.TaskDispatcherOutput: dm.ViewId("sp_powerops_models_temp", "TaskDispatcherOutput", "1"),
+            data_classes.TurbineEfficiencyCurve: dm.ViewId("sp_powerops_models_temp", "TurbineEfficiencyCurve", "1"),
+            data_classes.WaterValueBasedPartialBidConfiguration: dm.ViewId(
+                "sp_powerops_models_temp", "WaterValueBasedPartialBidConfiguration", "1"
+            ),
+            data_classes.WaterValueBasedPartialBidMatrixCalculationInput: dm.ViewId(
+                "sp_powerops_models_temp", "WaterValueBasedPartialBidMatrixCalculationInput", "1"
             ),
-            data_classes.Watercourse: dm.ViewId("sp_powerops_models", "Watercourse", "1"),
         }
         self._view_by_read_class = view_by_read_class
         self._client = client
 
         self.alert = AlertAPI(client, view_by_read_class)
-        self.bid_calculation_task = BidCalculationTaskAPI(client, view_by_read_class)
-        self.bid_configuration_water = BidConfigurationWaterAPI(client, view_by_read_class)
-        self.bid_matrix_raw = BidMatrixRawAPI(client, view_by_read_class)
-        self.bid_method_water_value = BidMethodWaterValueAPI(client, view_by_read_class)
+        self.bid_configuration = BidConfigurationAPI(client, view_by_read_class)
+        self.bid_matrix = BidMatrixAPI(client, view_by_read_class)
+        self.function_input = FunctionInputAPI(client, view_by_read_class)
+        self.function_output = FunctionOutputAPI(client, view_by_read_class)
         self.generator = GeneratorAPI(client, view_by_read_class)
         self.generator_efficiency_curve = GeneratorEfficiencyCurveAPI(client, view_by_read_class)
         self.market_configuration = MarketConfigurationAPI(client, view_by_read_class)
+        self.partial_bid_configuration = PartialBidConfigurationAPI(client, view_by_read_class)
+        self.partial_bid_matrix_calculation_input = PartialBidMatrixCalculationInputAPI(client, view_by_read_class)
+        self.partial_bid_matrix_calculation_output = PartialBidMatrixCalculationOutputAPI(client, view_by_read_class)
         self.plant = PlantAPI(client, view_by_read_class)
+        self.power_asset = PowerAssetAPI(client, view_by_read_class)
         self.price_area = PriceAreaAPI(client, view_by_read_class)
-        self.reservoir = ReservoirAPI(client, view_by_read_class)
-        self.task_dispatcher_water_input = TaskDispatcherWaterInputAPI(client, view_by_read_class)
-        self.task_dispatcher_water_output = TaskDispatcherWaterOutputAPI(client, view_by_read_class)
+        self.task_dispatcher_input = TaskDispatcherInputAPI(client, view_by_read_class)
+        self.task_dispatcher_output = TaskDispatcherOutputAPI(client, view_by_read_class)
         self.turbine_efficiency_curve = TurbineEfficiencyCurveAPI(client, view_by_read_class)
-        self.water_partial_bid_calculation_input = WaterPartialBidCalculationInputAPI(client, view_by_read_class)
-        self.water_partial_bid_calculation_output = WaterPartialBidCalculationOutputAPI(client, view_by_read_class)
-        self.watercourse = WatercourseAPI(client, view_by_read_class)
+        self.water_value_based_partial_bid_configuration = WaterValueBasedPartialBidConfigurationAPI(
+            client, view_by_read_class
+        )
+        self.water_value_based_partial_bid_matrix_calculation_input = (
+            WaterValueBasedPartialBidMatrixCalculationInputAPI(client, view_by_read_class)
+        )
 
     def graphql_query(self, query: str, variables: dict[str, Any] | None = None) -> GraphQLList:
         """Execute a GraphQl query against the compute_WaterValueBasedDayAheadBid data model.
 
         Args:
             query (str): The GraphQL query to issue.
             variables (dict[str, Any] | None): An optional dict of variables to pass to the query.
         """
-        data_model_id = dm.DataModelId("sp_powerops_models", "compute_WaterValueBasedDayAheadBid", "1")
+        data_model_id = dm.DataModelId("sp_powerops_models_temp", "compute_WaterValueBasedDayAheadBid", "1")
         result = self._client.data_modeling.graphql.query(data_model_id, query, variables)
         return GraphQLQueryResponse(data_model_id).parse(result)
 
 
 class DayAheadConfigurationAPIs:
     """
     DayAheadConfigurationAPIs
 
     Data Model:
-        space: sp_powerops_models
+        space: sp_powerops_models_temp
         externalId: config_DayAheadConfiguration
         version: 1
 
     """
 
     def __init__(self, client: CogniteClient):
         view_by_read_class = {
-            data_classes.BidConfiguration: dm.ViewId("sp_powerops_models", "BidConfiguration", "1"),
-            data_classes.BidConfigurationShop: dm.ViewId("sp_powerops_models", "BidConfigurationShop", "1"),
-            data_classes.BidConfigurationWater: dm.ViewId("sp_powerops_models", "BidConfigurationWater", "1"),
-            data_classes.BidMethod: dm.ViewId("sp_powerops_models", "BidMethod", "1"),
-            data_classes.BidMethodDayAhead: dm.ViewId("sp_powerops_models", "BidMethodDayAhead", "1"),
-            data_classes.BidMethodSHOPMultiScenario: dm.ViewId("sp_powerops_models", "BidMethodSHOPMultiScenario", "1"),
-            data_classes.BidMethodWaterValue: dm.ViewId("sp_powerops_models", "BidMethodWaterValue", "1"),
-            data_classes.Commands: dm.ViewId("sp_powerops_models", "Commands", "1"),
-            data_classes.Generator: dm.ViewId("sp_powerops_models", "Generator", "1"),
-            data_classes.GeneratorEfficiencyCurve: dm.ViewId("sp_powerops_models", "GeneratorEfficiencyCurve", "1"),
-            data_classes.Mapping: dm.ViewId("sp_powerops_models", "Mapping", "1"),
-            data_classes.MarketConfiguration: dm.ViewId("sp_powerops_models", "MarketConfiguration", "1"),
-            data_classes.ModelTemplate: dm.ViewId("sp_powerops_models", "ModelTemplate", "1"),
-            data_classes.Plant: dm.ViewId("sp_powerops_models", "Plant", "1"),
-            data_classes.PlantShop: dm.ViewId("sp_powerops_models", "PlantShop", "1"),
-            data_classes.PriceArea: dm.ViewId("sp_powerops_models", "PriceArea", "1"),
-            data_classes.Reservoir: dm.ViewId("sp_powerops_models", "Reservoir", "1"),
-            data_classes.Scenario: dm.ViewId("sp_powerops_models", "Scenario", "1"),
-            data_classes.TurbineEfficiencyCurve: dm.ViewId("sp_powerops_models", "TurbineEfficiencyCurve", "1"),
-            data_classes.Watercourse: dm.ViewId("sp_powerops_models", "Watercourse", "1"),
-            data_classes.WatercourseShop: dm.ViewId("sp_powerops_models", "WatercourseShop", "1"),
+            data_classes.BidConfiguration: dm.ViewId("sp_powerops_models_temp", "BidConfiguration", "1"),
+            data_classes.Commands: dm.ViewId("sp_powerops_models_temp", "Commands", "1"),
+            data_classes.Generator: dm.ViewId("sp_powerops_models_temp", "Generator", "1"),
+            data_classes.GeneratorEfficiencyCurve: dm.ViewId(
+                "sp_powerops_models_temp", "GeneratorEfficiencyCurve", "1"
+            ),
+            data_classes.Mapping: dm.ViewId("sp_powerops_models_temp", "Mapping", "1"),
+            data_classes.MarketConfiguration: dm.ViewId("sp_powerops_models_temp", "MarketConfiguration", "1"),
+            data_classes.ModelTemplate: dm.ViewId("sp_powerops_models_temp", "ModelTemplate", "1"),
+            data_classes.PartialBidConfiguration: dm.ViewId("sp_powerops_models_temp", "PartialBidConfiguration", "1"),
+            data_classes.Plant: dm.ViewId("sp_powerops_models_temp", "Plant", "1"),
+            data_classes.PowerAsset: dm.ViewId("sp_powerops_models_temp", "PowerAsset", "1"),
+            data_classes.PriceArea: dm.ViewId("sp_powerops_models_temp", "PriceArea", "1"),
+            data_classes.Scenario: dm.ViewId("sp_powerops_models_temp", "Scenario", "1"),
+            data_classes.ScenarioSet: dm.ViewId("sp_powerops_models_temp", "ScenarioSet", "1"),
+            data_classes.ShopBasedPartialBidConfiguration: dm.ViewId(
+                "sp_powerops_models_temp", "ShopBasedPartialBidConfiguration", "1"
+            ),
+            data_classes.TurbineEfficiencyCurve: dm.ViewId("sp_powerops_models_temp", "TurbineEfficiencyCurve", "1"),
+            data_classes.WaterValueBasedPartialBidConfiguration: dm.ViewId(
+                "sp_powerops_models_temp", "WaterValueBasedPartialBidConfiguration", "1"
+            ),
         }
         self._view_by_read_class = view_by_read_class
         self._client = client
 
         self.bid_configuration = BidConfigurationAPI(client, view_by_read_class)
-        self.bid_configuration_shop = BidConfigurationShopAPI(client, view_by_read_class)
-        self.bid_configuration_water = BidConfigurationWaterAPI(client, view_by_read_class)
-        self.bid_method = BidMethodAPI(client, view_by_read_class)
-        self.bid_method_day_ahead = BidMethodDayAheadAPI(client, view_by_read_class)
-        self.bid_method_shop_multi_scenario = BidMethodSHOPMultiScenarioAPI(client, view_by_read_class)
-        self.bid_method_water_value = BidMethodWaterValueAPI(client, view_by_read_class)
         self.commands = CommandsAPI(client, view_by_read_class)
         self.generator = GeneratorAPI(client, view_by_read_class)
         self.generator_efficiency_curve = GeneratorEfficiencyCurveAPI(client, view_by_read_class)
         self.mapping = MappingAPI(client, view_by_read_class)
         self.market_configuration = MarketConfigurationAPI(client, view_by_read_class)
         self.model_template = ModelTemplateAPI(client, view_by_read_class)
+        self.partial_bid_configuration = PartialBidConfigurationAPI(client, view_by_read_class)
         self.plant = PlantAPI(client, view_by_read_class)
-        self.plant_shop = PlantShopAPI(client, view_by_read_class)
+        self.power_asset = PowerAssetAPI(client, view_by_read_class)
         self.price_area = PriceAreaAPI(client, view_by_read_class)
-        self.reservoir = ReservoirAPI(client, view_by_read_class)
         self.scenario = ScenarioAPI(client, view_by_read_class)
+        self.scenario_set = ScenarioSetAPI(client, view_by_read_class)
+        self.shop_based_partial_bid_configuration = ShopBasedPartialBidConfigurationAPI(client, view_by_read_class)
         self.turbine_efficiency_curve = TurbineEfficiencyCurveAPI(client, view_by_read_class)
-        self.watercourse = WatercourseAPI(client, view_by_read_class)
-        self.watercourse_shop = WatercourseShopAPI(client, view_by_read_class)
+        self.water_value_based_partial_bid_configuration = WaterValueBasedPartialBidConfigurationAPI(
+            client, view_by_read_class
+        )
 
     def graphql_query(self, query: str, variables: dict[str, Any] | None = None) -> GraphQLList:
         """Execute a GraphQl query against the config_DayAheadConfiguration data model.
 
         Args:
             query (str): The GraphQL query to issue.
             variables (dict[str, Any] | None): An optional dict of variables to pass to the query.
         """
-        data_model_id = dm.DataModelId("sp_powerops_models", "config_DayAheadConfiguration", "1")
+        data_model_id = dm.DataModelId("sp_powerops_models_temp", "config_DayAheadConfiguration", "1")
         result = self._client.data_modeling.graphql.query(data_model_id, query, variables)
         return GraphQLQueryResponse(data_model_id).parse(result)
 
 
 class AFRRBidAPIs:
     """
     AFRRBidAPIs
 
     Data Model:
-        space: sp_powerops_models
+        space: sp_powerops_models_temp
         externalId: frontend_AFRRBid
         version: 1
 
     """
 
     def __init__(self, client: CogniteClient):
         view_by_read_class = {
-            data_classes.Alert: dm.ViewId("sp_powerops_models", "Alert", "1"),
-            data_classes.BidDocumentAFRR: dm.ViewId("sp_powerops_models", "BidDocumentAFRR", "1"),
-            data_classes.BidMethodAFRR: dm.ViewId("sp_powerops_models", "BidMethodAFRR", "1"),
-            data_classes.BidRow: dm.ViewId("sp_powerops_models", "BidRow", "1"),
-            data_classes.PriceAreaAFRR: dm.ViewId("sp_powerops_models", "PriceAreaAFRR", "1"),
+            data_classes.Alert: dm.ViewId("sp_powerops_models_temp", "Alert", "1"),
+            data_classes.BidDocument: dm.ViewId("sp_powerops_models_temp", "BidDocument", "1"),
+            data_classes.BidDocumentAFRR: dm.ViewId("sp_powerops_models_temp", "BidDocumentAFRR", "1"),
+            data_classes.BidRow: dm.ViewId("sp_powerops_models_temp", "BidRow", "1"),
+            data_classes.PowerAsset: dm.ViewId("sp_powerops_models_temp", "PowerAsset", "1"),
+            data_classes.PriceArea: dm.ViewId("sp_powerops_models_temp", "PriceArea", "1"),
+            data_classes.PriceAreaAFRR: dm.ViewId("sp_powerops_models_temp", "PriceAreaAFRR", "1"),
         }
         self._view_by_read_class = view_by_read_class
         self._client = client
 
         self.alert = AlertAPI(client, view_by_read_class)
+        self.bid_document = BidDocumentAPI(client, view_by_read_class)
         self.bid_document_afrr = BidDocumentAFRRAPI(client, view_by_read_class)
-        self.bid_method_afrr = BidMethodAFRRAPI(client, view_by_read_class)
         self.bid_row = BidRowAPI(client, view_by_read_class)
+        self.power_asset = PowerAssetAPI(client, view_by_read_class)
+        self.price_area = PriceAreaAPI(client, view_by_read_class)
         self.price_area_afrr = PriceAreaAFRRAPI(client, view_by_read_class)
 
     def graphql_query(self, query: str, variables: dict[str, Any] | None = None) -> GraphQLList:
         """Execute a GraphQl query against the frontend_AFRRBid data model.
 
         Args:
             query (str): The GraphQL query to issue.
             variables (dict[str, Any] | None): An optional dict of variables to pass to the query.
         """
-        data_model_id = dm.DataModelId("sp_powerops_models", "frontend_AFRRBid", "1")
+        data_model_id = dm.DataModelId("sp_powerops_models_temp", "frontend_AFRRBid", "1")
         result = self._client.data_modeling.graphql.query(data_model_id, query, variables)
         return GraphQLQueryResponse(data_model_id).parse(result)
 
 
 class PowerAssetAPIs:
     """
     PowerAssetAPIs
 
     Data Model:
-        space: sp_powerops_models
+        space: sp_powerops_models_temp
         externalId: frontend_Asset
         version: 1
 
     """
 
     def __init__(self, client: CogniteClient):
         view_by_read_class = {
-            data_classes.BidMethodDayAhead: dm.ViewId("sp_powerops_models", "BidMethodDayAhead", "1"),
-            data_classes.Generator: dm.ViewId("sp_powerops_models", "Generator", "1"),
-            data_classes.GeneratorEfficiencyCurve: dm.ViewId("sp_powerops_models", "GeneratorEfficiencyCurve", "1"),
-            data_classes.Plant: dm.ViewId("sp_powerops_models", "Plant", "1"),
-            data_classes.PriceAreaAsset: dm.ViewId("sp_powerops_models", "PriceAreaAsset", "1"),
-            data_classes.Reservoir: dm.ViewId("sp_powerops_models", "Reservoir", "1"),
-            data_classes.TurbineEfficiencyCurve: dm.ViewId("sp_powerops_models", "TurbineEfficiencyCurve", "1"),
-            data_classes.Watercourse: dm.ViewId("sp_powerops_models", "Watercourse", "1"),
+            data_classes.Generator: dm.ViewId("sp_powerops_models_temp", "Generator", "1"),
+            data_classes.GeneratorEfficiencyCurve: dm.ViewId(
+                "sp_powerops_models_temp", "GeneratorEfficiencyCurve", "1"
+            ),
+            data_classes.Plant: dm.ViewId("sp_powerops_models_temp", "Plant", "1"),
+            data_classes.PowerAsset: dm.ViewId("sp_powerops_models_temp", "PowerAsset", "1"),
+            data_classes.PriceArea: dm.ViewId("sp_powerops_models_temp", "PriceArea", "1"),
+            data_classes.TurbineEfficiencyCurve: dm.ViewId("sp_powerops_models_temp", "TurbineEfficiencyCurve", "1"),
         }
         self._view_by_read_class = view_by_read_class
         self._client = client
 
-        self.bid_method_day_ahead = BidMethodDayAheadAPI(client, view_by_read_class)
         self.generator = GeneratorAPI(client, view_by_read_class)
         self.generator_efficiency_curve = GeneratorEfficiencyCurveAPI(client, view_by_read_class)
         self.plant = PlantAPI(client, view_by_read_class)
-        self.price_area_asset = PriceAreaAssetAPI(client, view_by_read_class)
-        self.reservoir = ReservoirAPI(client, view_by_read_class)
+        self.power_asset = PowerAssetAPI(client, view_by_read_class)
+        self.price_area = PriceAreaAPI(client, view_by_read_class)
         self.turbine_efficiency_curve = TurbineEfficiencyCurveAPI(client, view_by_read_class)
-        self.watercourse = WatercourseAPI(client, view_by_read_class)
 
     def graphql_query(self, query: str, variables: dict[str, Any] | None = None) -> GraphQLList:
         """Execute a GraphQl query against the frontend_Asset data model.
 
         Args:
             query (str): The GraphQL query to issue.
             variables (dict[str, Any] | None): An optional dict of variables to pass to the query.
         """
-        data_model_id = dm.DataModelId("sp_powerops_models", "frontend_Asset", "1")
+        data_model_id = dm.DataModelId("sp_powerops_models_temp", "frontend_Asset", "1")
         result = self._client.data_modeling.graphql.query(data_model_id, query, variables)
         return GraphQLQueryResponse(data_model_id).parse(result)
 
 
 class DayAheadBidAPIs:
     """
     DayAheadBidAPIs
 
     Data Model:
-        space: sp_powerops_models
+        space: sp_powerops_models_temp
         externalId: frontend_DayAheadBid
         version: 1
 
     """
 
     def __init__(self, client: CogniteClient):
         view_by_read_class = {
-            data_classes.Alert: dm.ViewId("sp_powerops_models", "Alert", "1"),
-            data_classes.BasicBidMatrix: dm.ViewId("sp_powerops_models", "BasicBidMatrix", "1"),
-            data_classes.BidDocumentDayAhead: dm.ViewId("sp_powerops_models", "BidDocumentDayAhead", "1"),
-            data_classes.BidMatrix: dm.ViewId("sp_powerops_models", "BidMatrix", "1"),
-            data_classes.BidMethodCustom: dm.ViewId("sp_powerops_models", "BidMethodCustom", "1"),
-            data_classes.BidMethodDayAhead: dm.ViewId("sp_powerops_models", "BidMethodDayAhead", "1"),
-            data_classes.BidMethodSHOPMultiScenario: dm.ViewId("sp_powerops_models", "BidMethodSHOPMultiScenario", "1"),
-            data_classes.BidMethodWaterValue: dm.ViewId("sp_powerops_models", "BidMethodWaterValue", "1"),
-            data_classes.Case: dm.ViewId("sp_powerops_models", "Case", "1"),
-            data_classes.Commands: dm.ViewId("sp_powerops_models", "Commands", "1"),
-            data_classes.CustomBidMatrix: dm.ViewId("sp_powerops_models", "CustomBidMatrix", "1"),
-            data_classes.Mapping: dm.ViewId("sp_powerops_models", "Mapping", "1"),
-            data_classes.ModelTemplate: dm.ViewId("sp_powerops_models", "ModelTemplate", "1"),
-            data_classes.MultiScenarioMatrix: dm.ViewId("sp_powerops_models", "MultiScenarioMatrix", "1"),
-            data_classes.PriceArea: dm.ViewId("sp_powerops_models", "PriceArea", "1"),
-            data_classes.PriceProdCase: dm.ViewId("sp_powerops_models", "PriceProdCase", "1"),
-            data_classes.Scenario: dm.ViewId("sp_powerops_models", "Scenario", "1"),
-            data_classes.WatercourseShop: dm.ViewId("sp_powerops_models", "WatercourseShop", "1"),
+            data_classes.Alert: dm.ViewId("sp_powerops_models_temp", "Alert", "1"),
+            data_classes.BidConfiguration: dm.ViewId("sp_powerops_models_temp", "BidConfiguration", "1"),
+            data_classes.BidDocument: dm.ViewId("sp_powerops_models_temp", "BidDocument", "1"),
+            data_classes.BidDocumentDayAhead: dm.ViewId("sp_powerops_models_temp", "BidDocumentDayAhead", "1"),
+            data_classes.BidMatrix: dm.ViewId("sp_powerops_models_temp", "BidMatrix", "1"),
+            data_classes.Case: dm.ViewId("sp_powerops_models_temp", "Case", "1"),
+            data_classes.Commands: dm.ViewId("sp_powerops_models_temp", "Commands", "1"),
+            data_classes.Mapping: dm.ViewId("sp_powerops_models_temp", "Mapping", "1"),
+            data_classes.MarketConfiguration: dm.ViewId("sp_powerops_models_temp", "MarketConfiguration", "1"),
+            data_classes.ModelTemplate: dm.ViewId("sp_powerops_models_temp", "ModelTemplate", "1"),
+            data_classes.PartialBidConfiguration: dm.ViewId("sp_powerops_models_temp", "PartialBidConfiguration", "1"),
+            data_classes.PowerAsset: dm.ViewId("sp_powerops_models_temp", "PowerAsset", "1"),
+            data_classes.PriceArea: dm.ViewId("sp_powerops_models_temp", "PriceArea", "1"),
+            data_classes.PriceProduction: dm.ViewId("sp_powerops_models_temp", "PriceProduction", "1"),
+            data_classes.SHOPResult: dm.ViewId("sp_powerops_models_temp", "SHOPResult", "1"),
+            data_classes.SHOPTimeSeries: dm.ViewId("sp_powerops_models_temp", "SHOPTimeSeries", "1"),
+            data_classes.Scenario: dm.ViewId("sp_powerops_models_temp", "Scenario", "1"),
         }
         self._view_by_read_class = view_by_read_class
         self._client = client
 
         self.alert = AlertAPI(client, view_by_read_class)
-        self.basic_bid_matrix = BasicBidMatrixAPI(client, view_by_read_class)
+        self.bid_configuration = BidConfigurationAPI(client, view_by_read_class)
+        self.bid_document = BidDocumentAPI(client, view_by_read_class)
         self.bid_document_day_ahead = BidDocumentDayAheadAPI(client, view_by_read_class)
         self.bid_matrix = BidMatrixAPI(client, view_by_read_class)
-        self.bid_method_custom = BidMethodCustomAPI(client, view_by_read_class)
-        self.bid_method_day_ahead = BidMethodDayAheadAPI(client, view_by_read_class)
-        self.bid_method_shop_multi_scenario = BidMethodSHOPMultiScenarioAPI(client, view_by_read_class)
-        self.bid_method_water_value = BidMethodWaterValueAPI(client, view_by_read_class)
         self.case = CaseAPI(client, view_by_read_class)
         self.commands = CommandsAPI(client, view_by_read_class)
-        self.custom_bid_matrix = CustomBidMatrixAPI(client, view_by_read_class)
         self.mapping = MappingAPI(client, view_by_read_class)
+        self.market_configuration = MarketConfigurationAPI(client, view_by_read_class)
         self.model_template = ModelTemplateAPI(client, view_by_read_class)
-        self.multi_scenario_matrix = MultiScenarioMatrixAPI(client, view_by_read_class)
+        self.partial_bid_configuration = PartialBidConfigurationAPI(client, view_by_read_class)
+        self.power_asset = PowerAssetAPI(client, view_by_read_class)
         self.price_area = PriceAreaAPI(client, view_by_read_class)
-        self.price_prod_case = PriceProdCaseAPI(client, view_by_read_class)
+        self.price_production = PriceProductionAPI(client, view_by_read_class)
+        self.shop_result = SHOPResultAPI(client, view_by_read_class)
+        self.shop_time_series = SHOPTimeSeriesAPI(client, view_by_read_class)
         self.scenario = ScenarioAPI(client, view_by_read_class)
-        self.watercourse_shop = WatercourseShopAPI(client, view_by_read_class)
 
     def graphql_query(self, query: str, variables: dict[str, Any] | None = None) -> GraphQLList:
         """Execute a GraphQl query against the frontend_DayAheadBid data model.
 
         Args:
             query (str): The GraphQL query to issue.
             variables (dict[str, Any] | None): An optional dict of variables to pass to the query.
         """
-        data_model_id = dm.DataModelId("sp_powerops_models", "frontend_DayAheadBid", "1")
+        data_model_id = dm.DataModelId("sp_powerops_models_temp", "frontend_DayAheadBid", "1")
         result = self._client.data_modeling.graphql.query(data_model_id, query, variables)
         return GraphQLQueryResponse(data_model_id).parse(result)
 
 
 class PowerOpsModelsV1Client:
     """
     PowerOpsModelsV1Client
 
     Generated with:
-        pygen = 0.99.14
+        pygen = 0.99.17
         cognite-sdk = 7.26.2
         pydantic = 2.6.4
 
     """
 
     def __init__(self, config_or_client: CogniteClient | ClientConfig):
         if isinstance(config_or_client, CogniteClient):
             client = config_or_client
         elif isinstance(config_or_client, ClientConfig):
             client = CogniteClient(config_or_client)
         else:
             raise ValueError(f"Expected CogniteClient or ClientConfig, got {type(config_or_client)}")
         # The client name is used for aggregated logging of Pygen Usage
-        client.config.client_name = "CognitePygen:0.99.14"
+        client.config.client_name = "CognitePygen:0.99.17"
 
         self.shop_based_day_ahead_bid_process = SHOPBasedDayAheadBidProcesAPIs(client)
-        self.total_bid_calculation = TotalBidCalculationAPIs(client)
+        self.total_bid_matrix_calculation = TotalBidMatrixCalculationAPIs(client)
         self.water_value_based_day_ahead_bid_process = WaterValueBasedDayAheadBidProcesAPIs(client)
         self.day_ahead_configuration = DayAheadConfigurationAPIs(client)
         self.afrr_bid = AFRRBidAPIs(client)
         self.power_asset = PowerAssetAPIs(client)
         self.day_ahead_bid = DayAheadBidAPIs(client)
 
         self._client = client
         self._view_by_read_class = {
             k: v
             for api in [
                 self.shop_based_day_ahead_bid_process,
-                self.total_bid_calculation,
+                self.total_bid_matrix_calculation,
                 self.water_value_based_day_ahead_bid_process,
                 self.day_ahead_configuration,
                 self.afrr_bid,
                 self.power_asset,
                 self.day_ahead_bid,
             ]
             for k, v in api._view_by_read_class.items()
@@ -666,16 +664,16 @@
         Returns:
             The instance(s), i.e., nodes and edges which has been deleted. Empty list if nothing was deleted.
 
         Examples:
 
             Delete item by id:
 
-                >>> from omni import OmniClient
-                >>> client = OmniClient()
+                >>> from cognite.powerops.client._generated.v1 import PowerOpsModelsV1Client
+                >>> client = PowerOpsModelsV1Client()
                 >>> client.delete("my_node_external_id")
         """
         if isinstance(external_id, str):
             return self._client.data_modeling.instances.delete(nodes=(space, external_id))
         else:
             return self._client.data_modeling.instances.delete(
                 nodes=[(space, id) for id in external_id],
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_alert.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_alert.py`

 * *Files 6% similar despite different names*

```diff
@@ -32,21 +32,30 @@
     "AlertWriteList",
     "AlertApplyList",
     "AlertFields",
     "AlertTextFields",
 ]
 
 
-AlertTextFields = Literal["title", "description", "severity", "alert_type", "calculation_run"]
+AlertTextFields = Literal["process_id", "title", "description", "severity", "alert_type", "calculation_run"]
 AlertFields = Literal[
-    "time", "title", "description", "severity", "alert_type", "status_code", "event_ids", "calculation_run"
+    "time",
+    "process_id",
+    "title",
+    "description",
+    "severity",
+    "alert_type",
+    "status_code",
+    "event_ids",
+    "calculation_run",
 ]
 
 _ALERT_PROPERTIES_BY_FIELD = {
     "time": "time",
+    "process_id": "processId",
     "title": "title",
     "description": "description",
     "severity": "severity",
     "alert_type": "alertType",
     "status_code": "statusCode",
     "event_ids": "eventIds",
     "calculation_run": "calculationRun",
@@ -60,25 +69,27 @@
     It is used when retrieving data from CDF using GraphQL.
 
     Args:
         space: The space where the node is located.
         external_id: The external id of the alert.
         data_record: The data record of the alert node.
         time: Timestamp that the alert occurred (within the workflow)
+        process_id: Process ID in the workflow that the alert is related to
         title: Summary description of the alert
         description: Detailed description of the alert
         severity: CRITICAL (calculation could not completed) WARNING  (calculation completed, with major issue) INFO     (calculation completed, with minor issues)
         alert_type: Classification of the alert (not in current alerting implementation)
         status_code: Unique status code for the alert. May be used by the frontend to avoid use of hardcoded description (i.e. like a translation)
         event_ids: An array of associated alert CDF Events (e.g. SHOP Run events)
         calculation_run: The identifier of the parent Bid Calculation (required so tha alerts can be created befor the BidDocument)
     """
 
-    view_id = dm.ViewId("sp_powerops_models", "Alert", "1")
+    view_id = dm.ViewId("sp_powerops_models_temp", "Alert", "1")
     time: Optional[datetime.datetime] = None
+    process_id: Optional[str] = Field(None, alias="processId")
     title: Optional[str] = None
     description: Optional[str] = None
     severity: Optional[str] = None
     alert_type: Optional[str] = Field(None, alias="alertType")
     status_code: Optional[int] = Field(None, alias="statusCode")
     event_ids: Optional[list[int]] = Field(None, alias="eventIds")
     calculation_run: Optional[str] = Field(None, alias="calculationRun")
@@ -103,14 +114,15 @@
             external_id=self.external_id,
             data_record=DataRecord(
                 version=0,
                 last_updated_time=self.data_record.last_updated_time,
                 created_time=self.data_record.created_time,
             ),
             time=self.time,
+            process_id=self.process_id,
             title=self.title,
             description=self.description,
             severity=self.severity,
             alert_type=self.alert_type,
             status_code=self.status_code,
             event_ids=self.event_ids,
             calculation_run=self.calculation_run,
@@ -119,14 +131,15 @@
     def as_write(self) -> AlertWrite:
         """Convert this GraphQL format of alert to the writing format."""
         return AlertWrite(
             space=self.space,
             external_id=self.external_id,
             data_record=DataRecordWrite(existing_version=0),
             time=self.time,
+            process_id=self.process_id,
             title=self.title,
             description=self.description,
             severity=self.severity,
             alert_type=self.alert_type,
             status_code=self.status_code,
             event_ids=self.event_ids,
             calculation_run=self.calculation_run,
@@ -139,26 +152,28 @@
     It is used to when data is retrieved from CDF.
 
     Args:
         space: The space where the node is located.
         external_id: The external id of the alert.
         data_record: The data record of the alert node.
         time: Timestamp that the alert occurred (within the workflow)
+        process_id: Process ID in the workflow that the alert is related to
         title: Summary description of the alert
         description: Detailed description of the alert
         severity: CRITICAL (calculation could not completed) WARNING  (calculation completed, with major issue) INFO     (calculation completed, with minor issues)
         alert_type: Classification of the alert (not in current alerting implementation)
         status_code: Unique status code for the alert. May be used by the frontend to avoid use of hardcoded description (i.e. like a translation)
         event_ids: An array of associated alert CDF Events (e.g. SHOP Run events)
         calculation_run: The identifier of the parent Bid Calculation (required so tha alerts can be created befor the BidDocument)
     """
 
     space: str = DEFAULT_INSTANCE_SPACE
-    node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference("sp_powerops_types", "Alert")
+    node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference("sp_powerops_types_temp", "Alert")
     time: datetime.datetime
+    process_id: str = Field(alias="processId")
     title: str
     description: Optional[str] = None
     severity: Optional[str] = None
     alert_type: Optional[str] = Field(None, alias="alertType")
     status_code: Optional[int] = Field(None, alias="statusCode")
     event_ids: Optional[list[int]] = Field(None, alias="eventIds")
     calculation_run: Optional[str] = Field(None, alias="calculationRun")
@@ -166,14 +181,15 @@
     def as_write(self) -> AlertWrite:
         """Convert this read version of alert to the writing version."""
         return AlertWrite(
             space=self.space,
             external_id=self.external_id,
             data_record=DataRecordWrite(existing_version=self.data_record.version),
             time=self.time,
+            process_id=self.process_id,
             title=self.title,
             description=self.description,
             severity=self.severity,
             alert_type=self.alert_type,
             status_code=self.status_code,
             event_ids=self.event_ids,
             calculation_run=self.calculation_run,
@@ -195,26 +211,28 @@
     It is used to when data is sent to CDF.
 
     Args:
         space: The space where the node is located.
         external_id: The external id of the alert.
         data_record: The data record of the alert node.
         time: Timestamp that the alert occurred (within the workflow)
+        process_id: Process ID in the workflow that the alert is related to
         title: Summary description of the alert
         description: Detailed description of the alert
         severity: CRITICAL (calculation could not completed) WARNING  (calculation completed, with major issue) INFO     (calculation completed, with minor issues)
         alert_type: Classification of the alert (not in current alerting implementation)
         status_code: Unique status code for the alert. May be used by the frontend to avoid use of hardcoded description (i.e. like a translation)
         event_ids: An array of associated alert CDF Events (e.g. SHOP Run events)
         calculation_run: The identifier of the parent Bid Calculation (required so tha alerts can be created befor the BidDocument)
     """
 
     space: str = DEFAULT_INSTANCE_SPACE
-    node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference("sp_powerops_types", "Alert")
+    node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference("sp_powerops_types_temp", "Alert")
     time: datetime.datetime
+    process_id: str = Field(alias="processId")
     title: str
     description: Optional[str] = None
     severity: Optional[str] = None
     alert_type: Optional[str] = Field(None, alias="alertType")
     status_code: Optional[int] = Field(None, alias="statusCode")
     event_ids: Optional[list[int]] = Field(None, alias="eventIds")
     calculation_run: Optional[str] = Field(None, alias="calculationRun")
@@ -226,21 +244,24 @@
         write_none: bool = False,
         allow_version_increase: bool = False,
     ) -> ResourcesWrite:
         resources = ResourcesWrite()
         if self.as_tuple_id() in cache:
             return resources
 
-        write_view = (view_by_read_class or {}).get(Alert, dm.ViewId("sp_powerops_models", "Alert", "1"))
+        write_view = (view_by_read_class or {}).get(Alert, dm.ViewId("sp_powerops_models_temp", "Alert", "1"))
 
         properties: dict[str, Any] = {}
 
         if self.time is not None:
             properties["time"] = self.time.isoformat(timespec="milliseconds") if self.time else None
 
+        if self.process_id is not None:
+            properties["processId"] = self.process_id
+
         if self.title is not None:
             properties["title"] = self.title
 
         if self.description is not None or write_none:
             properties["description"] = self.description
 
         if self.severity is not None or write_none:
@@ -317,14 +338,16 @@
 class AlertApplyList(AlertWriteList): ...
 
 
 def _create_alert_filter(
     view_id: dm.ViewId,
     min_time: datetime.datetime | None = None,
     max_time: datetime.datetime | None = None,
+    process_id: str | list[str] | None = None,
+    process_id_prefix: str | None = None,
     title: str | list[str] | None = None,
     title_prefix: str | None = None,
     description: str | list[str] | None = None,
     description_prefix: str | None = None,
     severity: str | list[str] | None = None,
     severity_prefix: str | None = None,
     alert_type: str | list[str] | None = None,
@@ -342,14 +365,20 @@
         filters.append(
             dm.filters.Range(
                 view_id.as_property_ref("time"),
                 gte=min_time.isoformat(timespec="milliseconds") if min_time else None,
                 lte=max_time.isoformat(timespec="milliseconds") if max_time else None,
             )
         )
+    if isinstance(process_id, str):
+        filters.append(dm.filters.Equals(view_id.as_property_ref("processId"), value=process_id))
+    if process_id and isinstance(process_id, list):
+        filters.append(dm.filters.In(view_id.as_property_ref("processId"), values=process_id))
+    if process_id_prefix is not None:
+        filters.append(dm.filters.Prefix(view_id.as_property_ref("processId"), value=process_id_prefix))
     if isinstance(title, str):
         filters.append(dm.filters.Equals(view_id.as_property_ref("title"), value=title))
     if title and isinstance(title, list):
         filters.append(dm.filters.In(view_id.as_property_ref("title"), values=title))
     if title_prefix is not None:
         filters.append(dm.filters.Prefix(view_id.as_property_ref("title"), value=title_prefix))
     if isinstance(description, str):
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_basic_bid_matrix.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_basic_bid_matrix.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_bid_calculation_task.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_bid_calculation_task.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_bid_configuration_shop.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_bid_configuration_shop.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_bid_configuration_water.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_bid_configuration_water.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_bid_document_afrr.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_bid_document_afrr.py`

 * *Files 4% similar despite different names*

```diff
@@ -18,14 +18,15 @@
     DomainModelWrite,
     DomainModelWriteList,
     DomainModelList,
     DomainRelationWrite,
     GraphQLCore,
     ResourcesWrite,
 )
+from ._bid_document import BidDocument, BidDocumentWrite
 
 if TYPE_CHECKING:
     from ._alert import Alert, AlertGraphQL, AlertWrite
     from ._bid_row import BidRow, BidRowGraphQL, BidRowWrite
     from ._price_area_afrr import PriceAreaAFRR, PriceAreaAFRRGraphQL, PriceAreaAFRRWrite
 
 
@@ -37,19 +38,22 @@
     "BidDocumentAFRRWriteList",
     "BidDocumentAFRRApplyList",
     "BidDocumentAFRRFields",
     "BidDocumentAFRRTextFields",
 ]
 
 
-BidDocumentAFRRTextFields = Literal["name"]
-BidDocumentAFRRFields = Literal["name", "delivery_date", "start_calculation", "end_calculation", "is_complete"]
+BidDocumentAFRRTextFields = Literal["name", "process_id"]
+BidDocumentAFRRFields = Literal[
+    "name", "process_id", "delivery_date", "start_calculation", "end_calculation", "is_complete"
+]
 
 _BIDDOCUMENTAFRR_PROPERTIES_BY_FIELD = {
     "name": "name",
+    "process_id": "processId",
     "delivery_date": "deliveryDate",
     "start_calculation": "startCalculation",
     "end_calculation": "endCalculation",
     "is_complete": "isComplete",
 }
 
 
@@ -60,25 +64,27 @@
     It is used when retrieving data from CDF using GraphQL.
 
     Args:
         space: The space where the node is located.
         external_id: The external id of the bid document afrr.
         data_record: The data record of the bid document afrr node.
         name: Unique name for a given instance of a Bid Document. A combination of name, priceArea, date and startCalculation.
+        process_id: The process associated with the Bid calculation workflow.
         delivery_date: The date of the Bid.
         start_calculation: Timestamp of when the Bid calculation workflow started.
         end_calculation: Timestamp of when the Bid calculation workflow completed.
         is_complete: Indicates that the Bid calculation workflow has completed (although has not necessarily succeeded).
         alerts: An array of calculation level Alerts.
         price_area: The price area field.
         bids: An array of BidRows containing the Bid data.
     """
 
-    view_id = dm.ViewId("sp_powerops_models", "BidDocumentAFRR", "1")
+    view_id = dm.ViewId("sp_powerops_models_temp", "BidDocumentAFRR", "1")
     name: Optional[str] = None
+    process_id: Optional[str] = Field(None, alias="processId")
     delivery_date: Optional[datetime.date] = Field(None, alias="deliveryDate")
     start_calculation: Optional[datetime.datetime] = Field(None, alias="startCalculation")
     end_calculation: Optional[datetime.datetime] = Field(None, alias="endCalculation")
     is_complete: Optional[bool] = Field(None, alias="isComplete")
     alerts: Optional[list[AlertGraphQL]] = Field(default=None, repr=False)
     price_area: Optional[PriceAreaAFRRGraphQL] = Field(None, repr=False, alias="priceArea")
     bids: Optional[list[BidRowGraphQL]] = Field(default=None, repr=False)
@@ -111,14 +117,15 @@
             external_id=self.external_id,
             data_record=DataRecord(
                 version=0,
                 last_updated_time=self.data_record.last_updated_time,
                 created_time=self.data_record.created_time,
             ),
             name=self.name,
+            process_id=self.process_id,
             delivery_date=self.delivery_date,
             start_calculation=self.start_calculation,
             end_calculation=self.end_calculation,
             is_complete=self.is_complete,
             alerts=[alert.as_read() if isinstance(alert, GraphQLCore) else alert for alert in self.alerts or []],
             price_area=self.price_area.as_read() if isinstance(self.price_area, GraphQLCore) else self.price_area,
             bids=[bid.as_read() if isinstance(bid, GraphQLCore) else bid for bid in self.bids or []],
@@ -127,63 +134,59 @@
     def as_write(self) -> BidDocumentAFRRWrite:
         """Convert this GraphQL format of bid document afrr to the writing format."""
         return BidDocumentAFRRWrite(
             space=self.space,
             external_id=self.external_id,
             data_record=DataRecordWrite(existing_version=0),
             name=self.name,
+            process_id=self.process_id,
             delivery_date=self.delivery_date,
             start_calculation=self.start_calculation,
             end_calculation=self.end_calculation,
             is_complete=self.is_complete,
             alerts=[alert.as_write() if isinstance(alert, DomainModel) else alert for alert in self.alerts or []],
             price_area=self.price_area.as_write() if isinstance(self.price_area, DomainModel) else self.price_area,
             bids=[bid.as_write() if isinstance(bid, DomainModel) else bid for bid in self.bids or []],
         )
 
 
-class BidDocumentAFRR(DomainModel):
+class BidDocumentAFRR(BidDocument):
     """This represents the reading version of bid document afrr.
 
     It is used to when data is retrieved from CDF.
 
     Args:
         space: The space where the node is located.
         external_id: The external id of the bid document afrr.
         data_record: The data record of the bid document afrr node.
         name: Unique name for a given instance of a Bid Document. A combination of name, priceArea, date and startCalculation.
+        process_id: The process associated with the Bid calculation workflow.
         delivery_date: The date of the Bid.
         start_calculation: Timestamp of when the Bid calculation workflow started.
         end_calculation: Timestamp of when the Bid calculation workflow completed.
         is_complete: Indicates that the Bid calculation workflow has completed (although has not necessarily succeeded).
         alerts: An array of calculation level Alerts.
         price_area: The price area field.
         bids: An array of BidRows containing the Bid data.
     """
 
-    space: str = DEFAULT_INSTANCE_SPACE
     node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference(
-        "sp_powerops_types", "AFRRBidDocument"
+        "sp_powerops_types_temp", "AFRRBidDocument"
     )
-    name: Optional[str] = None
-    delivery_date: datetime.date = Field(alias="deliveryDate")
-    start_calculation: Optional[datetime.datetime] = Field(None, alias="startCalculation")
-    end_calculation: Optional[datetime.datetime] = Field(None, alias="endCalculation")
-    is_complete: Optional[bool] = Field(None, alias="isComplete")
-    alerts: Union[list[Alert], list[str], list[dm.NodeId], None] = Field(default=None, repr=False)
     price_area: Union[PriceAreaAFRR, str, dm.NodeId, None] = Field(None, repr=False, alias="priceArea")
     bids: Union[list[BidRow], list[str], list[dm.NodeId], None] = Field(default=None, repr=False)
 
     def as_write(self) -> BidDocumentAFRRWrite:
         """Convert this read version of bid document afrr to the writing version."""
         return BidDocumentAFRRWrite(
             space=self.space,
             external_id=self.external_id,
             data_record=DataRecordWrite(existing_version=self.data_record.version),
             name=self.name,
+            process_id=self.process_id,
             delivery_date=self.delivery_date,
             start_calculation=self.start_calculation,
             end_calculation=self.end_calculation,
             is_complete=self.is_complete,
             alerts=[alert.as_write() if isinstance(alert, DomainModel) else alert for alert in self.alerts or []],
             price_area=self.price_area.as_write() if isinstance(self.price_area, DomainModel) else self.price_area,
             bids=[bid.as_write() if isinstance(bid, DomainModel) else bid for bid in self.bids or []],
@@ -195,43 +198,37 @@
             "as_apply is deprecated and will be removed in v1.0. Use as_write instead.",
             UserWarning,
             stacklevel=2,
         )
         return self.as_write()
 
 
-class BidDocumentAFRRWrite(DomainModelWrite):
+class BidDocumentAFRRWrite(BidDocumentWrite):
     """This represents the writing version of bid document afrr.
 
     It is used to when data is sent to CDF.
 
     Args:
         space: The space where the node is located.
         external_id: The external id of the bid document afrr.
         data_record: The data record of the bid document afrr node.
         name: Unique name for a given instance of a Bid Document. A combination of name, priceArea, date and startCalculation.
+        process_id: The process associated with the Bid calculation workflow.
         delivery_date: The date of the Bid.
         start_calculation: Timestamp of when the Bid calculation workflow started.
         end_calculation: Timestamp of when the Bid calculation workflow completed.
         is_complete: Indicates that the Bid calculation workflow has completed (although has not necessarily succeeded).
         alerts: An array of calculation level Alerts.
         price_area: The price area field.
         bids: An array of BidRows containing the Bid data.
     """
 
-    space: str = DEFAULT_INSTANCE_SPACE
     node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference(
-        "sp_powerops_types", "AFRRBidDocument"
+        "sp_powerops_types_temp", "AFRRBidDocument"
     )
-    name: Optional[str] = None
-    delivery_date: datetime.date = Field(alias="deliveryDate")
-    start_calculation: Optional[datetime.datetime] = Field(None, alias="startCalculation")
-    end_calculation: Optional[datetime.datetime] = Field(None, alias="endCalculation")
-    is_complete: Optional[bool] = Field(None, alias="isComplete")
-    alerts: Union[list[AlertWrite], list[str], list[dm.NodeId], None] = Field(default=None, repr=False)
     price_area: Union[PriceAreaAFRRWrite, str, dm.NodeId, None] = Field(None, repr=False, alias="priceArea")
     bids: Union[list[BidRowWrite], list[str], list[dm.NodeId], None] = Field(default=None, repr=False)
 
     def _to_instances_write(
         self,
         cache: set[tuple[str, str]],
         view_by_read_class: dict[type[DomainModelCore], dm.ViewId] | None,
@@ -239,22 +236,25 @@
         allow_version_increase: bool = False,
     ) -> ResourcesWrite:
         resources = ResourcesWrite()
         if self.as_tuple_id() in cache:
             return resources
 
         write_view = (view_by_read_class or {}).get(
-            BidDocumentAFRR, dm.ViewId("sp_powerops_models", "BidDocumentAFRR", "1")
+            BidDocumentAFRR, dm.ViewId("sp_powerops_models_temp", "BidDocumentAFRR", "1")
         )
 
         properties: dict[str, Any] = {}
 
         if self.name is not None or write_none:
             properties["name"] = self.name
 
+        if self.process_id is not None or write_none:
+            properties["processId"] = self.process_id
+
         if self.delivery_date is not None:
             properties["deliveryDate"] = self.delivery_date.isoformat() if self.delivery_date else None
 
         if self.start_calculation is not None or write_none:
             properties["startCalculation"] = (
                 self.start_calculation.isoformat(timespec="milliseconds") if self.start_calculation else None
             )
@@ -285,28 +285,28 @@
                         properties=properties,
                     )
                 ],
             )
             resources.nodes.append(this_node)
             cache.add(self.as_tuple_id())
 
-        edge_type = dm.DirectRelationReference("sp_powerops_types", "calculationIssue")
+        edge_type = dm.DirectRelationReference("sp_powerops_types_temp", "calculationIssue")
         for alert in self.alerts or []:
             other_resources = DomainRelationWrite.from_edge_to_resources(
                 cache,
                 start_node=self,
                 end_node=alert,
                 edge_type=edge_type,
                 view_by_read_class=view_by_read_class,
                 write_none=write_none,
                 allow_version_increase=allow_version_increase,
             )
             resources.extend(other_resources)
 
-        edge_type = dm.DirectRelationReference("sp_powerops_types", "partialBid")
+        edge_type = dm.DirectRelationReference("sp_powerops_types_temp", "partialBid")
         for bid in self.bids or []:
             other_resources = DomainRelationWrite.from_edge_to_resources(
                 cache,
                 start_node=self,
                 end_node=bid,
                 edge_type=edge_type,
                 view_by_read_class=view_by_read_class,
@@ -362,14 +362,16 @@
 class BidDocumentAFRRApplyList(BidDocumentAFRRWriteList): ...
 
 
 def _create_bid_document_afrr_filter(
     view_id: dm.ViewId,
     name: str | list[str] | None = None,
     name_prefix: str | None = None,
+    process_id: str | list[str] | None = None,
+    process_id_prefix: str | None = None,
     min_delivery_date: datetime.date | None = None,
     max_delivery_date: datetime.date | None = None,
     min_start_calculation: datetime.datetime | None = None,
     max_start_calculation: datetime.datetime | None = None,
     min_end_calculation: datetime.datetime | None = None,
     max_end_calculation: datetime.datetime | None = None,
     is_complete: bool | None = None,
@@ -381,14 +383,20 @@
     filters = []
     if isinstance(name, str):
         filters.append(dm.filters.Equals(view_id.as_property_ref("name"), value=name))
     if name and isinstance(name, list):
         filters.append(dm.filters.In(view_id.as_property_ref("name"), values=name))
     if name_prefix is not None:
         filters.append(dm.filters.Prefix(view_id.as_property_ref("name"), value=name_prefix))
+    if isinstance(process_id, str):
+        filters.append(dm.filters.Equals(view_id.as_property_ref("processId"), value=process_id))
+    if process_id and isinstance(process_id, list):
+        filters.append(dm.filters.In(view_id.as_property_ref("processId"), values=process_id))
+    if process_id_prefix is not None:
+        filters.append(dm.filters.Prefix(view_id.as_property_ref("processId"), value=process_id_prefix))
     if min_delivery_date is not None or max_delivery_date is not None:
         filters.append(
             dm.filters.Range(
                 view_id.as_property_ref("deliveryDate"),
                 gte=min_delivery_date.isoformat() if min_delivery_date else None,
                 lte=max_delivery_date.isoformat() if max_delivery_date else None,
             )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_bid_document_day_ahead.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_bid_document_day_ahead.py`

 * *Files 7% similar despite different names*

```diff
@@ -18,39 +18,42 @@
     DomainModelWrite,
     DomainModelWriteList,
     DomainModelList,
     DomainRelationWrite,
     GraphQLCore,
     ResourcesWrite,
 )
+from ._bid_document import BidDocument, BidDocumentWrite
 
 if TYPE_CHECKING:
     from ._alert import Alert, AlertGraphQL, AlertWrite
+    from ._bid_configuration import BidConfiguration, BidConfigurationGraphQL, BidConfigurationWrite
     from ._bid_matrix import BidMatrix, BidMatrixGraphQL, BidMatrixWrite
-    from ._bid_method_day_ahead import BidMethodDayAhead, BidMethodDayAheadGraphQL, BidMethodDayAheadWrite
-    from ._price_area import PriceArea, PriceAreaGraphQL, PriceAreaWrite
 
 
 __all__ = [
     "BidDocumentDayAhead",
     "BidDocumentDayAheadWrite",
     "BidDocumentDayAheadApply",
     "BidDocumentDayAheadList",
     "BidDocumentDayAheadWriteList",
     "BidDocumentDayAheadApplyList",
     "BidDocumentDayAheadFields",
     "BidDocumentDayAheadTextFields",
 ]
 
 
-BidDocumentDayAheadTextFields = Literal["name"]
-BidDocumentDayAheadFields = Literal["name", "delivery_date", "start_calculation", "end_calculation", "is_complete"]
+BidDocumentDayAheadTextFields = Literal["name", "process_id"]
+BidDocumentDayAheadFields = Literal[
+    "name", "process_id", "delivery_date", "start_calculation", "end_calculation", "is_complete"
+]
 
 _BIDDOCUMENTDAYAHEAD_PROPERTIES_BY_FIELD = {
     "name": "name",
+    "process_id": "processId",
     "delivery_date": "deliveryDate",
     "start_calculation": "startCalculation",
     "end_calculation": "endCalculation",
     "is_complete": "isComplete",
 }
 
 
@@ -61,49 +64,49 @@
     It is used when retrieving data from CDF using GraphQL.
 
     Args:
         space: The space where the node is located.
         external_id: The external id of the bid document day ahead.
         data_record: The data record of the bid document day ahead node.
         name: Unique name for a given instance of a Bid Document. A combination of name, priceArea, date and startCalculation.
+        process_id: The process associated with the Bid calculation workflow.
         delivery_date: The date of the Bid.
         start_calculation: Timestamp of when the Bid calculation workflow started.
         end_calculation: Timestamp of when the Bid calculation workflow completed.
         is_complete: Indicates that the Bid calculation workflow has completed (although has not necessarily succeeded).
         alerts: An array of calculation level Alerts.
-        price_area: The price area field.
-        method: The method field.
+        bid_configuration: The bid configuration field.
         total: The total field.
         partials: The partial field.
     """
 
-    view_id = dm.ViewId("sp_powerops_models", "BidDocumentDayAhead", "1")
+    view_id = dm.ViewId("sp_powerops_models_temp", "BidDocumentDayAhead", "1")
     name: Optional[str] = None
+    process_id: Optional[str] = Field(None, alias="processId")
     delivery_date: Optional[datetime.date] = Field(None, alias="deliveryDate")
     start_calculation: Optional[datetime.datetime] = Field(None, alias="startCalculation")
     end_calculation: Optional[datetime.datetime] = Field(None, alias="endCalculation")
     is_complete: Optional[bool] = Field(None, alias="isComplete")
     alerts: Optional[list[AlertGraphQL]] = Field(default=None, repr=False)
-    price_area: Optional[PriceAreaGraphQL] = Field(None, repr=False, alias="priceArea")
-    method: Optional[BidMethodDayAheadGraphQL] = Field(None, repr=False)
+    bid_configuration: Optional[BidConfigurationGraphQL] = Field(None, repr=False, alias="bidConfiguration")
     total: Optional[BidMatrixGraphQL] = Field(None, repr=False)
     partials: Optional[list[BidMatrixGraphQL]] = Field(default=None, repr=False)
 
     @model_validator(mode="before")
     def parse_data_record(cls, values: Any) -> Any:
         if not isinstance(values, dict):
             return values
         if "lastUpdatedTime" in values or "createdTime" in values:
             values["dataRecord"] = DataRecordGraphQL(
                 created_time=values.pop("createdTime", None),
                 last_updated_time=values.pop("lastUpdatedTime", None),
             )
         return values
 
-    @field_validator("alerts", "price_area", "method", "total", "partials", mode="before")
+    @field_validator("alerts", "bid_configuration", "total", "partials", mode="before")
     def parse_graphql(cls, value: Any) -> Any:
         if not isinstance(value, dict):
             return value
         if "items" in value:
             return value["items"]
         return value
 
@@ -116,98 +119,102 @@
             external_id=self.external_id,
             data_record=DataRecord(
                 version=0,
                 last_updated_time=self.data_record.last_updated_time,
                 created_time=self.data_record.created_time,
             ),
             name=self.name,
+            process_id=self.process_id,
             delivery_date=self.delivery_date,
             start_calculation=self.start_calculation,
             end_calculation=self.end_calculation,
             is_complete=self.is_complete,
             alerts=[alert.as_read() if isinstance(alert, GraphQLCore) else alert for alert in self.alerts or []],
-            price_area=self.price_area.as_read() if isinstance(self.price_area, GraphQLCore) else self.price_area,
-            method=self.method.as_read() if isinstance(self.method, GraphQLCore) else self.method,
+            bid_configuration=(
+                self.bid_configuration.as_read()
+                if isinstance(self.bid_configuration, GraphQLCore)
+                else self.bid_configuration
+            ),
             total=self.total.as_read() if isinstance(self.total, GraphQLCore) else self.total,
             partials=[
                 partial.as_read() if isinstance(partial, GraphQLCore) else partial for partial in self.partials or []
             ],
         )
 
     def as_write(self) -> BidDocumentDayAheadWrite:
         """Convert this GraphQL format of bid document day ahead to the writing format."""
         return BidDocumentDayAheadWrite(
             space=self.space,
             external_id=self.external_id,
             data_record=DataRecordWrite(existing_version=0),
             name=self.name,
+            process_id=self.process_id,
             delivery_date=self.delivery_date,
             start_calculation=self.start_calculation,
             end_calculation=self.end_calculation,
             is_complete=self.is_complete,
             alerts=[alert.as_write() if isinstance(alert, DomainModel) else alert for alert in self.alerts or []],
-            price_area=self.price_area.as_write() if isinstance(self.price_area, DomainModel) else self.price_area,
-            method=self.method.as_write() if isinstance(self.method, DomainModel) else self.method,
+            bid_configuration=(
+                self.bid_configuration.as_write()
+                if isinstance(self.bid_configuration, DomainModel)
+                else self.bid_configuration
+            ),
             total=self.total.as_write() if isinstance(self.total, DomainModel) else self.total,
             partials=[
                 partial.as_write() if isinstance(partial, DomainModel) else partial for partial in self.partials or []
             ],
         )
 
 
-class BidDocumentDayAhead(DomainModel):
+class BidDocumentDayAhead(BidDocument):
     """This represents the reading version of bid document day ahead.
 
     It is used to when data is retrieved from CDF.
 
     Args:
         space: The space where the node is located.
         external_id: The external id of the bid document day ahead.
         data_record: The data record of the bid document day ahead node.
         name: Unique name for a given instance of a Bid Document. A combination of name, priceArea, date and startCalculation.
+        process_id: The process associated with the Bid calculation workflow.
         delivery_date: The date of the Bid.
         start_calculation: Timestamp of when the Bid calculation workflow started.
         end_calculation: Timestamp of when the Bid calculation workflow completed.
         is_complete: Indicates that the Bid calculation workflow has completed (although has not necessarily succeeded).
         alerts: An array of calculation level Alerts.
-        price_area: The price area field.
-        method: The method field.
+        bid_configuration: The bid configuration field.
         total: The total field.
         partials: The partial field.
     """
 
-    space: str = DEFAULT_INSTANCE_SPACE
     node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference(
-        "sp_powerops_types", "DayAheadBidDocument"
+        "sp_powerops_types_temp", "DayAheadBidDocument"
     )
-    name: Optional[str] = None
-    delivery_date: datetime.date = Field(alias="deliveryDate")
-    start_calculation: Optional[datetime.datetime] = Field(None, alias="startCalculation")
-    end_calculation: Optional[datetime.datetime] = Field(None, alias="endCalculation")
-    is_complete: Optional[bool] = Field(None, alias="isComplete")
-    alerts: Union[list[Alert], list[str], list[dm.NodeId], None] = Field(default=None, repr=False)
-    price_area: Union[PriceArea, str, dm.NodeId, None] = Field(None, repr=False, alias="priceArea")
-    method: Union[BidMethodDayAhead, str, dm.NodeId, None] = Field(None, repr=False)
+    bid_configuration: Union[BidConfiguration, str, dm.NodeId, None] = Field(None, repr=False, alias="bidConfiguration")
     total: Union[BidMatrix, str, dm.NodeId, None] = Field(None, repr=False)
     partials: Union[list[BidMatrix], list[str], list[dm.NodeId], None] = Field(default=None, repr=False)
 
     def as_write(self) -> BidDocumentDayAheadWrite:
         """Convert this read version of bid document day ahead to the writing version."""
         return BidDocumentDayAheadWrite(
             space=self.space,
             external_id=self.external_id,
             data_record=DataRecordWrite(existing_version=self.data_record.version),
             name=self.name,
+            process_id=self.process_id,
             delivery_date=self.delivery_date,
             start_calculation=self.start_calculation,
             end_calculation=self.end_calculation,
             is_complete=self.is_complete,
             alerts=[alert.as_write() if isinstance(alert, DomainModel) else alert for alert in self.alerts or []],
-            price_area=self.price_area.as_write() if isinstance(self.price_area, DomainModel) else self.price_area,
-            method=self.method.as_write() if isinstance(self.method, DomainModel) else self.method,
+            bid_configuration=(
+                self.bid_configuration.as_write()
+                if isinstance(self.bid_configuration, DomainModel)
+                else self.bid_configuration
+            ),
             total=self.total.as_write() if isinstance(self.total, DomainModel) else self.total,
             partials=[
                 partial.as_write() if isinstance(partial, DomainModel) else partial for partial in self.partials or []
             ],
         )
 
     def as_apply(self) -> BidDocumentDayAheadWrite:
@@ -216,47 +223,41 @@
             "as_apply is deprecated and will be removed in v1.0. Use as_write instead.",
             UserWarning,
             stacklevel=2,
         )
         return self.as_write()
 
 
-class BidDocumentDayAheadWrite(DomainModelWrite):
+class BidDocumentDayAheadWrite(BidDocumentWrite):
     """This represents the writing version of bid document day ahead.
 
     It is used to when data is sent to CDF.
 
     Args:
         space: The space where the node is located.
         external_id: The external id of the bid document day ahead.
         data_record: The data record of the bid document day ahead node.
         name: Unique name for a given instance of a Bid Document. A combination of name, priceArea, date and startCalculation.
+        process_id: The process associated with the Bid calculation workflow.
         delivery_date: The date of the Bid.
         start_calculation: Timestamp of when the Bid calculation workflow started.
         end_calculation: Timestamp of when the Bid calculation workflow completed.
         is_complete: Indicates that the Bid calculation workflow has completed (although has not necessarily succeeded).
         alerts: An array of calculation level Alerts.
-        price_area: The price area field.
-        method: The method field.
+        bid_configuration: The bid configuration field.
         total: The total field.
         partials: The partial field.
     """
 
-    space: str = DEFAULT_INSTANCE_SPACE
     node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference(
-        "sp_powerops_types", "DayAheadBidDocument"
+        "sp_powerops_types_temp", "DayAheadBidDocument"
+    )
+    bid_configuration: Union[BidConfigurationWrite, str, dm.NodeId, None] = Field(
+        None, repr=False, alias="bidConfiguration"
     )
-    name: Optional[str] = None
-    delivery_date: datetime.date = Field(alias="deliveryDate")
-    start_calculation: Optional[datetime.datetime] = Field(None, alias="startCalculation")
-    end_calculation: Optional[datetime.datetime] = Field(None, alias="endCalculation")
-    is_complete: Optional[bool] = Field(None, alias="isComplete")
-    alerts: Union[list[AlertWrite], list[str], list[dm.NodeId], None] = Field(default=None, repr=False)
-    price_area: Union[PriceAreaWrite, str, dm.NodeId, None] = Field(None, repr=False, alias="priceArea")
-    method: Union[BidMethodDayAheadWrite, str, dm.NodeId, None] = Field(None, repr=False)
     total: Union[BidMatrixWrite, str, dm.NodeId, None] = Field(None, repr=False)
     partials: Union[list[BidMatrixWrite], list[str], list[dm.NodeId], None] = Field(default=None, repr=False)
 
     def _to_instances_write(
         self,
         cache: set[tuple[str, str]],
         view_by_read_class: dict[type[DomainModelCore], dm.ViewId] | None,
@@ -264,22 +265,25 @@
         allow_version_increase: bool = False,
     ) -> ResourcesWrite:
         resources = ResourcesWrite()
         if self.as_tuple_id() in cache:
             return resources
 
         write_view = (view_by_read_class or {}).get(
-            BidDocumentDayAhead, dm.ViewId("sp_powerops_models", "BidDocumentDayAhead", "1")
+            BidDocumentDayAhead, dm.ViewId("sp_powerops_models_temp", "BidDocumentDayAhead", "1")
         )
 
         properties: dict[str, Any] = {}
 
         if self.name is not None or write_none:
             properties["name"] = self.name
 
+        if self.process_id is not None or write_none:
+            properties["processId"] = self.process_id
+
         if self.delivery_date is not None:
             properties["deliveryDate"] = self.delivery_date.isoformat() if self.delivery_date else None
 
         if self.start_calculation is not None or write_none:
             properties["startCalculation"] = (
                 self.start_calculation.isoformat(timespec="milliseconds") if self.start_calculation else None
             )
@@ -288,24 +292,22 @@
             properties["endCalculation"] = (
                 self.end_calculation.isoformat(timespec="milliseconds") if self.end_calculation else None
             )
 
         if self.is_complete is not None or write_none:
             properties["isComplete"] = self.is_complete
 
-        if self.price_area is not None:
-            properties["priceArea"] = {
-                "space": self.space if isinstance(self.price_area, str) else self.price_area.space,
-                "externalId": self.price_area if isinstance(self.price_area, str) else self.price_area.external_id,
-            }
-
-        if self.method is not None:
-            properties["method"] = {
-                "space": self.space if isinstance(self.method, str) else self.method.space,
-                "externalId": self.method if isinstance(self.method, str) else self.method.external_id,
+        if self.bid_configuration is not None:
+            properties["bidConfiguration"] = {
+                "space": self.space if isinstance(self.bid_configuration, str) else self.bid_configuration.space,
+                "externalId": (
+                    self.bid_configuration
+                    if isinstance(self.bid_configuration, str)
+                    else self.bid_configuration.external_id
+                ),
             }
 
         if self.total is not None:
             properties["total"] = {
                 "space": self.space if isinstance(self.total, str) else self.total.space,
                 "externalId": self.total if isinstance(self.total, str) else self.total.external_id,
             }
@@ -322,46 +324,42 @@
                         properties=properties,
                     )
                 ],
             )
             resources.nodes.append(this_node)
             cache.add(self.as_tuple_id())
 
-        edge_type = dm.DirectRelationReference("sp_powerops_types", "calculationIssue")
+        edge_type = dm.DirectRelationReference("sp_powerops_types_temp", "calculationIssue")
         for alert in self.alerts or []:
             other_resources = DomainRelationWrite.from_edge_to_resources(
                 cache,
                 start_node=self,
                 end_node=alert,
                 edge_type=edge_type,
                 view_by_read_class=view_by_read_class,
                 write_none=write_none,
                 allow_version_increase=allow_version_increase,
             )
             resources.extend(other_resources)
 
-        edge_type = dm.DirectRelationReference("sp_powerops_types", "partialBid")
+        edge_type = dm.DirectRelationReference("sp_powerops_types_temp", "partialBid")
         for partial in self.partials or []:
             other_resources = DomainRelationWrite.from_edge_to_resources(
                 cache,
                 start_node=self,
                 end_node=partial,
                 edge_type=edge_type,
                 view_by_read_class=view_by_read_class,
                 write_none=write_none,
                 allow_version_increase=allow_version_increase,
             )
             resources.extend(other_resources)
 
-        if isinstance(self.price_area, DomainModelWrite):
-            other_resources = self.price_area._to_instances_write(cache, view_by_read_class)
-            resources.extend(other_resources)
-
-        if isinstance(self.method, DomainModelWrite):
-            other_resources = self.method._to_instances_write(cache, view_by_read_class)
+        if isinstance(self.bid_configuration, DomainModelWrite):
+            other_resources = self.bid_configuration._to_instances_write(cache, view_by_read_class)
             resources.extend(other_resources)
 
         if isinstance(self.total, DomainModelWrite):
             other_resources = self.total._to_instances_write(cache, view_by_read_class)
             resources.extend(other_resources)
 
         return resources
@@ -407,35 +405,42 @@
 class BidDocumentDayAheadApplyList(BidDocumentDayAheadWriteList): ...
 
 
 def _create_bid_document_day_ahead_filter(
     view_id: dm.ViewId,
     name: str | list[str] | None = None,
     name_prefix: str | None = None,
+    process_id: str | list[str] | None = None,
+    process_id_prefix: str | None = None,
     min_delivery_date: datetime.date | None = None,
     max_delivery_date: datetime.date | None = None,
     min_start_calculation: datetime.datetime | None = None,
     max_start_calculation: datetime.datetime | None = None,
     min_end_calculation: datetime.datetime | None = None,
     max_end_calculation: datetime.datetime | None = None,
     is_complete: bool | None = None,
-    price_area: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
-    method: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+    bid_configuration: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
     total: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
     external_id_prefix: str | None = None,
     space: str | list[str] | None = None,
     filter: dm.Filter | None = None,
 ) -> dm.Filter | None:
     filters = []
     if isinstance(name, str):
         filters.append(dm.filters.Equals(view_id.as_property_ref("name"), value=name))
     if name and isinstance(name, list):
         filters.append(dm.filters.In(view_id.as_property_ref("name"), values=name))
     if name_prefix is not None:
         filters.append(dm.filters.Prefix(view_id.as_property_ref("name"), value=name_prefix))
+    if isinstance(process_id, str):
+        filters.append(dm.filters.Equals(view_id.as_property_ref("processId"), value=process_id))
+    if process_id and isinstance(process_id, list):
+        filters.append(dm.filters.In(view_id.as_property_ref("processId"), values=process_id))
+    if process_id_prefix is not None:
+        filters.append(dm.filters.Prefix(view_id.as_property_ref("processId"), value=process_id_prefix))
     if min_delivery_date is not None or max_delivery_date is not None:
         filters.append(
             dm.filters.Range(
                 view_id.as_property_ref("deliveryDate"),
                 gte=min_delivery_date.isoformat() if min_delivery_date else None,
                 lte=max_delivery_date.isoformat() if max_delivery_date else None,
             )
@@ -454,61 +459,40 @@
                 view_id.as_property_ref("endCalculation"),
                 gte=min_end_calculation.isoformat(timespec="milliseconds") if min_end_calculation else None,
                 lte=max_end_calculation.isoformat(timespec="milliseconds") if max_end_calculation else None,
             )
         )
     if isinstance(is_complete, bool):
         filters.append(dm.filters.Equals(view_id.as_property_ref("isComplete"), value=is_complete))
-    if price_area and isinstance(price_area, str):
-        filters.append(
-            dm.filters.Equals(
-                view_id.as_property_ref("priceArea"), value={"space": DEFAULT_INSTANCE_SPACE, "externalId": price_area}
-            )
-        )
-    if price_area and isinstance(price_area, tuple):
+    if bid_configuration and isinstance(bid_configuration, str):
         filters.append(
             dm.filters.Equals(
-                view_id.as_property_ref("priceArea"), value={"space": price_area[0], "externalId": price_area[1]}
-            )
-        )
-    if price_area and isinstance(price_area, list) and isinstance(price_area[0], str):
-        filters.append(
-            dm.filters.In(
-                view_id.as_property_ref("priceArea"),
-                values=[{"space": DEFAULT_INSTANCE_SPACE, "externalId": item} for item in price_area],
-            )
-        )
-    if price_area and isinstance(price_area, list) and isinstance(price_area[0], tuple):
-        filters.append(
-            dm.filters.In(
-                view_id.as_property_ref("priceArea"),
-                values=[{"space": item[0], "externalId": item[1]} for item in price_area],
+                view_id.as_property_ref("bidConfiguration"),
+                value={"space": DEFAULT_INSTANCE_SPACE, "externalId": bid_configuration},
             )
         )
-    if method and isinstance(method, str):
+    if bid_configuration and isinstance(bid_configuration, tuple):
         filters.append(
             dm.filters.Equals(
-                view_id.as_property_ref("method"), value={"space": DEFAULT_INSTANCE_SPACE, "externalId": method}
+                view_id.as_property_ref("bidConfiguration"),
+                value={"space": bid_configuration[0], "externalId": bid_configuration[1]},
             )
         )
-    if method and isinstance(method, tuple):
-        filters.append(
-            dm.filters.Equals(view_id.as_property_ref("method"), value={"space": method[0], "externalId": method[1]})
-        )
-    if method and isinstance(method, list) and isinstance(method[0], str):
+    if bid_configuration and isinstance(bid_configuration, list) and isinstance(bid_configuration[0], str):
         filters.append(
             dm.filters.In(
-                view_id.as_property_ref("method"),
-                values=[{"space": DEFAULT_INSTANCE_SPACE, "externalId": item} for item in method],
+                view_id.as_property_ref("bidConfiguration"),
+                values=[{"space": DEFAULT_INSTANCE_SPACE, "externalId": item} for item in bid_configuration],
             )
         )
-    if method and isinstance(method, list) and isinstance(method[0], tuple):
+    if bid_configuration and isinstance(bid_configuration, list) and isinstance(bid_configuration[0], tuple):
         filters.append(
             dm.filters.In(
-                view_id.as_property_ref("method"), values=[{"space": item[0], "externalId": item[1]} for item in method]
+                view_id.as_property_ref("bidConfiguration"),
+                values=[{"space": item[0], "externalId": item[1]} for item in bid_configuration],
             )
         )
     if total and isinstance(total, str):
         filters.append(
             dm.filters.Equals(
                 view_id.as_property_ref("total"), value={"space": DEFAULT_INSTANCE_SPACE, "externalId": total}
             )
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_bid_matrix.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_bid_matrix_raw.py`

 * *Files 9% similar despite different names*

```diff
@@ -17,62 +17,63 @@
     DomainModelWrite,
     DomainModelWriteList,
     DomainModelList,
     DomainRelationWrite,
     GraphQLCore,
     ResourcesWrite,
 )
+from ._bid_matrix import BidMatrix, BidMatrixWrite
 
 if TYPE_CHECKING:
     from ._alert import Alert, AlertGraphQL, AlertWrite
 
 
 __all__ = [
-    "BidMatrix",
-    "BidMatrixWrite",
-    "BidMatrixApply",
-    "BidMatrixList",
-    "BidMatrixWriteList",
-    "BidMatrixApplyList",
-    "BidMatrixFields",
-    "BidMatrixTextFields",
+    "BidMatrixRaw",
+    "BidMatrixRawWrite",
+    "BidMatrixRawApply",
+    "BidMatrixRawList",
+    "BidMatrixRawWriteList",
+    "BidMatrixRawApplyList",
+    "BidMatrixRawFields",
+    "BidMatrixRawTextFields",
 ]
 
 
-BidMatrixTextFields = Literal["resource_cost", "matrix", "asset_type", "asset_id"]
-BidMatrixFields = Literal["resource_cost", "matrix", "asset_type", "asset_id", "is_processed"]
+BidMatrixRawTextFields = Literal["resource_cost", "matrix", "asset_type", "asset_id"]
+BidMatrixRawFields = Literal["resource_cost", "matrix", "asset_type", "asset_id", "is_processed"]
 
-_BIDMATRIX_PROPERTIES_BY_FIELD = {
+_BIDMATRIXRAW_PROPERTIES_BY_FIELD = {
     "resource_cost": "resourceCost",
     "matrix": "matrix",
     "asset_type": "assetType",
     "asset_id": "assetId",
     "is_processed": "isProcessed",
 }
 
 
-class BidMatrixGraphQL(GraphQLCore):
-    """This represents the reading version of bid matrix, used
+class BidMatrixRawGraphQL(GraphQLCore):
+    """This represents the reading version of bid matrix raw, used
     when data is retrieved from CDF using GraphQL.
 
     It is used when retrieving data from CDF using GraphQL.
 
     Args:
         space: The space where the node is located.
-        external_id: The external id of the bid matrix.
-        data_record: The data record of the bid matrix node.
+        external_id: The external id of the bid matrix raw.
+        data_record: The data record of the bid matrix raw node.
         resource_cost: The resource cost field.
         matrix: The matrix field.
         asset_type: The asset type field.
         asset_id: The asset id field.
         is_processed: Whether the bid matrix has been processed by the bid matrix processor or not
         alerts: The alert field.
     """
 
-    view_id = dm.ViewId("sp_powerops_models", "BidMatrix", "1")
+    view_id = dm.ViewId("sp_powerops_models", "BidMatrixRaw", "1")
     resource_cost: Optional[str] = Field(None, alias="resourceCost")
     matrix: Union[str, None] = None
     asset_type: Optional[str] = Field(None, alias="assetType")
     asset_id: Optional[str] = Field(None, alias="assetId")
     is_processed: Optional[bool] = Field(None, alias="isProcessed")
     alerts: Optional[list[AlertGraphQL]] = Field(default=None, repr=False)
 
@@ -91,19 +92,19 @@
     def parse_graphql(cls, value: Any) -> Any:
         if not isinstance(value, dict):
             return value
         if "items" in value:
             return value["items"]
         return value
 
-    def as_read(self) -> BidMatrix:
-        """Convert this GraphQL format of bid matrix to the reading format."""
+    def as_read(self) -> BidMatrixRaw:
+        """Convert this GraphQL format of bid matrix raw to the reading format."""
         if self.data_record is None:
             raise ValueError("This object cannot be converted to a read format because it lacks a data record.")
-        return BidMatrix(
+        return BidMatrixRaw(
             space=self.space,
             external_id=self.external_id,
             data_record=DataRecord(
                 version=0,
                 last_updated_time=self.data_record.last_updated_time,
                 created_time=self.data_record.created_time,
             ),
@@ -111,117 +112,103 @@
             matrix=self.matrix,
             asset_type=self.asset_type,
             asset_id=self.asset_id,
             is_processed=self.is_processed,
             alerts=[alert.as_read() if isinstance(alert, GraphQLCore) else alert for alert in self.alerts or []],
         )
 
-    def as_write(self) -> BidMatrixWrite:
-        """Convert this GraphQL format of bid matrix to the writing format."""
-        return BidMatrixWrite(
+    def as_write(self) -> BidMatrixRawWrite:
+        """Convert this GraphQL format of bid matrix raw to the writing format."""
+        return BidMatrixRawWrite(
             space=self.space,
             external_id=self.external_id,
             data_record=DataRecordWrite(existing_version=0),
             resource_cost=self.resource_cost,
             matrix=self.matrix,
             asset_type=self.asset_type,
             asset_id=self.asset_id,
             is_processed=self.is_processed,
             alerts=[alert.as_write() if isinstance(alert, DomainModel) else alert for alert in self.alerts or []],
         )
 
 
-class BidMatrix(DomainModel):
-    """This represents the reading version of bid matrix.
+class BidMatrixRaw(BidMatrix):
+    """This represents the reading version of bid matrix raw.
 
     It is used to when data is retrieved from CDF.
 
     Args:
         space: The space where the node is located.
-        external_id: The external id of the bid matrix.
-        data_record: The data record of the bid matrix node.
+        external_id: The external id of the bid matrix raw.
+        data_record: The data record of the bid matrix raw node.
         resource_cost: The resource cost field.
         matrix: The matrix field.
         asset_type: The asset type field.
         asset_id: The asset id field.
         is_processed: Whether the bid matrix has been processed by the bid matrix processor or not
         alerts: The alert field.
     """
 
-    space: str = DEFAULT_INSTANCE_SPACE
     node_type: Union[dm.DirectRelationReference, None] = None
-    resource_cost: Optional[str] = Field(None, alias="resourceCost")
-    matrix: Union[str, None] = None
-    asset_type: Optional[str] = Field(None, alias="assetType")
-    asset_id: Optional[str] = Field(None, alias="assetId")
-    is_processed: Optional[bool] = Field(None, alias="isProcessed")
-    alerts: Union[list[Alert], list[str], list[dm.NodeId], None] = Field(default=None, repr=False)
 
-    def as_write(self) -> BidMatrixWrite:
-        """Convert this read version of bid matrix to the writing version."""
-        return BidMatrixWrite(
+    def as_write(self) -> BidMatrixRawWrite:
+        """Convert this read version of bid matrix raw to the writing version."""
+        return BidMatrixRawWrite(
             space=self.space,
             external_id=self.external_id,
             data_record=DataRecordWrite(existing_version=self.data_record.version),
             resource_cost=self.resource_cost,
             matrix=self.matrix,
             asset_type=self.asset_type,
             asset_id=self.asset_id,
             is_processed=self.is_processed,
             alerts=[alert.as_write() if isinstance(alert, DomainModel) else alert for alert in self.alerts or []],
         )
 
-    def as_apply(self) -> BidMatrixWrite:
-        """Convert this read version of bid matrix to the writing version."""
+    def as_apply(self) -> BidMatrixRawWrite:
+        """Convert this read version of bid matrix raw to the writing version."""
         warnings.warn(
             "as_apply is deprecated and will be removed in v1.0. Use as_write instead.",
             UserWarning,
             stacklevel=2,
         )
         return self.as_write()
 
 
-class BidMatrixWrite(DomainModelWrite):
-    """This represents the writing version of bid matrix.
+class BidMatrixRawWrite(BidMatrixWrite):
+    """This represents the writing version of bid matrix raw.
 
     It is used to when data is sent to CDF.
 
     Args:
         space: The space where the node is located.
-        external_id: The external id of the bid matrix.
-        data_record: The data record of the bid matrix node.
+        external_id: The external id of the bid matrix raw.
+        data_record: The data record of the bid matrix raw node.
         resource_cost: The resource cost field.
         matrix: The matrix field.
         asset_type: The asset type field.
         asset_id: The asset id field.
         is_processed: Whether the bid matrix has been processed by the bid matrix processor or not
         alerts: The alert field.
     """
 
-    space: str = DEFAULT_INSTANCE_SPACE
     node_type: Union[dm.DirectRelationReference, None] = None
-    resource_cost: Optional[str] = Field(None, alias="resourceCost")
-    matrix: Union[str, None] = None
-    asset_type: Optional[str] = Field(None, alias="assetType")
-    asset_id: Optional[str] = Field(None, alias="assetId")
-    is_processed: Optional[bool] = Field(None, alias="isProcessed")
-    alerts: Union[list[AlertWrite], list[str], list[dm.NodeId], None] = Field(default=None, repr=False)
 
     def _to_instances_write(
         self,
         cache: set[tuple[str, str]],
         view_by_read_class: dict[type[DomainModelCore], dm.ViewId] | None,
         write_none: bool = False,
         allow_version_increase: bool = False,
     ) -> ResourcesWrite:
         resources = ResourcesWrite()
         if self.as_tuple_id() in cache:
             return resources
 
-        write_view = (view_by_read_class or {}).get(BidMatrix, dm.ViewId("sp_powerops_models", "BidMatrix", "1"))
+        write_view = (view_by_read_class or {}).get(BidMatrixRaw, dm.ViewId("sp_powerops_models", "BidMatrixRaw", "1"))
 
         properties: dict[str, Any] = {}
 
         if self.resource_cost is not None or write_none:
             properties["resourceCost"] = self.resource_cost
 
         if self.matrix is not None or write_none:
@@ -264,55 +251,55 @@
                 allow_version_increase=allow_version_increase,
             )
             resources.extend(other_resources)
 
         return resources
 
 
-class BidMatrixApply(BidMatrixWrite):
-    def __new__(cls, *args, **kwargs) -> BidMatrixApply:
+class BidMatrixRawApply(BidMatrixRawWrite):
+    def __new__(cls, *args, **kwargs) -> BidMatrixRawApply:
         warnings.warn(
-            "BidMatrixApply is deprecated and will be removed in v1.0. Use BidMatrixWrite instead."
+            "BidMatrixRawApply is deprecated and will be removed in v1.0. Use BidMatrixRawWrite instead."
             "The motivation for this change is that Write is a more descriptive name for the writing version of the"
-            "BidMatrix.",
+            "BidMatrixRaw.",
             UserWarning,
             stacklevel=2,
         )
         return super().__new__(cls)
 
 
-class BidMatrixList(DomainModelList[BidMatrix]):
-    """List of bid matrixes in the read version."""
+class BidMatrixRawList(DomainModelList[BidMatrixRaw]):
+    """List of bid matrix raws in the read version."""
 
-    _INSTANCE = BidMatrix
+    _INSTANCE = BidMatrixRaw
 
-    def as_write(self) -> BidMatrixWriteList:
-        """Convert these read versions of bid matrix to the writing versions."""
-        return BidMatrixWriteList([node.as_write() for node in self.data])
+    def as_write(self) -> BidMatrixRawWriteList:
+        """Convert these read versions of bid matrix raw to the writing versions."""
+        return BidMatrixRawWriteList([node.as_write() for node in self.data])
 
-    def as_apply(self) -> BidMatrixWriteList:
+    def as_apply(self) -> BidMatrixRawWriteList:
         """Convert these read versions of primitive nullable to the writing versions."""
         warnings.warn(
             "as_apply is deprecated and will be removed in v1.0. Use as_write instead.",
             UserWarning,
             stacklevel=2,
         )
         return self.as_write()
 
 
-class BidMatrixWriteList(DomainModelWriteList[BidMatrixWrite]):
-    """List of bid matrixes in the writing version."""
+class BidMatrixRawWriteList(DomainModelWriteList[BidMatrixRawWrite]):
+    """List of bid matrix raws in the writing version."""
 
-    _INSTANCE = BidMatrixWrite
+    _INSTANCE = BidMatrixRawWrite
 
 
-class BidMatrixApplyList(BidMatrixWriteList): ...
+class BidMatrixRawApplyList(BidMatrixRawWriteList): ...
 
 
-def _create_bid_matrix_filter(
+def _create_bid_matrix_raw_filter(
     view_id: dm.ViewId,
     resource_cost: str | list[str] | None = None,
     resource_cost_prefix: str | None = None,
     asset_type: str | list[str] | None = None,
     asset_type_prefix: str | None = None,
     asset_id: str | list[str] | None = None,
     asset_id_prefix: str | None = None,
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_bid_matrix_raw.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_shop_time_series.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,13 +1,14 @@
 from __future__ import annotations
 
 import warnings
-from typing import TYPE_CHECKING, Any, Literal, Optional, Union
+from typing import Any, Literal, Optional, Union
 
 from cognite.client import data_modeling as dm
+from cognite.client.data_classes import TimeSeries as CogniteTimeSeries
 from pydantic import Field
 from pydantic import field_validator, model_validator
 
 from ._core import (
     DEFAULT_INSTANCE_SPACE,
     DataRecord,
     DataRecordGraphQL,
@@ -16,216 +17,206 @@
     DomainModelCore,
     DomainModelWrite,
     DomainModelWriteList,
     DomainModelList,
     DomainRelationWrite,
     GraphQLCore,
     ResourcesWrite,
+    TimeSeries,
 )
-from ._bid_matrix import BidMatrix, BidMatrixWrite
-
-if TYPE_CHECKING:
-    from ._alert import Alert, AlertGraphQL, AlertWrite
 
 
 __all__ = [
-    "BidMatrixRaw",
-    "BidMatrixRawWrite",
-    "BidMatrixRawApply",
-    "BidMatrixRawList",
-    "BidMatrixRawWriteList",
-    "BidMatrixRawApplyList",
-    "BidMatrixRawFields",
-    "BidMatrixRawTextFields",
+    "SHOPTimeSeries",
+    "SHOPTimeSeriesWrite",
+    "SHOPTimeSeriesApply",
+    "SHOPTimeSeriesList",
+    "SHOPTimeSeriesWriteList",
+    "SHOPTimeSeriesApplyList",
+    "SHOPTimeSeriesFields",
+    "SHOPTimeSeriesTextFields",
 ]
 
 
-BidMatrixRawTextFields = Literal["resource_cost", "matrix", "asset_type", "asset_id"]
-BidMatrixRawFields = Literal["resource_cost", "matrix", "asset_type", "asset_id", "is_processed"]
+SHOPTimeSeriesTextFields = Literal["object_type", "object_name", "attribute_name", "timeseries"]
+SHOPTimeSeriesFields = Literal["object_type", "object_name", "attribute_name", "timeseries"]
 
-_BIDMATRIXRAW_PROPERTIES_BY_FIELD = {
-    "resource_cost": "resourceCost",
-    "matrix": "matrix",
-    "asset_type": "assetType",
-    "asset_id": "assetId",
-    "is_processed": "isProcessed",
+_SHOPTIMESERIES_PROPERTIES_BY_FIELD = {
+    "object_type": "objectType",
+    "object_name": "objectName",
+    "attribute_name": "attributeName",
+    "timeseries": "timeseries",
 }
 
 
-class BidMatrixRawGraphQL(GraphQLCore):
-    """This represents the reading version of bid matrix raw, used
+class SHOPTimeSeriesGraphQL(GraphQLCore):
+    """This represents the reading version of shop time series, used
     when data is retrieved from CDF using GraphQL.
 
     It is used when retrieving data from CDF using GraphQL.
 
     Args:
         space: The space where the node is located.
-        external_id: The external id of the bid matrix raw.
-        data_record: The data record of the bid matrix raw node.
-        resource_cost: The resource cost field.
-        matrix: The matrix field.
-        asset_type: The asset type field.
-        asset_id: The asset id field.
-        is_processed: Whether the bid matrix has been processed by the bid matrix processor or not
-        alerts: The alert field.
+        external_id: The external id of the shop time series.
+        data_record: The data record of the shop time series node.
+        object_type: The type of the object
+        object_name: The name of the object
+        attribute_name: The name of the attribute
+        timeseries: Timeseries object from output of SHOP stored as a timeseries in cdf
     """
 
-    view_id = dm.ViewId("sp_powerops_models", "BidMatrixRaw", "1")
-    resource_cost: Optional[str] = Field(None, alias="resourceCost")
-    matrix: Union[str, None] = None
-    asset_type: Optional[str] = Field(None, alias="assetType")
-    asset_id: Optional[str] = Field(None, alias="assetId")
-    is_processed: Optional[bool] = Field(None, alias="isProcessed")
-    alerts: Optional[list[AlertGraphQL]] = Field(default=None, repr=False)
+    view_id = dm.ViewId("sp_powerops_models_temp", "SHOPTimeSeries", "1")
+    object_type: Optional[str] = Field(None, alias="objectType")
+    object_name: Optional[str] = Field(None, alias="objectName")
+    attribute_name: Optional[str] = Field(None, alias="attributeName")
+    timeseries: Union[TimeSeries, str, None] = None
 
     @model_validator(mode="before")
     def parse_data_record(cls, values: Any) -> Any:
         if not isinstance(values, dict):
             return values
         if "lastUpdatedTime" in values or "createdTime" in values:
             values["dataRecord"] = DataRecordGraphQL(
                 created_time=values.pop("createdTime", None),
                 last_updated_time=values.pop("lastUpdatedTime", None),
             )
         return values
 
-    @field_validator("alerts", mode="before")
-    def parse_graphql(cls, value: Any) -> Any:
-        if not isinstance(value, dict):
-            return value
-        if "items" in value:
-            return value["items"]
-        return value
-
-    def as_read(self) -> BidMatrixRaw:
-        """Convert this GraphQL format of bid matrix raw to the reading format."""
+    def as_read(self) -> SHOPTimeSeries:
+        """Convert this GraphQL format of shop time series to the reading format."""
         if self.data_record is None:
             raise ValueError("This object cannot be converted to a read format because it lacks a data record.")
-        return BidMatrixRaw(
+        return SHOPTimeSeries(
             space=self.space,
             external_id=self.external_id,
             data_record=DataRecord(
                 version=0,
                 last_updated_time=self.data_record.last_updated_time,
                 created_time=self.data_record.created_time,
             ),
-            resource_cost=self.resource_cost,
-            matrix=self.matrix,
-            asset_type=self.asset_type,
-            asset_id=self.asset_id,
-            is_processed=self.is_processed,
-            alerts=[alert.as_read() if isinstance(alert, GraphQLCore) else alert for alert in self.alerts or []],
+            object_type=self.object_type,
+            object_name=self.object_name,
+            attribute_name=self.attribute_name,
+            timeseries=self.timeseries,
         )
 
-    def as_write(self) -> BidMatrixRawWrite:
-        """Convert this GraphQL format of bid matrix raw to the writing format."""
-        return BidMatrixRawWrite(
+    def as_write(self) -> SHOPTimeSeriesWrite:
+        """Convert this GraphQL format of shop time series to the writing format."""
+        return SHOPTimeSeriesWrite(
             space=self.space,
             external_id=self.external_id,
             data_record=DataRecordWrite(existing_version=0),
-            resource_cost=self.resource_cost,
-            matrix=self.matrix,
-            asset_type=self.asset_type,
-            asset_id=self.asset_id,
-            is_processed=self.is_processed,
-            alerts=[alert.as_write() if isinstance(alert, DomainModel) else alert for alert in self.alerts or []],
+            object_type=self.object_type,
+            object_name=self.object_name,
+            attribute_name=self.attribute_name,
+            timeseries=self.timeseries,
         )
 
 
-class BidMatrixRaw(BidMatrix):
-    """This represents the reading version of bid matrix raw.
+class SHOPTimeSeries(DomainModel):
+    """This represents the reading version of shop time series.
 
     It is used to when data is retrieved from CDF.
 
     Args:
         space: The space where the node is located.
-        external_id: The external id of the bid matrix raw.
-        data_record: The data record of the bid matrix raw node.
-        resource_cost: The resource cost field.
-        matrix: The matrix field.
-        asset_type: The asset type field.
-        asset_id: The asset id field.
-        is_processed: Whether the bid matrix has been processed by the bid matrix processor or not
-        alerts: The alert field.
+        external_id: The external id of the shop time series.
+        data_record: The data record of the shop time series node.
+        object_type: The type of the object
+        object_name: The name of the object
+        attribute_name: The name of the attribute
+        timeseries: Timeseries object from output of SHOP stored as a timeseries in cdf
     """
 
-    node_type: Union[dm.DirectRelationReference, None] = None
-
-    def as_write(self) -> BidMatrixRawWrite:
-        """Convert this read version of bid matrix raw to the writing version."""
-        return BidMatrixRawWrite(
+    space: str = DEFAULT_INSTANCE_SPACE
+    node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference(
+        "sp_powerops_types_temp", "SHOPTimeSeries"
+    )
+    object_type: Optional[str] = Field(None, alias="objectType")
+    object_name: Optional[str] = Field(None, alias="objectName")
+    attribute_name: Optional[str] = Field(None, alias="attributeName")
+    timeseries: Union[TimeSeries, str, None] = None
+
+    def as_write(self) -> SHOPTimeSeriesWrite:
+        """Convert this read version of shop time series to the writing version."""
+        return SHOPTimeSeriesWrite(
             space=self.space,
             external_id=self.external_id,
             data_record=DataRecordWrite(existing_version=self.data_record.version),
-            resource_cost=self.resource_cost,
-            matrix=self.matrix,
-            asset_type=self.asset_type,
-            asset_id=self.asset_id,
-            is_processed=self.is_processed,
-            alerts=[alert.as_write() if isinstance(alert, DomainModel) else alert for alert in self.alerts or []],
+            object_type=self.object_type,
+            object_name=self.object_name,
+            attribute_name=self.attribute_name,
+            timeseries=self.timeseries,
         )
 
-    def as_apply(self) -> BidMatrixRawWrite:
-        """Convert this read version of bid matrix raw to the writing version."""
+    def as_apply(self) -> SHOPTimeSeriesWrite:
+        """Convert this read version of shop time series to the writing version."""
         warnings.warn(
             "as_apply is deprecated and will be removed in v1.0. Use as_write instead.",
             UserWarning,
             stacklevel=2,
         )
         return self.as_write()
 
 
-class BidMatrixRawWrite(BidMatrixWrite):
-    """This represents the writing version of bid matrix raw.
+class SHOPTimeSeriesWrite(DomainModelWrite):
+    """This represents the writing version of shop time series.
 
     It is used to when data is sent to CDF.
 
     Args:
         space: The space where the node is located.
-        external_id: The external id of the bid matrix raw.
-        data_record: The data record of the bid matrix raw node.
-        resource_cost: The resource cost field.
-        matrix: The matrix field.
-        asset_type: The asset type field.
-        asset_id: The asset id field.
-        is_processed: Whether the bid matrix has been processed by the bid matrix processor or not
-        alerts: The alert field.
+        external_id: The external id of the shop time series.
+        data_record: The data record of the shop time series node.
+        object_type: The type of the object
+        object_name: The name of the object
+        attribute_name: The name of the attribute
+        timeseries: Timeseries object from output of SHOP stored as a timeseries in cdf
     """
 
-    node_type: Union[dm.DirectRelationReference, None] = None
+    space: str = DEFAULT_INSTANCE_SPACE
+    node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference(
+        "sp_powerops_types_temp", "SHOPTimeSeries"
+    )
+    object_type: Optional[str] = Field(None, alias="objectType")
+    object_name: Optional[str] = Field(None, alias="objectName")
+    attribute_name: Optional[str] = Field(None, alias="attributeName")
+    timeseries: Union[TimeSeries, str, None] = None
 
     def _to_instances_write(
         self,
         cache: set[tuple[str, str]],
         view_by_read_class: dict[type[DomainModelCore], dm.ViewId] | None,
         write_none: bool = False,
         allow_version_increase: bool = False,
     ) -> ResourcesWrite:
         resources = ResourcesWrite()
         if self.as_tuple_id() in cache:
             return resources
 
-        write_view = (view_by_read_class or {}).get(BidMatrixRaw, dm.ViewId("sp_powerops_models", "BidMatrixRaw", "1"))
+        write_view = (view_by_read_class or {}).get(
+            SHOPTimeSeries, dm.ViewId("sp_powerops_models_temp", "SHOPTimeSeries", "1")
+        )
 
         properties: dict[str, Any] = {}
 
-        if self.resource_cost is not None or write_none:
-            properties["resourceCost"] = self.resource_cost
-
-        if self.matrix is not None or write_none:
-            properties["matrix"] = self.matrix
+        if self.object_type is not None or write_none:
+            properties["objectType"] = self.object_type
 
-        if self.asset_type is not None or write_none:
-            properties["assetType"] = self.asset_type
+        if self.object_name is not None or write_none:
+            properties["objectName"] = self.object_name
 
-        if self.asset_id is not None or write_none:
-            properties["assetId"] = self.asset_id
+        if self.attribute_name is not None or write_none:
+            properties["attributeName"] = self.attribute_name
 
-        if self.is_processed is not None or write_none:
-            properties["isProcessed"] = self.is_processed
+        if self.timeseries is not None or write_none:
+            if isinstance(self.timeseries, str) or self.timeseries is None:
+                properties["timeseries"] = self.timeseries
+            else:
+                properties["timeseries"] = self.timeseries.external_id
 
         if properties:
             this_node = dm.NodeApply(
                 space=self.space,
                 external_id=self.external_id,
                 existing_version=None if allow_version_increase else self.data_record.existing_version,
                 type=self.node_type,
@@ -235,104 +226,91 @@
                         properties=properties,
                     )
                 ],
             )
             resources.nodes.append(this_node)
             cache.add(self.as_tuple_id())
 
-        edge_type = dm.DirectRelationReference("sp_powerops_types", "calculationIssue")
-        for alert in self.alerts or []:
-            other_resources = DomainRelationWrite.from_edge_to_resources(
-                cache,
-                start_node=self,
-                end_node=alert,
-                edge_type=edge_type,
-                view_by_read_class=view_by_read_class,
-                write_none=write_none,
-                allow_version_increase=allow_version_increase,
-            )
-            resources.extend(other_resources)
+        if isinstance(self.timeseries, CogniteTimeSeries):
+            resources.time_series.append(self.timeseries)
 
         return resources
 
 
-class BidMatrixRawApply(BidMatrixRawWrite):
-    def __new__(cls, *args, **kwargs) -> BidMatrixRawApply:
+class SHOPTimeSeriesApply(SHOPTimeSeriesWrite):
+    def __new__(cls, *args, **kwargs) -> SHOPTimeSeriesApply:
         warnings.warn(
-            "BidMatrixRawApply is deprecated and will be removed in v1.0. Use BidMatrixRawWrite instead."
+            "SHOPTimeSeriesApply is deprecated and will be removed in v1.0. Use SHOPTimeSeriesWrite instead."
             "The motivation for this change is that Write is a more descriptive name for the writing version of the"
-            "BidMatrixRaw.",
+            "SHOPTimeSeries.",
             UserWarning,
             stacklevel=2,
         )
         return super().__new__(cls)
 
 
-class BidMatrixRawList(DomainModelList[BidMatrixRaw]):
-    """List of bid matrix raws in the read version."""
+class SHOPTimeSeriesList(DomainModelList[SHOPTimeSeries]):
+    """List of shop time series in the read version."""
 
-    _INSTANCE = BidMatrixRaw
+    _INSTANCE = SHOPTimeSeries
 
-    def as_write(self) -> BidMatrixRawWriteList:
-        """Convert these read versions of bid matrix raw to the writing versions."""
-        return BidMatrixRawWriteList([node.as_write() for node in self.data])
+    def as_write(self) -> SHOPTimeSeriesWriteList:
+        """Convert these read versions of shop time series to the writing versions."""
+        return SHOPTimeSeriesWriteList([node.as_write() for node in self.data])
 
-    def as_apply(self) -> BidMatrixRawWriteList:
+    def as_apply(self) -> SHOPTimeSeriesWriteList:
         """Convert these read versions of primitive nullable to the writing versions."""
         warnings.warn(
             "as_apply is deprecated and will be removed in v1.0. Use as_write instead.",
             UserWarning,
             stacklevel=2,
         )
         return self.as_write()
 
 
-class BidMatrixRawWriteList(DomainModelWriteList[BidMatrixRawWrite]):
-    """List of bid matrix raws in the writing version."""
+class SHOPTimeSeriesWriteList(DomainModelWriteList[SHOPTimeSeriesWrite]):
+    """List of shop time series in the writing version."""
 
-    _INSTANCE = BidMatrixRawWrite
+    _INSTANCE = SHOPTimeSeriesWrite
 
 
-class BidMatrixRawApplyList(BidMatrixRawWriteList): ...
+class SHOPTimeSeriesApplyList(SHOPTimeSeriesWriteList): ...
 
 
-def _create_bid_matrix_raw_filter(
+def _create_shop_time_series_filter(
     view_id: dm.ViewId,
-    resource_cost: str | list[str] | None = None,
-    resource_cost_prefix: str | None = None,
-    asset_type: str | list[str] | None = None,
-    asset_type_prefix: str | None = None,
-    asset_id: str | list[str] | None = None,
-    asset_id_prefix: str | None = None,
-    is_processed: bool | None = None,
+    object_type: str | list[str] | None = None,
+    object_type_prefix: str | None = None,
+    object_name: str | list[str] | None = None,
+    object_name_prefix: str | None = None,
+    attribute_name: str | list[str] | None = None,
+    attribute_name_prefix: str | None = None,
     external_id_prefix: str | None = None,
     space: str | list[str] | None = None,
     filter: dm.Filter | None = None,
 ) -> dm.Filter | None:
     filters = []
-    if isinstance(resource_cost, str):
-        filters.append(dm.filters.Equals(view_id.as_property_ref("resourceCost"), value=resource_cost))
-    if resource_cost and isinstance(resource_cost, list):
-        filters.append(dm.filters.In(view_id.as_property_ref("resourceCost"), values=resource_cost))
-    if resource_cost_prefix is not None:
-        filters.append(dm.filters.Prefix(view_id.as_property_ref("resourceCost"), value=resource_cost_prefix))
-    if isinstance(asset_type, str):
-        filters.append(dm.filters.Equals(view_id.as_property_ref("assetType"), value=asset_type))
-    if asset_type and isinstance(asset_type, list):
-        filters.append(dm.filters.In(view_id.as_property_ref("assetType"), values=asset_type))
-    if asset_type_prefix is not None:
-        filters.append(dm.filters.Prefix(view_id.as_property_ref("assetType"), value=asset_type_prefix))
-    if isinstance(asset_id, str):
-        filters.append(dm.filters.Equals(view_id.as_property_ref("assetId"), value=asset_id))
-    if asset_id and isinstance(asset_id, list):
-        filters.append(dm.filters.In(view_id.as_property_ref("assetId"), values=asset_id))
-    if asset_id_prefix is not None:
-        filters.append(dm.filters.Prefix(view_id.as_property_ref("assetId"), value=asset_id_prefix))
-    if isinstance(is_processed, bool):
-        filters.append(dm.filters.Equals(view_id.as_property_ref("isProcessed"), value=is_processed))
+    if isinstance(object_type, str):
+        filters.append(dm.filters.Equals(view_id.as_property_ref("objectType"), value=object_type))
+    if object_type and isinstance(object_type, list):
+        filters.append(dm.filters.In(view_id.as_property_ref("objectType"), values=object_type))
+    if object_type_prefix is not None:
+        filters.append(dm.filters.Prefix(view_id.as_property_ref("objectType"), value=object_type_prefix))
+    if isinstance(object_name, str):
+        filters.append(dm.filters.Equals(view_id.as_property_ref("objectName"), value=object_name))
+    if object_name and isinstance(object_name, list):
+        filters.append(dm.filters.In(view_id.as_property_ref("objectName"), values=object_name))
+    if object_name_prefix is not None:
+        filters.append(dm.filters.Prefix(view_id.as_property_ref("objectName"), value=object_name_prefix))
+    if isinstance(attribute_name, str):
+        filters.append(dm.filters.Equals(view_id.as_property_ref("attributeName"), value=attribute_name))
+    if attribute_name and isinstance(attribute_name, list):
+        filters.append(dm.filters.In(view_id.as_property_ref("attributeName"), values=attribute_name))
+    if attribute_name_prefix is not None:
+        filters.append(dm.filters.Prefix(view_id.as_property_ref("attributeName"), value=attribute_name_prefix))
     if external_id_prefix is not None:
         filters.append(dm.filters.Prefix(["node", "externalId"], value=external_id_prefix))
     if isinstance(space, str):
         filters.append(dm.filters.Equals(["node", "space"], value=space))
     if space and isinstance(space, list):
         filters.append(dm.filters.In(["node", "space"], values=space))
     if filter:
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_bid_method.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_bid_method.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_bid_method_afrr.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_bid_method_afrr.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_bid_method_custom.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_bid_method_custom.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_bid_method_day_ahead.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_bid_method_day_ahead.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_bid_method_shop_multi_scenario.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_bid_method_shop_multi_scenario.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_bid_method_water_value.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_bid_method_water_value.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_bid_row.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_bid_row.py`

 * *Files 12% similar despite different names*

```diff
@@ -20,53 +20,43 @@
     DomainRelationWrite,
     GraphQLCore,
     ResourcesWrite,
 )
 
 if TYPE_CHECKING:
     from ._alert import Alert, AlertGraphQL, AlertWrite
-    from ._bid_method_afrr import BidMethodAFRR, BidMethodAFRRGraphQL, BidMethodAFRRWrite
     from ._bid_row import BidRow, BidRowGraphQL, BidRowWrite
+    from ._power_asset import PowerAsset, PowerAssetGraphQL, PowerAssetWrite
 
 
 __all__ = [
     "BidRow",
     "BidRowWrite",
     "BidRowApply",
     "BidRowList",
     "BidRowWriteList",
     "BidRowApplyList",
     "BidRowFields",
     "BidRowTextFields",
 ]
 
 
-BidRowTextFields = Literal["product", "exclusive_group_id", "asset_type", "asset_id"]
+BidRowTextFields = Literal["product", "exclusive_group_id"]
 BidRowFields = Literal[
-    "price",
-    "quantity_per_hour",
-    "product",
-    "is_divisible",
-    "min_quantity",
-    "is_block",
-    "exclusive_group_id",
-    "asset_type",
-    "asset_id",
+    "price", "quantity_per_hour", "product", "is_divisible", "min_quantity", "is_block", "exclusive_group_id"
 ]
 
 _BIDROW_PROPERTIES_BY_FIELD = {
     "price": "price",
     "quantity_per_hour": "quantityPerHour",
     "product": "product",
     "is_divisible": "isDivisible",
     "min_quantity": "minQuantity",
     "is_block": "isBlock",
     "exclusive_group_id": "exclusiveGroupId",
-    "asset_type": "assetType",
-    "asset_id": "assetId",
 }
 
 
 class BidRowGraphQL(GraphQLCore):
     """This represents the reading version of bid row, used
     when data is retrieved from CDF using GraphQL.
 
@@ -80,46 +70,42 @@
         quantity_per_hour: The capacity offered, per hour, in MW, rounded to nearest step size (5?)
         product: The product field.
         is_divisible: The is divisible field.
         min_quantity: Min quantity, per hour. Only relevant for divisible Bids. The minimum capacity that must be accepted; this must be lower than capacityPerHour and is rounded to the nearest step (5 MW?)).
         is_block: Indication if the row is part of a Block bid. If true: quantityPerHour must have the same value for consecutive hours (and no breaks). Block bids must be accepted for all hours or none.
         exclusive_group_id: Other bids with the same ID are part of an exclusive group - only one of them can be accepted, and they must have the same direction (product). Not allowed for block bids.
         linked_bid: The linked bid must have the opposite direction (link means that both or none must be accepted). Should be bi-directional.
-        asset_type: The asset type field.
-        asset_id: The asset id field.
-        method: The method field.
+        power_asset: TODO description
         alerts: An array of associated alerts.
     """
 
-    view_id = dm.ViewId("sp_powerops_models", "BidRow", "1")
+    view_id = dm.ViewId("sp_powerops_models_temp", "BidRow", "1")
     price: Optional[float] = None
     quantity_per_hour: Optional[list[float]] = Field(None, alias="quantityPerHour")
     product: Optional[str] = None
     is_divisible: Optional[bool] = Field(None, alias="isDivisible")
     min_quantity: Optional[list[float]] = Field(None, alias="minQuantity")
     is_block: Optional[bool] = Field(None, alias="isBlock")
     exclusive_group_id: Optional[str] = Field(None, alias="exclusiveGroupId")
     linked_bid: Optional[BidRowGraphQL] = Field(None, repr=False, alias="linkedBid")
-    asset_type: Optional[str] = Field(None, alias="assetType")
-    asset_id: Optional[str] = Field(None, alias="assetId")
-    method: Optional[BidMethodAFRRGraphQL] = Field(None, repr=False)
+    power_asset: Optional[PowerAssetGraphQL] = Field(None, repr=False, alias="powerAsset")
     alerts: Optional[list[AlertGraphQL]] = Field(default=None, repr=False)
 
     @model_validator(mode="before")
     def parse_data_record(cls, values: Any) -> Any:
         if not isinstance(values, dict):
             return values
         if "lastUpdatedTime" in values or "createdTime" in values:
             values["dataRecord"] = DataRecordGraphQL(
                 created_time=values.pop("createdTime", None),
                 last_updated_time=values.pop("lastUpdatedTime", None),
             )
         return values
 
-    @field_validator("linked_bid", "method", "alerts", mode="before")
+    @field_validator("linked_bid", "power_asset", "alerts", mode="before")
     def parse_graphql(cls, value: Any) -> Any:
         if not isinstance(value, dict):
             return value
         if "items" in value:
             return value["items"]
         return value
 
@@ -139,17 +125,15 @@
             quantity_per_hour=self.quantity_per_hour,
             product=self.product,
             is_divisible=self.is_divisible,
             min_quantity=self.min_quantity,
             is_block=self.is_block,
             exclusive_group_id=self.exclusive_group_id,
             linked_bid=self.linked_bid.as_read() if isinstance(self.linked_bid, GraphQLCore) else self.linked_bid,
-            asset_type=self.asset_type,
-            asset_id=self.asset_id,
-            method=self.method.as_read() if isinstance(self.method, GraphQLCore) else self.method,
+            power_asset=self.power_asset.as_read() if isinstance(self.power_asset, GraphQLCore) else self.power_asset,
             alerts=[alert.as_read() if isinstance(alert, GraphQLCore) else alert for alert in self.alerts or []],
         )
 
     def as_write(self) -> BidRowWrite:
         """Convert this GraphQL format of bid row to the writing format."""
         return BidRowWrite(
             space=self.space,
@@ -159,17 +143,15 @@
             quantity_per_hour=self.quantity_per_hour,
             product=self.product,
             is_divisible=self.is_divisible,
             min_quantity=self.min_quantity,
             is_block=self.is_block,
             exclusive_group_id=self.exclusive_group_id,
             linked_bid=self.linked_bid.as_write() if isinstance(self.linked_bid, DomainModel) else self.linked_bid,
-            asset_type=self.asset_type,
-            asset_id=self.asset_id,
-            method=self.method.as_write() if isinstance(self.method, DomainModel) else self.method,
+            power_asset=self.power_asset.as_write() if isinstance(self.power_asset, DomainModel) else self.power_asset,
             alerts=[alert.as_write() if isinstance(alert, DomainModel) else alert for alert in self.alerts or []],
         )
 
 
 class BidRow(DomainModel):
     """This represents the reading version of bid row.
 
@@ -183,33 +165,29 @@
         quantity_per_hour: The capacity offered, per hour, in MW, rounded to nearest step size (5?)
         product: The product field.
         is_divisible: The is divisible field.
         min_quantity: Min quantity, per hour. Only relevant for divisible Bids. The minimum capacity that must be accepted; this must be lower than capacityPerHour and is rounded to the nearest step (5 MW?)).
         is_block: Indication if the row is part of a Block bid. If true: quantityPerHour must have the same value for consecutive hours (and no breaks). Block bids must be accepted for all hours or none.
         exclusive_group_id: Other bids with the same ID are part of an exclusive group - only one of them can be accepted, and they must have the same direction (product). Not allowed for block bids.
         linked_bid: The linked bid must have the opposite direction (link means that both or none must be accepted). Should be bi-directional.
-        asset_type: The asset type field.
-        asset_id: The asset id field.
-        method: The method field.
+        power_asset: TODO description
         alerts: An array of associated alerts.
     """
 
     space: str = DEFAULT_INSTANCE_SPACE
-    node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference("sp_powerops_types", "BidRow")
+    node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference("sp_powerops_types_temp", "BidRow")
     price: Optional[float] = None
     quantity_per_hour: Optional[list[float]] = Field(None, alias="quantityPerHour")
     product: Optional[str] = None
     is_divisible: Optional[bool] = Field(None, alias="isDivisible")
     min_quantity: Optional[list[float]] = Field(None, alias="minQuantity")
     is_block: Optional[bool] = Field(None, alias="isBlock")
     exclusive_group_id: Optional[str] = Field(None, alias="exclusiveGroupId")
     linked_bid: Union[BidRow, str, dm.NodeId, None] = Field(None, repr=False, alias="linkedBid")
-    asset_type: Optional[str] = Field(None, alias="assetType")
-    asset_id: Optional[str] = Field(None, alias="assetId")
-    method: Union[BidMethodAFRR, str, dm.NodeId, None] = Field(None, repr=False)
+    power_asset: Union[PowerAsset, str, dm.NodeId, None] = Field(None, repr=False, alias="powerAsset")
     alerts: Union[list[Alert], list[str], list[dm.NodeId], None] = Field(default=None, repr=False)
 
     def as_write(self) -> BidRowWrite:
         """Convert this read version of bid row to the writing version."""
         return BidRowWrite(
             space=self.space,
             external_id=self.external_id,
@@ -218,17 +196,15 @@
             quantity_per_hour=self.quantity_per_hour,
             product=self.product,
             is_divisible=self.is_divisible,
             min_quantity=self.min_quantity,
             is_block=self.is_block,
             exclusive_group_id=self.exclusive_group_id,
             linked_bid=self.linked_bid.as_write() if isinstance(self.linked_bid, DomainModel) else self.linked_bid,
-            asset_type=self.asset_type,
-            asset_id=self.asset_id,
-            method=self.method.as_write() if isinstance(self.method, DomainModel) else self.method,
+            power_asset=self.power_asset.as_write() if isinstance(self.power_asset, DomainModel) else self.power_asset,
             alerts=[alert.as_write() if isinstance(alert, DomainModel) else alert for alert in self.alerts or []],
         )
 
     def as_apply(self) -> BidRowWrite:
         """Convert this read version of bid row to the writing version."""
         warnings.warn(
             "as_apply is deprecated and will be removed in v1.0. Use as_write instead.",
@@ -251,47 +227,43 @@
         quantity_per_hour: The capacity offered, per hour, in MW, rounded to nearest step size (5?)
         product: The product field.
         is_divisible: The is divisible field.
         min_quantity: Min quantity, per hour. Only relevant for divisible Bids. The minimum capacity that must be accepted; this must be lower than capacityPerHour and is rounded to the nearest step (5 MW?)).
         is_block: Indication if the row is part of a Block bid. If true: quantityPerHour must have the same value for consecutive hours (and no breaks). Block bids must be accepted for all hours or none.
         exclusive_group_id: Other bids with the same ID are part of an exclusive group - only one of them can be accepted, and they must have the same direction (product). Not allowed for block bids.
         linked_bid: The linked bid must have the opposite direction (link means that both or none must be accepted). Should be bi-directional.
-        asset_type: The asset type field.
-        asset_id: The asset id field.
-        method: The method field.
+        power_asset: TODO description
         alerts: An array of associated alerts.
     """
 
     space: str = DEFAULT_INSTANCE_SPACE
-    node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference("sp_powerops_types", "BidRow")
+    node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference("sp_powerops_types_temp", "BidRow")
     price: Optional[float] = None
     quantity_per_hour: Optional[list[float]] = Field(None, alias="quantityPerHour")
     product: Optional[str] = None
     is_divisible: Optional[bool] = Field(None, alias="isDivisible")
     min_quantity: Optional[list[float]] = Field(None, alias="minQuantity")
     is_block: Optional[bool] = Field(None, alias="isBlock")
     exclusive_group_id: Optional[str] = Field(None, alias="exclusiveGroupId")
     linked_bid: Union[BidRowWrite, str, dm.NodeId, None] = Field(None, repr=False, alias="linkedBid")
-    asset_type: Optional[str] = Field(None, alias="assetType")
-    asset_id: Optional[str] = Field(None, alias="assetId")
-    method: Union[BidMethodAFRRWrite, str, dm.NodeId, None] = Field(None, repr=False)
+    power_asset: Union[PowerAssetWrite, str, dm.NodeId, None] = Field(None, repr=False, alias="powerAsset")
     alerts: Union[list[AlertWrite], list[str], list[dm.NodeId], None] = Field(default=None, repr=False)
 
     def _to_instances_write(
         self,
         cache: set[tuple[str, str]],
         view_by_read_class: dict[type[DomainModelCore], dm.ViewId] | None,
         write_none: bool = False,
         allow_version_increase: bool = False,
     ) -> ResourcesWrite:
         resources = ResourcesWrite()
         if self.as_tuple_id() in cache:
             return resources
 
-        write_view = (view_by_read_class or {}).get(BidRow, dm.ViewId("sp_powerops_models", "BidRow", "1"))
+        write_view = (view_by_read_class or {}).get(BidRow, dm.ViewId("sp_powerops_models_temp", "BidRow", "1"))
 
         properties: dict[str, Any] = {}
 
         if self.price is not None or write_none:
             properties["price"] = self.price
 
         if self.quantity_per_hour is not None or write_none:
@@ -314,24 +286,18 @@
 
         if self.linked_bid is not None:
             properties["linkedBid"] = {
                 "space": self.space if isinstance(self.linked_bid, str) else self.linked_bid.space,
                 "externalId": self.linked_bid if isinstance(self.linked_bid, str) else self.linked_bid.external_id,
             }
 
-        if self.asset_type is not None or write_none:
-            properties["assetType"] = self.asset_type
-
-        if self.asset_id is not None or write_none:
-            properties["assetId"] = self.asset_id
-
-        if self.method is not None:
-            properties["method"] = {
-                "space": self.space if isinstance(self.method, str) else self.method.space,
-                "externalId": self.method if isinstance(self.method, str) else self.method.external_id,
+        if self.power_asset is not None:
+            properties["powerAsset"] = {
+                "space": self.space if isinstance(self.power_asset, str) else self.power_asset.space,
+                "externalId": self.power_asset if isinstance(self.power_asset, str) else self.power_asset.external_id,
             }
 
         if properties:
             this_node = dm.NodeApply(
                 space=self.space,
                 external_id=self.external_id,
                 existing_version=None if allow_version_increase else self.data_record.existing_version,
@@ -342,15 +308,15 @@
                         properties=properties,
                     )
                 ],
             )
             resources.nodes.append(this_node)
             cache.add(self.as_tuple_id())
 
-        edge_type = dm.DirectRelationReference("sp_powerops_types", "calculationIssue")
+        edge_type = dm.DirectRelationReference("sp_powerops_types_temp", "calculationIssue")
         for alert in self.alerts or []:
             other_resources = DomainRelationWrite.from_edge_to_resources(
                 cache,
                 start_node=self,
                 end_node=alert,
                 edge_type=edge_type,
                 view_by_read_class=view_by_read_class,
@@ -359,16 +325,16 @@
             )
             resources.extend(other_resources)
 
         if isinstance(self.linked_bid, DomainModelWrite):
             other_resources = self.linked_bid._to_instances_write(cache, view_by_read_class)
             resources.extend(other_resources)
 
-        if isinstance(self.method, DomainModelWrite):
-            other_resources = self.method._to_instances_write(cache, view_by_read_class)
+        if isinstance(self.power_asset, DomainModelWrite):
+            other_resources = self.power_asset._to_instances_write(cache, view_by_read_class)
             resources.extend(other_resources)
 
         return resources
 
 
 class BidRowApply(BidRowWrite):
     def __new__(cls, *args, **kwargs) -> BidRowApply:
@@ -417,19 +383,15 @@
     product: str | list[str] | None = None,
     product_prefix: str | None = None,
     is_divisible: bool | None = None,
     is_block: bool | None = None,
     exclusive_group_id: str | list[str] | None = None,
     exclusive_group_id_prefix: str | None = None,
     linked_bid: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
-    asset_type: str | list[str] | None = None,
-    asset_type_prefix: str | None = None,
-    asset_id: str | list[str] | None = None,
-    asset_id_prefix: str | None = None,
-    method: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+    power_asset: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
     external_id_prefix: str | None = None,
     space: str | list[str] | None = None,
     filter: dm.Filter | None = None,
 ) -> dm.Filter | None:
     filters = []
     if min_price is not None or max_price is not None:
         filters.append(dm.filters.Range(view_id.as_property_ref("price"), gte=min_price, lte=max_price))
@@ -471,47 +433,39 @@
     if linked_bid and isinstance(linked_bid, list) and isinstance(linked_bid[0], tuple):
         filters.append(
             dm.filters.In(
                 view_id.as_property_ref("linkedBid"),
                 values=[{"space": item[0], "externalId": item[1]} for item in linked_bid],
             )
         )
-    if isinstance(asset_type, str):
-        filters.append(dm.filters.Equals(view_id.as_property_ref("assetType"), value=asset_type))
-    if asset_type and isinstance(asset_type, list):
-        filters.append(dm.filters.In(view_id.as_property_ref("assetType"), values=asset_type))
-    if asset_type_prefix is not None:
-        filters.append(dm.filters.Prefix(view_id.as_property_ref("assetType"), value=asset_type_prefix))
-    if isinstance(asset_id, str):
-        filters.append(dm.filters.Equals(view_id.as_property_ref("assetId"), value=asset_id))
-    if asset_id and isinstance(asset_id, list):
-        filters.append(dm.filters.In(view_id.as_property_ref("assetId"), values=asset_id))
-    if asset_id_prefix is not None:
-        filters.append(dm.filters.Prefix(view_id.as_property_ref("assetId"), value=asset_id_prefix))
-    if method and isinstance(method, str):
+    if power_asset and isinstance(power_asset, str):
         filters.append(
             dm.filters.Equals(
-                view_id.as_property_ref("method"), value={"space": DEFAULT_INSTANCE_SPACE, "externalId": method}
+                view_id.as_property_ref("powerAsset"),
+                value={"space": DEFAULT_INSTANCE_SPACE, "externalId": power_asset},
             )
         )
-    if method and isinstance(method, tuple):
+    if power_asset and isinstance(power_asset, tuple):
         filters.append(
-            dm.filters.Equals(view_id.as_property_ref("method"), value={"space": method[0], "externalId": method[1]})
+            dm.filters.Equals(
+                view_id.as_property_ref("powerAsset"), value={"space": power_asset[0], "externalId": power_asset[1]}
+            )
         )
-    if method and isinstance(method, list) and isinstance(method[0], str):
+    if power_asset and isinstance(power_asset, list) and isinstance(power_asset[0], str):
         filters.append(
             dm.filters.In(
-                view_id.as_property_ref("method"),
-                values=[{"space": DEFAULT_INSTANCE_SPACE, "externalId": item} for item in method],
+                view_id.as_property_ref("powerAsset"),
+                values=[{"space": DEFAULT_INSTANCE_SPACE, "externalId": item} for item in power_asset],
             )
         )
-    if method and isinstance(method, list) and isinstance(method[0], tuple):
+    if power_asset and isinstance(power_asset, list) and isinstance(power_asset[0], tuple):
         filters.append(
             dm.filters.In(
-                view_id.as_property_ref("method"), values=[{"space": item[0], "externalId": item[1]} for item in method]
+                view_id.as_property_ref("powerAsset"),
+                values=[{"space": item[0], "externalId": item[1]} for item in power_asset],
             )
         )
     if external_id_prefix is not None:
         filters.append(dm.filters.Prefix(["node", "externalId"], value=external_id_prefix))
     if isinstance(space, str):
         filters.append(dm.filters.Equals(["node", "space"], value=space))
     if space and isinstance(space, list):
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_case.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_case.py`

 * *Files 4% similar despite different names*

```diff
@@ -77,23 +77,23 @@
         cut_order_files: Cut order files (Module series in PRODRISK)
         extra_files: The extra file field.
         cog_shop_files_config: Configuration for in what order to load the various files into pyshop
         start_time: The start time of the case
         end_time: The end time of the case
     """
 
-    view_id = dm.ViewId("sp_powerops_models", "Case", "1")
+    view_id = dm.ViewId("sp_powerops_models_temp", "Case", "1")
     scenario: Optional[ScenarioGraphQL] = Field(None, repr=False)
     case_file: Union[str, None] = Field(None, alias="caseFile")
     reservoir_mapping: Optional[list[str]] = Field(None, alias="reservoirMapping")
     cut_order_files: Optional[list[str]] = Field(None, alias="cutOrderFiles")
     extra_files: Optional[list[str]] = Field(None, alias="extraFiles")
     cog_shop_files_config: Optional[list[dict]] = Field(None, alias="cogShopFilesConfig")
-    start_time: Optional[datetime.date] = Field(None, alias="startTime")
-    end_time: Optional[datetime.date] = Field(None, alias="endTime")
+    start_time: Optional[datetime.datetime] = Field(None, alias="startTime")
+    end_time: Optional[datetime.datetime] = Field(None, alias="endTime")
 
     @model_validator(mode="before")
     def parse_data_record(cls, values: Any) -> Any:
         if not isinstance(values, dict):
             return values
         if "lastUpdatedTime" in values or "createdTime" in values:
             values["dataRecord"] = DataRecordGraphQL(
@@ -165,23 +165,23 @@
         extra_files: The extra file field.
         cog_shop_files_config: Configuration for in what order to load the various files into pyshop
         start_time: The start time of the case
         end_time: The end time of the case
     """
 
     space: str = DEFAULT_INSTANCE_SPACE
-    node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference("sp_powerops_types", "Case")
+    node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference("sp_powerops_types_temp", "Case")
     scenario: Union[Scenario, str, dm.NodeId, None] = Field(None, repr=False)
     case_file: Union[str, None] = Field(None, alias="caseFile")
     reservoir_mapping: Optional[list[str]] = Field(None, alias="reservoirMapping")
     cut_order_files: Optional[list[str]] = Field(None, alias="cutOrderFiles")
     extra_files: Optional[list[str]] = Field(None, alias="extraFiles")
     cog_shop_files_config: Optional[list[dict]] = Field(None, alias="cogShopFilesConfig")
-    start_time: Optional[datetime.date] = Field(None, alias="startTime")
-    end_time: Optional[datetime.date] = Field(None, alias="endTime")
+    start_time: Optional[datetime.datetime] = Field(None, alias="startTime")
+    end_time: Optional[datetime.datetime] = Field(None, alias="endTime")
 
     def as_write(self) -> CaseWrite:
         """Convert this read version of case to the writing version."""
         return CaseWrite(
             space=self.space,
             external_id=self.external_id,
             data_record=DataRecordWrite(existing_version=self.data_record.version),
@@ -221,36 +221,36 @@
         extra_files: The extra file field.
         cog_shop_files_config: Configuration for in what order to load the various files into pyshop
         start_time: The start time of the case
         end_time: The end time of the case
     """
 
     space: str = DEFAULT_INSTANCE_SPACE
-    node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference("sp_powerops_types", "Case")
+    node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference("sp_powerops_types_temp", "Case")
     scenario: Union[ScenarioWrite, str, dm.NodeId, None] = Field(None, repr=False)
     case_file: Union[str, None] = Field(None, alias="caseFile")
     reservoir_mapping: Optional[list[str]] = Field(None, alias="reservoirMapping")
     cut_order_files: Optional[list[str]] = Field(None, alias="cutOrderFiles")
     extra_files: Optional[list[str]] = Field(None, alias="extraFiles")
     cog_shop_files_config: Optional[list[dict]] = Field(None, alias="cogShopFilesConfig")
-    start_time: Optional[datetime.date] = Field(None, alias="startTime")
-    end_time: Optional[datetime.date] = Field(None, alias="endTime")
+    start_time: Optional[datetime.datetime] = Field(None, alias="startTime")
+    end_time: Optional[datetime.datetime] = Field(None, alias="endTime")
 
     def _to_instances_write(
         self,
         cache: set[tuple[str, str]],
         view_by_read_class: dict[type[DomainModelCore], dm.ViewId] | None,
         write_none: bool = False,
         allow_version_increase: bool = False,
     ) -> ResourcesWrite:
         resources = ResourcesWrite()
         if self.as_tuple_id() in cache:
             return resources
 
-        write_view = (view_by_read_class or {}).get(Case, dm.ViewId("sp_powerops_models", "Case", "1"))
+        write_view = (view_by_read_class or {}).get(Case, dm.ViewId("sp_powerops_models_temp", "Case", "1"))
 
         properties: dict[str, Any] = {}
 
         if self.scenario is not None:
             properties["scenario"] = {
                 "space": self.space if isinstance(self.scenario, str) else self.scenario.space,
                 "externalId": self.scenario if isinstance(self.scenario, str) else self.scenario.external_id,
@@ -268,18 +268,18 @@
         if self.extra_files is not None or write_none:
             properties["extraFiles"] = self.extra_files
 
         if self.cog_shop_files_config is not None or write_none:
             properties["cogShopFilesConfig"] = self.cog_shop_files_config
 
         if self.start_time is not None or write_none:
-            properties["startTime"] = self.start_time.isoformat() if self.start_time else None
+            properties["startTime"] = self.start_time.isoformat(timespec="milliseconds") if self.start_time else None
 
         if self.end_time is not None or write_none:
-            properties["endTime"] = self.end_time.isoformat() if self.end_time else None
+            properties["endTime"] = self.end_time.isoformat(timespec="milliseconds") if self.end_time else None
 
         if properties:
             this_node = dm.NodeApply(
                 space=self.space,
                 external_id=self.external_id,
                 existing_version=None if allow_version_increase else self.data_record.existing_version,
                 type=self.node_type,
@@ -339,18 +339,18 @@
 
 class CaseApplyList(CaseWriteList): ...
 
 
 def _create_case_filter(
     view_id: dm.ViewId,
     scenario: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
-    min_start_time: datetime.date | None = None,
-    max_start_time: datetime.date | None = None,
-    min_end_time: datetime.date | None = None,
-    max_end_time: datetime.date | None = None,
+    min_start_time: datetime.datetime | None = None,
+    max_start_time: datetime.datetime | None = None,
+    min_end_time: datetime.datetime | None = None,
+    max_end_time: datetime.datetime | None = None,
     external_id_prefix: str | None = None,
     space: str | list[str] | None = None,
     filter: dm.Filter | None = None,
 ) -> dm.Filter | None:
     filters = []
     if scenario and isinstance(scenario, str):
         filters.append(
@@ -378,24 +378,24 @@
                 values=[{"space": item[0], "externalId": item[1]} for item in scenario],
             )
         )
     if min_start_time is not None or max_start_time is not None:
         filters.append(
             dm.filters.Range(
                 view_id.as_property_ref("startTime"),
-                gte=min_start_time.isoformat() if min_start_time else None,
-                lte=max_start_time.isoformat() if max_start_time else None,
+                gte=min_start_time.isoformat(timespec="milliseconds") if min_start_time else None,
+                lte=max_start_time.isoformat(timespec="milliseconds") if max_start_time else None,
             )
         )
     if min_end_time is not None or max_end_time is not None:
         filters.append(
             dm.filters.Range(
                 view_id.as_property_ref("endTime"),
-                gte=min_end_time.isoformat() if min_end_time else None,
-                lte=max_end_time.isoformat() if max_end_time else None,
+                gte=min_end_time.isoformat(timespec="milliseconds") if min_end_time else None,
+                lte=max_end_time.isoformat(timespec="milliseconds") if max_end_time else None,
             )
         )
     if external_id_prefix is not None:
         filters.append(dm.filters.Prefix(["node", "externalId"], value=external_id_prefix))
     if isinstance(space, str):
         filters.append(dm.filters.Equals(["node", "space"], value=space))
     if space and isinstance(space, list):
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_commands.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_commands.py`

 * *Files 10% similar despite different names*

```diff
@@ -30,36 +30,39 @@
     "CommandsWriteList",
     "CommandsApplyList",
     "CommandsFields",
     "CommandsTextFields",
 ]
 
 
-CommandsTextFields = Literal["commands"]
-CommandsFields = Literal["commands"]
+CommandsTextFields = Literal["name", "commands"]
+CommandsFields = Literal["name", "commands"]
 
 _COMMANDS_PROPERTIES_BY_FIELD = {
+    "name": "name",
     "commands": "commands",
 }
 
 
 class CommandsGraphQL(GraphQLCore):
     """This represents the reading version of command, used
     when data is retrieved from CDF using GraphQL.
 
     It is used when retrieving data from CDF using GraphQL.
 
     Args:
         space: The space where the node is located.
         external_id: The external id of the command.
         data_record: The data record of the command node.
+        name: Name for the Commands
         commands: The commands used in the shop model file
     """
 
-    view_id = dm.ViewId("sp_powerops_models", "Commands", "1")
+    view_id = dm.ViewId("sp_powerops_models_temp", "Commands", "1")
+    name: Optional[str] = None
     commands: Optional[list[str]] = None
 
     @model_validator(mode="before")
     def parse_data_record(cls, values: Any) -> Any:
         if not isinstance(values, dict):
             return values
         if "lastUpdatedTime" in values or "createdTime" in values:
@@ -77,49 +80,56 @@
             space=self.space,
             external_id=self.external_id,
             data_record=DataRecord(
                 version=0,
                 last_updated_time=self.data_record.last_updated_time,
                 created_time=self.data_record.created_time,
             ),
+            name=self.name,
             commands=self.commands,
         )
 
     def as_write(self) -> CommandsWrite:
         """Convert this GraphQL format of command to the writing format."""
         return CommandsWrite(
             space=self.space,
             external_id=self.external_id,
             data_record=DataRecordWrite(existing_version=0),
+            name=self.name,
             commands=self.commands,
         )
 
 
 class Commands(DomainModel):
     """This represents the reading version of command.
 
     It is used to when data is retrieved from CDF.
 
     Args:
         space: The space where the node is located.
         external_id: The external id of the command.
         data_record: The data record of the command node.
+        name: Name for the Commands
         commands: The commands used in the shop model file
     """
 
     space: str = DEFAULT_INSTANCE_SPACE
-    node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference("sp_powerops_types", "Commands")
+    node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference(
+        "sp_powerops_types_temp", "Commands"
+    )
+    name: str
     commands: Optional[list[str]] = None
 
     def as_write(self) -> CommandsWrite:
         """Convert this read version of command to the writing version."""
         return CommandsWrite(
             space=self.space,
             external_id=self.external_id,
             data_record=DataRecordWrite(existing_version=self.data_record.version),
+            name=self.name,
             commands=self.commands,
         )
 
     def as_apply(self) -> CommandsWrite:
         """Convert this read version of command to the writing version."""
         warnings.warn(
             "as_apply is deprecated and will be removed in v1.0. Use as_write instead.",
@@ -134,36 +144,43 @@
 
     It is used to when data is sent to CDF.
 
     Args:
         space: The space where the node is located.
         external_id: The external id of the command.
         data_record: The data record of the command node.
+        name: Name for the Commands
         commands: The commands used in the shop model file
     """
 
     space: str = DEFAULT_INSTANCE_SPACE
-    node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference("sp_powerops_types", "Commands")
+    node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference(
+        "sp_powerops_types_temp", "Commands"
+    )
+    name: str
     commands: list[str]
 
     def _to_instances_write(
         self,
         cache: set[tuple[str, str]],
         view_by_read_class: dict[type[DomainModelCore], dm.ViewId] | None,
         write_none: bool = False,
         allow_version_increase: bool = False,
     ) -> ResourcesWrite:
         resources = ResourcesWrite()
         if self.as_tuple_id() in cache:
             return resources
 
-        write_view = (view_by_read_class or {}).get(Commands, dm.ViewId("sp_powerops_models", "Commands", "1"))
+        write_view = (view_by_read_class or {}).get(Commands, dm.ViewId("sp_powerops_models_temp", "Commands", "1"))
 
         properties: dict[str, Any] = {}
 
+        if self.name is not None:
+            properties["name"] = self.name
+
         if self.commands is not None:
             properties["commands"] = self.commands
 
         if properties:
             this_node = dm.NodeApply(
                 space=self.space,
                 external_id=self.external_id,
@@ -220,19 +237,27 @@
 
 
 class CommandsApplyList(CommandsWriteList): ...
 
 
 def _create_command_filter(
     view_id: dm.ViewId,
+    name: str | list[str] | None = None,
+    name_prefix: str | None = None,
     external_id_prefix: str | None = None,
     space: str | list[str] | None = None,
     filter: dm.Filter | None = None,
 ) -> dm.Filter | None:
     filters = []
+    if isinstance(name, str):
+        filters.append(dm.filters.Equals(view_id.as_property_ref("name"), value=name))
+    if name and isinstance(name, list):
+        filters.append(dm.filters.In(view_id.as_property_ref("name"), values=name))
+    if name_prefix is not None:
+        filters.append(dm.filters.Prefix(view_id.as_property_ref("name"), value=name_prefix))
     if external_id_prefix is not None:
         filters.append(dm.filters.Prefix(["node", "externalId"], value=external_id_prefix))
     if isinstance(space, str):
         filters.append(dm.filters.Equals(["node", "space"], value=space))
     if space and isinstance(space, list):
         filters.append(dm.filters.In(["node", "space"], values=space))
     if filter:
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_core.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/day_ahead_bid/data_classes/_core.py`

 * *Files 2% similar despite different names*

```diff
@@ -40,15 +40,15 @@
         return_type=dict,
         when_used="unless-none",
     ),
     BeforeValidator(lambda v: CogniteTimeSeries.load(v) if isinstance(v, dict) else v),
 ]
 
 
-DEFAULT_INSTANCE_SPACE = "sp_powerops_instance"
+DEFAULT_INSTANCE_SPACE = "power-ops-instance"
 
 
 @dataclass
 class ResourcesWrite:
     nodes: dm.NodeApplyList = field(default_factory=lambda: dm.NodeApplyList([]))
     edges: dm.EdgeApplyList = field(default_factory=lambda: dm.EdgeApplyList([]))
     time_series: TimeSeriesList = field(default_factory=lambda: TimeSeriesList([]))
@@ -462,32 +462,37 @@
         )
 
 
 T_DomainRelation = TypeVar("T_DomainRelation", bound=DomainRelation)
 
 
 def default_edge_external_id_factory(
-    start_node: DomainModelWrite | str, end_node: DomainModelWrite | str, edge_type: dm.DirectRelationReference
+    start_node: DomainModelWrite | str | dm.NodeId,
+    end_node: DomainModelWrite | str | dm.NodeId,
+    edge_type: dm.DirectRelationReference,
 ) -> str:
     start = start_node if isinstance(start_node, str) else start_node.external_id
     end = end_node if isinstance(end_node, str) else end_node.external_id
     return f"{start}:{end}"
 
 
 class DomainRelationWrite(BaseModel, extra=Extra.forbid, populate_by_name=True):
     external_id_factory: ClassVar[
-        Callable[[Union[DomainModelWrite, str], Union[DomainModelWrite, str], dm.DirectRelationReference], str]
+        Callable[
+            [
+                Union[DomainModelWrite, str, dm.NodeId],
+                Union[DomainModelWrite, str, dm.NodeId],
+                dm.DirectRelationReference,
+            ],
+            str,
+        ]
     ] = default_edge_external_id_factory
     data_record: DataRecordWrite = Field(default_factory=DataRecordWrite)
     external_id: Optional[str] = Field(None, min_length=1, max_length=255)
 
-    @property
-    def data_records(self) -> DataRecordWriteList:
-        return DataRecordWriteList([node.data_record for node in self])
-
     @abstractmethod
     def _to_instances_write(
         self,
         cache: set[tuple[str, str]],
         start_node: DomainModelWrite,
         edge_type: dm.DirectRelationReference,
         view_by_read_class: dict[type[DomainModelCore], dm.ViewId] | None,
@@ -569,24 +574,32 @@
                 write_none,
                 allow_version_increase,
             )
             resources.extend(other_resources)
 
         return resources
 
+    @classmethod
+    def reset_external_id_factory(cls) -> None:
+        cls.external_id_factory = default_edge_external_id_factory
+
 
 T_DomainRelationWrite = TypeVar("T_DomainRelationWrite", bound=DomainRelationWrite)
 
 
 class DomainRelationList(CoreList[T_DomainRelation]):
     _PARENT_CLASS = DomainRelation
 
     def as_edge_ids(self) -> list[dm.EdgeId]:
         return [edge.as_id() for edge in self]
 
+    @property
+    def data_records(self) -> DataRecordWriteList:
+        return DataRecordWriteList([connection.data_record for connection in self])
+
 
 T_DomainRelationList = TypeVar("T_DomainRelationList", bound=DomainRelationList)
 
 
 def unpack_properties(properties: Properties) -> Mapping[str, PropertyValue]:
     unpacked: dict[str, PropertyValue] = {}
     for view_properties in properties.values():
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_custom_bid_matrix.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_custom_bid_matrix.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_generator.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_total_bid_matrix_calculation_output.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,14 +1,13 @@
 from __future__ import annotations
 
 import warnings
 from typing import TYPE_CHECKING, Any, Literal, Optional, Union
 
 from cognite.client import data_modeling as dm
-from cognite.client.data_classes import TimeSeries as CogniteTimeSeries
 from pydantic import Field
 from pydantic import field_validator, model_validator
 
 from ._core import (
     DEFAULT_INSTANCE_SPACE,
     DataRecord,
     DataRecordGraphQL,
@@ -17,337 +16,257 @@
     DomainModelCore,
     DomainModelWrite,
     DomainModelWriteList,
     DomainModelList,
     DomainRelationWrite,
     GraphQLCore,
     ResourcesWrite,
-    TimeSeries,
 )
+from ._function_output import FunctionOutput, FunctionOutputWrite
 
 if TYPE_CHECKING:
-    from ._generator_efficiency_curve import (
-        GeneratorEfficiencyCurve,
-        GeneratorEfficiencyCurveGraphQL,
-        GeneratorEfficiencyCurveWrite,
-    )
-    from ._turbine_efficiency_curve import (
-        TurbineEfficiencyCurve,
-        TurbineEfficiencyCurveGraphQL,
-        TurbineEfficiencyCurveWrite,
+    from ._alert import Alert, AlertGraphQL, AlertWrite
+    from ._bid_document_day_ahead import BidDocumentDayAhead, BidDocumentDayAheadGraphQL, BidDocumentDayAheadWrite
+    from ._total_bid_matrix_calculation_input import (
+        TotalBidMatrixCalculationInput,
+        TotalBidMatrixCalculationInputGraphQL,
+        TotalBidMatrixCalculationInputWrite,
     )
 
 
 __all__ = [
-    "Generator",
-    "GeneratorWrite",
-    "GeneratorApply",
-    "GeneratorList",
-    "GeneratorWriteList",
-    "GeneratorApplyList",
-    "GeneratorFields",
-    "GeneratorTextFields",
+    "TotalBidMatrixCalculationOutput",
+    "TotalBidMatrixCalculationOutputWrite",
+    "TotalBidMatrixCalculationOutputApply",
+    "TotalBidMatrixCalculationOutputList",
+    "TotalBidMatrixCalculationOutputWriteList",
+    "TotalBidMatrixCalculationOutputApplyList",
+    "TotalBidMatrixCalculationOutputFields",
+    "TotalBidMatrixCalculationOutputTextFields",
 ]
 
 
-GeneratorTextFields = Literal["name", "display_name", "start_stop_cost", "is_available_time_series"]
-GeneratorFields = Literal[
-    "name", "display_name", "ordering", "p_min", "penstock", "start_cost", "start_stop_cost", "is_available_time_series"
-]
+TotalBidMatrixCalculationOutputTextFields = Literal["process_id", "function_name", "function_call_id"]
+TotalBidMatrixCalculationOutputFields = Literal["process_id", "process_step", "function_name", "function_call_id"]
 
-_GENERATOR_PROPERTIES_BY_FIELD = {
-    "name": "name",
-    "display_name": "displayName",
-    "ordering": "ordering",
-    "p_min": "pMin",
-    "penstock": "penstock",
-    "start_cost": "startCost",
-    "start_stop_cost": "startStopCost",
-    "is_available_time_series": "isAvailableTimeSeries",
+_TOTALBIDMATRIXCALCULATIONOUTPUT_PROPERTIES_BY_FIELD = {
+    "process_id": "processId",
+    "process_step": "processStep",
+    "function_name": "functionName",
+    "function_call_id": "functionCallId",
 }
 
 
-class GeneratorGraphQL(GraphQLCore):
-    """This represents the reading version of generator, used
+class TotalBidMatrixCalculationOutputGraphQL(GraphQLCore):
+    """This represents the reading version of total bid matrix calculation output, used
     when data is retrieved from CDF using GraphQL.
 
     It is used when retrieving data from CDF using GraphQL.
 
     Args:
         space: The space where the node is located.
-        external_id: The external id of the generator.
-        data_record: The data record of the generator node.
-        name: Name for the Asset
-        display_name: Display name for the Asset.
-        ordering: The ordering of the asset
-        p_min: The p min field.
-        penstock: The penstock field.
-        start_cost: The start cost field.
-        start_stop_cost: The start stop cost field.
-        is_available_time_series: The is available time series field.
-        efficiency_curve: The efficiency curve field.
-        turbine_curves: The watercourses that are connected to the PriceArea.
+        external_id: The external id of the total bid matrix calculation output.
+        data_record: The data record of the total bid matrix calculation output node.
+        process_id: The process associated with the function execution
+        process_step: This is the step in the process.
+        function_name: The name of the function
+        function_call_id: The function call id
+        alerts: An array of calculation level Alerts.
+        bid_document: The bid document field.
+        input_: The previous step in the process.
     """
 
-    view_id = dm.ViewId("sp_powerops_models", "Generator", "1")
-    name: Optional[str] = None
-    display_name: Optional[str] = Field(None, alias="displayName")
-    ordering: Optional[int] = None
-    p_min: Optional[float] = Field(None, alias="pMin")
-    penstock: Optional[int] = None
-    start_cost: Optional[float] = Field(None, alias="startCost")
-    start_stop_cost: Union[TimeSeries, str, None] = Field(None, alias="startStopCost")
-    is_available_time_series: Union[TimeSeries, str, None] = Field(None, alias="isAvailableTimeSeries")
-    efficiency_curve: Optional[GeneratorEfficiencyCurveGraphQL] = Field(None, repr=False, alias="efficiencyCurve")
-    turbine_curves: Optional[list[TurbineEfficiencyCurveGraphQL]] = Field(
-        default=None, repr=False, alias="turbineCurves"
-    )
+    view_id = dm.ViewId("sp_powerops_models_temp", "TotalBidMatrixCalculationOutput", "1")
+    process_id: Optional[str] = Field(None, alias="processId")
+    process_step: Optional[int] = Field(None, alias="processStep")
+    function_name: Optional[str] = Field(None, alias="functionName")
+    function_call_id: Optional[str] = Field(None, alias="functionCallId")
+    alerts: Optional[list[AlertGraphQL]] = Field(default=None, repr=False)
+    bid_document: Optional[BidDocumentDayAheadGraphQL] = Field(None, repr=False, alias="bidDocument")
+    input_: Optional[TotalBidMatrixCalculationInputGraphQL] = Field(None, repr=False, alias="input")
 
     @model_validator(mode="before")
     def parse_data_record(cls, values: Any) -> Any:
         if not isinstance(values, dict):
             return values
         if "lastUpdatedTime" in values or "createdTime" in values:
             values["dataRecord"] = DataRecordGraphQL(
                 created_time=values.pop("createdTime", None),
                 last_updated_time=values.pop("lastUpdatedTime", None),
             )
         return values
 
-    @field_validator("efficiency_curve", "turbine_curves", mode="before")
+    @field_validator("alerts", "bid_document", "input_", mode="before")
     def parse_graphql(cls, value: Any) -> Any:
         if not isinstance(value, dict):
             return value
         if "items" in value:
             return value["items"]
         return value
 
-    def as_read(self) -> Generator:
-        """Convert this GraphQL format of generator to the reading format."""
+    def as_read(self) -> TotalBidMatrixCalculationOutput:
+        """Convert this GraphQL format of total bid matrix calculation output to the reading format."""
         if self.data_record is None:
             raise ValueError("This object cannot be converted to a read format because it lacks a data record.")
-        return Generator(
+        return TotalBidMatrixCalculationOutput(
             space=self.space,
             external_id=self.external_id,
             data_record=DataRecord(
                 version=0,
                 last_updated_time=self.data_record.last_updated_time,
                 created_time=self.data_record.created_time,
             ),
-            name=self.name,
-            display_name=self.display_name,
-            ordering=self.ordering,
-            p_min=self.p_min,
-            penstock=self.penstock,
-            start_cost=self.start_cost,
-            start_stop_cost=self.start_stop_cost,
-            is_available_time_series=self.is_available_time_series,
-            efficiency_curve=(
-                self.efficiency_curve.as_read()
-                if isinstance(self.efficiency_curve, GraphQLCore)
-                else self.efficiency_curve
+            process_id=self.process_id,
+            process_step=self.process_step,
+            function_name=self.function_name,
+            function_call_id=self.function_call_id,
+            alerts=[alert.as_read() if isinstance(alert, GraphQLCore) else alert for alert in self.alerts or []],
+            bid_document=(
+                self.bid_document.as_read() if isinstance(self.bid_document, GraphQLCore) else self.bid_document
             ),
-            turbine_curves=[
-                turbine_curve.as_read() if isinstance(turbine_curve, GraphQLCore) else turbine_curve
-                for turbine_curve in self.turbine_curves or []
-            ],
+            input_=self.input_.as_read() if isinstance(self.input_, GraphQLCore) else self.input_,
         )
 
-    def as_write(self) -> GeneratorWrite:
-        """Convert this GraphQL format of generator to the writing format."""
-        return GeneratorWrite(
+    def as_write(self) -> TotalBidMatrixCalculationOutputWrite:
+        """Convert this GraphQL format of total bid matrix calculation output to the writing format."""
+        return TotalBidMatrixCalculationOutputWrite(
             space=self.space,
             external_id=self.external_id,
             data_record=DataRecordWrite(existing_version=0),
-            name=self.name,
-            display_name=self.display_name,
-            ordering=self.ordering,
-            p_min=self.p_min,
-            penstock=self.penstock,
-            start_cost=self.start_cost,
-            start_stop_cost=self.start_stop_cost,
-            is_available_time_series=self.is_available_time_series,
-            efficiency_curve=(
-                self.efficiency_curve.as_write()
-                if isinstance(self.efficiency_curve, DomainModel)
-                else self.efficiency_curve
+            process_id=self.process_id,
+            process_step=self.process_step,
+            function_name=self.function_name,
+            function_call_id=self.function_call_id,
+            alerts=[alert.as_write() if isinstance(alert, DomainModel) else alert for alert in self.alerts or []],
+            bid_document=(
+                self.bid_document.as_write() if isinstance(self.bid_document, DomainModel) else self.bid_document
             ),
-            turbine_curves=[
-                turbine_curve.as_write() if isinstance(turbine_curve, DomainModel) else turbine_curve
-                for turbine_curve in self.turbine_curves or []
-            ],
+            input_=self.input_.as_write() if isinstance(self.input_, DomainModel) else self.input_,
         )
 
 
-class Generator(DomainModel):
-    """This represents the reading version of generator.
+class TotalBidMatrixCalculationOutput(FunctionOutput):
+    """This represents the reading version of total bid matrix calculation output.
 
     It is used to when data is retrieved from CDF.
 
     Args:
         space: The space where the node is located.
-        external_id: The external id of the generator.
-        data_record: The data record of the generator node.
-        name: Name for the Asset
-        display_name: Display name for the Asset.
-        ordering: The ordering of the asset
-        p_min: The p min field.
-        penstock: The penstock field.
-        start_cost: The start cost field.
-        start_stop_cost: The start stop cost field.
-        is_available_time_series: The is available time series field.
-        efficiency_curve: The efficiency curve field.
-        turbine_curves: The watercourses that are connected to the PriceArea.
+        external_id: The external id of the total bid matrix calculation output.
+        data_record: The data record of the total bid matrix calculation output node.
+        process_id: The process associated with the function execution
+        process_step: This is the step in the process.
+        function_name: The name of the function
+        function_call_id: The function call id
+        alerts: An array of calculation level Alerts.
+        bid_document: The bid document field.
+        input_: The previous step in the process.
     """
 
-    space: str = DEFAULT_INSTANCE_SPACE
-    node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference("sp_powerops_types", "Generator")
-    name: str
-    display_name: Optional[str] = Field(None, alias="displayName")
-    ordering: Optional[int] = None
-    p_min: Optional[float] = Field(None, alias="pMin")
-    penstock: Optional[int] = None
-    start_cost: Optional[float] = Field(None, alias="startCost")
-    start_stop_cost: Union[TimeSeries, str, None] = Field(None, alias="startStopCost")
-    is_available_time_series: Union[TimeSeries, str, None] = Field(None, alias="isAvailableTimeSeries")
-    efficiency_curve: Union[GeneratorEfficiencyCurve, str, dm.NodeId, None] = Field(
-        None, repr=False, alias="efficiencyCurve"
-    )
-    turbine_curves: Union[list[TurbineEfficiencyCurve], list[str], list[dm.NodeId], None] = Field(
-        default=None, repr=False, alias="turbineCurves"
+    node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference(
+        "sp_powerops_types_temp", "TotalBidMatrixCalculationOutput"
     )
+    bid_document: Union[BidDocumentDayAhead, str, dm.NodeId, None] = Field(None, repr=False, alias="bidDocument")
+    input_: Union[TotalBidMatrixCalculationInput, str, dm.NodeId, None] = Field(None, repr=False, alias="input")
 
-    def as_write(self) -> GeneratorWrite:
-        """Convert this read version of generator to the writing version."""
-        return GeneratorWrite(
+    def as_write(self) -> TotalBidMatrixCalculationOutputWrite:
+        """Convert this read version of total bid matrix calculation output to the writing version."""
+        return TotalBidMatrixCalculationOutputWrite(
             space=self.space,
             external_id=self.external_id,
             data_record=DataRecordWrite(existing_version=self.data_record.version),
-            name=self.name,
-            display_name=self.display_name,
-            ordering=self.ordering,
-            p_min=self.p_min,
-            penstock=self.penstock,
-            start_cost=self.start_cost,
-            start_stop_cost=self.start_stop_cost,
-            is_available_time_series=self.is_available_time_series,
-            efficiency_curve=(
-                self.efficiency_curve.as_write()
-                if isinstance(self.efficiency_curve, DomainModel)
-                else self.efficiency_curve
+            process_id=self.process_id,
+            process_step=self.process_step,
+            function_name=self.function_name,
+            function_call_id=self.function_call_id,
+            alerts=[alert.as_write() if isinstance(alert, DomainModel) else alert for alert in self.alerts or []],
+            bid_document=(
+                self.bid_document.as_write() if isinstance(self.bid_document, DomainModel) else self.bid_document
             ),
-            turbine_curves=[
-                turbine_curve.as_write() if isinstance(turbine_curve, DomainModel) else turbine_curve
-                for turbine_curve in self.turbine_curves or []
-            ],
+            input_=self.input_.as_write() if isinstance(self.input_, DomainModel) else self.input_,
         )
 
-    def as_apply(self) -> GeneratorWrite:
-        """Convert this read version of generator to the writing version."""
+    def as_apply(self) -> TotalBidMatrixCalculationOutputWrite:
+        """Convert this read version of total bid matrix calculation output to the writing version."""
         warnings.warn(
             "as_apply is deprecated and will be removed in v1.0. Use as_write instead.",
             UserWarning,
             stacklevel=2,
         )
         return self.as_write()
 
 
-class GeneratorWrite(DomainModelWrite):
-    """This represents the writing version of generator.
+class TotalBidMatrixCalculationOutputWrite(FunctionOutputWrite):
+    """This represents the writing version of total bid matrix calculation output.
 
     It is used to when data is sent to CDF.
 
     Args:
         space: The space where the node is located.
-        external_id: The external id of the generator.
-        data_record: The data record of the generator node.
-        name: Name for the Asset
-        display_name: Display name for the Asset.
-        ordering: The ordering of the asset
-        p_min: The p min field.
-        penstock: The penstock field.
-        start_cost: The start cost field.
-        start_stop_cost: The start stop cost field.
-        is_available_time_series: The is available time series field.
-        efficiency_curve: The efficiency curve field.
-        turbine_curves: The watercourses that are connected to the PriceArea.
+        external_id: The external id of the total bid matrix calculation output.
+        data_record: The data record of the total bid matrix calculation output node.
+        process_id: The process associated with the function execution
+        process_step: This is the step in the process.
+        function_name: The name of the function
+        function_call_id: The function call id
+        alerts: An array of calculation level Alerts.
+        bid_document: The bid document field.
+        input_: The previous step in the process.
     """
 
-    space: str = DEFAULT_INSTANCE_SPACE
-    node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference("sp_powerops_types", "Generator")
-    name: str
-    display_name: Optional[str] = Field(None, alias="displayName")
-    ordering: Optional[int] = None
-    p_min: Optional[float] = Field(None, alias="pMin")
-    penstock: Optional[int] = None
-    start_cost: Optional[float] = Field(None, alias="startCost")
-    start_stop_cost: Union[TimeSeries, str, None] = Field(None, alias="startStopCost")
-    is_available_time_series: Union[TimeSeries, str, None] = Field(None, alias="isAvailableTimeSeries")
-    efficiency_curve: Union[GeneratorEfficiencyCurveWrite, str, dm.NodeId, None] = Field(
-        None, repr=False, alias="efficiencyCurve"
-    )
-    turbine_curves: Union[list[TurbineEfficiencyCurveWrite], list[str], list[dm.NodeId], None] = Field(
-        default=None, repr=False, alias="turbineCurves"
+    node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference(
+        "sp_powerops_types_temp", "TotalBidMatrixCalculationOutput"
     )
+    bid_document: Union[BidDocumentDayAheadWrite, str, dm.NodeId, None] = Field(None, repr=False, alias="bidDocument")
+    input_: Union[TotalBidMatrixCalculationInputWrite, str, dm.NodeId, None] = Field(None, repr=False, alias="input")
 
     def _to_instances_write(
         self,
         cache: set[tuple[str, str]],
         view_by_read_class: dict[type[DomainModelCore], dm.ViewId] | None,
         write_none: bool = False,
         allow_version_increase: bool = False,
     ) -> ResourcesWrite:
         resources = ResourcesWrite()
         if self.as_tuple_id() in cache:
             return resources
 
-        write_view = (view_by_read_class or {}).get(Generator, dm.ViewId("sp_powerops_models", "Generator", "1"))
+        write_view = (view_by_read_class or {}).get(
+            TotalBidMatrixCalculationOutput,
+            dm.ViewId("sp_powerops_models_temp", "TotalBidMatrixCalculationOutput", "1"),
+        )
 
         properties: dict[str, Any] = {}
 
-        if self.name is not None:
-            properties["name"] = self.name
+        if self.process_id is not None:
+            properties["processId"] = self.process_id
+
+        if self.process_step is not None:
+            properties["processStep"] = self.process_step
 
-        if self.display_name is not None or write_none:
-            properties["displayName"] = self.display_name
+        if self.function_name is not None:
+            properties["functionName"] = self.function_name
 
-        if self.ordering is not None or write_none:
-            properties["ordering"] = self.ordering
+        if self.function_call_id is not None:
+            properties["functionCallId"] = self.function_call_id
 
-        if self.p_min is not None or write_none:
-            properties["pMin"] = self.p_min
-
-        if self.penstock is not None or write_none:
-            properties["penstock"] = self.penstock
-
-        if self.start_cost is not None or write_none:
-            properties["startCost"] = self.start_cost
-
-        if self.start_stop_cost is not None or write_none:
-            if isinstance(self.start_stop_cost, str) or self.start_stop_cost is None:
-                properties["startStopCost"] = self.start_stop_cost
-            else:
-                properties["startStopCost"] = self.start_stop_cost.external_id
-
-        if self.is_available_time_series is not None or write_none:
-            if isinstance(self.is_available_time_series, str) or self.is_available_time_series is None:
-                properties["isAvailableTimeSeries"] = self.is_available_time_series
-            else:
-                properties["isAvailableTimeSeries"] = self.is_available_time_series.external_id
-
-        if self.efficiency_curve is not None:
-            properties["efficiencyCurve"] = {
-                "space": self.space if isinstance(self.efficiency_curve, str) else self.efficiency_curve.space,
+        if self.bid_document is not None:
+            properties["bidDocument"] = {
+                "space": self.space if isinstance(self.bid_document, str) else self.bid_document.space,
                 "externalId": (
-                    self.efficiency_curve
-                    if isinstance(self.efficiency_curve, str)
-                    else self.efficiency_curve.external_id
+                    self.bid_document if isinstance(self.bid_document, str) else self.bid_document.external_id
                 ),
             }
 
+        if self.input_ is not None:
+            properties["input"] = {
+                "space": self.space if isinstance(self.input_, str) else self.input_.space,
+                "externalId": self.input_ if isinstance(self.input_, str) else self.input_.external_id,
+            }
+
         if properties:
             this_node = dm.NodeApply(
                 space=self.space,
                 external_id=self.external_id,
                 existing_version=None if allow_version_increase else self.data_record.existing_version,
                 type=self.node_type,
                 sources=[
@@ -356,146 +275,165 @@
                         properties=properties,
                     )
                 ],
             )
             resources.nodes.append(this_node)
             cache.add(self.as_tuple_id())
 
-        edge_type = dm.DirectRelationReference("sp_powerops_types", "isSubAssetOf")
-        for turbine_curve in self.turbine_curves or []:
+        edge_type = dm.DirectRelationReference("sp_powerops_types_temp", "calculationIssue")
+        for alert in self.alerts or []:
             other_resources = DomainRelationWrite.from_edge_to_resources(
                 cache,
                 start_node=self,
-                end_node=turbine_curve,
+                end_node=alert,
                 edge_type=edge_type,
                 view_by_read_class=view_by_read_class,
                 write_none=write_none,
                 allow_version_increase=allow_version_increase,
             )
             resources.extend(other_resources)
 
-        if isinstance(self.efficiency_curve, DomainModelWrite):
-            other_resources = self.efficiency_curve._to_instances_write(cache, view_by_read_class)
+        if isinstance(self.bid_document, DomainModelWrite):
+            other_resources = self.bid_document._to_instances_write(cache, view_by_read_class)
             resources.extend(other_resources)
 
-        if isinstance(self.start_stop_cost, CogniteTimeSeries):
-            resources.time_series.append(self.start_stop_cost)
-
-        if isinstance(self.is_available_time_series, CogniteTimeSeries):
-            resources.time_series.append(self.is_available_time_series)
+        if isinstance(self.input_, DomainModelWrite):
+            other_resources = self.input_._to_instances_write(cache, view_by_read_class)
+            resources.extend(other_resources)
 
         return resources
 
 
-class GeneratorApply(GeneratorWrite):
-    def __new__(cls, *args, **kwargs) -> GeneratorApply:
+class TotalBidMatrixCalculationOutputApply(TotalBidMatrixCalculationOutputWrite):
+    def __new__(cls, *args, **kwargs) -> TotalBidMatrixCalculationOutputApply:
         warnings.warn(
-            "GeneratorApply is deprecated and will be removed in v1.0. Use GeneratorWrite instead."
+            "TotalBidMatrixCalculationOutputApply is deprecated and will be removed in v1.0. Use TotalBidMatrixCalculationOutputWrite instead."
             "The motivation for this change is that Write is a more descriptive name for the writing version of the"
-            "Generator.",
+            "TotalBidMatrixCalculationOutput.",
             UserWarning,
             stacklevel=2,
         )
         return super().__new__(cls)
 
 
-class GeneratorList(DomainModelList[Generator]):
-    """List of generators in the read version."""
+class TotalBidMatrixCalculationOutputList(DomainModelList[TotalBidMatrixCalculationOutput]):
+    """List of total bid matrix calculation outputs in the read version."""
 
-    _INSTANCE = Generator
+    _INSTANCE = TotalBidMatrixCalculationOutput
 
-    def as_write(self) -> GeneratorWriteList:
-        """Convert these read versions of generator to the writing versions."""
-        return GeneratorWriteList([node.as_write() for node in self.data])
+    def as_write(self) -> TotalBidMatrixCalculationOutputWriteList:
+        """Convert these read versions of total bid matrix calculation output to the writing versions."""
+        return TotalBidMatrixCalculationOutputWriteList([node.as_write() for node in self.data])
 
-    def as_apply(self) -> GeneratorWriteList:
+    def as_apply(self) -> TotalBidMatrixCalculationOutputWriteList:
         """Convert these read versions of primitive nullable to the writing versions."""
         warnings.warn(
             "as_apply is deprecated and will be removed in v1.0. Use as_write instead.",
             UserWarning,
             stacklevel=2,
         )
         return self.as_write()
 
 
-class GeneratorWriteList(DomainModelWriteList[GeneratorWrite]):
-    """List of generators in the writing version."""
+class TotalBidMatrixCalculationOutputWriteList(DomainModelWriteList[TotalBidMatrixCalculationOutputWrite]):
+    """List of total bid matrix calculation outputs in the writing version."""
 
-    _INSTANCE = GeneratorWrite
+    _INSTANCE = TotalBidMatrixCalculationOutputWrite
 
 
-class GeneratorApplyList(GeneratorWriteList): ...
+class TotalBidMatrixCalculationOutputApplyList(TotalBidMatrixCalculationOutputWriteList): ...
 
 
-def _create_generator_filter(
+def _create_total_bid_matrix_calculation_output_filter(
     view_id: dm.ViewId,
-    name: str | list[str] | None = None,
-    name_prefix: str | None = None,
-    display_name: str | list[str] | None = None,
-    display_name_prefix: str | None = None,
-    min_ordering: int | None = None,
-    max_ordering: int | None = None,
-    min_p_min: float | None = None,
-    max_p_min: float | None = None,
-    min_penstock: int | None = None,
-    max_penstock: int | None = None,
-    min_start_cost: float | None = None,
-    max_start_cost: float | None = None,
-    efficiency_curve: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+    process_id: str | list[str] | None = None,
+    process_id_prefix: str | None = None,
+    min_process_step: int | None = None,
+    max_process_step: int | None = None,
+    function_name: str | list[str] | None = None,
+    function_name_prefix: str | None = None,
+    function_call_id: str | list[str] | None = None,
+    function_call_id_prefix: str | None = None,
+    bid_document: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+    input_: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
     external_id_prefix: str | None = None,
     space: str | list[str] | None = None,
     filter: dm.Filter | None = None,
 ) -> dm.Filter | None:
     filters = []
-    if isinstance(name, str):
-        filters.append(dm.filters.Equals(view_id.as_property_ref("name"), value=name))
-    if name and isinstance(name, list):
-        filters.append(dm.filters.In(view_id.as_property_ref("name"), values=name))
-    if name_prefix is not None:
-        filters.append(dm.filters.Prefix(view_id.as_property_ref("name"), value=name_prefix))
-    if isinstance(display_name, str):
-        filters.append(dm.filters.Equals(view_id.as_property_ref("displayName"), value=display_name))
-    if display_name and isinstance(display_name, list):
-        filters.append(dm.filters.In(view_id.as_property_ref("displayName"), values=display_name))
-    if display_name_prefix is not None:
-        filters.append(dm.filters.Prefix(view_id.as_property_ref("displayName"), value=display_name_prefix))
-    if min_ordering is not None or max_ordering is not None:
-        filters.append(dm.filters.Range(view_id.as_property_ref("ordering"), gte=min_ordering, lte=max_ordering))
-    if min_p_min is not None or max_p_min is not None:
-        filters.append(dm.filters.Range(view_id.as_property_ref("pMin"), gte=min_p_min, lte=max_p_min))
-    if min_penstock is not None or max_penstock is not None:
-        filters.append(dm.filters.Range(view_id.as_property_ref("penstock"), gte=min_penstock, lte=max_penstock))
-    if min_start_cost is not None or max_start_cost is not None:
-        filters.append(dm.filters.Range(view_id.as_property_ref("startCost"), gte=min_start_cost, lte=max_start_cost))
-    if efficiency_curve and isinstance(efficiency_curve, str):
+    if isinstance(process_id, str):
+        filters.append(dm.filters.Equals(view_id.as_property_ref("processId"), value=process_id))
+    if process_id and isinstance(process_id, list):
+        filters.append(dm.filters.In(view_id.as_property_ref("processId"), values=process_id))
+    if process_id_prefix is not None:
+        filters.append(dm.filters.Prefix(view_id.as_property_ref("processId"), value=process_id_prefix))
+    if min_process_step is not None or max_process_step is not None:
+        filters.append(
+            dm.filters.Range(view_id.as_property_ref("processStep"), gte=min_process_step, lte=max_process_step)
+        )
+    if isinstance(function_name, str):
+        filters.append(dm.filters.Equals(view_id.as_property_ref("functionName"), value=function_name))
+    if function_name and isinstance(function_name, list):
+        filters.append(dm.filters.In(view_id.as_property_ref("functionName"), values=function_name))
+    if function_name_prefix is not None:
+        filters.append(dm.filters.Prefix(view_id.as_property_ref("functionName"), value=function_name_prefix))
+    if isinstance(function_call_id, str):
+        filters.append(dm.filters.Equals(view_id.as_property_ref("functionCallId"), value=function_call_id))
+    if function_call_id and isinstance(function_call_id, list):
+        filters.append(dm.filters.In(view_id.as_property_ref("functionCallId"), values=function_call_id))
+    if function_call_id_prefix is not None:
+        filters.append(dm.filters.Prefix(view_id.as_property_ref("functionCallId"), value=function_call_id_prefix))
+    if bid_document and isinstance(bid_document, str):
         filters.append(
             dm.filters.Equals(
-                view_id.as_property_ref("efficiencyCurve"),
-                value={"space": DEFAULT_INSTANCE_SPACE, "externalId": efficiency_curve},
+                view_id.as_property_ref("bidDocument"),
+                value={"space": DEFAULT_INSTANCE_SPACE, "externalId": bid_document},
             )
         )
-    if efficiency_curve and isinstance(efficiency_curve, tuple):
+    if bid_document and isinstance(bid_document, tuple):
         filters.append(
             dm.filters.Equals(
-                view_id.as_property_ref("efficiencyCurve"),
-                value={"space": efficiency_curve[0], "externalId": efficiency_curve[1]},
+                view_id.as_property_ref("bidDocument"), value={"space": bid_document[0], "externalId": bid_document[1]}
             )
         )
-    if efficiency_curve and isinstance(efficiency_curve, list) and isinstance(efficiency_curve[0], str):
+    if bid_document and isinstance(bid_document, list) and isinstance(bid_document[0], str):
+        filters.append(
+            dm.filters.In(
+                view_id.as_property_ref("bidDocument"),
+                values=[{"space": DEFAULT_INSTANCE_SPACE, "externalId": item} for item in bid_document],
+            )
+        )
+    if bid_document and isinstance(bid_document, list) and isinstance(bid_document[0], tuple):
+        filters.append(
+            dm.filters.In(
+                view_id.as_property_ref("bidDocument"),
+                values=[{"space": item[0], "externalId": item[1]} for item in bid_document],
+            )
+        )
+    if input_ and isinstance(input_, str):
+        filters.append(
+            dm.filters.Equals(
+                view_id.as_property_ref("input"), value={"space": DEFAULT_INSTANCE_SPACE, "externalId": input_}
+            )
+        )
+    if input_ and isinstance(input_, tuple):
+        filters.append(
+            dm.filters.Equals(view_id.as_property_ref("input"), value={"space": input_[0], "externalId": input_[1]})
+        )
+    if input_ and isinstance(input_, list) and isinstance(input_[0], str):
         filters.append(
             dm.filters.In(
-                view_id.as_property_ref("efficiencyCurve"),
-                values=[{"space": DEFAULT_INSTANCE_SPACE, "externalId": item} for item in efficiency_curve],
+                view_id.as_property_ref("input"),
+                values=[{"space": DEFAULT_INSTANCE_SPACE, "externalId": item} for item in input_],
             )
         )
-    if efficiency_curve and isinstance(efficiency_curve, list) and isinstance(efficiency_curve[0], tuple):
+    if input_ and isinstance(input_, list) and isinstance(input_[0], tuple):
         filters.append(
             dm.filters.In(
-                view_id.as_property_ref("efficiencyCurve"),
-                values=[{"space": item[0], "externalId": item[1]} for item in efficiency_curve],
+                view_id.as_property_ref("input"), values=[{"space": item[0], "externalId": item[1]} for item in input_]
             )
         )
     if external_id_prefix is not None:
         filters.append(dm.filters.Prefix(["node", "externalId"], value=external_id_prefix))
     if isinstance(space, str):
         filters.append(dm.filters.Equals(["node", "space"], value=space))
     if space and isinstance(space, list):
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_generator_efficiency_curve.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_generator_efficiency_curve.py`

 * *Files 0% similar despite different names*

```diff
@@ -52,15 +52,15 @@
         external_id: The external id of the generator efficiency curve.
         data_record: The data record of the generator efficiency curve node.
         ref: The reference value
         power: The generator power values
         efficiency: The generator efficiency values
     """
 
-    view_id = dm.ViewId("sp_powerops_models", "GeneratorEfficiencyCurve", "1")
+    view_id = dm.ViewId("sp_powerops_models_temp", "GeneratorEfficiencyCurve", "1")
     ref: Optional[float] = None
     power: Optional[list[float]] = None
     efficiency: Optional[list[float]] = None
 
     @model_validator(mode="before")
     def parse_data_record(cls, values: Any) -> Any:
         if not isinstance(values, dict):
@@ -113,15 +113,15 @@
         ref: The reference value
         power: The generator power values
         efficiency: The generator efficiency values
     """
 
     space: str = DEFAULT_INSTANCE_SPACE
     node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference(
-        "sp_powerops_types", "GeneratorEfficiencyCurve"
+        "sp_powerops_types_temp", "GeneratorEfficiencyCurve"
     )
     ref: Optional[float] = None
     power: Optional[list[float]] = None
     efficiency: Optional[list[float]] = None
 
     def as_write(self) -> GeneratorEfficiencyCurveWrite:
         """Convert this read version of generator efficiency curve to the writing version."""
@@ -156,15 +156,15 @@
         ref: The reference value
         power: The generator power values
         efficiency: The generator efficiency values
     """
 
     space: str = DEFAULT_INSTANCE_SPACE
     node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference(
-        "sp_powerops_types", "GeneratorEfficiencyCurve"
+        "sp_powerops_types_temp", "GeneratorEfficiencyCurve"
     )
     ref: Optional[float] = None
     power: list[float]
     efficiency: list[float]
 
     def _to_instances_write(
         self,
@@ -174,15 +174,15 @@
         allow_version_increase: bool = False,
     ) -> ResourcesWrite:
         resources = ResourcesWrite()
         if self.as_tuple_id() in cache:
             return resources
 
         write_view = (view_by_read_class or {}).get(
-            GeneratorEfficiencyCurve, dm.ViewId("sp_powerops_models", "GeneratorEfficiencyCurve", "1")
+            GeneratorEfficiencyCurve, dm.ViewId("sp_powerops_models_temp", "GeneratorEfficiencyCurve", "1")
         )
 
         properties: dict[str, Any] = {}
 
         if self.ref is not None or write_none:
             properties["ref"] = self.ref
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_mapping.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_mapping.py`

 * *Files 1% similar despite different names*

```diff
@@ -62,15 +62,15 @@
         shop_path: The key in shop file to map to
         timeseries: The time series to map to
         transformations: The transformations to apply to the time series
         retrieve: How to retrieve time series data
         aggregation: How to aggregate time series data
     """
 
-    view_id = dm.ViewId("sp_powerops_models", "Mapping", "1")
+    view_id = dm.ViewId("sp_powerops_models_temp", "Mapping", "1")
     shop_path: Optional[str] = Field(None, alias="shopPath")
     timeseries: Union[TimeSeries, str, None] = None
     transformations: Optional[list[dict]] = None
     retrieve: Optional[str] = None
     aggregation: Optional[str] = None
 
     @model_validator(mode="before")
@@ -130,15 +130,15 @@
         timeseries: The time series to map to
         transformations: The transformations to apply to the time series
         retrieve: How to retrieve time series data
         aggregation: How to aggregate time series data
     """
 
     space: str = DEFAULT_INSTANCE_SPACE
-    node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference("sp_powerops_types", "Mapping")
+    node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference("sp_powerops_types_temp", "Mapping")
     shop_path: str = Field(alias="shopPath")
     timeseries: Union[TimeSeries, str, None] = None
     transformations: Optional[list[dict]] = None
     retrieve: Optional[str] = None
     aggregation: Optional[str] = None
 
     def as_write(self) -> MappingWrite:
@@ -177,15 +177,15 @@
         timeseries: The time series to map to
         transformations: The transformations to apply to the time series
         retrieve: How to retrieve time series data
         aggregation: How to aggregate time series data
     """
 
     space: str = DEFAULT_INSTANCE_SPACE
-    node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference("sp_powerops_types", "Mapping")
+    node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference("sp_powerops_types_temp", "Mapping")
     shop_path: str = Field(alias="shopPath")
     timeseries: Union[TimeSeries, str, None] = None
     transformations: Optional[list[dict]] = None
     retrieve: Optional[str] = None
     aggregation: Optional[str] = None
 
     def _to_instances_write(
@@ -195,15 +195,15 @@
         write_none: bool = False,
         allow_version_increase: bool = False,
     ) -> ResourcesWrite:
         resources = ResourcesWrite()
         if self.as_tuple_id() in cache:
             return resources
 
-        write_view = (view_by_read_class or {}).get(Mapping, dm.ViewId("sp_powerops_models", "Mapping", "1"))
+        write_view = (view_by_read_class or {}).get(Mapping, dm.ViewId("sp_powerops_models_temp", "Mapping", "1"))
 
         properties: dict[str, Any] = {}
 
         if self.shop_path is not None:
             properties["shopPath"] = self.shop_path
 
         if self.timeseries is not None or write_none:
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_market_configuration.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_market_configuration.py`

 * *Files 0% similar despite different names*

```diff
@@ -70,15 +70,15 @@
         price_unit: Unit of measurement for the price ('EUR/MWh')
         price_steps: The maximum number of price steps
         tick_size: 'Granularity' of the price; tick size = 0.1 means that prices must be 'rounded to nearest 0.1' (i. e. 66.43 is not allowed, but 66.4 is)
         time_unit: The time unit ('1h')
         trade_lot: 'Granularity' of the volumes; trade lot = 0.2 means that volumes must be 'rounded to nearest 0.2' (i. e. 66.5 is not allowed, but 66.4 is)
     """
 
-    view_id = dm.ViewId("sp_powerops_models", "MarketConfiguration", "1")
+    view_id = dm.ViewId("sp_powerops_models_temp", "MarketConfiguration", "1")
     name: Optional[str] = None
     max_price: Optional[float] = Field(None, alias="maxPrice")
     min_price: Optional[float] = Field(None, alias="minPrice")
     time_zone: Optional[str] = Field(None, alias="timeZone")
     price_unit: Optional[str] = Field(None, alias="priceUnit")
     price_steps: Optional[int] = Field(None, alias="priceSteps")
     tick_size: Optional[float] = Field(None, alias="tickSize")
@@ -155,15 +155,15 @@
         tick_size: 'Granularity' of the price; tick size = 0.1 means that prices must be 'rounded to nearest 0.1' (i. e. 66.43 is not allowed, but 66.4 is)
         time_unit: The time unit ('1h')
         trade_lot: 'Granularity' of the volumes; trade lot = 0.2 means that volumes must be 'rounded to nearest 0.2' (i. e. 66.5 is not allowed, but 66.4 is)
     """
 
     space: str = DEFAULT_INSTANCE_SPACE
     node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference(
-        "sp_powerops_types", "MarketConfiguration"
+        "sp_powerops_types_temp", "MarketConfiguration"
     )
     name: Optional[str] = None
     max_price: float = Field(alias="maxPrice")
     min_price: float = Field(alias="minPrice")
     time_zone: str = Field(alias="timeZone")
     price_unit: str = Field(alias="priceUnit")
     price_steps: int = Field(alias="priceSteps")
@@ -216,15 +216,15 @@
         tick_size: 'Granularity' of the price; tick size = 0.1 means that prices must be 'rounded to nearest 0.1' (i. e. 66.43 is not allowed, but 66.4 is)
         time_unit: The time unit ('1h')
         trade_lot: 'Granularity' of the volumes; trade lot = 0.2 means that volumes must be 'rounded to nearest 0.2' (i. e. 66.5 is not allowed, but 66.4 is)
     """
 
     space: str = DEFAULT_INSTANCE_SPACE
     node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference(
-        "sp_powerops_types", "MarketConfiguration"
+        "sp_powerops_types_temp", "MarketConfiguration"
     )
     name: Optional[str] = None
     max_price: float = Field(alias="maxPrice")
     min_price: float = Field(alias="minPrice")
     time_zone: str = Field(alias="timeZone")
     price_unit: str = Field(alias="priceUnit")
     price_steps: int = Field(alias="priceSteps")
@@ -240,15 +240,15 @@
         allow_version_increase: bool = False,
     ) -> ResourcesWrite:
         resources = ResourcesWrite()
         if self.as_tuple_id() in cache:
             return resources
 
         write_view = (view_by_read_class or {}).get(
-            MarketConfiguration, dm.ViewId("sp_powerops_models", "MarketConfiguration", "1")
+            MarketConfiguration, dm.ViewId("sp_powerops_models_temp", "MarketConfiguration", "1")
         )
 
         properties: dict[str, Any] = {}
 
         if self.name is not None or write_none:
             properties["name"] = self.name
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_model_template.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_model_template.py`

 * *Files 14% similar despite different names*

```diff
@@ -20,35 +20,38 @@
     DomainRelationWrite,
     GraphQLCore,
     ResourcesWrite,
 )
 
 if TYPE_CHECKING:
     from ._mapping import Mapping, MappingGraphQL, MappingWrite
-    from ._watercourse_shop import WatercourseShop, WatercourseShopGraphQL, WatercourseShopWrite
 
 
 __all__ = [
     "ModelTemplate",
     "ModelTemplateWrite",
     "ModelTemplateApply",
     "ModelTemplateList",
     "ModelTemplateWriteList",
     "ModelTemplateApplyList",
     "ModelTemplateFields",
     "ModelTemplateTextFields",
 ]
 
 
-ModelTemplateTextFields = Literal["version_", "shop_version", "model", "extra_files"]
-ModelTemplateFields = Literal["version_", "shop_version", "model", "cog_shop_files_config", "extra_files"]
+ModelTemplateTextFields = Literal["name", "version_", "shop_version", "model", "extra_files"]
+ModelTemplateFields = Literal[
+    "name", "version_", "shop_version", "penalty_limit", "model", "cog_shop_files_config", "extra_files"
+]
 
 _MODELTEMPLATE_PROPERTIES_BY_FIELD = {
+    "name": "name",
     "version_": "version",
     "shop_version": "shopVersion",
+    "penalty_limit": "penaltyLimit",
     "model": "model",
     "cog_shop_files_config": "cogShopFilesConfig",
     "extra_files": "extraFiles",
 }
 
 
 class ModelTemplateGraphQL(GraphQLCore, protected_namespaces=()):
@@ -57,27 +60,29 @@
 
     It is used when retrieving data from CDF using GraphQL.
 
     Args:
         space: The space where the node is located.
         external_id: The external id of the model template.
         data_record: The data record of the model template node.
+        name: TODO
         version_: The version of the model
         shop_version: The version of SHOP to run
-        watercourse: The watercourse to run the model for
+        penalty_limit: TODO
         model: The shop model file to use as template before applying base mapping
         cog_shop_files_config: Configuration for in what order to load the various files into pyshop
         extra_files: Extra files related to a model template
         base_mappings: The base mappings for the model
     """
 
-    view_id = dm.ViewId("sp_powerops_models", "ModelTemplate", "1")
+    view_id = dm.ViewId("sp_powerops_models_temp", "ModelTemplate", "1")
+    name: Optional[str] = None
     version_: Optional[str] = Field(None, alias="version")
     shop_version: Optional[str] = Field(None, alias="shopVersion")
-    watercourse: Optional[WatercourseShopGraphQL] = Field(None, repr=False)
+    penalty_limit: Optional[float] = Field(None, alias="penaltyLimit")
     model: Union[str, None] = None
     cog_shop_files_config: Optional[list[dict]] = Field(None, alias="cogShopFilesConfig")
     extra_files: Optional[list[str]] = Field(None, alias="extraFiles")
     base_mappings: Optional[list[MappingGraphQL]] = Field(default=None, repr=False, alias="baseMappings")
 
     @model_validator(mode="before")
     def parse_data_record(cls, values: Any) -> Any:
@@ -86,15 +91,15 @@
         if "lastUpdatedTime" in values or "createdTime" in values:
             values["dataRecord"] = DataRecordGraphQL(
                 created_time=values.pop("createdTime", None),
                 last_updated_time=values.pop("lastUpdatedTime", None),
             )
         return values
 
-    @field_validator("watercourse", "base_mappings", mode="before")
+    @field_validator("base_mappings", mode="before")
     def parse_graphql(cls, value: Any) -> Any:
         if not isinstance(value, dict):
             return value
         if "items" in value:
             return value["items"]
         return value
 
@@ -106,17 +111,18 @@
             space=self.space,
             external_id=self.external_id,
             data_record=DataRecord(
                 version=0,
                 last_updated_time=self.data_record.last_updated_time,
                 created_time=self.data_record.created_time,
             ),
+            name=self.name,
             version_=self.version_,
             shop_version=self.shop_version,
-            watercourse=self.watercourse.as_read() if isinstance(self.watercourse, GraphQLCore) else self.watercourse,
+            penalty_limit=self.penalty_limit,
             model=self.model,
             cog_shop_files_config=self.cog_shop_files_config,
             extra_files=self.extra_files,
             base_mappings=[
                 base_mapping.as_read() if isinstance(base_mapping, GraphQLCore) else base_mapping
                 for base_mapping in self.base_mappings or []
             ],
@@ -124,17 +130,18 @@
 
     def as_write(self) -> ModelTemplateWrite:
         """Convert this GraphQL format of model template to the writing format."""
         return ModelTemplateWrite(
             space=self.space,
             external_id=self.external_id,
             data_record=DataRecordWrite(existing_version=0),
+            name=self.name,
             version_=self.version_,
             shop_version=self.shop_version,
-            watercourse=self.watercourse.as_write() if isinstance(self.watercourse, DomainModel) else self.watercourse,
+            penalty_limit=self.penalty_limit,
             model=self.model,
             cog_shop_files_config=self.cog_shop_files_config,
             extra_files=self.extra_files,
             base_mappings=[
                 base_mapping.as_write() if isinstance(base_mapping, DomainModel) else base_mapping
                 for base_mapping in self.base_mappings or []
             ],
@@ -146,46 +153,49 @@
 
     It is used to when data is retrieved from CDF.
 
     Args:
         space: The space where the node is located.
         external_id: The external id of the model template.
         data_record: The data record of the model template node.
+        name: TODO
         version_: The version of the model
         shop_version: The version of SHOP to run
-        watercourse: The watercourse to run the model for
+        penalty_limit: TODO
         model: The shop model file to use as template before applying base mapping
         cog_shop_files_config: Configuration for in what order to load the various files into pyshop
         extra_files: Extra files related to a model template
         base_mappings: The base mappings for the model
     """
 
     space: str = DEFAULT_INSTANCE_SPACE
     node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference(
-        "sp_powerops_types", "ModelTemplate"
+        "sp_powerops_types_temp", "ModelTemplate"
     )
+    name: str
     version_: Optional[str] = Field(None, alias="version")
     shop_version: str = Field(alias="shopVersion")
-    watercourse: Union[WatercourseShop, str, dm.NodeId, None] = Field(None, repr=False)
+    penalty_limit: Optional[float] = Field(None, alias="penaltyLimit")
     model: Union[str, None] = None
     cog_shop_files_config: Optional[list[dict]] = Field(None, alias="cogShopFilesConfig")
     extra_files: Optional[list[str]] = Field(None, alias="extraFiles")
     base_mappings: Union[list[Mapping], list[str], list[dm.NodeId], None] = Field(
         default=None, repr=False, alias="baseMappings"
     )
 
     def as_write(self) -> ModelTemplateWrite:
         """Convert this read version of model template to the writing version."""
         return ModelTemplateWrite(
             space=self.space,
             external_id=self.external_id,
             data_record=DataRecordWrite(existing_version=self.data_record.version),
+            name=self.name,
             version_=self.version_,
             shop_version=self.shop_version,
-            watercourse=self.watercourse.as_write() if isinstance(self.watercourse, DomainModel) else self.watercourse,
+            penalty_limit=self.penalty_limit,
             model=self.model,
             cog_shop_files_config=self.cog_shop_files_config,
             extra_files=self.extra_files,
             base_mappings=[
                 base_mapping.as_write() if isinstance(base_mapping, DomainModel) else base_mapping
                 for base_mapping in self.base_mappings or []
             ],
@@ -206,30 +216,32 @@
 
     It is used to when data is sent to CDF.
 
     Args:
         space: The space where the node is located.
         external_id: The external id of the model template.
         data_record: The data record of the model template node.
+        name: TODO
         version_: The version of the model
         shop_version: The version of SHOP to run
-        watercourse: The watercourse to run the model for
+        penalty_limit: TODO
         model: The shop model file to use as template before applying base mapping
         cog_shop_files_config: Configuration for in what order to load the various files into pyshop
         extra_files: Extra files related to a model template
         base_mappings: The base mappings for the model
     """
 
     space: str = DEFAULT_INSTANCE_SPACE
     node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference(
-        "sp_powerops_types", "ModelTemplate"
+        "sp_powerops_types_temp", "ModelTemplate"
     )
+    name: str
     version_: Optional[str] = Field(None, alias="version")
     shop_version: str = Field(alias="shopVersion")
-    watercourse: Union[WatercourseShopWrite, str, dm.NodeId, None] = Field(None, repr=False)
+    penalty_limit: Optional[float] = Field(None, alias="penaltyLimit")
     model: Union[str, None] = None
     cog_shop_files_config: Optional[list[dict]] = Field(None, alias="cogShopFilesConfig")
     extra_files: Optional[list[str]] = Field(None, alias="extraFiles")
     base_mappings: Union[list[MappingWrite], list[str], list[dm.NodeId], None] = Field(
         default=None, repr=False, alias="baseMappings"
     )
 
@@ -241,30 +253,30 @@
         allow_version_increase: bool = False,
     ) -> ResourcesWrite:
         resources = ResourcesWrite()
         if self.as_tuple_id() in cache:
             return resources
 
         write_view = (view_by_read_class or {}).get(
-            ModelTemplate, dm.ViewId("sp_powerops_models", "ModelTemplate", "1")
+            ModelTemplate, dm.ViewId("sp_powerops_models_temp", "ModelTemplate", "1")
         )
 
         properties: dict[str, Any] = {}
 
+        if self.name is not None:
+            properties["name"] = self.name
+
         if self.version_ is not None or write_none:
             properties["version"] = self.version_
 
         if self.shop_version is not None:
             properties["shopVersion"] = self.shop_version
 
-        if self.watercourse is not None:
-            properties["watercourse"] = {
-                "space": self.space if isinstance(self.watercourse, str) else self.watercourse.space,
-                "externalId": self.watercourse if isinstance(self.watercourse, str) else self.watercourse.external_id,
-            }
+        if self.penalty_limit is not None or write_none:
+            properties["penaltyLimit"] = self.penalty_limit
 
         if self.model is not None:
             properties["model"] = self.model
 
         if self.cog_shop_files_config is not None or write_none:
             properties["cogShopFilesConfig"] = self.cog_shop_files_config
 
@@ -283,31 +295,27 @@
                         properties=properties,
                     )
                 ],
             )
             resources.nodes.append(this_node)
             cache.add(self.as_tuple_id())
 
-        edge_type = dm.DirectRelationReference("sp_powerops_types", "ModelTemplate.baseMappings")
+        edge_type = dm.DirectRelationReference("sp_powerops_types_temp", "ModelTemplate.baseMappings")
         for base_mapping in self.base_mappings or []:
             other_resources = DomainRelationWrite.from_edge_to_resources(
                 cache,
                 start_node=self,
                 end_node=base_mapping,
                 edge_type=edge_type,
                 view_by_read_class=view_by_read_class,
                 write_none=write_none,
                 allow_version_increase=allow_version_increase,
             )
             resources.extend(other_resources)
 
-        if isinstance(self.watercourse, DomainModelWrite):
-            other_resources = self.watercourse._to_instances_write(cache, view_by_read_class)
-            resources.extend(other_resources)
-
         return resources
 
 
 class ModelTemplateApply(ModelTemplateWrite):
     def __new__(cls, *args, **kwargs) -> ModelTemplateApply:
         warnings.warn(
             "ModelTemplateApply is deprecated and will be removed in v1.0. Use ModelTemplateWrite instead."
@@ -345,62 +353,48 @@
 
 
 class ModelTemplateApplyList(ModelTemplateWriteList): ...
 
 
 def _create_model_template_filter(
     view_id: dm.ViewId,
+    name: str | list[str] | None = None,
+    name_prefix: str | None = None,
     version_: str | list[str] | None = None,
     version_prefix: str | None = None,
     shop_version: str | list[str] | None = None,
     shop_version_prefix: str | None = None,
-    watercourse: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+    min_penalty_limit: float | None = None,
+    max_penalty_limit: float | None = None,
     external_id_prefix: str | None = None,
     space: str | list[str] | None = None,
     filter: dm.Filter | None = None,
 ) -> dm.Filter | None:
     filters = []
+    if isinstance(name, str):
+        filters.append(dm.filters.Equals(view_id.as_property_ref("name"), value=name))
+    if name and isinstance(name, list):
+        filters.append(dm.filters.In(view_id.as_property_ref("name"), values=name))
+    if name_prefix is not None:
+        filters.append(dm.filters.Prefix(view_id.as_property_ref("name"), value=name_prefix))
     if isinstance(version_, str):
         filters.append(dm.filters.Equals(view_id.as_property_ref("version"), value=version_))
     if version_ and isinstance(version_, list):
         filters.append(dm.filters.In(view_id.as_property_ref("version"), values=version_))
     if version_prefix is not None:
         filters.append(dm.filters.Prefix(view_id.as_property_ref("version"), value=version_prefix))
     if isinstance(shop_version, str):
         filters.append(dm.filters.Equals(view_id.as_property_ref("shopVersion"), value=shop_version))
     if shop_version and isinstance(shop_version, list):
         filters.append(dm.filters.In(view_id.as_property_ref("shopVersion"), values=shop_version))
     if shop_version_prefix is not None:
         filters.append(dm.filters.Prefix(view_id.as_property_ref("shopVersion"), value=shop_version_prefix))
-    if watercourse and isinstance(watercourse, str):
-        filters.append(
-            dm.filters.Equals(
-                view_id.as_property_ref("watercourse"),
-                value={"space": DEFAULT_INSTANCE_SPACE, "externalId": watercourse},
-            )
-        )
-    if watercourse and isinstance(watercourse, tuple):
+    if min_penalty_limit is not None or max_penalty_limit is not None:
         filters.append(
-            dm.filters.Equals(
-                view_id.as_property_ref("watercourse"), value={"space": watercourse[0], "externalId": watercourse[1]}
-            )
-        )
-    if watercourse and isinstance(watercourse, list) and isinstance(watercourse[0], str):
-        filters.append(
-            dm.filters.In(
-                view_id.as_property_ref("watercourse"),
-                values=[{"space": DEFAULT_INSTANCE_SPACE, "externalId": item} for item in watercourse],
-            )
-        )
-    if watercourse and isinstance(watercourse, list) and isinstance(watercourse[0], tuple):
-        filters.append(
-            dm.filters.In(
-                view_id.as_property_ref("watercourse"),
-                values=[{"space": item[0], "externalId": item[1]} for item in watercourse],
-            )
+            dm.filters.Range(view_id.as_property_ref("penaltyLimit"), gte=min_penalty_limit, lte=max_penalty_limit)
         )
     if external_id_prefix is not None:
         filters.append(dm.filters.Prefix(["node", "externalId"], value=external_id_prefix))
     if isinstance(space, str):
         filters.append(dm.filters.Equals(["node", "space"], value=space))
     if space and isinstance(space, list):
         filters.append(dm.filters.In(["node", "space"], values=space))
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_multi_scenario_matrix.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_multi_scenario_matrix.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_multi_scenario_matrix_raw.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_multi_scenario_matrix_raw.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_partial_post_processing_input.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_partial_post_processing_input.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_partial_post_processing_output.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_partial_post_processing_output.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_plant.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_plant.py`

 * *Files 9% similar despite different names*

```diff
@@ -19,19 +19,18 @@
     DomainModelWriteList,
     DomainModelList,
     DomainRelationWrite,
     GraphQLCore,
     ResourcesWrite,
     TimeSeries,
 )
+from ._power_asset import PowerAsset, PowerAssetWrite
 
 if TYPE_CHECKING:
     from ._generator import Generator, GeneratorGraphQL, GeneratorWrite
-    from ._reservoir import Reservoir, ReservoirGraphQL, ReservoirWrite
-    from ._watercourse import Watercourse, WatercourseGraphQL, WatercourseWrite
 
 
 __all__ = [
     "Plant",
     "PlantWrite",
     "PlantApply",
     "PlantList",
@@ -41,53 +40,56 @@
     "PlantTextFields",
 ]
 
 
 PlantTextFields = Literal[
     "name",
     "display_name",
-    "p_max_time_series",
-    "p_min_time_series",
+    "asset_type",
+    "production_max_time_series",
+    "production_min_time_series",
     "water_value_time_series",
     "feeding_fee_time_series",
     "outlet_level_time_series",
     "inlet_level_time_series",
     "head_direct_time_series",
 ]
 PlantFields = Literal[
     "name",
     "display_name",
     "ordering",
+    "asset_type",
     "head_loss_factor",
     "outlet_level",
-    "p_max",
-    "p_min",
+    "production_max",
+    "production_min",
     "penstock_head_loss_factors",
     "connection_losses",
-    "p_max_time_series",
-    "p_min_time_series",
+    "production_max_time_series",
+    "production_min_time_series",
     "water_value_time_series",
     "feeding_fee_time_series",
     "outlet_level_time_series",
     "inlet_level_time_series",
     "head_direct_time_series",
 ]
 
 _PLANT_PROPERTIES_BY_FIELD = {
     "name": "name",
     "display_name": "displayName",
     "ordering": "ordering",
+    "asset_type": "assetType",
     "head_loss_factor": "headLossFactor",
     "outlet_level": "outletLevel",
-    "p_max": "pMax",
-    "p_min": "pMin",
+    "production_max": "productionMax",
+    "production_min": "productionMin",
     "penstock_head_loss_factors": "penstockHeadLossFactors",
     "connection_losses": "connectionLosses",
-    "p_max_time_series": "pMaxTimeSeries",
-    "p_min_time_series": "pMinTimeSeries",
+    "production_max_time_series": "productionMaxTimeSeries",
+    "production_min_time_series": "productionMinTimeSeries",
     "water_value_time_series": "waterValueTimeSeries",
     "feeding_fee_time_series": "feedingFeeTimeSeries",
     "outlet_level_time_series": "outletLevelTimeSeries",
     "inlet_level_time_series": "inletLevelTimeSeries",
     "head_direct_time_series": "headDirectTimeSeries",
 }
 
@@ -101,65 +103,63 @@
     Args:
         space: The space where the node is located.
         external_id: The external id of the plant.
         data_record: The data record of the plant node.
         name: Name for the Asset
         display_name: Display name for the Asset.
         ordering: The ordering of the asset
+        asset_type: The type of the asset
         head_loss_factor: The head loss factor field.
         outlet_level: The outlet level field.
-        p_max: The p max field.
-        p_min: The p min field.
+        production_max: The production max field.
+        production_min: The production min field.
         penstock_head_loss_factors: The penstock head loss factor field.
-        watercourse: The watercourse field.
         connection_losses: The connection loss field.
-        p_max_time_series: The p max time series field.
-        p_min_time_series: The p min time series field.
+        production_max_time_series: The production max time series field.
+        production_min_time_series: The production min time series field.
         water_value_time_series: The water value time series field.
         feeding_fee_time_series: The feeding fee time series field.
         outlet_level_time_series: The outlet level time series field.
         inlet_level_time_series: The inlet level time series field.
         head_direct_time_series: The head direct time series field.
-        inlet_reservoir: The inlet reservoir field.
         generators: The generator field.
     """
 
-    view_id = dm.ViewId("sp_powerops_models", "Plant", "1")
+    view_id = dm.ViewId("sp_powerops_models_temp", "Plant", "1")
     name: Optional[str] = None
     display_name: Optional[str] = Field(None, alias="displayName")
     ordering: Optional[int] = None
+    asset_type: Optional[str] = Field(None, alias="assetType")
     head_loss_factor: Optional[float] = Field(None, alias="headLossFactor")
     outlet_level: Optional[float] = Field(None, alias="outletLevel")
-    p_max: Optional[float] = Field(None, alias="pMax")
-    p_min: Optional[float] = Field(None, alias="pMin")
+    production_max: Optional[float] = Field(None, alias="productionMax")
+    production_min: Optional[float] = Field(None, alias="productionMin")
     penstock_head_loss_factors: Optional[dict] = Field(None, alias="penstockHeadLossFactors")
-    watercourse: Optional[WatercourseGraphQL] = Field(None, repr=False)
     connection_losses: Optional[float] = Field(None, alias="connectionLosses")
-    p_max_time_series: Union[TimeSeries, str, None] = Field(None, alias="pMaxTimeSeries")
-    p_min_time_series: Union[TimeSeries, str, None] = Field(None, alias="pMinTimeSeries")
+    production_max_time_series: Union[TimeSeries, str, None] = Field(None, alias="productionMaxTimeSeries")
+    production_min_time_series: Union[TimeSeries, str, None] = Field(None, alias="productionMinTimeSeries")
     water_value_time_series: Union[TimeSeries, str, None] = Field(None, alias="waterValueTimeSeries")
     feeding_fee_time_series: Union[TimeSeries, str, None] = Field(None, alias="feedingFeeTimeSeries")
     outlet_level_time_series: Union[TimeSeries, str, None] = Field(None, alias="outletLevelTimeSeries")
     inlet_level_time_series: Union[TimeSeries, str, None] = Field(None, alias="inletLevelTimeSeries")
     head_direct_time_series: Union[TimeSeries, str, None] = Field(None, alias="headDirectTimeSeries")
-    inlet_reservoir: Optional[ReservoirGraphQL] = Field(None, repr=False, alias="inletReservoir")
     generators: Optional[list[GeneratorGraphQL]] = Field(default=None, repr=False)
 
     @model_validator(mode="before")
     def parse_data_record(cls, values: Any) -> Any:
         if not isinstance(values, dict):
             return values
         if "lastUpdatedTime" in values or "createdTime" in values:
             values["dataRecord"] = DataRecordGraphQL(
                 created_time=values.pop("createdTime", None),
                 last_updated_time=values.pop("lastUpdatedTime", None),
             )
         return values
 
-    @field_validator("watercourse", "inlet_reservoir", "generators", mode="before")
+    @field_validator("generators", mode="before")
     def parse_graphql(cls, value: Any) -> Any:
         if not isinstance(value, dict):
             return value
         if "items" in value:
             return value["items"]
         return value
 
@@ -174,33 +174,28 @@
                 version=0,
                 last_updated_time=self.data_record.last_updated_time,
                 created_time=self.data_record.created_time,
             ),
             name=self.name,
             display_name=self.display_name,
             ordering=self.ordering,
+            asset_type=self.asset_type,
             head_loss_factor=self.head_loss_factor,
             outlet_level=self.outlet_level,
-            p_max=self.p_max,
-            p_min=self.p_min,
+            production_max=self.production_max,
+            production_min=self.production_min,
             penstock_head_loss_factors=self.penstock_head_loss_factors,
-            watercourse=self.watercourse.as_read() if isinstance(self.watercourse, GraphQLCore) else self.watercourse,
             connection_losses=self.connection_losses,
-            p_max_time_series=self.p_max_time_series,
-            p_min_time_series=self.p_min_time_series,
+            production_max_time_series=self.production_max_time_series,
+            production_min_time_series=self.production_min_time_series,
             water_value_time_series=self.water_value_time_series,
             feeding_fee_time_series=self.feeding_fee_time_series,
             outlet_level_time_series=self.outlet_level_time_series,
             inlet_level_time_series=self.inlet_level_time_series,
             head_direct_time_series=self.head_direct_time_series,
-            inlet_reservoir=(
-                self.inlet_reservoir.as_read()
-                if isinstance(self.inlet_reservoir, GraphQLCore)
-                else self.inlet_reservoir
-            ),
             generators=[
                 generator.as_read() if isinstance(generator, GraphQLCore) else generator
                 for generator in self.generators or []
             ],
         )
 
     def as_write(self) -> PlantWrite:
@@ -208,120 +203,103 @@
         return PlantWrite(
             space=self.space,
             external_id=self.external_id,
             data_record=DataRecordWrite(existing_version=0),
             name=self.name,
             display_name=self.display_name,
             ordering=self.ordering,
+            asset_type=self.asset_type,
             head_loss_factor=self.head_loss_factor,
             outlet_level=self.outlet_level,
-            p_max=self.p_max,
-            p_min=self.p_min,
+            production_max=self.production_max,
+            production_min=self.production_min,
             penstock_head_loss_factors=self.penstock_head_loss_factors,
-            watercourse=self.watercourse.as_write() if isinstance(self.watercourse, DomainModel) else self.watercourse,
             connection_losses=self.connection_losses,
-            p_max_time_series=self.p_max_time_series,
-            p_min_time_series=self.p_min_time_series,
+            production_max_time_series=self.production_max_time_series,
+            production_min_time_series=self.production_min_time_series,
             water_value_time_series=self.water_value_time_series,
             feeding_fee_time_series=self.feeding_fee_time_series,
             outlet_level_time_series=self.outlet_level_time_series,
             inlet_level_time_series=self.inlet_level_time_series,
             head_direct_time_series=self.head_direct_time_series,
-            inlet_reservoir=(
-                self.inlet_reservoir.as_write()
-                if isinstance(self.inlet_reservoir, DomainModel)
-                else self.inlet_reservoir
-            ),
             generators=[
                 generator.as_write() if isinstance(generator, DomainModel) else generator
                 for generator in self.generators or []
             ],
         )
 
 
-class Plant(DomainModel):
+class Plant(PowerAsset):
     """This represents the reading version of plant.
 
     It is used to when data is retrieved from CDF.
 
     Args:
         space: The space where the node is located.
         external_id: The external id of the plant.
         data_record: The data record of the plant node.
         name: Name for the Asset
         display_name: Display name for the Asset.
         ordering: The ordering of the asset
+        asset_type: The type of the asset
         head_loss_factor: The head loss factor field.
         outlet_level: The outlet level field.
-        p_max: The p max field.
-        p_min: The p min field.
+        production_max: The production max field.
+        production_min: The production min field.
         penstock_head_loss_factors: The penstock head loss factor field.
-        watercourse: The watercourse field.
         connection_losses: The connection loss field.
-        p_max_time_series: The p max time series field.
-        p_min_time_series: The p min time series field.
+        production_max_time_series: The production max time series field.
+        production_min_time_series: The production min time series field.
         water_value_time_series: The water value time series field.
         feeding_fee_time_series: The feeding fee time series field.
         outlet_level_time_series: The outlet level time series field.
         inlet_level_time_series: The inlet level time series field.
         head_direct_time_series: The head direct time series field.
-        inlet_reservoir: The inlet reservoir field.
         generators: The generator field.
     """
 
-    space: str = DEFAULT_INSTANCE_SPACE
-    node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference("sp_powerops_types", "Plant")
-    name: str
-    display_name: Optional[str] = Field(None, alias="displayName")
-    ordering: Optional[int] = None
+    node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference("sp_powerops_types_temp", "Plant")
     head_loss_factor: Optional[float] = Field(None, alias="headLossFactor")
     outlet_level: Optional[float] = Field(None, alias="outletLevel")
-    p_max: Optional[float] = Field(None, alias="pMax")
-    p_min: Optional[float] = Field(None, alias="pMin")
+    production_max: Optional[float] = Field(None, alias="productionMax")
+    production_min: Optional[float] = Field(None, alias="productionMin")
     penstock_head_loss_factors: Optional[dict] = Field(None, alias="penstockHeadLossFactors")
-    watercourse: Union[Watercourse, str, dm.NodeId, None] = Field(None, repr=False)
     connection_losses: Optional[float] = Field(None, alias="connectionLosses")
-    p_max_time_series: Union[TimeSeries, str, None] = Field(None, alias="pMaxTimeSeries")
-    p_min_time_series: Union[TimeSeries, str, None] = Field(None, alias="pMinTimeSeries")
+    production_max_time_series: Union[TimeSeries, str, None] = Field(None, alias="productionMaxTimeSeries")
+    production_min_time_series: Union[TimeSeries, str, None] = Field(None, alias="productionMinTimeSeries")
     water_value_time_series: Union[TimeSeries, str, None] = Field(None, alias="waterValueTimeSeries")
     feeding_fee_time_series: Union[TimeSeries, str, None] = Field(None, alias="feedingFeeTimeSeries")
     outlet_level_time_series: Union[TimeSeries, str, None] = Field(None, alias="outletLevelTimeSeries")
     inlet_level_time_series: Union[TimeSeries, str, None] = Field(None, alias="inletLevelTimeSeries")
     head_direct_time_series: Union[TimeSeries, str, None] = Field(None, alias="headDirectTimeSeries")
-    inlet_reservoir: Union[Reservoir, str, dm.NodeId, None] = Field(None, repr=False, alias="inletReservoir")
     generators: Union[list[Generator], list[str], list[dm.NodeId], None] = Field(default=None, repr=False)
 
     def as_write(self) -> PlantWrite:
         """Convert this read version of plant to the writing version."""
         return PlantWrite(
             space=self.space,
             external_id=self.external_id,
             data_record=DataRecordWrite(existing_version=self.data_record.version),
             name=self.name,
             display_name=self.display_name,
             ordering=self.ordering,
+            asset_type=self.asset_type,
             head_loss_factor=self.head_loss_factor,
             outlet_level=self.outlet_level,
-            p_max=self.p_max,
-            p_min=self.p_min,
+            production_max=self.production_max,
+            production_min=self.production_min,
             penstock_head_loss_factors=self.penstock_head_loss_factors,
-            watercourse=self.watercourse.as_write() if isinstance(self.watercourse, DomainModel) else self.watercourse,
             connection_losses=self.connection_losses,
-            p_max_time_series=self.p_max_time_series,
-            p_min_time_series=self.p_min_time_series,
+            production_max_time_series=self.production_max_time_series,
+            production_min_time_series=self.production_min_time_series,
             water_value_time_series=self.water_value_time_series,
             feeding_fee_time_series=self.feeding_fee_time_series,
             outlet_level_time_series=self.outlet_level_time_series,
             inlet_level_time_series=self.inlet_level_time_series,
             head_direct_time_series=self.head_direct_time_series,
-            inlet_reservoir=(
-                self.inlet_reservoir.as_write()
-                if isinstance(self.inlet_reservoir, DomainModel)
-                else self.inlet_reservoir
-            ),
             generators=[
                 generator.as_write() if isinstance(generator, DomainModel) else generator
                 for generator in self.generators or []
             ],
         )
 
     def as_apply(self) -> PlantWrite:
@@ -330,125 +308,115 @@
             "as_apply is deprecated and will be removed in v1.0. Use as_write instead.",
             UserWarning,
             stacklevel=2,
         )
         return self.as_write()
 
 
-class PlantWrite(DomainModelWrite):
+class PlantWrite(PowerAssetWrite):
     """This represents the writing version of plant.
 
     It is used to when data is sent to CDF.
 
     Args:
         space: The space where the node is located.
         external_id: The external id of the plant.
         data_record: The data record of the plant node.
         name: Name for the Asset
         display_name: Display name for the Asset.
         ordering: The ordering of the asset
+        asset_type: The type of the asset
         head_loss_factor: The head loss factor field.
         outlet_level: The outlet level field.
-        p_max: The p max field.
-        p_min: The p min field.
+        production_max: The production max field.
+        production_min: The production min field.
         penstock_head_loss_factors: The penstock head loss factor field.
-        watercourse: The watercourse field.
         connection_losses: The connection loss field.
-        p_max_time_series: The p max time series field.
-        p_min_time_series: The p min time series field.
+        production_max_time_series: The production max time series field.
+        production_min_time_series: The production min time series field.
         water_value_time_series: The water value time series field.
         feeding_fee_time_series: The feeding fee time series field.
         outlet_level_time_series: The outlet level time series field.
         inlet_level_time_series: The inlet level time series field.
         head_direct_time_series: The head direct time series field.
-        inlet_reservoir: The inlet reservoir field.
         generators: The generator field.
     """
 
-    space: str = DEFAULT_INSTANCE_SPACE
-    node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference("sp_powerops_types", "Plant")
-    name: str
-    display_name: Optional[str] = Field(None, alias="displayName")
-    ordering: Optional[int] = None
+    node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference("sp_powerops_types_temp", "Plant")
     head_loss_factor: Optional[float] = Field(None, alias="headLossFactor")
     outlet_level: Optional[float] = Field(None, alias="outletLevel")
-    p_max: Optional[float] = Field(None, alias="pMax")
-    p_min: Optional[float] = Field(None, alias="pMin")
+    production_max: Optional[float] = Field(None, alias="productionMax")
+    production_min: Optional[float] = Field(None, alias="productionMin")
     penstock_head_loss_factors: Optional[dict] = Field(None, alias="penstockHeadLossFactors")
-    watercourse: Union[WatercourseWrite, str, dm.NodeId, None] = Field(None, repr=False)
     connection_losses: Optional[float] = Field(None, alias="connectionLosses")
-    p_max_time_series: Union[TimeSeries, str, None] = Field(None, alias="pMaxTimeSeries")
-    p_min_time_series: Union[TimeSeries, str, None] = Field(None, alias="pMinTimeSeries")
+    production_max_time_series: Union[TimeSeries, str, None] = Field(None, alias="productionMaxTimeSeries")
+    production_min_time_series: Union[TimeSeries, str, None] = Field(None, alias="productionMinTimeSeries")
     water_value_time_series: Union[TimeSeries, str, None] = Field(None, alias="waterValueTimeSeries")
     feeding_fee_time_series: Union[TimeSeries, str, None] = Field(None, alias="feedingFeeTimeSeries")
     outlet_level_time_series: Union[TimeSeries, str, None] = Field(None, alias="outletLevelTimeSeries")
     inlet_level_time_series: Union[TimeSeries, str, None] = Field(None, alias="inletLevelTimeSeries")
     head_direct_time_series: Union[TimeSeries, str, None] = Field(None, alias="headDirectTimeSeries")
-    inlet_reservoir: Union[ReservoirWrite, str, dm.NodeId, None] = Field(None, repr=False, alias="inletReservoir")
     generators: Union[list[GeneratorWrite], list[str], list[dm.NodeId], None] = Field(default=None, repr=False)
 
     def _to_instances_write(
         self,
         cache: set[tuple[str, str]],
         view_by_read_class: dict[type[DomainModelCore], dm.ViewId] | None,
         write_none: bool = False,
         allow_version_increase: bool = False,
     ) -> ResourcesWrite:
         resources = ResourcesWrite()
         if self.as_tuple_id() in cache:
             return resources
 
-        write_view = (view_by_read_class or {}).get(Plant, dm.ViewId("sp_powerops_models", "Plant", "1"))
+        write_view = (view_by_read_class or {}).get(Plant, dm.ViewId("sp_powerops_models_temp", "Plant", "1"))
 
         properties: dict[str, Any] = {}
 
         if self.name is not None:
             properties["name"] = self.name
 
         if self.display_name is not None or write_none:
             properties["displayName"] = self.display_name
 
         if self.ordering is not None or write_none:
             properties["ordering"] = self.ordering
 
+        if self.asset_type is not None or write_none:
+            properties["assetType"] = self.asset_type
+
         if self.head_loss_factor is not None or write_none:
             properties["headLossFactor"] = self.head_loss_factor
 
         if self.outlet_level is not None or write_none:
             properties["outletLevel"] = self.outlet_level
 
-        if self.p_max is not None or write_none:
-            properties["pMax"] = self.p_max
+        if self.production_max is not None or write_none:
+            properties["productionMax"] = self.production_max
 
-        if self.p_min is not None or write_none:
-            properties["pMin"] = self.p_min
+        if self.production_min is not None or write_none:
+            properties["productionMin"] = self.production_min
 
         if self.penstock_head_loss_factors is not None or write_none:
             properties["penstockHeadLossFactors"] = self.penstock_head_loss_factors
 
-        if self.watercourse is not None:
-            properties["watercourse"] = {
-                "space": self.space if isinstance(self.watercourse, str) else self.watercourse.space,
-                "externalId": self.watercourse if isinstance(self.watercourse, str) else self.watercourse.external_id,
-            }
-
         if self.connection_losses is not None or write_none:
             properties["connectionLosses"] = self.connection_losses
 
-        if self.p_max_time_series is not None or write_none:
-            if isinstance(self.p_max_time_series, str) or self.p_max_time_series is None:
-                properties["pMaxTimeSeries"] = self.p_max_time_series
+        if self.production_max_time_series is not None or write_none:
+            if isinstance(self.production_max_time_series, str) or self.production_max_time_series is None:
+                properties["productionMaxTimeSeries"] = self.production_max_time_series
             else:
-                properties["pMaxTimeSeries"] = self.p_max_time_series.external_id
+                properties["productionMaxTimeSeries"] = self.production_max_time_series.external_id
 
-        if self.p_min_time_series is not None or write_none:
-            if isinstance(self.p_min_time_series, str) or self.p_min_time_series is None:
-                properties["pMinTimeSeries"] = self.p_min_time_series
+        if self.production_min_time_series is not None or write_none:
+            if isinstance(self.production_min_time_series, str) or self.production_min_time_series is None:
+                properties["productionMinTimeSeries"] = self.production_min_time_series
             else:
-                properties["pMinTimeSeries"] = self.p_min_time_series.external_id
+                properties["productionMinTimeSeries"] = self.production_min_time_series.external_id
 
         if self.water_value_time_series is not None or write_none:
             if isinstance(self.water_value_time_series, str) or self.water_value_time_series is None:
                 properties["waterValueTimeSeries"] = self.water_value_time_series
             else:
                 properties["waterValueTimeSeries"] = self.water_value_time_series.external_id
 
@@ -472,22 +440,14 @@
 
         if self.head_direct_time_series is not None or write_none:
             if isinstance(self.head_direct_time_series, str) or self.head_direct_time_series is None:
                 properties["headDirectTimeSeries"] = self.head_direct_time_series
             else:
                 properties["headDirectTimeSeries"] = self.head_direct_time_series.external_id
 
-        if self.inlet_reservoir is not None:
-            properties["inletReservoir"] = {
-                "space": self.space if isinstance(self.inlet_reservoir, str) else self.inlet_reservoir.space,
-                "externalId": (
-                    self.inlet_reservoir if isinstance(self.inlet_reservoir, str) else self.inlet_reservoir.external_id
-                ),
-            }
-
         if properties:
             this_node = dm.NodeApply(
                 space=self.space,
                 external_id=self.external_id,
                 existing_version=None if allow_version_increase else self.data_record.existing_version,
                 type=self.node_type,
                 sources=[
@@ -496,40 +456,32 @@
                         properties=properties,
                     )
                 ],
             )
             resources.nodes.append(this_node)
             cache.add(self.as_tuple_id())
 
-        edge_type = dm.DirectRelationReference("sp_powerops_types", "isSubAssetOf")
+        edge_type = dm.DirectRelationReference("sp_powerops_types_temp", "isSubAssetOf")
         for generator in self.generators or []:
             other_resources = DomainRelationWrite.from_edge_to_resources(
                 cache,
                 start_node=self,
                 end_node=generator,
                 edge_type=edge_type,
                 view_by_read_class=view_by_read_class,
                 write_none=write_none,
                 allow_version_increase=allow_version_increase,
             )
             resources.extend(other_resources)
 
-        if isinstance(self.watercourse, DomainModelWrite):
-            other_resources = self.watercourse._to_instances_write(cache, view_by_read_class)
-            resources.extend(other_resources)
-
-        if isinstance(self.inlet_reservoir, DomainModelWrite):
-            other_resources = self.inlet_reservoir._to_instances_write(cache, view_by_read_class)
-            resources.extend(other_resources)
-
-        if isinstance(self.p_max_time_series, CogniteTimeSeries):
-            resources.time_series.append(self.p_max_time_series)
+        if isinstance(self.production_max_time_series, CogniteTimeSeries):
+            resources.time_series.append(self.production_max_time_series)
 
-        if isinstance(self.p_min_time_series, CogniteTimeSeries):
-            resources.time_series.append(self.p_min_time_series)
+        if isinstance(self.production_min_time_series, CogniteTimeSeries):
+            resources.time_series.append(self.production_min_time_series)
 
         if isinstance(self.water_value_time_series, CogniteTimeSeries):
             resources.time_series.append(self.water_value_time_series)
 
         if isinstance(self.feeding_fee_time_series, CogniteTimeSeries):
             resources.time_series.append(self.feeding_fee_time_series)
 
@@ -589,26 +541,26 @@
     view_id: dm.ViewId,
     name: str | list[str] | None = None,
     name_prefix: str | None = None,
     display_name: str | list[str] | None = None,
     display_name_prefix: str | None = None,
     min_ordering: int | None = None,
     max_ordering: int | None = None,
+    asset_type: str | list[str] | None = None,
+    asset_type_prefix: str | None = None,
     min_head_loss_factor: float | None = None,
     max_head_loss_factor: float | None = None,
     min_outlet_level: float | None = None,
     max_outlet_level: float | None = None,
-    min_p_max: float | None = None,
-    max_p_max: float | None = None,
-    min_p_min: float | None = None,
-    max_p_min: float | None = None,
-    watercourse: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+    min_production_max: float | None = None,
+    max_production_max: float | None = None,
+    min_production_min: float | None = None,
+    max_production_min: float | None = None,
     min_connection_losses: float | None = None,
     max_connection_losses: float | None = None,
-    inlet_reservoir: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
     external_id_prefix: str | None = None,
     space: str | list[str] | None = None,
     filter: dm.Filter | None = None,
 ) -> dm.Filter | None:
     filters = []
     if isinstance(name, str):
         filters.append(dm.filters.Equals(view_id.as_property_ref("name"), value=name))
@@ -620,89 +572,44 @@
         filters.append(dm.filters.Equals(view_id.as_property_ref("displayName"), value=display_name))
     if display_name and isinstance(display_name, list):
         filters.append(dm.filters.In(view_id.as_property_ref("displayName"), values=display_name))
     if display_name_prefix is not None:
         filters.append(dm.filters.Prefix(view_id.as_property_ref("displayName"), value=display_name_prefix))
     if min_ordering is not None or max_ordering is not None:
         filters.append(dm.filters.Range(view_id.as_property_ref("ordering"), gte=min_ordering, lte=max_ordering))
+    if isinstance(asset_type, str):
+        filters.append(dm.filters.Equals(view_id.as_property_ref("assetType"), value=asset_type))
+    if asset_type and isinstance(asset_type, list):
+        filters.append(dm.filters.In(view_id.as_property_ref("assetType"), values=asset_type))
+    if asset_type_prefix is not None:
+        filters.append(dm.filters.Prefix(view_id.as_property_ref("assetType"), value=asset_type_prefix))
     if min_head_loss_factor is not None or max_head_loss_factor is not None:
         filters.append(
             dm.filters.Range(
                 view_id.as_property_ref("headLossFactor"), gte=min_head_loss_factor, lte=max_head_loss_factor
             )
         )
     if min_outlet_level is not None or max_outlet_level is not None:
         filters.append(
             dm.filters.Range(view_id.as_property_ref("outletLevel"), gte=min_outlet_level, lte=max_outlet_level)
         )
-    if min_p_max is not None or max_p_max is not None:
-        filters.append(dm.filters.Range(view_id.as_property_ref("pMax"), gte=min_p_max, lte=max_p_max))
-    if min_p_min is not None or max_p_min is not None:
-        filters.append(dm.filters.Range(view_id.as_property_ref("pMin"), gte=min_p_min, lte=max_p_min))
-    if watercourse and isinstance(watercourse, str):
-        filters.append(
-            dm.filters.Equals(
-                view_id.as_property_ref("watercourse"),
-                value={"space": DEFAULT_INSTANCE_SPACE, "externalId": watercourse},
-            )
-        )
-    if watercourse and isinstance(watercourse, tuple):
+    if min_production_max is not None or max_production_max is not None:
         filters.append(
-            dm.filters.Equals(
-                view_id.as_property_ref("watercourse"), value={"space": watercourse[0], "externalId": watercourse[1]}
-            )
+            dm.filters.Range(view_id.as_property_ref("productionMax"), gte=min_production_max, lte=max_production_max)
         )
-    if watercourse and isinstance(watercourse, list) and isinstance(watercourse[0], str):
+    if min_production_min is not None or max_production_min is not None:
         filters.append(
-            dm.filters.In(
-                view_id.as_property_ref("watercourse"),
-                values=[{"space": DEFAULT_INSTANCE_SPACE, "externalId": item} for item in watercourse],
-            )
-        )
-    if watercourse and isinstance(watercourse, list) and isinstance(watercourse[0], tuple):
-        filters.append(
-            dm.filters.In(
-                view_id.as_property_ref("watercourse"),
-                values=[{"space": item[0], "externalId": item[1]} for item in watercourse],
-            )
+            dm.filters.Range(view_id.as_property_ref("productionMin"), gte=min_production_min, lte=max_production_min)
         )
     if min_connection_losses is not None or max_connection_losses is not None:
         filters.append(
             dm.filters.Range(
                 view_id.as_property_ref("connectionLosses"), gte=min_connection_losses, lte=max_connection_losses
             )
         )
-    if inlet_reservoir and isinstance(inlet_reservoir, str):
-        filters.append(
-            dm.filters.Equals(
-                view_id.as_property_ref("inletReservoir"),
-                value={"space": DEFAULT_INSTANCE_SPACE, "externalId": inlet_reservoir},
-            )
-        )
-    if inlet_reservoir and isinstance(inlet_reservoir, tuple):
-        filters.append(
-            dm.filters.Equals(
-                view_id.as_property_ref("inletReservoir"),
-                value={"space": inlet_reservoir[0], "externalId": inlet_reservoir[1]},
-            )
-        )
-    if inlet_reservoir and isinstance(inlet_reservoir, list) and isinstance(inlet_reservoir[0], str):
-        filters.append(
-            dm.filters.In(
-                view_id.as_property_ref("inletReservoir"),
-                values=[{"space": DEFAULT_INSTANCE_SPACE, "externalId": item} for item in inlet_reservoir],
-            )
-        )
-    if inlet_reservoir and isinstance(inlet_reservoir, list) and isinstance(inlet_reservoir[0], tuple):
-        filters.append(
-            dm.filters.In(
-                view_id.as_property_ref("inletReservoir"),
-                values=[{"space": item[0], "externalId": item[1]} for item in inlet_reservoir],
-            )
-        )
     if external_id_prefix is not None:
         filters.append(dm.filters.Prefix(["node", "externalId"], value=external_id_prefix))
     if isinstance(space, str):
         filters.append(dm.filters.Equals(["node", "space"], value=space))
     if space and isinstance(space, list):
         filters.append(dm.filters.In(["node", "space"], values=space))
     if filter:
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_plant_shop.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_plant_shop.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_preprocessor_input.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_preprocessor_input.py`

 * *Files 4% similar despite different names*

```diff
@@ -18,14 +18,15 @@
     DomainModelWrite,
     DomainModelWriteList,
     DomainModelList,
     DomainRelationWrite,
     GraphQLCore,
     ResourcesWrite,
 )
+from ._function_input import FunctionInput, FunctionInputWrite
 
 if TYPE_CHECKING:
     from ._scenario import Scenario, ScenarioGraphQL, ScenarioWrite
 
 
 __all__ = [
     "PreprocessorInput",
@@ -69,22 +70,22 @@
         function_name: The name of the function
         function_call_id: The function call id
         scenario: The scenario to run shop with
         shop_start: Start date of bid period
         shop_end: End date of bid period
     """
 
-    view_id = dm.ViewId("sp_powerops_models", "PreprocessorInput", "1")
+    view_id = dm.ViewId("sp_powerops_models_temp", "PreprocessorInput", "1")
     process_id: Optional[str] = Field(None, alias="processId")
     process_step: Optional[int] = Field(None, alias="processStep")
     function_name: Optional[str] = Field(None, alias="functionName")
     function_call_id: Optional[str] = Field(None, alias="functionCallId")
     scenario: Optional[ScenarioGraphQL] = Field(None, repr=False)
-    shop_start: Optional[datetime.date] = Field(None, alias="shopStart")
-    shop_end: Optional[datetime.date] = Field(None, alias="shopEnd")
+    shop_start: Optional[datetime.datetime] = Field(None, alias="shopStart")
+    shop_end: Optional[datetime.datetime] = Field(None, alias="shopEnd")
 
     @model_validator(mode="before")
     def parse_data_record(cls, values: Any) -> Any:
         if not isinstance(values, dict):
             return values
         if "lastUpdatedTime" in values or "createdTime" in values:
             values["dataRecord"] = DataRecordGraphQL(
@@ -134,15 +135,15 @@
             function_call_id=self.function_call_id,
             scenario=self.scenario.as_write() if isinstance(self.scenario, DomainModel) else self.scenario,
             shop_start=self.shop_start,
             shop_end=self.shop_end,
         )
 
 
-class PreprocessorInput(DomainModel):
+class PreprocessorInput(FunctionInput):
     """This represents the reading version of preprocessor input.
 
     It is used to when data is retrieved from CDF.
 
     Args:
         space: The space where the node is located.
         external_id: The external id of the preprocessor input.
@@ -152,25 +153,20 @@
         function_name: The name of the function
         function_call_id: The function call id
         scenario: The scenario to run shop with
         shop_start: Start date of bid period
         shop_end: End date of bid period
     """
 
-    space: str = DEFAULT_INSTANCE_SPACE
     node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference(
-        "sp_powerops_types", "PreprocessorInput"
+        "sp_powerops_types_temp", "PreprocessorInput"
     )
-    process_id: str = Field(alias="processId")
-    process_step: int = Field(alias="processStep")
-    function_name: str = Field(alias="functionName")
-    function_call_id: str = Field(alias="functionCallId")
     scenario: Union[Scenario, str, dm.NodeId, None] = Field(None, repr=False)
-    shop_start: Optional[datetime.date] = Field(None, alias="shopStart")
-    shop_end: Optional[datetime.date] = Field(None, alias="shopEnd")
+    shop_start: Optional[datetime.datetime] = Field(None, alias="shopStart")
+    shop_end: Optional[datetime.datetime] = Field(None, alias="shopEnd")
 
     def as_write(self) -> PreprocessorInputWrite:
         """Convert this read version of preprocessor input to the writing version."""
         return PreprocessorInputWrite(
             space=self.space,
             external_id=self.external_id,
             data_record=DataRecordWrite(existing_version=self.data_record.version),
@@ -189,15 +185,15 @@
             "as_apply is deprecated and will be removed in v1.0. Use as_write instead.",
             UserWarning,
             stacklevel=2,
         )
         return self.as_write()
 
 
-class PreprocessorInputWrite(DomainModelWrite):
+class PreprocessorInputWrite(FunctionInputWrite):
     """This represents the writing version of preprocessor input.
 
     It is used to when data is sent to CDF.
 
     Args:
         space: The space where the node is located.
         external_id: The external id of the preprocessor input.
@@ -207,39 +203,34 @@
         function_name: The name of the function
         function_call_id: The function call id
         scenario: The scenario to run shop with
         shop_start: Start date of bid period
         shop_end: End date of bid period
     """
 
-    space: str = DEFAULT_INSTANCE_SPACE
     node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference(
-        "sp_powerops_types", "PreprocessorInput"
+        "sp_powerops_types_temp", "PreprocessorInput"
     )
-    process_id: str = Field(alias="processId")
-    process_step: int = Field(alias="processStep")
-    function_name: str = Field(alias="functionName")
-    function_call_id: str = Field(alias="functionCallId")
     scenario: Union[ScenarioWrite, str, dm.NodeId, None] = Field(None, repr=False)
-    shop_start: Optional[datetime.date] = Field(None, alias="shopStart")
-    shop_end: Optional[datetime.date] = Field(None, alias="shopEnd")
+    shop_start: Optional[datetime.datetime] = Field(None, alias="shopStart")
+    shop_end: Optional[datetime.datetime] = Field(None, alias="shopEnd")
 
     def _to_instances_write(
         self,
         cache: set[tuple[str, str]],
         view_by_read_class: dict[type[DomainModelCore], dm.ViewId] | None,
         write_none: bool = False,
         allow_version_increase: bool = False,
     ) -> ResourcesWrite:
         resources = ResourcesWrite()
         if self.as_tuple_id() in cache:
             return resources
 
         write_view = (view_by_read_class or {}).get(
-            PreprocessorInput, dm.ViewId("sp_powerops_models", "PreprocessorInput", "1")
+            PreprocessorInput, dm.ViewId("sp_powerops_models_temp", "PreprocessorInput", "1")
         )
 
         properties: dict[str, Any] = {}
 
         if self.process_id is not None:
             properties["processId"] = self.process_id
 
@@ -255,18 +246,18 @@
         if self.scenario is not None:
             properties["scenario"] = {
                 "space": self.space if isinstance(self.scenario, str) else self.scenario.space,
                 "externalId": self.scenario if isinstance(self.scenario, str) else self.scenario.external_id,
             }
 
         if self.shop_start is not None or write_none:
-            properties["shopStart"] = self.shop_start.isoformat() if self.shop_start else None
+            properties["shopStart"] = self.shop_start.isoformat(timespec="milliseconds") if self.shop_start else None
 
         if self.shop_end is not None or write_none:
-            properties["shopEnd"] = self.shop_end.isoformat() if self.shop_end else None
+            properties["shopEnd"] = self.shop_end.isoformat(timespec="milliseconds") if self.shop_end else None
 
         if properties:
             this_node = dm.NodeApply(
                 space=self.space,
                 external_id=self.external_id,
                 existing_version=None if allow_version_increase else self.data_record.existing_version,
                 type=self.node_type,
@@ -334,18 +325,18 @@
     min_process_step: int | None = None,
     max_process_step: int | None = None,
     function_name: str | list[str] | None = None,
     function_name_prefix: str | None = None,
     function_call_id: str | list[str] | None = None,
     function_call_id_prefix: str | None = None,
     scenario: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
-    min_shop_start: datetime.date | None = None,
-    max_shop_start: datetime.date | None = None,
-    min_shop_end: datetime.date | None = None,
-    max_shop_end: datetime.date | None = None,
+    min_shop_start: datetime.datetime | None = None,
+    max_shop_start: datetime.datetime | None = None,
+    min_shop_end: datetime.datetime | None = None,
+    max_shop_end: datetime.datetime | None = None,
     external_id_prefix: str | None = None,
     space: str | list[str] | None = None,
     filter: dm.Filter | None = None,
 ) -> dm.Filter | None:
     filters = []
     if isinstance(process_id, str):
         filters.append(dm.filters.Equals(view_id.as_property_ref("processId"), value=process_id))
@@ -395,24 +386,24 @@
                 values=[{"space": item[0], "externalId": item[1]} for item in scenario],
             )
         )
     if min_shop_start is not None or max_shop_start is not None:
         filters.append(
             dm.filters.Range(
                 view_id.as_property_ref("shopStart"),
-                gte=min_shop_start.isoformat() if min_shop_start else None,
-                lte=max_shop_start.isoformat() if max_shop_start else None,
+                gte=min_shop_start.isoformat(timespec="milliseconds") if min_shop_start else None,
+                lte=max_shop_start.isoformat(timespec="milliseconds") if max_shop_start else None,
             )
         )
     if min_shop_end is not None or max_shop_end is not None:
         filters.append(
             dm.filters.Range(
                 view_id.as_property_ref("shopEnd"),
-                gte=min_shop_end.isoformat() if min_shop_end else None,
-                lte=max_shop_end.isoformat() if max_shop_end else None,
+                gte=min_shop_end.isoformat(timespec="milliseconds") if min_shop_end else None,
+                lte=max_shop_end.isoformat(timespec="milliseconds") if max_shop_end else None,
             )
         )
     if external_id_prefix is not None:
         filters.append(dm.filters.Prefix(["node", "externalId"], value=external_id_prefix))
     if isinstance(space, str):
         filters.append(dm.filters.Equals(["node", "space"], value=space))
     if space and isinstance(space, list):
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_preprocessor_output.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_preprocessor_output.py`

 * *Files 6% similar despite different names*

```diff
@@ -17,14 +17,15 @@
     DomainModelWrite,
     DomainModelWriteList,
     DomainModelList,
     DomainRelationWrite,
     GraphQLCore,
     ResourcesWrite,
 )
+from ._function_output import FunctionOutput, FunctionOutputWrite
 
 if TYPE_CHECKING:
     from ._alert import Alert, AlertGraphQL, AlertWrite
     from ._case import Case, CaseGraphQL, CaseWrite
     from ._preprocessor_input import PreprocessorInput, PreprocessorInputGraphQL, PreprocessorInputWrite
 
 
@@ -66,15 +67,15 @@
         function_name: The name of the function
         function_call_id: The function call id
         alerts: An array of calculation level Alerts.
         case: The Case to trigger shop with
         input_: The prepped and processed scenario to send to shop trigger
     """
 
-    view_id = dm.ViewId("sp_powerops_models", "PreprocessorOutput", "1")
+    view_id = dm.ViewId("sp_powerops_models_temp", "PreprocessorOutput", "1")
     process_id: Optional[str] = Field(None, alias="processId")
     process_step: Optional[int] = Field(None, alias="processStep")
     function_name: Optional[str] = Field(None, alias="functionName")
     function_call_id: Optional[str] = Field(None, alias="functionCallId")
     alerts: Optional[list[AlertGraphQL]] = Field(default=None, repr=False)
     case: Optional[CaseGraphQL] = Field(None, repr=False)
     input_: Optional[PreprocessorInputGraphQL] = Field(None, repr=False, alias="input")
@@ -131,15 +132,15 @@
             function_call_id=self.function_call_id,
             alerts=[alert.as_write() if isinstance(alert, DomainModel) else alert for alert in self.alerts or []],
             case=self.case.as_write() if isinstance(self.case, DomainModel) else self.case,
             input_=self.input_.as_write() if isinstance(self.input_, DomainModel) else self.input_,
         )
 
 
-class PreprocessorOutput(DomainModel):
+class PreprocessorOutput(FunctionOutput):
     """This represents the reading version of preprocessor output.
 
     It is used to when data is retrieved from CDF.
 
     Args:
         space: The space where the node is located.
         external_id: The external id of the preprocessor output.
@@ -149,23 +150,17 @@
         function_name: The name of the function
         function_call_id: The function call id
         alerts: An array of calculation level Alerts.
         case: The Case to trigger shop with
         input_: The prepped and processed scenario to send to shop trigger
     """
 
-    space: str = DEFAULT_INSTANCE_SPACE
     node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference(
-        "sp_powerops_types", "PreprocessorOutput"
+        "sp_powerops_types_temp", "PreprocessorOutput"
     )
-    process_id: str = Field(alias="processId")
-    process_step: int = Field(alias="processStep")
-    function_name: str = Field(alias="functionName")
-    function_call_id: str = Field(alias="functionCallId")
-    alerts: Union[list[Alert], list[str], list[dm.NodeId], None] = Field(default=None, repr=False)
     case: Union[Case, str, dm.NodeId, None] = Field(None, repr=False)
     input_: Union[PreprocessorInput, str, dm.NodeId, None] = Field(None, repr=False, alias="input")
 
     def as_write(self) -> PreprocessorOutputWrite:
         """Convert this read version of preprocessor output to the writing version."""
         return PreprocessorOutputWrite(
             space=self.space,
@@ -186,15 +181,15 @@
             "as_apply is deprecated and will be removed in v1.0. Use as_write instead.",
             UserWarning,
             stacklevel=2,
         )
         return self.as_write()
 
 
-class PreprocessorOutputWrite(DomainModelWrite):
+class PreprocessorOutputWrite(FunctionOutputWrite):
     """This represents the writing version of preprocessor output.
 
     It is used to when data is sent to CDF.
 
     Args:
         space: The space where the node is located.
         external_id: The external id of the preprocessor output.
@@ -204,23 +199,17 @@
         function_name: The name of the function
         function_call_id: The function call id
         alerts: An array of calculation level Alerts.
         case: The Case to trigger shop with
         input_: The prepped and processed scenario to send to shop trigger
     """
 
-    space: str = DEFAULT_INSTANCE_SPACE
     node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference(
-        "sp_powerops_types", "PreprocessorOutput"
+        "sp_powerops_types_temp", "PreprocessorOutput"
     )
-    process_id: str = Field(alias="processId")
-    process_step: int = Field(alias="processStep")
-    function_name: str = Field(alias="functionName")
-    function_call_id: str = Field(alias="functionCallId")
-    alerts: Union[list[AlertWrite], list[str], list[dm.NodeId], None] = Field(default=None, repr=False)
     case: Union[CaseWrite, str, dm.NodeId, None] = Field(None, repr=False)
     input_: Union[PreprocessorInputWrite, str, dm.NodeId, None] = Field(None, repr=False, alias="input")
 
     def _to_instances_write(
         self,
         cache: set[tuple[str, str]],
         view_by_read_class: dict[type[DomainModelCore], dm.ViewId] | None,
@@ -228,15 +217,15 @@
         allow_version_increase: bool = False,
     ) -> ResourcesWrite:
         resources = ResourcesWrite()
         if self.as_tuple_id() in cache:
             return resources
 
         write_view = (view_by_read_class or {}).get(
-            PreprocessorOutput, dm.ViewId("sp_powerops_models", "PreprocessorOutput", "1")
+            PreprocessorOutput, dm.ViewId("sp_powerops_models_temp", "PreprocessorOutput", "1")
         )
 
         properties: dict[str, Any] = {}
 
         if self.process_id is not None:
             properties["processId"] = self.process_id
 
@@ -273,15 +262,15 @@
                         properties=properties,
                     )
                 ],
             )
             resources.nodes.append(this_node)
             cache.add(self.as_tuple_id())
 
-        edge_type = dm.DirectRelationReference("sp_powerops_types", "calculationIssue")
+        edge_type = dm.DirectRelationReference("sp_powerops_types_temp", "calculationIssue")
         for alert in self.alerts or []:
             other_resources = DomainRelationWrite.from_edge_to_resources(
                 cache,
                 start_node=self,
                 end_node=alert,
                 edge_type=edge_type,
                 view_by_read_class=view_by_read_class,
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_price_area.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_watercourse_shop.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,13 +1,14 @@
 from __future__ import annotations
 
 import warnings
 from typing import Any, Literal, Optional, Union
 
 from cognite.client import data_modeling as dm
+from pydantic import Field
 from pydantic import field_validator, model_validator
 
 from ._core import (
     DEFAULT_INSTANCE_SPACE,
     DataRecord,
     DataRecordGraphQL,
     DataRecordWrite,
@@ -19,166 +20,185 @@
     DomainRelationWrite,
     GraphQLCore,
     ResourcesWrite,
 )
 
 
 __all__ = [
-    "PriceArea",
-    "PriceAreaWrite",
-    "PriceAreaApply",
-    "PriceAreaList",
-    "PriceAreaWriteList",
-    "PriceAreaApplyList",
-    "PriceAreaFields",
-    "PriceAreaTextFields",
+    "WatercourseShop",
+    "WatercourseShopWrite",
+    "WatercourseShopApply",
+    "WatercourseShopList",
+    "WatercourseShopWriteList",
+    "WatercourseShopApplyList",
+    "WatercourseShopFields",
+    "WatercourseShopTextFields",
 ]
 
 
-PriceAreaTextFields = Literal["name", "timezone"]
-PriceAreaFields = Literal["name", "timezone"]
+WatercourseShopTextFields = Literal["name", "display_name"]
+WatercourseShopFields = Literal["name", "display_name", "ordering"]
 
-_PRICEAREA_PROPERTIES_BY_FIELD = {
+_WATERCOURSESHOP_PROPERTIES_BY_FIELD = {
     "name": "name",
-    "timezone": "timezone",
+    "display_name": "displayName",
+    "ordering": "ordering",
 }
 
 
-class PriceAreaGraphQL(GraphQLCore):
-    """This represents the reading version of price area, used
+class WatercourseShopGraphQL(GraphQLCore):
+    """This represents the reading version of watercourse shop, used
     when data is retrieved from CDF using GraphQL.
 
     It is used when retrieving data from CDF using GraphQL.
 
     Args:
         space: The space where the node is located.
-        external_id: The external id of the price area.
-        data_record: The data record of the price area node.
-        name: The name of the price area
-        timezone: The timezone of the price area
+        external_id: The external id of the watercourse shop.
+        data_record: The data record of the watercourse shop node.
+        name: Name for the Asset
+        display_name: Display name for the Asset.
+        ordering: The ordering of the asset
     """
 
-    view_id = dm.ViewId("sp_powerops_models", "PriceArea", "1")
+    view_id = dm.ViewId("sp_powerops_models", "WatercourseShop", "1")
     name: Optional[str] = None
-    timezone: Optional[str] = None
+    display_name: Optional[str] = Field(None, alias="displayName")
+    ordering: Optional[int] = None
 
     @model_validator(mode="before")
     def parse_data_record(cls, values: Any) -> Any:
         if not isinstance(values, dict):
             return values
         if "lastUpdatedTime" in values or "createdTime" in values:
             values["dataRecord"] = DataRecordGraphQL(
                 created_time=values.pop("createdTime", None),
                 last_updated_time=values.pop("lastUpdatedTime", None),
             )
         return values
 
-    def as_read(self) -> PriceArea:
-        """Convert this GraphQL format of price area to the reading format."""
+    def as_read(self) -> WatercourseShop:
+        """Convert this GraphQL format of watercourse shop to the reading format."""
         if self.data_record is None:
             raise ValueError("This object cannot be converted to a read format because it lacks a data record.")
-        return PriceArea(
+        return WatercourseShop(
             space=self.space,
             external_id=self.external_id,
             data_record=DataRecord(
                 version=0,
                 last_updated_time=self.data_record.last_updated_time,
                 created_time=self.data_record.created_time,
             ),
             name=self.name,
-            timezone=self.timezone,
+            display_name=self.display_name,
+            ordering=self.ordering,
         )
 
-    def as_write(self) -> PriceAreaWrite:
-        """Convert this GraphQL format of price area to the writing format."""
-        return PriceAreaWrite(
+    def as_write(self) -> WatercourseShopWrite:
+        """Convert this GraphQL format of watercourse shop to the writing format."""
+        return WatercourseShopWrite(
             space=self.space,
             external_id=self.external_id,
             data_record=DataRecordWrite(existing_version=0),
             name=self.name,
-            timezone=self.timezone,
+            display_name=self.display_name,
+            ordering=self.ordering,
         )
 
 
-class PriceArea(DomainModel):
-    """This represents the reading version of price area.
+class WatercourseShop(DomainModel):
+    """This represents the reading version of watercourse shop.
 
     It is used to when data is retrieved from CDF.
 
     Args:
         space: The space where the node is located.
-        external_id: The external id of the price area.
-        data_record: The data record of the price area node.
-        name: The name of the price area
-        timezone: The timezone of the price area
+        external_id: The external id of the watercourse shop.
+        data_record: The data record of the watercourse shop node.
+        name: Name for the Asset
+        display_name: Display name for the Asset.
+        ordering: The ordering of the asset
     """
 
     space: str = DEFAULT_INSTANCE_SPACE
-    node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference("sp_powerops_types", "PriceArea")
+    node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference(
+        "sp_powerops_types", "WatercourseShop"
+    )
     name: str
-    timezone: str
+    display_name: Optional[str] = Field(None, alias="displayName")
+    ordering: Optional[int] = None
 
-    def as_write(self) -> PriceAreaWrite:
-        """Convert this read version of price area to the writing version."""
-        return PriceAreaWrite(
+    def as_write(self) -> WatercourseShopWrite:
+        """Convert this read version of watercourse shop to the writing version."""
+        return WatercourseShopWrite(
             space=self.space,
             external_id=self.external_id,
             data_record=DataRecordWrite(existing_version=self.data_record.version),
             name=self.name,
-            timezone=self.timezone,
+            display_name=self.display_name,
+            ordering=self.ordering,
         )
 
-    def as_apply(self) -> PriceAreaWrite:
-        """Convert this read version of price area to the writing version."""
+    def as_apply(self) -> WatercourseShopWrite:
+        """Convert this read version of watercourse shop to the writing version."""
         warnings.warn(
             "as_apply is deprecated and will be removed in v1.0. Use as_write instead.",
             UserWarning,
             stacklevel=2,
         )
         return self.as_write()
 
 
-class PriceAreaWrite(DomainModelWrite):
-    """This represents the writing version of price area.
+class WatercourseShopWrite(DomainModelWrite):
+    """This represents the writing version of watercourse shop.
 
     It is used to when data is sent to CDF.
 
     Args:
         space: The space where the node is located.
-        external_id: The external id of the price area.
-        data_record: The data record of the price area node.
-        name: The name of the price area
-        timezone: The timezone of the price area
+        external_id: The external id of the watercourse shop.
+        data_record: The data record of the watercourse shop node.
+        name: Name for the Asset
+        display_name: Display name for the Asset.
+        ordering: The ordering of the asset
     """
 
     space: str = DEFAULT_INSTANCE_SPACE
-    node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference("sp_powerops_types", "PriceArea")
+    node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference(
+        "sp_powerops_types", "WatercourseShop"
+    )
     name: str
-    timezone: str
+    display_name: Optional[str] = Field(None, alias="displayName")
+    ordering: Optional[int] = None
 
     def _to_instances_write(
         self,
         cache: set[tuple[str, str]],
         view_by_read_class: dict[type[DomainModelCore], dm.ViewId] | None,
         write_none: bool = False,
         allow_version_increase: bool = False,
     ) -> ResourcesWrite:
         resources = ResourcesWrite()
         if self.as_tuple_id() in cache:
             return resources
 
-        write_view = (view_by_read_class or {}).get(PriceArea, dm.ViewId("sp_powerops_models", "PriceArea", "1"))
+        write_view = (view_by_read_class or {}).get(
+            WatercourseShop, dm.ViewId("sp_powerops_models", "WatercourseShop", "1")
+        )
 
         properties: dict[str, Any] = {}
 
         if self.name is not None:
             properties["name"] = self.name
 
-        if self.timezone is not None:
-            properties["timezone"] = self.timezone
+        if self.display_name is not None or write_none:
+            properties["displayName"] = self.display_name
+
+        if self.ordering is not None or write_none:
+            properties["ordering"] = self.ordering
 
         if properties:
             this_node = dm.NodeApply(
                 space=self.space,
                 external_id=self.external_id,
                 existing_version=None if allow_version_increase else self.data_record.existing_version,
                 type=self.node_type,
@@ -191,77 +211,81 @@
             )
             resources.nodes.append(this_node)
             cache.add(self.as_tuple_id())
 
         return resources
 
 
-class PriceAreaApply(PriceAreaWrite):
-    def __new__(cls, *args, **kwargs) -> PriceAreaApply:
+class WatercourseShopApply(WatercourseShopWrite):
+    def __new__(cls, *args, **kwargs) -> WatercourseShopApply:
         warnings.warn(
-            "PriceAreaApply is deprecated and will be removed in v1.0. Use PriceAreaWrite instead."
+            "WatercourseShopApply is deprecated and will be removed in v1.0. Use WatercourseShopWrite instead."
             "The motivation for this change is that Write is a more descriptive name for the writing version of the"
-            "PriceArea.",
+            "WatercourseShop.",
             UserWarning,
             stacklevel=2,
         )
         return super().__new__(cls)
 
 
-class PriceAreaList(DomainModelList[PriceArea]):
-    """List of price areas in the read version."""
+class WatercourseShopList(DomainModelList[WatercourseShop]):
+    """List of watercourse shops in the read version."""
 
-    _INSTANCE = PriceArea
+    _INSTANCE = WatercourseShop
 
-    def as_write(self) -> PriceAreaWriteList:
-        """Convert these read versions of price area to the writing versions."""
-        return PriceAreaWriteList([node.as_write() for node in self.data])
+    def as_write(self) -> WatercourseShopWriteList:
+        """Convert these read versions of watercourse shop to the writing versions."""
+        return WatercourseShopWriteList([node.as_write() for node in self.data])
 
-    def as_apply(self) -> PriceAreaWriteList:
+    def as_apply(self) -> WatercourseShopWriteList:
         """Convert these read versions of primitive nullable to the writing versions."""
         warnings.warn(
             "as_apply is deprecated and will be removed in v1.0. Use as_write instead.",
             UserWarning,
             stacklevel=2,
         )
         return self.as_write()
 
 
-class PriceAreaWriteList(DomainModelWriteList[PriceAreaWrite]):
-    """List of price areas in the writing version."""
+class WatercourseShopWriteList(DomainModelWriteList[WatercourseShopWrite]):
+    """List of watercourse shops in the writing version."""
 
-    _INSTANCE = PriceAreaWrite
+    _INSTANCE = WatercourseShopWrite
 
 
-class PriceAreaApplyList(PriceAreaWriteList): ...
+class WatercourseShopApplyList(WatercourseShopWriteList): ...
 
 
-def _create_price_area_filter(
+def _create_watercourse_shop_filter(
     view_id: dm.ViewId,
     name: str | list[str] | None = None,
     name_prefix: str | None = None,
-    timezone: str | list[str] | None = None,
-    timezone_prefix: str | None = None,
+    display_name: str | list[str] | None = None,
+    display_name_prefix: str | None = None,
+    min_ordering: int | None = None,
+    max_ordering: int | None = None,
     external_id_prefix: str | None = None,
     space: str | list[str] | None = None,
     filter: dm.Filter | None = None,
 ) -> dm.Filter | None:
     filters = []
     if isinstance(name, str):
         filters.append(dm.filters.Equals(view_id.as_property_ref("name"), value=name))
     if name and isinstance(name, list):
         filters.append(dm.filters.In(view_id.as_property_ref("name"), values=name))
     if name_prefix is not None:
         filters.append(dm.filters.Prefix(view_id.as_property_ref("name"), value=name_prefix))
-    if isinstance(timezone, str):
-        filters.append(dm.filters.Equals(view_id.as_property_ref("timezone"), value=timezone))
-    if timezone and isinstance(timezone, list):
-        filters.append(dm.filters.In(view_id.as_property_ref("timezone"), values=timezone))
-    if timezone_prefix is not None:
-        filters.append(dm.filters.Prefix(view_id.as_property_ref("timezone"), value=timezone_prefix))
+    if isinstance(display_name, str):
+        filters.append(dm.filters.Equals(view_id.as_property_ref("displayName"), value=display_name))
+    if display_name and isinstance(display_name, list):
+        filters.append(dm.filters.In(view_id.as_property_ref("displayName"), values=display_name))
+    if display_name_prefix is not None:
+        filters.append(dm.filters.Prefix(view_id.as_property_ref("displayName"), value=display_name_prefix))
+    if min_ordering is not None or max_ordering is not None:
+        filters.append(dm.filters.Range(view_id.as_property_ref("ordering"), gte=min_ordering, lte=max_ordering))
     if external_id_prefix is not None:
         filters.append(dm.filters.Prefix(["node", "externalId"], value=external_id_prefix))
     if isinstance(space, str):
         filters.append(dm.filters.Equals(["node", "space"], value=space))
     if space and isinstance(space, list):
         filters.append(dm.filters.In(["node", "space"], values=space))
     if filter:
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_price_area_afrr.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_price_area_afrr.py`

 * *Files 21% similar despite different names*

```diff
@@ -36,41 +36,49 @@
     "PriceAreaAFRRFields",
     "PriceAreaAFRRTextFields",
 ]
 
 
 PriceAreaAFRRTextFields = Literal[
     "name",
+    "display_name",
+    "asset_type",
     "timezone",
     "capacity_price_up",
     "capacity_price_down",
     "activation_price_up",
     "activation_price_down",
     "relative_activation",
     "total_capacity_allocation_up",
     "total_capacity_allocation_down",
     "own_capacity_allocation_up",
     "own_capacity_allocation_down",
 ]
 PriceAreaAFRRFields = Literal[
     "name",
+    "display_name",
+    "ordering",
+    "asset_type",
     "timezone",
     "capacity_price_up",
     "capacity_price_down",
     "activation_price_up",
     "activation_price_down",
     "relative_activation",
     "total_capacity_allocation_up",
     "total_capacity_allocation_down",
     "own_capacity_allocation_up",
     "own_capacity_allocation_down",
 ]
 
 _PRICEAREAAFRR_PROPERTIES_BY_FIELD = {
     "name": "name",
+    "display_name": "displayName",
+    "ordering": "ordering",
+    "asset_type": "assetType",
     "timezone": "timezone",
     "capacity_price_up": "capacityPriceUp",
     "capacity_price_down": "capacityPriceDown",
     "activation_price_up": "activationPriceUp",
     "activation_price_down": "activationPriceDown",
     "relative_activation": "relativeActivation",
     "total_capacity_allocation_up": "totalCapacityAllocationUp",
@@ -86,29 +94,35 @@
 
     It is used when retrieving data from CDF using GraphQL.
 
     Args:
         space: The space where the node is located.
         external_id: The external id of the price area afrr.
         data_record: The data record of the price area afrr node.
-        name: The name of the price area
+        name: Name for the Asset
+        display_name: Display name for the Asset.
+        ordering: The ordering of the asset
+        asset_type: The type of the asset
         timezone: The timezone of the price area
         capacity_price_up: The capacity price up field.
         capacity_price_down: The capacity price down field.
         activation_price_up: The mFRR activation price (TBC)
         activation_price_down: The mFRR activate price (TBC)
         relative_activation: Value between -1 (100 % activation down) and 1 (100 % activation down)
         total_capacity_allocation_up: The total capacity allocation up field.
         total_capacity_allocation_down: The total capacity allocation down field.
         own_capacity_allocation_up: The own capacity allocation up field.
         own_capacity_allocation_down: The own capacity allocation down field.
     """
 
-    view_id = dm.ViewId("sp_powerops_models", "PriceAreaAFRR", "1")
+    view_id = dm.ViewId("sp_powerops_models_temp", "PriceAreaAFRR", "1")
     name: Optional[str] = None
+    display_name: Optional[str] = Field(None, alias="displayName")
+    ordering: Optional[int] = None
+    asset_type: Optional[str] = Field(None, alias="assetType")
     timezone: Optional[str] = None
     capacity_price_up: Union[TimeSeries, str, None] = Field(None, alias="capacityPriceUp")
     capacity_price_down: Union[TimeSeries, str, None] = Field(None, alias="capacityPriceDown")
     activation_price_up: Union[TimeSeries, str, None] = Field(None, alias="activationPriceUp")
     activation_price_down: Union[TimeSeries, str, None] = Field(None, alias="activationPriceDown")
     relative_activation: Union[TimeSeries, str, None] = Field(None, alias="relativeActivation")
     total_capacity_allocation_up: Union[TimeSeries, str, None] = Field(None, alias="totalCapacityAllocationUp")
@@ -136,14 +150,17 @@
             external_id=self.external_id,
             data_record=DataRecord(
                 version=0,
                 last_updated_time=self.data_record.last_updated_time,
                 created_time=self.data_record.created_time,
             ),
             name=self.name,
+            display_name=self.display_name,
+            ordering=self.ordering,
+            asset_type=self.asset_type,
             timezone=self.timezone,
             capacity_price_up=self.capacity_price_up,
             capacity_price_down=self.capacity_price_down,
             activation_price_up=self.activation_price_up,
             activation_price_down=self.activation_price_down,
             relative_activation=self.relative_activation,
             total_capacity_allocation_up=self.total_capacity_allocation_up,
@@ -155,14 +172,17 @@
     def as_write(self) -> PriceAreaAFRRWrite:
         """Convert this GraphQL format of price area afrr to the writing format."""
         return PriceAreaAFRRWrite(
             space=self.space,
             external_id=self.external_id,
             data_record=DataRecordWrite(existing_version=0),
             name=self.name,
+            display_name=self.display_name,
+            ordering=self.ordering,
+            asset_type=self.asset_type,
             timezone=self.timezone,
             capacity_price_up=self.capacity_price_up,
             capacity_price_down=self.capacity_price_down,
             activation_price_up=self.activation_price_up,
             activation_price_down=self.activation_price_down,
             relative_activation=self.relative_activation,
             total_capacity_allocation_up=self.total_capacity_allocation_up,
@@ -177,28 +197,33 @@
 
     It is used to when data is retrieved from CDF.
 
     Args:
         space: The space where the node is located.
         external_id: The external id of the price area afrr.
         data_record: The data record of the price area afrr node.
-        name: The name of the price area
+        name: Name for the Asset
+        display_name: Display name for the Asset.
+        ordering: The ordering of the asset
+        asset_type: The type of the asset
         timezone: The timezone of the price area
         capacity_price_up: The capacity price up field.
         capacity_price_down: The capacity price down field.
         activation_price_up: The mFRR activation price (TBC)
         activation_price_down: The mFRR activate price (TBC)
         relative_activation: Value between -1 (100 % activation down) and 1 (100 % activation down)
         total_capacity_allocation_up: The total capacity allocation up field.
         total_capacity_allocation_down: The total capacity allocation down field.
         own_capacity_allocation_up: The own capacity allocation up field.
         own_capacity_allocation_down: The own capacity allocation down field.
     """
 
-    node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference("sp_powerops_types", "PriceArea")
+    node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference(
+        "sp_powerops_types_temp", "PriceArea"
+    )
     capacity_price_up: Union[TimeSeries, str, None] = Field(None, alias="capacityPriceUp")
     capacity_price_down: Union[TimeSeries, str, None] = Field(None, alias="capacityPriceDown")
     activation_price_up: Union[TimeSeries, str, None] = Field(None, alias="activationPriceUp")
     activation_price_down: Union[TimeSeries, str, None] = Field(None, alias="activationPriceDown")
     relative_activation: Union[TimeSeries, str, None] = Field(None, alias="relativeActivation")
     total_capacity_allocation_up: Union[TimeSeries, str, None] = Field(None, alias="totalCapacityAllocationUp")
     total_capacity_allocation_down: Union[TimeSeries, str, None] = Field(None, alias="totalCapacityAllocationDown")
@@ -208,14 +233,17 @@
     def as_write(self) -> PriceAreaAFRRWrite:
         """Convert this read version of price area afrr to the writing version."""
         return PriceAreaAFRRWrite(
             space=self.space,
             external_id=self.external_id,
             data_record=DataRecordWrite(existing_version=self.data_record.version),
             name=self.name,
+            display_name=self.display_name,
+            ordering=self.ordering,
+            asset_type=self.asset_type,
             timezone=self.timezone,
             capacity_price_up=self.capacity_price_up,
             capacity_price_down=self.capacity_price_down,
             activation_price_up=self.activation_price_up,
             activation_price_down=self.activation_price_down,
             relative_activation=self.relative_activation,
             total_capacity_allocation_up=self.total_capacity_allocation_up,
@@ -239,28 +267,33 @@
 
     It is used to when data is sent to CDF.
 
     Args:
         space: The space where the node is located.
         external_id: The external id of the price area afrr.
         data_record: The data record of the price area afrr node.
-        name: The name of the price area
+        name: Name for the Asset
+        display_name: Display name for the Asset.
+        ordering: The ordering of the asset
+        asset_type: The type of the asset
         timezone: The timezone of the price area
         capacity_price_up: The capacity price up field.
         capacity_price_down: The capacity price down field.
         activation_price_up: The mFRR activation price (TBC)
         activation_price_down: The mFRR activate price (TBC)
         relative_activation: Value between -1 (100 % activation down) and 1 (100 % activation down)
         total_capacity_allocation_up: The total capacity allocation up field.
         total_capacity_allocation_down: The total capacity allocation down field.
         own_capacity_allocation_up: The own capacity allocation up field.
         own_capacity_allocation_down: The own capacity allocation down field.
     """
 
-    node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference("sp_powerops_types", "PriceArea")
+    node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference(
+        "sp_powerops_types_temp", "PriceArea"
+    )
     capacity_price_up: Union[TimeSeries, str, None] = Field(None, alias="capacityPriceUp")
     capacity_price_down: Union[TimeSeries, str, None] = Field(None, alias="capacityPriceDown")
     activation_price_up: Union[TimeSeries, str, None] = Field(None, alias="activationPriceUp")
     activation_price_down: Union[TimeSeries, str, None] = Field(None, alias="activationPriceDown")
     relative_activation: Union[TimeSeries, str, None] = Field(None, alias="relativeActivation")
     total_capacity_allocation_up: Union[TimeSeries, str, None] = Field(None, alias="totalCapacityAllocationUp")
     total_capacity_allocation_down: Union[TimeSeries, str, None] = Field(None, alias="totalCapacityAllocationDown")
@@ -275,22 +308,31 @@
         allow_version_increase: bool = False,
     ) -> ResourcesWrite:
         resources = ResourcesWrite()
         if self.as_tuple_id() in cache:
             return resources
 
         write_view = (view_by_read_class or {}).get(
-            PriceAreaAFRR, dm.ViewId("sp_powerops_models", "PriceAreaAFRR", "1")
+            PriceAreaAFRR, dm.ViewId("sp_powerops_models_temp", "PriceAreaAFRR", "1")
         )
 
         properties: dict[str, Any] = {}
 
         if self.name is not None:
             properties["name"] = self.name
 
+        if self.display_name is not None or write_none:
+            properties["displayName"] = self.display_name
+
+        if self.ordering is not None or write_none:
+            properties["ordering"] = self.ordering
+
+        if self.asset_type is not None or write_none:
+            properties["assetType"] = self.asset_type
+
         if self.timezone is not None:
             properties["timezone"] = self.timezone
 
         if self.capacity_price_up is not None or write_none:
             if isinstance(self.capacity_price_up, str) or self.capacity_price_up is None:
                 properties["capacityPriceUp"] = self.capacity_price_up
             else:
@@ -430,27 +472,47 @@
 class PriceAreaAFRRApplyList(PriceAreaAFRRWriteList): ...
 
 
 def _create_price_area_afrr_filter(
     view_id: dm.ViewId,
     name: str | list[str] | None = None,
     name_prefix: str | None = None,
+    display_name: str | list[str] | None = None,
+    display_name_prefix: str | None = None,
+    min_ordering: int | None = None,
+    max_ordering: int | None = None,
+    asset_type: str | list[str] | None = None,
+    asset_type_prefix: str | None = None,
     timezone: str | list[str] | None = None,
     timezone_prefix: str | None = None,
     external_id_prefix: str | None = None,
     space: str | list[str] | None = None,
     filter: dm.Filter | None = None,
 ) -> dm.Filter | None:
     filters = []
     if isinstance(name, str):
         filters.append(dm.filters.Equals(view_id.as_property_ref("name"), value=name))
     if name and isinstance(name, list):
         filters.append(dm.filters.In(view_id.as_property_ref("name"), values=name))
     if name_prefix is not None:
         filters.append(dm.filters.Prefix(view_id.as_property_ref("name"), value=name_prefix))
+    if isinstance(display_name, str):
+        filters.append(dm.filters.Equals(view_id.as_property_ref("displayName"), value=display_name))
+    if display_name and isinstance(display_name, list):
+        filters.append(dm.filters.In(view_id.as_property_ref("displayName"), values=display_name))
+    if display_name_prefix is not None:
+        filters.append(dm.filters.Prefix(view_id.as_property_ref("displayName"), value=display_name_prefix))
+    if min_ordering is not None or max_ordering is not None:
+        filters.append(dm.filters.Range(view_id.as_property_ref("ordering"), gte=min_ordering, lte=max_ordering))
+    if isinstance(asset_type, str):
+        filters.append(dm.filters.Equals(view_id.as_property_ref("assetType"), value=asset_type))
+    if asset_type and isinstance(asset_type, list):
+        filters.append(dm.filters.In(view_id.as_property_ref("assetType"), values=asset_type))
+    if asset_type_prefix is not None:
+        filters.append(dm.filters.Prefix(view_id.as_property_ref("assetType"), value=asset_type_prefix))
     if isinstance(timezone, str):
         filters.append(dm.filters.Equals(view_id.as_property_ref("timezone"), value=timezone))
     if timezone and isinstance(timezone, list):
         filters.append(dm.filters.In(view_id.as_property_ref("timezone"), values=timezone))
     if timezone_prefix is not None:
         filters.append(dm.filters.Prefix(view_id.as_property_ref("timezone"), value=timezone_prefix))
     if external_id_prefix is not None:
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_price_area_asset.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_price_area_asset.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_price_area_day_ahead.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_price_area_day_ahead.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_price_prod_case.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_price_prod_case.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_price_scenario.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_price_scenario.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_reservoir.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_reservoir.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_scenario.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_scenario.py`

 * *Files 1% similar despite different names*

```diff
@@ -62,15 +62,15 @@
         name: The name of the scenario to run
         model_template: The model template to use when running the scenario
         commands: The commands to run
         source: The source of the scenario
         mappings_override: An array of base mappings to override in shop model file
     """
 
-    view_id = dm.ViewId("sp_powerops_models", "Scenario", "1")
+    view_id = dm.ViewId("sp_powerops_models_temp", "Scenario", "1")
     name: Optional[str] = None
     model_template: Optional[ModelTemplateGraphQL] = Field(None, repr=False, alias="modelTemplate")
     commands: Optional[CommandsGraphQL] = Field(None, repr=False)
     source: Optional[str] = None
     mappings_override: Optional[list[MappingGraphQL]] = Field(default=None, repr=False, alias="mappingsOverride")
 
     @model_validator(mode="before")
@@ -222,15 +222,15 @@
         write_none: bool = False,
         allow_version_increase: bool = False,
     ) -> ResourcesWrite:
         resources = ResourcesWrite()
         if self.as_tuple_id() in cache:
             return resources
 
-        write_view = (view_by_read_class or {}).get(Scenario, dm.ViewId("sp_powerops_models", "Scenario", "1"))
+        write_view = (view_by_read_class or {}).get(Scenario, dm.ViewId("sp_powerops_models_temp", "Scenario", "1"))
 
         properties: dict[str, Any] = {}
 
         if self.name is not None:
             properties["name"] = self.name
 
         if self.model_template is not None:
@@ -262,15 +262,15 @@
                         properties=properties,
                     )
                 ],
             )
             resources.nodes.append(this_node)
             cache.add(self.as_tuple_id())
 
-        edge_type = dm.DirectRelationReference("sp_powerops_types", "Mapping")
+        edge_type = dm.DirectRelationReference("sp_powerops_types_temp", "Mapping")
         for mappings_override in self.mappings_override or []:
             other_resources = DomainRelationWrite.from_edge_to_resources(
                 cache,
                 start_node=self,
                 end_node=mappings_override,
                 edge_type=edge_type,
                 view_by_read_class=view_by_read_class,
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_scenario_raw.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_scenario_raw.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_shop_partial_bid_calculation_input.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_shop_partial_bid_calculation_input.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_shop_partial_bid_calculation_output.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_shop_partial_bid_calculation_output.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_shop_result.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_shop_result.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,14 +1,13 @@
 from __future__ import annotations
 
 import warnings
 from typing import TYPE_CHECKING, Any, Literal, Optional, Union
 
 from cognite.client import data_modeling as dm
-from cognite.client.data_classes import TimeSeries as CogniteTimeSeries
 from pydantic import Field
 from pydantic import field_validator, model_validator
 
 from ._core import (
     DEFAULT_INSTANCE_SPACE,
     DataRecord,
     DataRecordGraphQL,
@@ -17,43 +16,38 @@
     DomainModelCore,
     DomainModelWrite,
     DomainModelWriteList,
     DomainModelList,
     DomainRelationWrite,
     GraphQLCore,
     ResourcesWrite,
-    TimeSeries,
 )
 
 if TYPE_CHECKING:
     from ._alert import Alert, AlertGraphQL, AlertWrite
     from ._case import Case, CaseGraphQL, CaseWrite
+    from ._shop_time_series import SHOPTimeSeries, SHOPTimeSeriesGraphQL, SHOPTimeSeriesWrite
 
 
 __all__ = [
     "SHOPResult",
     "SHOPResultWrite",
     "SHOPResultApply",
     "SHOPResultList",
     "SHOPResultWriteList",
     "SHOPResultApplyList",
     "SHOPResultFields",
     "SHOPResultTextFields",
 ]
 
 
-SHOPResultTextFields = Literal[
-    "output_timeseries", "objective_sequence", "pre_run", "post_run", "shop_messages", "cplex_logs"
-]
-SHOPResultFields = Literal[
-    "output_timeseries", "objective_sequence", "pre_run", "post_run", "shop_messages", "cplex_logs"
-]
+SHOPResultTextFields = Literal["objective_sequence", "pre_run", "post_run", "shop_messages", "cplex_logs"]
+SHOPResultFields = Literal["objective_sequence", "pre_run", "post_run", "shop_messages", "cplex_logs"]
 
 _SHOPRESULT_PROPERTIES_BY_FIELD = {
-    "output_timeseries": "outputTimeseries",
     "objective_sequence": "objectiveSequence",
     "pre_run": "preRun",
     "post_run": "postRun",
     "shop_messages": "shopMessages",
     "cplex_logs": "cplexLogs",
 }
 
@@ -65,45 +59,45 @@
     It is used when retrieving data from CDF using GraphQL.
 
     Args:
         space: The space where the node is located.
         external_id: The external id of the shop result.
         data_record: The data record of the shop result node.
         case: The case that was used to produce this result
-        output_timeseries: A general placeholder for all timeseries that stem from a shop run
         objective_sequence: The sequence of the objective function
         pre_run: The pre-run data for the SHOP run
         post_run: The post-run data for the SHOP run
         shop_messages: The messages from the SHOP run
         cplex_logs: The logs from CPLEX
         alerts: An array of calculation level Alerts.
+        output_timeseries: TODO
     """
 
-    view_id = dm.ViewId("sp_powerops_models", "SHOPResult", "1")
+    view_id = dm.ViewId("sp_powerops_models_temp", "SHOPResult", "1")
     case: Optional[CaseGraphQL] = Field(None, repr=False)
-    output_timeseries: Union[list[TimeSeries], list[str], None] = Field(None, alias="outputTimeseries")
     objective_sequence: Union[str, None] = Field(None, alias="objectiveSequence")
     pre_run: Union[str, None] = Field(None, alias="preRun")
     post_run: Union[str, None] = Field(None, alias="postRun")
     shop_messages: Union[str, None] = Field(None, alias="shopMessages")
     cplex_logs: Union[str, None] = Field(None, alias="cplexLogs")
     alerts: Optional[list[AlertGraphQL]] = Field(default=None, repr=False)
+    output_timeseries: Optional[list[SHOPTimeSeriesGraphQL]] = Field(default=None, repr=False, alias="outputTimeseries")
 
     @model_validator(mode="before")
     def parse_data_record(cls, values: Any) -> Any:
         if not isinstance(values, dict):
             return values
         if "lastUpdatedTime" in values or "createdTime" in values:
             values["dataRecord"] = DataRecordGraphQL(
                 created_time=values.pop("createdTime", None),
                 last_updated_time=values.pop("lastUpdatedTime", None),
             )
         return values
 
-    @field_validator("case", "alerts", mode="before")
+    @field_validator("case", "alerts", "output_timeseries", mode="before")
     def parse_graphql(cls, value: Any) -> Any:
         if not isinstance(value, dict):
             return value
         if "items" in value:
             return value["items"]
         return value
 
@@ -116,84 +110,95 @@
             external_id=self.external_id,
             data_record=DataRecord(
                 version=0,
                 last_updated_time=self.data_record.last_updated_time,
                 created_time=self.data_record.created_time,
             ),
             case=self.case.as_read() if isinstance(self.case, GraphQLCore) else self.case,
-            output_timeseries=self.output_timeseries,
             objective_sequence=self.objective_sequence,
             pre_run=self.pre_run,
             post_run=self.post_run,
             shop_messages=self.shop_messages,
             cplex_logs=self.cplex_logs,
             alerts=[alert.as_read() if isinstance(alert, GraphQLCore) else alert for alert in self.alerts or []],
+            output_timeseries=[
+                output_timesery.as_read() if isinstance(output_timesery, GraphQLCore) else output_timesery
+                for output_timesery in self.output_timeseries or []
+            ],
         )
 
     def as_write(self) -> SHOPResultWrite:
         """Convert this GraphQL format of shop result to the writing format."""
         return SHOPResultWrite(
             space=self.space,
             external_id=self.external_id,
             data_record=DataRecordWrite(existing_version=0),
             case=self.case.as_write() if isinstance(self.case, DomainModel) else self.case,
-            output_timeseries=self.output_timeseries,
             objective_sequence=self.objective_sequence,
             pre_run=self.pre_run,
             post_run=self.post_run,
             shop_messages=self.shop_messages,
             cplex_logs=self.cplex_logs,
             alerts=[alert.as_write() if isinstance(alert, DomainModel) else alert for alert in self.alerts or []],
+            output_timeseries=[
+                output_timesery.as_write() if isinstance(output_timesery, DomainModel) else output_timesery
+                for output_timesery in self.output_timeseries or []
+            ],
         )
 
 
 class SHOPResult(DomainModel):
     """This represents the reading version of shop result.
 
     It is used to when data is retrieved from CDF.
 
     Args:
         space: The space where the node is located.
         external_id: The external id of the shop result.
         data_record: The data record of the shop result node.
         case: The case that was used to produce this result
-        output_timeseries: A general placeholder for all timeseries that stem from a shop run
         objective_sequence: The sequence of the objective function
         pre_run: The pre-run data for the SHOP run
         post_run: The post-run data for the SHOP run
         shop_messages: The messages from the SHOP run
         cplex_logs: The logs from CPLEX
         alerts: An array of calculation level Alerts.
+        output_timeseries: TODO
     """
 
     space: str = DEFAULT_INSTANCE_SPACE
     node_type: Union[dm.DirectRelationReference, None] = None
     case: Union[Case, str, dm.NodeId, None] = Field(None, repr=False)
-    output_timeseries: Union[list[TimeSeries], list[str], None] = Field(None, alias="outputTimeseries")
     objective_sequence: Union[str, None] = Field(None, alias="objectiveSequence")
     pre_run: Union[str, None] = Field(None, alias="preRun")
     post_run: Union[str, None] = Field(None, alias="postRun")
     shop_messages: Union[str, None] = Field(None, alias="shopMessages")
     cplex_logs: Union[str, None] = Field(None, alias="cplexLogs")
     alerts: Union[list[Alert], list[str], list[dm.NodeId], None] = Field(default=None, repr=False)
+    output_timeseries: Union[list[SHOPTimeSeries], list[str], list[dm.NodeId], None] = Field(
+        default=None, repr=False, alias="outputTimeseries"
+    )
 
     def as_write(self) -> SHOPResultWrite:
         """Convert this read version of shop result to the writing version."""
         return SHOPResultWrite(
             space=self.space,
             external_id=self.external_id,
             data_record=DataRecordWrite(existing_version=self.data_record.version),
             case=self.case.as_write() if isinstance(self.case, DomainModel) else self.case,
-            output_timeseries=self.output_timeseries,
             objective_sequence=self.objective_sequence,
             pre_run=self.pre_run,
             post_run=self.post_run,
             shop_messages=self.shop_messages,
             cplex_logs=self.cplex_logs,
             alerts=[alert.as_write() if isinstance(alert, DomainModel) else alert for alert in self.alerts or []],
+            output_timeseries=[
+                output_timesery.as_write() if isinstance(output_timesery, DomainModel) else output_timesery
+                for output_timesery in self.output_timeseries or []
+            ],
         )
 
     def as_apply(self) -> SHOPResultWrite:
         """Convert this read version of shop result to the writing version."""
         warnings.warn(
             "as_apply is deprecated and will be removed in v1.0. Use as_write instead.",
             UserWarning,
@@ -208,60 +213,57 @@
     It is used to when data is sent to CDF.
 
     Args:
         space: The space where the node is located.
         external_id: The external id of the shop result.
         data_record: The data record of the shop result node.
         case: The case that was used to produce this result
-        output_timeseries: A general placeholder for all timeseries that stem from a shop run
         objective_sequence: The sequence of the objective function
         pre_run: The pre-run data for the SHOP run
         post_run: The post-run data for the SHOP run
         shop_messages: The messages from the SHOP run
         cplex_logs: The logs from CPLEX
         alerts: An array of calculation level Alerts.
+        output_timeseries: TODO
     """
 
     space: str = DEFAULT_INSTANCE_SPACE
     node_type: Union[dm.DirectRelationReference, None] = None
     case: Union[CaseWrite, str, dm.NodeId, None] = Field(None, repr=False)
-    output_timeseries: Union[list[TimeSeries], list[str], None] = Field(None, alias="outputTimeseries")
     objective_sequence: Union[str, None] = Field(None, alias="objectiveSequence")
     pre_run: Union[str, None] = Field(None, alias="preRun")
     post_run: Union[str, None] = Field(None, alias="postRun")
     shop_messages: Union[str, None] = Field(None, alias="shopMessages")
     cplex_logs: Union[str, None] = Field(None, alias="cplexLogs")
     alerts: Union[list[AlertWrite], list[str], list[dm.NodeId], None] = Field(default=None, repr=False)
+    output_timeseries: Union[list[SHOPTimeSeriesWrite], list[str], list[dm.NodeId], None] = Field(
+        default=None, repr=False, alias="outputTimeseries"
+    )
 
     def _to_instances_write(
         self,
         cache: set[tuple[str, str]],
         view_by_read_class: dict[type[DomainModelCore], dm.ViewId] | None,
         write_none: bool = False,
         allow_version_increase: bool = False,
     ) -> ResourcesWrite:
         resources = ResourcesWrite()
         if self.as_tuple_id() in cache:
             return resources
 
-        write_view = (view_by_read_class or {}).get(SHOPResult, dm.ViewId("sp_powerops_models", "SHOPResult", "1"))
+        write_view = (view_by_read_class or {}).get(SHOPResult, dm.ViewId("sp_powerops_models_temp", "SHOPResult", "1"))
 
         properties: dict[str, Any] = {}
 
         if self.case is not None:
             properties["case"] = {
                 "space": self.space if isinstance(self.case, str) else self.case.space,
                 "externalId": self.case if isinstance(self.case, str) else self.case.external_id,
             }
 
-        if self.output_timeseries is not None or write_none:
-            properties["outputTimeseries"] = [
-                value if isinstance(value, str) else value.external_id for value in self.output_timeseries or []
-            ] or None
-
         if self.objective_sequence is not None or write_none:
             properties["objectiveSequence"] = self.objective_sequence
 
         if self.pre_run is not None or write_none:
             properties["preRun"] = self.pre_run
 
         if self.post_run is not None or write_none:
@@ -285,34 +287,44 @@
                         properties=properties,
                     )
                 ],
             )
             resources.nodes.append(this_node)
             cache.add(self.as_tuple_id())
 
-        edge_type = dm.DirectRelationReference("sp_powerops_types", "calculationIssue")
+        edge_type = dm.DirectRelationReference("sp_powerops_types_temp", "calculationIssue")
         for alert in self.alerts or []:
             other_resources = DomainRelationWrite.from_edge_to_resources(
                 cache,
                 start_node=self,
                 end_node=alert,
                 edge_type=edge_type,
                 view_by_read_class=view_by_read_class,
                 write_none=write_none,
                 allow_version_increase=allow_version_increase,
             )
             resources.extend(other_resources)
 
+        edge_type = dm.DirectRelationReference("sp_powerops_types_temp", "SHOPResult.outputTimeseries")
+        for output_timesery in self.output_timeseries or []:
+            other_resources = DomainRelationWrite.from_edge_to_resources(
+                cache,
+                start_node=self,
+                end_node=output_timesery,
+                edge_type=edge_type,
+                view_by_read_class=view_by_read_class,
+                write_none=write_none,
+                allow_version_increase=allow_version_increase,
+            )
+            resources.extend(other_resources)
+
         if isinstance(self.case, DomainModelWrite):
             other_resources = self.case._to_instances_write(cache, view_by_read_class)
             resources.extend(other_resources)
 
-        if isinstance(self.output_timeseries, CogniteTimeSeries):
-            resources.time_series.append(self.output_timeseries)
-
         return resources
 
 
 class SHOPResultApply(SHOPResultWrite):
     def __new__(cls, *args, **kwargs) -> SHOPResultApply:
         warnings.warn(
             "SHOPResultApply is deprecated and will be removed in v1.0. Use SHOPResultWrite instead."
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_shop_result_price_prod.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_shop_result_price_prod.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_shop_trigger_input.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_water_partial_bid_calculation_input.py`

 * *Files 22% similar despite different names*

```diff
@@ -19,214 +19,218 @@
     DomainModelList,
     DomainRelationWrite,
     GraphQLCore,
     ResourcesWrite,
 )
 
 if TYPE_CHECKING:
-    from ._scenario import Scenario, ScenarioGraphQL, ScenarioWrite
+    from ._bid_calculation_task import BidCalculationTask, BidCalculationTaskGraphQL, BidCalculationTaskWrite
 
 
 __all__ = [
-    "SHOPTriggerInput",
-    "SHOPTriggerInputWrite",
-    "SHOPTriggerInputApply",
-    "SHOPTriggerInputList",
-    "SHOPTriggerInputWriteList",
-    "SHOPTriggerInputApplyList",
-    "SHOPTriggerInputFields",
-    "SHOPTriggerInputTextFields",
+    "WaterPartialBidCalculationInput",
+    "WaterPartialBidCalculationInputWrite",
+    "WaterPartialBidCalculationInputApply",
+    "WaterPartialBidCalculationInputList",
+    "WaterPartialBidCalculationInputWriteList",
+    "WaterPartialBidCalculationInputApplyList",
+    "WaterPartialBidCalculationInputFields",
+    "WaterPartialBidCalculationInputTextFields",
 ]
 
 
-SHOPTriggerInputTextFields = Literal["process_id", "function_name", "function_call_id", "cog_shop_tag"]
-SHOPTriggerInputFields = Literal["process_id", "process_step", "function_name", "function_call_id", "cog_shop_tag"]
+WaterPartialBidCalculationInputTextFields = Literal["process_id", "function_name", "function_call_id"]
+WaterPartialBidCalculationInputFields = Literal["process_id", "process_step", "function_name", "function_call_id"]
 
-_SHOPTRIGGERINPUT_PROPERTIES_BY_FIELD = {
+_WATERPARTIALBIDCALCULATIONINPUT_PROPERTIES_BY_FIELD = {
     "process_id": "processId",
     "process_step": "processStep",
     "function_name": "functionName",
     "function_call_id": "functionCallId",
-    "cog_shop_tag": "cogShopTag",
 }
 
 
-class SHOPTriggerInputGraphQL(GraphQLCore):
-    """This represents the reading version of shop trigger input, used
+class WaterPartialBidCalculationInputGraphQL(GraphQLCore):
+    """This represents the reading version of water partial bid calculation input, used
     when data is retrieved from CDF using GraphQL.
 
     It is used when retrieving data from CDF using GraphQL.
 
     Args:
         space: The space where the node is located.
-        external_id: The external id of the shop trigger input.
-        data_record: The data record of the shop trigger input node.
+        external_id: The external id of the water partial bid calculation input.
+        data_record: The data record of the water partial bid calculation input node.
         process_id: The process associated with the function execution
         process_step: This is the step in the process.
         function_name: The name of the function
         function_call_id: The function call id
-        cog_shop_tag: Optionally specify cogshop tag to trigger
-        scenario: The scenario that is used in the shop run
+        calculation_task: The calculation task field.
     """
 
-    view_id = dm.ViewId("sp_powerops_models", "SHOPTriggerInput", "1")
+    view_id = dm.ViewId("sp_powerops_models", "WaterPartialBidCalculationInput", "1")
     process_id: Optional[str] = Field(None, alias="processId")
     process_step: Optional[int] = Field(None, alias="processStep")
     function_name: Optional[str] = Field(None, alias="functionName")
     function_call_id: Optional[str] = Field(None, alias="functionCallId")
-    cog_shop_tag: Optional[str] = Field(None, alias="cogShopTag")
-    scenario: Optional[ScenarioGraphQL] = Field(None, repr=False)
+    calculation_task: Optional[BidCalculationTaskGraphQL] = Field(None, repr=False, alias="calculationTask")
 
     @model_validator(mode="before")
     def parse_data_record(cls, values: Any) -> Any:
         if not isinstance(values, dict):
             return values
         if "lastUpdatedTime" in values or "createdTime" in values:
             values["dataRecord"] = DataRecordGraphQL(
                 created_time=values.pop("createdTime", None),
                 last_updated_time=values.pop("lastUpdatedTime", None),
             )
         return values
 
-    @field_validator("scenario", mode="before")
+    @field_validator("calculation_task", mode="before")
     def parse_graphql(cls, value: Any) -> Any:
         if not isinstance(value, dict):
             return value
         if "items" in value:
             return value["items"]
         return value
 
-    def as_read(self) -> SHOPTriggerInput:
-        """Convert this GraphQL format of shop trigger input to the reading format."""
+    def as_read(self) -> WaterPartialBidCalculationInput:
+        """Convert this GraphQL format of water partial bid calculation input to the reading format."""
         if self.data_record is None:
             raise ValueError("This object cannot be converted to a read format because it lacks a data record.")
-        return SHOPTriggerInput(
+        return WaterPartialBidCalculationInput(
             space=self.space,
             external_id=self.external_id,
             data_record=DataRecord(
                 version=0,
                 last_updated_time=self.data_record.last_updated_time,
                 created_time=self.data_record.created_time,
             ),
             process_id=self.process_id,
             process_step=self.process_step,
             function_name=self.function_name,
             function_call_id=self.function_call_id,
-            cog_shop_tag=self.cog_shop_tag,
-            scenario=self.scenario.as_read() if isinstance(self.scenario, GraphQLCore) else self.scenario,
+            calculation_task=(
+                self.calculation_task.as_read()
+                if isinstance(self.calculation_task, GraphQLCore)
+                else self.calculation_task
+            ),
         )
 
-    def as_write(self) -> SHOPTriggerInputWrite:
-        """Convert this GraphQL format of shop trigger input to the writing format."""
-        return SHOPTriggerInputWrite(
+    def as_write(self) -> WaterPartialBidCalculationInputWrite:
+        """Convert this GraphQL format of water partial bid calculation input to the writing format."""
+        return WaterPartialBidCalculationInputWrite(
             space=self.space,
             external_id=self.external_id,
             data_record=DataRecordWrite(existing_version=0),
             process_id=self.process_id,
             process_step=self.process_step,
             function_name=self.function_name,
             function_call_id=self.function_call_id,
-            cog_shop_tag=self.cog_shop_tag,
-            scenario=self.scenario.as_write() if isinstance(self.scenario, DomainModel) else self.scenario,
+            calculation_task=(
+                self.calculation_task.as_write()
+                if isinstance(self.calculation_task, DomainModel)
+                else self.calculation_task
+            ),
         )
 
 
-class SHOPTriggerInput(DomainModel):
-    """This represents the reading version of shop trigger input.
+class WaterPartialBidCalculationInput(DomainModel):
+    """This represents the reading version of water partial bid calculation input.
 
     It is used to when data is retrieved from CDF.
 
     Args:
         space: The space where the node is located.
-        external_id: The external id of the shop trigger input.
-        data_record: The data record of the shop trigger input node.
+        external_id: The external id of the water partial bid calculation input.
+        data_record: The data record of the water partial bid calculation input node.
         process_id: The process associated with the function execution
         process_step: This is the step in the process.
         function_name: The name of the function
         function_call_id: The function call id
-        cog_shop_tag: Optionally specify cogshop tag to trigger
-        scenario: The scenario that is used in the shop run
+        calculation_task: The calculation task field.
     """
 
     space: str = DEFAULT_INSTANCE_SPACE
     node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference(
-        "sp_powerops_types", "SHOPTriggerInput"
+        "sp_powerops_types", "WaterPartialBidCalculationInput"
     )
     process_id: str = Field(alias="processId")
     process_step: int = Field(alias="processStep")
     function_name: str = Field(alias="functionName")
     function_call_id: str = Field(alias="functionCallId")
-    cog_shop_tag: Optional[str] = Field(None, alias="cogShopTag")
-    scenario: Union[Scenario, str, dm.NodeId, None] = Field(None, repr=False)
+    calculation_task: Union[BidCalculationTask, str, dm.NodeId, None] = Field(None, repr=False, alias="calculationTask")
 
-    def as_write(self) -> SHOPTriggerInputWrite:
-        """Convert this read version of shop trigger input to the writing version."""
-        return SHOPTriggerInputWrite(
+    def as_write(self) -> WaterPartialBidCalculationInputWrite:
+        """Convert this read version of water partial bid calculation input to the writing version."""
+        return WaterPartialBidCalculationInputWrite(
             space=self.space,
             external_id=self.external_id,
             data_record=DataRecordWrite(existing_version=self.data_record.version),
             process_id=self.process_id,
             process_step=self.process_step,
             function_name=self.function_name,
             function_call_id=self.function_call_id,
-            cog_shop_tag=self.cog_shop_tag,
-            scenario=self.scenario.as_write() if isinstance(self.scenario, DomainModel) else self.scenario,
+            calculation_task=(
+                self.calculation_task.as_write()
+                if isinstance(self.calculation_task, DomainModel)
+                else self.calculation_task
+            ),
         )
 
-    def as_apply(self) -> SHOPTriggerInputWrite:
-        """Convert this read version of shop trigger input to the writing version."""
+    def as_apply(self) -> WaterPartialBidCalculationInputWrite:
+        """Convert this read version of water partial bid calculation input to the writing version."""
         warnings.warn(
             "as_apply is deprecated and will be removed in v1.0. Use as_write instead.",
             UserWarning,
             stacklevel=2,
         )
         return self.as_write()
 
 
-class SHOPTriggerInputWrite(DomainModelWrite):
-    """This represents the writing version of shop trigger input.
+class WaterPartialBidCalculationInputWrite(DomainModelWrite):
+    """This represents the writing version of water partial bid calculation input.
 
     It is used to when data is sent to CDF.
 
     Args:
         space: The space where the node is located.
-        external_id: The external id of the shop trigger input.
-        data_record: The data record of the shop trigger input node.
+        external_id: The external id of the water partial bid calculation input.
+        data_record: The data record of the water partial bid calculation input node.
         process_id: The process associated with the function execution
         process_step: This is the step in the process.
         function_name: The name of the function
         function_call_id: The function call id
-        cog_shop_tag: Optionally specify cogshop tag to trigger
-        scenario: The scenario that is used in the shop run
+        calculation_task: The calculation task field.
     """
 
     space: str = DEFAULT_INSTANCE_SPACE
     node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference(
-        "sp_powerops_types", "SHOPTriggerInput"
+        "sp_powerops_types", "WaterPartialBidCalculationInput"
     )
     process_id: str = Field(alias="processId")
     process_step: int = Field(alias="processStep")
     function_name: str = Field(alias="functionName")
     function_call_id: str = Field(alias="functionCallId")
-    cog_shop_tag: Optional[str] = Field(None, alias="cogShopTag")
-    scenario: Union[ScenarioWrite, str, dm.NodeId, None] = Field(None, repr=False)
+    calculation_task: Union[BidCalculationTaskWrite, str, dm.NodeId, None] = Field(
+        None, repr=False, alias="calculationTask"
+    )
 
     def _to_instances_write(
         self,
         cache: set[tuple[str, str]],
         view_by_read_class: dict[type[DomainModelCore], dm.ViewId] | None,
         write_none: bool = False,
         allow_version_increase: bool = False,
     ) -> ResourcesWrite:
         resources = ResourcesWrite()
         if self.as_tuple_id() in cache:
             return resources
 
         write_view = (view_by_read_class or {}).get(
-            SHOPTriggerInput, dm.ViewId("sp_powerops_models", "SHOPTriggerInput", "1")
+            WaterPartialBidCalculationInput, dm.ViewId("sp_powerops_models", "WaterPartialBidCalculationInput", "1")
         )
 
         properties: dict[str, Any] = {}
 
         if self.process_id is not None:
             properties["processId"] = self.process_id
 
@@ -235,21 +239,22 @@
 
         if self.function_name is not None:
             properties["functionName"] = self.function_name
 
         if self.function_call_id is not None:
             properties["functionCallId"] = self.function_call_id
 
-        if self.cog_shop_tag is not None or write_none:
-            properties["cogShopTag"] = self.cog_shop_tag
-
-        if self.scenario is not None:
-            properties["scenario"] = {
-                "space": self.space if isinstance(self.scenario, str) else self.scenario.space,
-                "externalId": self.scenario if isinstance(self.scenario, str) else self.scenario.external_id,
+        if self.calculation_task is not None:
+            properties["calculationTask"] = {
+                "space": self.space if isinstance(self.calculation_task, str) else self.calculation_task.space,
+                "externalId": (
+                    self.calculation_task
+                    if isinstance(self.calculation_task, str)
+                    else self.calculation_task.external_id
+                ),
             }
 
         if properties:
             this_node = dm.NodeApply(
                 space=self.space,
                 external_id=self.external_id,
                 existing_version=None if allow_version_increase else self.data_record.existing_version,
@@ -260,74 +265,72 @@
                         properties=properties,
                     )
                 ],
             )
             resources.nodes.append(this_node)
             cache.add(self.as_tuple_id())
 
-        if isinstance(self.scenario, DomainModelWrite):
-            other_resources = self.scenario._to_instances_write(cache, view_by_read_class)
+        if isinstance(self.calculation_task, DomainModelWrite):
+            other_resources = self.calculation_task._to_instances_write(cache, view_by_read_class)
             resources.extend(other_resources)
 
         return resources
 
 
-class SHOPTriggerInputApply(SHOPTriggerInputWrite):
-    def __new__(cls, *args, **kwargs) -> SHOPTriggerInputApply:
+class WaterPartialBidCalculationInputApply(WaterPartialBidCalculationInputWrite):
+    def __new__(cls, *args, **kwargs) -> WaterPartialBidCalculationInputApply:
         warnings.warn(
-            "SHOPTriggerInputApply is deprecated and will be removed in v1.0. Use SHOPTriggerInputWrite instead."
+            "WaterPartialBidCalculationInputApply is deprecated and will be removed in v1.0. Use WaterPartialBidCalculationInputWrite instead."
             "The motivation for this change is that Write is a more descriptive name for the writing version of the"
-            "SHOPTriggerInput.",
+            "WaterPartialBidCalculationInput.",
             UserWarning,
             stacklevel=2,
         )
         return super().__new__(cls)
 
 
-class SHOPTriggerInputList(DomainModelList[SHOPTriggerInput]):
-    """List of shop trigger inputs in the read version."""
+class WaterPartialBidCalculationInputList(DomainModelList[WaterPartialBidCalculationInput]):
+    """List of water partial bid calculation inputs in the read version."""
 
-    _INSTANCE = SHOPTriggerInput
+    _INSTANCE = WaterPartialBidCalculationInput
 
-    def as_write(self) -> SHOPTriggerInputWriteList:
-        """Convert these read versions of shop trigger input to the writing versions."""
-        return SHOPTriggerInputWriteList([node.as_write() for node in self.data])
+    def as_write(self) -> WaterPartialBidCalculationInputWriteList:
+        """Convert these read versions of water partial bid calculation input to the writing versions."""
+        return WaterPartialBidCalculationInputWriteList([node.as_write() for node in self.data])
 
-    def as_apply(self) -> SHOPTriggerInputWriteList:
+    def as_apply(self) -> WaterPartialBidCalculationInputWriteList:
         """Convert these read versions of primitive nullable to the writing versions."""
         warnings.warn(
             "as_apply is deprecated and will be removed in v1.0. Use as_write instead.",
             UserWarning,
             stacklevel=2,
         )
         return self.as_write()
 
 
-class SHOPTriggerInputWriteList(DomainModelWriteList[SHOPTriggerInputWrite]):
-    """List of shop trigger inputs in the writing version."""
+class WaterPartialBidCalculationInputWriteList(DomainModelWriteList[WaterPartialBidCalculationInputWrite]):
+    """List of water partial bid calculation inputs in the writing version."""
 
-    _INSTANCE = SHOPTriggerInputWrite
+    _INSTANCE = WaterPartialBidCalculationInputWrite
 
 
-class SHOPTriggerInputApplyList(SHOPTriggerInputWriteList): ...
+class WaterPartialBidCalculationInputApplyList(WaterPartialBidCalculationInputWriteList): ...
 
 
-def _create_shop_trigger_input_filter(
+def _create_water_partial_bid_calculation_input_filter(
     view_id: dm.ViewId,
     process_id: str | list[str] | None = None,
     process_id_prefix: str | None = None,
     min_process_step: int | None = None,
     max_process_step: int | None = None,
     function_name: str | list[str] | None = None,
     function_name_prefix: str | None = None,
     function_call_id: str | list[str] | None = None,
     function_call_id_prefix: str | None = None,
-    cog_shop_tag: str | list[str] | None = None,
-    cog_shop_tag_prefix: str | None = None,
-    scenario: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+    calculation_task: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
     external_id_prefix: str | None = None,
     space: str | list[str] | None = None,
     filter: dm.Filter | None = None,
 ) -> dm.Filter | None:
     filters = []
     if isinstance(process_id, str):
         filters.append(dm.filters.Equals(view_id.as_property_ref("processId"), value=process_id))
@@ -347,44 +350,40 @@
         filters.append(dm.filters.Prefix(view_id.as_property_ref("functionName"), value=function_name_prefix))
     if isinstance(function_call_id, str):
         filters.append(dm.filters.Equals(view_id.as_property_ref("functionCallId"), value=function_call_id))
     if function_call_id and isinstance(function_call_id, list):
         filters.append(dm.filters.In(view_id.as_property_ref("functionCallId"), values=function_call_id))
     if function_call_id_prefix is not None:
         filters.append(dm.filters.Prefix(view_id.as_property_ref("functionCallId"), value=function_call_id_prefix))
-    if isinstance(cog_shop_tag, str):
-        filters.append(dm.filters.Equals(view_id.as_property_ref("cogShopTag"), value=cog_shop_tag))
-    if cog_shop_tag and isinstance(cog_shop_tag, list):
-        filters.append(dm.filters.In(view_id.as_property_ref("cogShopTag"), values=cog_shop_tag))
-    if cog_shop_tag_prefix is not None:
-        filters.append(dm.filters.Prefix(view_id.as_property_ref("cogShopTag"), value=cog_shop_tag_prefix))
-    if scenario and isinstance(scenario, str):
+    if calculation_task and isinstance(calculation_task, str):
         filters.append(
             dm.filters.Equals(
-                view_id.as_property_ref("scenario"), value={"space": DEFAULT_INSTANCE_SPACE, "externalId": scenario}
+                view_id.as_property_ref("calculationTask"),
+                value={"space": DEFAULT_INSTANCE_SPACE, "externalId": calculation_task},
             )
         )
-    if scenario and isinstance(scenario, tuple):
+    if calculation_task and isinstance(calculation_task, tuple):
         filters.append(
             dm.filters.Equals(
-                view_id.as_property_ref("scenario"), value={"space": scenario[0], "externalId": scenario[1]}
+                view_id.as_property_ref("calculationTask"),
+                value={"space": calculation_task[0], "externalId": calculation_task[1]},
             )
         )
-    if scenario and isinstance(scenario, list) and isinstance(scenario[0], str):
+    if calculation_task and isinstance(calculation_task, list) and isinstance(calculation_task[0], str):
         filters.append(
             dm.filters.In(
-                view_id.as_property_ref("scenario"),
-                values=[{"space": DEFAULT_INSTANCE_SPACE, "externalId": item} for item in scenario],
+                view_id.as_property_ref("calculationTask"),
+                values=[{"space": DEFAULT_INSTANCE_SPACE, "externalId": item} for item in calculation_task],
             )
         )
-    if scenario and isinstance(scenario, list) and isinstance(scenario[0], tuple):
+    if calculation_task and isinstance(calculation_task, list) and isinstance(calculation_task[0], tuple):
         filters.append(
             dm.filters.In(
-                view_id.as_property_ref("scenario"),
-                values=[{"space": item[0], "externalId": item[1]} for item in scenario],
+                view_id.as_property_ref("calculationTask"),
+                values=[{"space": item[0], "externalId": item[1]} for item in calculation_task],
             )
         )
     if external_id_prefix is not None:
         filters.append(dm.filters.Prefix(["node", "externalId"], value=external_id_prefix))
     if isinstance(space, str):
         filters.append(dm.filters.Equals(["node", "space"], value=space))
     if space and isinstance(space, list):
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_shop_trigger_output.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_shop_trigger_output.py`

 * *Files 3% similar despite different names*

```diff
@@ -17,14 +17,15 @@
     DomainModelWrite,
     DomainModelWriteList,
     DomainModelList,
     DomainRelationWrite,
     GraphQLCore,
     ResourcesWrite,
 )
+from ._function_output import FunctionOutput, FunctionOutputWrite
 
 if TYPE_CHECKING:
     from ._alert import Alert, AlertGraphQL, AlertWrite
     from ._shop_result import SHOPResult, SHOPResultGraphQL, SHOPResultWrite
     from ._shop_trigger_input import SHOPTriggerInput, SHOPTriggerInputGraphQL, SHOPTriggerInputWrite
 
 
@@ -66,15 +67,15 @@
         function_name: The name of the function
         function_call_id: The function call id
         alerts: An array of calculation level Alerts.
         shop_result: The shop result field.
         input_: The prepped and processed scenario to send to shop trigger
     """
 
-    view_id = dm.ViewId("sp_powerops_models", "SHOPTriggerOutput", "1")
+    view_id = dm.ViewId("sp_powerops_models_temp", "SHOPTriggerOutput", "1")
     process_id: Optional[str] = Field(None, alias="processId")
     process_step: Optional[int] = Field(None, alias="processStep")
     function_name: Optional[str] = Field(None, alias="functionName")
     function_call_id: Optional[str] = Field(None, alias="functionCallId")
     alerts: Optional[list[AlertGraphQL]] = Field(default=None, repr=False)
     shop_result: Optional[SHOPResultGraphQL] = Field(None, repr=False, alias="shopResult")
     input_: Optional[SHOPTriggerInputGraphQL] = Field(None, repr=False, alias="input")
@@ -131,15 +132,15 @@
             function_call_id=self.function_call_id,
             alerts=[alert.as_write() if isinstance(alert, DomainModel) else alert for alert in self.alerts or []],
             shop_result=self.shop_result.as_write() if isinstance(self.shop_result, DomainModel) else self.shop_result,
             input_=self.input_.as_write() if isinstance(self.input_, DomainModel) else self.input_,
         )
 
 
-class SHOPTriggerOutput(DomainModel):
+class SHOPTriggerOutput(FunctionOutput):
     """This represents the reading version of shop trigger output.
 
     It is used to when data is retrieved from CDF.
 
     Args:
         space: The space where the node is located.
         external_id: The external id of the shop trigger output.
@@ -149,23 +150,17 @@
         function_name: The name of the function
         function_call_id: The function call id
         alerts: An array of calculation level Alerts.
         shop_result: The shop result field.
         input_: The prepped and processed scenario to send to shop trigger
     """
 
-    space: str = DEFAULT_INSTANCE_SPACE
     node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference(
-        "sp_powerops_types", "SHOPTriggerOutput"
+        "sp_powerops_types_temp", "SHOPTriggerOutput"
     )
-    process_id: str = Field(alias="processId")
-    process_step: int = Field(alias="processStep")
-    function_name: str = Field(alias="functionName")
-    function_call_id: str = Field(alias="functionCallId")
-    alerts: Union[list[Alert], list[str], list[dm.NodeId], None] = Field(default=None, repr=False)
     shop_result: Union[SHOPResult, str, dm.NodeId, None] = Field(None, repr=False, alias="shopResult")
     input_: Union[SHOPTriggerInput, str, dm.NodeId, None] = Field(None, repr=False, alias="input")
 
     def as_write(self) -> SHOPTriggerOutputWrite:
         """Convert this read version of shop trigger output to the writing version."""
         return SHOPTriggerOutputWrite(
             space=self.space,
@@ -186,15 +181,15 @@
             "as_apply is deprecated and will be removed in v1.0. Use as_write instead.",
             UserWarning,
             stacklevel=2,
         )
         return self.as_write()
 
 
-class SHOPTriggerOutputWrite(DomainModelWrite):
+class SHOPTriggerOutputWrite(FunctionOutputWrite):
     """This represents the writing version of shop trigger output.
 
     It is used to when data is sent to CDF.
 
     Args:
         space: The space where the node is located.
         external_id: The external id of the shop trigger output.
@@ -204,23 +199,17 @@
         function_name: The name of the function
         function_call_id: The function call id
         alerts: An array of calculation level Alerts.
         shop_result: The shop result field.
         input_: The prepped and processed scenario to send to shop trigger
     """
 
-    space: str = DEFAULT_INSTANCE_SPACE
     node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference(
-        "sp_powerops_types", "SHOPTriggerOutput"
+        "sp_powerops_types_temp", "SHOPTriggerOutput"
     )
-    process_id: str = Field(alias="processId")
-    process_step: int = Field(alias="processStep")
-    function_name: str = Field(alias="functionName")
-    function_call_id: str = Field(alias="functionCallId")
-    alerts: Union[list[AlertWrite], list[str], list[dm.NodeId], None] = Field(default=None, repr=False)
     shop_result: Union[SHOPResultWrite, str, dm.NodeId, None] = Field(None, repr=False, alias="shopResult")
     input_: Union[SHOPTriggerInputWrite, str, dm.NodeId, None] = Field(None, repr=False, alias="input")
 
     def _to_instances_write(
         self,
         cache: set[tuple[str, str]],
         view_by_read_class: dict[type[DomainModelCore], dm.ViewId] | None,
@@ -228,15 +217,15 @@
         allow_version_increase: bool = False,
     ) -> ResourcesWrite:
         resources = ResourcesWrite()
         if self.as_tuple_id() in cache:
             return resources
 
         write_view = (view_by_read_class or {}).get(
-            SHOPTriggerOutput, dm.ViewId("sp_powerops_models", "SHOPTriggerOutput", "1")
+            SHOPTriggerOutput, dm.ViewId("sp_powerops_models_temp", "SHOPTriggerOutput", "1")
         )
 
         properties: dict[str, Any] = {}
 
         if self.process_id is not None:
             properties["processId"] = self.process_id
 
@@ -273,15 +262,15 @@
                         properties=properties,
                     )
                 ],
             )
             resources.nodes.append(this_node)
             cache.add(self.as_tuple_id())
 
-        edge_type = dm.DirectRelationReference("sp_powerops_types", "calculationIssue")
+        edge_type = dm.DirectRelationReference("sp_powerops_types_temp", "calculationIssue")
         for alert in self.alerts or []:
             other_resources = DomainRelationWrite.from_edge_to_resources(
                 cache,
                 start_node=self,
                 end_node=alert,
                 edge_type=edge_type,
                 view_by_read_class=view_by_read_class,
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_task_dispatcher_shop_input.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_task_dispatcher_shop_input.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_task_dispatcher_shop_output.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_task_dispatcher_shop_output.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_task_dispatcher_water_input.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_task_dispatcher_water_input.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_task_dispatcher_water_output.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_task_dispatcher_water_output.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_total_bid_matrix_calculation_input.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_partial_bid_matrix_calculation_input.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 from __future__ import annotations
 
+import datetime
 import warnings
 from typing import TYPE_CHECKING, Any, Literal, Optional, Union
 
 from cognite.client import data_modeling as dm
 from pydantic import Field
 from pydantic import field_validator, model_validator
 
@@ -17,219 +18,220 @@
     DomainModelWrite,
     DomainModelWriteList,
     DomainModelList,
     DomainRelationWrite,
     GraphQLCore,
     ResourcesWrite,
 )
+from ._function_input import FunctionInput, FunctionInputWrite
 
 if TYPE_CHECKING:
-    from ._bid_matrix import BidMatrix, BidMatrixGraphQL, BidMatrixWrite
+    from ._bid_configuration import BidConfiguration, BidConfigurationGraphQL, BidConfigurationWrite
 
 
 __all__ = [
-    "TotalBidMatrixCalculationInput",
-    "TotalBidMatrixCalculationInputWrite",
-    "TotalBidMatrixCalculationInputApply",
-    "TotalBidMatrixCalculationInputList",
-    "TotalBidMatrixCalculationInputWriteList",
-    "TotalBidMatrixCalculationInputApplyList",
-    "TotalBidMatrixCalculationInputFields",
-    "TotalBidMatrixCalculationInputTextFields",
+    "PartialBidMatrixCalculationInput",
+    "PartialBidMatrixCalculationInputWrite",
+    "PartialBidMatrixCalculationInputApply",
+    "PartialBidMatrixCalculationInputList",
+    "PartialBidMatrixCalculationInputWriteList",
+    "PartialBidMatrixCalculationInputApplyList",
+    "PartialBidMatrixCalculationInputFields",
+    "PartialBidMatrixCalculationInputTextFields",
 ]
 
 
-TotalBidMatrixCalculationInputTextFields = Literal["process_id", "function_name", "function_call_id"]
-TotalBidMatrixCalculationInputFields = Literal["process_id", "process_step", "function_name", "function_call_id"]
+PartialBidMatrixCalculationInputTextFields = Literal["process_id", "function_name", "function_call_id"]
+PartialBidMatrixCalculationInputFields = Literal[
+    "process_id", "process_step", "function_name", "function_call_id", "bid_date"
+]
 
-_TOTALBIDMATRIXCALCULATIONINPUT_PROPERTIES_BY_FIELD = {
+_PARTIALBIDMATRIXCALCULATIONINPUT_PROPERTIES_BY_FIELD = {
     "process_id": "processId",
     "process_step": "processStep",
     "function_name": "functionName",
     "function_call_id": "functionCallId",
+    "bid_date": "bidDate",
 }
 
 
-class TotalBidMatrixCalculationInputGraphQL(GraphQLCore):
-    """This represents the reading version of total bid matrix calculation input, used
+class PartialBidMatrixCalculationInputGraphQL(GraphQLCore):
+    """This represents the reading version of partial bid matrix calculation input, used
     when data is retrieved from CDF using GraphQL.
 
     It is used when retrieving data from CDF using GraphQL.
 
     Args:
         space: The space where the node is located.
-        external_id: The external id of the total bid matrix calculation input.
-        data_record: The data record of the total bid matrix calculation input node.
+        external_id: The external id of the partial bid matrix calculation input.
+        data_record: The data record of the partial bid matrix calculation input node.
         process_id: The process associated with the function execution
         process_step: This is the step in the process.
         function_name: The name of the function
         function_call_id: The function call id
-        partial_bid_matrices: The partial bid matrices that are used to calculate the total bid matrix.
+        bid_date: The bid date
+        bid_configuration: TODO description
     """
 
-    view_id = dm.ViewId("sp_powerops_models", "TotalBidMatrixCalculationInput", "1")
+    view_id = dm.ViewId("sp_powerops_models_temp", "PartialBidMatrixCalculationInput", "1")
     process_id: Optional[str] = Field(None, alias="processId")
     process_step: Optional[int] = Field(None, alias="processStep")
     function_name: Optional[str] = Field(None, alias="functionName")
     function_call_id: Optional[str] = Field(None, alias="functionCallId")
-    partial_bid_matrices: Optional[list[BidMatrixGraphQL]] = Field(default=None, repr=False, alias="partialBidMatrices")
+    bid_date: Optional[datetime.date] = Field(None, alias="bidDate")
+    bid_configuration: Optional[BidConfigurationGraphQL] = Field(None, repr=False, alias="bidConfiguration")
 
     @model_validator(mode="before")
     def parse_data_record(cls, values: Any) -> Any:
         if not isinstance(values, dict):
             return values
         if "lastUpdatedTime" in values or "createdTime" in values:
             values["dataRecord"] = DataRecordGraphQL(
                 created_time=values.pop("createdTime", None),
                 last_updated_time=values.pop("lastUpdatedTime", None),
             )
         return values
 
-    @field_validator("partial_bid_matrices", mode="before")
+    @field_validator("bid_configuration", mode="before")
     def parse_graphql(cls, value: Any) -> Any:
         if not isinstance(value, dict):
             return value
         if "items" in value:
             return value["items"]
         return value
 
-    def as_read(self) -> TotalBidMatrixCalculationInput:
-        """Convert this GraphQL format of total bid matrix calculation input to the reading format."""
+    def as_read(self) -> PartialBidMatrixCalculationInput:
+        """Convert this GraphQL format of partial bid matrix calculation input to the reading format."""
         if self.data_record is None:
             raise ValueError("This object cannot be converted to a read format because it lacks a data record.")
-        return TotalBidMatrixCalculationInput(
+        return PartialBidMatrixCalculationInput(
             space=self.space,
             external_id=self.external_id,
             data_record=DataRecord(
                 version=0,
                 last_updated_time=self.data_record.last_updated_time,
                 created_time=self.data_record.created_time,
             ),
             process_id=self.process_id,
             process_step=self.process_step,
             function_name=self.function_name,
             function_call_id=self.function_call_id,
-            partial_bid_matrices=[
-                partial_bid_matrice.as_read() if isinstance(partial_bid_matrice, GraphQLCore) else partial_bid_matrice
-                for partial_bid_matrice in self.partial_bid_matrices or []
-            ],
+            bid_date=self.bid_date,
+            bid_configuration=(
+                self.bid_configuration.as_read()
+                if isinstance(self.bid_configuration, GraphQLCore)
+                else self.bid_configuration
+            ),
         )
 
-    def as_write(self) -> TotalBidMatrixCalculationInputWrite:
-        """Convert this GraphQL format of total bid matrix calculation input to the writing format."""
-        return TotalBidMatrixCalculationInputWrite(
+    def as_write(self) -> PartialBidMatrixCalculationInputWrite:
+        """Convert this GraphQL format of partial bid matrix calculation input to the writing format."""
+        return PartialBidMatrixCalculationInputWrite(
             space=self.space,
             external_id=self.external_id,
             data_record=DataRecordWrite(existing_version=0),
             process_id=self.process_id,
             process_step=self.process_step,
             function_name=self.function_name,
             function_call_id=self.function_call_id,
-            partial_bid_matrices=[
-                partial_bid_matrice.as_write() if isinstance(partial_bid_matrice, DomainModel) else partial_bid_matrice
-                for partial_bid_matrice in self.partial_bid_matrices or []
-            ],
+            bid_date=self.bid_date,
+            bid_configuration=(
+                self.bid_configuration.as_write()
+                if isinstance(self.bid_configuration, DomainModel)
+                else self.bid_configuration
+            ),
         )
 
 
-class TotalBidMatrixCalculationInput(DomainModel):
-    """This represents the reading version of total bid matrix calculation input.
+class PartialBidMatrixCalculationInput(FunctionInput):
+    """This represents the reading version of partial bid matrix calculation input.
 
     It is used to when data is retrieved from CDF.
 
     Args:
         space: The space where the node is located.
-        external_id: The external id of the total bid matrix calculation input.
-        data_record: The data record of the total bid matrix calculation input node.
+        external_id: The external id of the partial bid matrix calculation input.
+        data_record: The data record of the partial bid matrix calculation input node.
         process_id: The process associated with the function execution
         process_step: This is the step in the process.
         function_name: The name of the function
         function_call_id: The function call id
-        partial_bid_matrices: The partial bid matrices that are used to calculate the total bid matrix.
+        bid_date: The bid date
+        bid_configuration: TODO description
     """
 
-    space: str = DEFAULT_INSTANCE_SPACE
-    node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference(
-        "sp_powerops_types", "TotalBidMatrixCalculationInput"
-    )
-    process_id: str = Field(alias="processId")
-    process_step: int = Field(alias="processStep")
-    function_name: str = Field(alias="functionName")
-    function_call_id: str = Field(alias="functionCallId")
-    partial_bid_matrices: Union[list[BidMatrix], list[str], list[dm.NodeId], None] = Field(
-        default=None, repr=False, alias="partialBidMatrices"
-    )
-
-    def as_write(self) -> TotalBidMatrixCalculationInputWrite:
-        """Convert this read version of total bid matrix calculation input to the writing version."""
-        return TotalBidMatrixCalculationInputWrite(
+    node_type: Union[dm.DirectRelationReference, None] = None
+    bid_date: Optional[datetime.date] = Field(None, alias="bidDate")
+    bid_configuration: Union[BidConfiguration, str, dm.NodeId, None] = Field(None, repr=False, alias="bidConfiguration")
+
+    def as_write(self) -> PartialBidMatrixCalculationInputWrite:
+        """Convert this read version of partial bid matrix calculation input to the writing version."""
+        return PartialBidMatrixCalculationInputWrite(
             space=self.space,
             external_id=self.external_id,
             data_record=DataRecordWrite(existing_version=self.data_record.version),
             process_id=self.process_id,
             process_step=self.process_step,
             function_name=self.function_name,
             function_call_id=self.function_call_id,
-            partial_bid_matrices=[
-                partial_bid_matrice.as_write() if isinstance(partial_bid_matrice, DomainModel) else partial_bid_matrice
-                for partial_bid_matrice in self.partial_bid_matrices or []
-            ],
+            bid_date=self.bid_date,
+            bid_configuration=(
+                self.bid_configuration.as_write()
+                if isinstance(self.bid_configuration, DomainModel)
+                else self.bid_configuration
+            ),
         )
 
-    def as_apply(self) -> TotalBidMatrixCalculationInputWrite:
-        """Convert this read version of total bid matrix calculation input to the writing version."""
+    def as_apply(self) -> PartialBidMatrixCalculationInputWrite:
+        """Convert this read version of partial bid matrix calculation input to the writing version."""
         warnings.warn(
             "as_apply is deprecated and will be removed in v1.0. Use as_write instead.",
             UserWarning,
             stacklevel=2,
         )
         return self.as_write()
 
 
-class TotalBidMatrixCalculationInputWrite(DomainModelWrite):
-    """This represents the writing version of total bid matrix calculation input.
+class PartialBidMatrixCalculationInputWrite(FunctionInputWrite):
+    """This represents the writing version of partial bid matrix calculation input.
 
     It is used to when data is sent to CDF.
 
     Args:
         space: The space where the node is located.
-        external_id: The external id of the total bid matrix calculation input.
-        data_record: The data record of the total bid matrix calculation input node.
+        external_id: The external id of the partial bid matrix calculation input.
+        data_record: The data record of the partial bid matrix calculation input node.
         process_id: The process associated with the function execution
         process_step: This is the step in the process.
         function_name: The name of the function
         function_call_id: The function call id
-        partial_bid_matrices: The partial bid matrices that are used to calculate the total bid matrix.
+        bid_date: The bid date
+        bid_configuration: TODO description
     """
 
-    space: str = DEFAULT_INSTANCE_SPACE
-    node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference(
-        "sp_powerops_types", "TotalBidMatrixCalculationInput"
-    )
-    process_id: str = Field(alias="processId")
-    process_step: int = Field(alias="processStep")
-    function_name: str = Field(alias="functionName")
-    function_call_id: str = Field(alias="functionCallId")
-    partial_bid_matrices: Union[list[BidMatrixWrite], list[str], list[dm.NodeId], None] = Field(
-        default=None, repr=False, alias="partialBidMatrices"
+    node_type: Union[dm.DirectRelationReference, None] = None
+    bid_date: Optional[datetime.date] = Field(None, alias="bidDate")
+    bid_configuration: Union[BidConfigurationWrite, str, dm.NodeId, None] = Field(
+        None, repr=False, alias="bidConfiguration"
     )
 
     def _to_instances_write(
         self,
         cache: set[tuple[str, str]],
         view_by_read_class: dict[type[DomainModelCore], dm.ViewId] | None,
         write_none: bool = False,
         allow_version_increase: bool = False,
     ) -> ResourcesWrite:
         resources = ResourcesWrite()
         if self.as_tuple_id() in cache:
             return resources
 
         write_view = (view_by_read_class or {}).get(
-            TotalBidMatrixCalculationInput, dm.ViewId("sp_powerops_models", "TotalBidMatrixCalculationInput", "1")
+            PartialBidMatrixCalculationInput,
+            dm.ViewId("sp_powerops_models_temp", "PartialBidMatrixCalculationInput", "1"),
         )
 
         properties: dict[str, Any] = {}
 
         if self.process_id is not None:
             properties["processId"] = self.process_id
 
@@ -238,14 +240,27 @@
 
         if self.function_name is not None:
             properties["functionName"] = self.function_name
 
         if self.function_call_id is not None:
             properties["functionCallId"] = self.function_call_id
 
+        if self.bid_date is not None or write_none:
+            properties["bidDate"] = self.bid_date.isoformat() if self.bid_date else None
+
+        if self.bid_configuration is not None:
+            properties["bidConfiguration"] = {
+                "space": self.space if isinstance(self.bid_configuration, str) else self.bid_configuration.space,
+                "externalId": (
+                    self.bid_configuration
+                    if isinstance(self.bid_configuration, str)
+                    else self.bid_configuration.external_id
+                ),
+            }
+
         if properties:
             this_node = dm.NodeApply(
                 space=self.space,
                 external_id=self.external_id,
                 existing_version=None if allow_version_increase else self.data_record.existing_version,
                 type=self.node_type,
                 sources=[
@@ -254,80 +269,74 @@
                         properties=properties,
                     )
                 ],
             )
             resources.nodes.append(this_node)
             cache.add(self.as_tuple_id())
 
-        edge_type = dm.DirectRelationReference("sp_powerops_types", "BidMatrix")
-        for partial_bid_matrice in self.partial_bid_matrices or []:
-            other_resources = DomainRelationWrite.from_edge_to_resources(
-                cache,
-                start_node=self,
-                end_node=partial_bid_matrice,
-                edge_type=edge_type,
-                view_by_read_class=view_by_read_class,
-                write_none=write_none,
-                allow_version_increase=allow_version_increase,
-            )
+        if isinstance(self.bid_configuration, DomainModelWrite):
+            other_resources = self.bid_configuration._to_instances_write(cache, view_by_read_class)
             resources.extend(other_resources)
 
         return resources
 
 
-class TotalBidMatrixCalculationInputApply(TotalBidMatrixCalculationInputWrite):
-    def __new__(cls, *args, **kwargs) -> TotalBidMatrixCalculationInputApply:
+class PartialBidMatrixCalculationInputApply(PartialBidMatrixCalculationInputWrite):
+    def __new__(cls, *args, **kwargs) -> PartialBidMatrixCalculationInputApply:
         warnings.warn(
-            "TotalBidMatrixCalculationInputApply is deprecated and will be removed in v1.0. Use TotalBidMatrixCalculationInputWrite instead."
+            "PartialBidMatrixCalculationInputApply is deprecated and will be removed in v1.0. Use PartialBidMatrixCalculationInputWrite instead."
             "The motivation for this change is that Write is a more descriptive name for the writing version of the"
-            "TotalBidMatrixCalculationInput.",
+            "PartialBidMatrixCalculationInput.",
             UserWarning,
             stacklevel=2,
         )
         return super().__new__(cls)
 
 
-class TotalBidMatrixCalculationInputList(DomainModelList[TotalBidMatrixCalculationInput]):
-    """List of total bid matrix calculation inputs in the read version."""
+class PartialBidMatrixCalculationInputList(DomainModelList[PartialBidMatrixCalculationInput]):
+    """List of partial bid matrix calculation inputs in the read version."""
 
-    _INSTANCE = TotalBidMatrixCalculationInput
+    _INSTANCE = PartialBidMatrixCalculationInput
 
-    def as_write(self) -> TotalBidMatrixCalculationInputWriteList:
-        """Convert these read versions of total bid matrix calculation input to the writing versions."""
-        return TotalBidMatrixCalculationInputWriteList([node.as_write() for node in self.data])
+    def as_write(self) -> PartialBidMatrixCalculationInputWriteList:
+        """Convert these read versions of partial bid matrix calculation input to the writing versions."""
+        return PartialBidMatrixCalculationInputWriteList([node.as_write() for node in self.data])
 
-    def as_apply(self) -> TotalBidMatrixCalculationInputWriteList:
+    def as_apply(self) -> PartialBidMatrixCalculationInputWriteList:
         """Convert these read versions of primitive nullable to the writing versions."""
         warnings.warn(
             "as_apply is deprecated and will be removed in v1.0. Use as_write instead.",
             UserWarning,
             stacklevel=2,
         )
         return self.as_write()
 
 
-class TotalBidMatrixCalculationInputWriteList(DomainModelWriteList[TotalBidMatrixCalculationInputWrite]):
-    """List of total bid matrix calculation inputs in the writing version."""
+class PartialBidMatrixCalculationInputWriteList(DomainModelWriteList[PartialBidMatrixCalculationInputWrite]):
+    """List of partial bid matrix calculation inputs in the writing version."""
 
-    _INSTANCE = TotalBidMatrixCalculationInputWrite
+    _INSTANCE = PartialBidMatrixCalculationInputWrite
 
 
-class TotalBidMatrixCalculationInputApplyList(TotalBidMatrixCalculationInputWriteList): ...
+class PartialBidMatrixCalculationInputApplyList(PartialBidMatrixCalculationInputWriteList): ...
 
 
-def _create_total_bid_matrix_calculation_input_filter(
+def _create_partial_bid_matrix_calculation_input_filter(
     view_id: dm.ViewId,
     process_id: str | list[str] | None = None,
     process_id_prefix: str | None = None,
     min_process_step: int | None = None,
     max_process_step: int | None = None,
     function_name: str | list[str] | None = None,
     function_name_prefix: str | None = None,
     function_call_id: str | list[str] | None = None,
     function_call_id_prefix: str | None = None,
+    min_bid_date: datetime.date | None = None,
+    max_bid_date: datetime.date | None = None,
+    bid_configuration: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
     external_id_prefix: str | None = None,
     space: str | list[str] | None = None,
     filter: dm.Filter | None = None,
 ) -> dm.Filter | None:
     filters = []
     if isinstance(process_id, str):
         filters.append(dm.filters.Equals(view_id.as_property_ref("processId"), value=process_id))
@@ -347,14 +356,50 @@
         filters.append(dm.filters.Prefix(view_id.as_property_ref("functionName"), value=function_name_prefix))
     if isinstance(function_call_id, str):
         filters.append(dm.filters.Equals(view_id.as_property_ref("functionCallId"), value=function_call_id))
     if function_call_id and isinstance(function_call_id, list):
         filters.append(dm.filters.In(view_id.as_property_ref("functionCallId"), values=function_call_id))
     if function_call_id_prefix is not None:
         filters.append(dm.filters.Prefix(view_id.as_property_ref("functionCallId"), value=function_call_id_prefix))
+    if min_bid_date is not None or max_bid_date is not None:
+        filters.append(
+            dm.filters.Range(
+                view_id.as_property_ref("bidDate"),
+                gte=min_bid_date.isoformat() if min_bid_date else None,
+                lte=max_bid_date.isoformat() if max_bid_date else None,
+            )
+        )
+    if bid_configuration and isinstance(bid_configuration, str):
+        filters.append(
+            dm.filters.Equals(
+                view_id.as_property_ref("bidConfiguration"),
+                value={"space": DEFAULT_INSTANCE_SPACE, "externalId": bid_configuration},
+            )
+        )
+    if bid_configuration and isinstance(bid_configuration, tuple):
+        filters.append(
+            dm.filters.Equals(
+                view_id.as_property_ref("bidConfiguration"),
+                value={"space": bid_configuration[0], "externalId": bid_configuration[1]},
+            )
+        )
+    if bid_configuration and isinstance(bid_configuration, list) and isinstance(bid_configuration[0], str):
+        filters.append(
+            dm.filters.In(
+                view_id.as_property_ref("bidConfiguration"),
+                values=[{"space": DEFAULT_INSTANCE_SPACE, "externalId": item} for item in bid_configuration],
+            )
+        )
+    if bid_configuration and isinstance(bid_configuration, list) and isinstance(bid_configuration[0], tuple):
+        filters.append(
+            dm.filters.In(
+                view_id.as_property_ref("bidConfiguration"),
+                values=[{"space": item[0], "externalId": item[1]} for item in bid_configuration],
+            )
+        )
     if external_id_prefix is not None:
         filters.append(dm.filters.Prefix(["node", "externalId"], value=external_id_prefix))
     if isinstance(space, str):
         filters.append(dm.filters.Equals(["node", "space"], value=space))
     if space and isinstance(space, list):
         filters.append(dm.filters.In(["node", "space"], values=space))
     if filter:
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_total_bid_matrix_calculation_output.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_total_bid_matrix_calculation_input.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 from __future__ import annotations
 
+import datetime
 import warnings
 from typing import TYPE_CHECKING, Any, Literal, Optional, Union
 
 from cognite.client import data_modeling as dm
 from pydantic import Field
 from pydantic import field_validator, model_validator
 
@@ -17,236 +18,246 @@
     DomainModelWrite,
     DomainModelWriteList,
     DomainModelList,
     DomainRelationWrite,
     GraphQLCore,
     ResourcesWrite,
 )
+from ._function_input import FunctionInput, FunctionInputWrite
 
 if TYPE_CHECKING:
-    from ._alert import Alert, AlertGraphQL, AlertWrite
-    from ._bid_document_day_ahead import BidDocumentDayAhead, BidDocumentDayAheadGraphQL, BidDocumentDayAheadWrite
-    from ._total_bid_matrix_calculation_input import (
-        TotalBidMatrixCalculationInput,
-        TotalBidMatrixCalculationInputGraphQL,
-        TotalBidMatrixCalculationInputWrite,
-    )
+    from ._bid_configuration import BidConfiguration, BidConfigurationGraphQL, BidConfigurationWrite
+    from ._bid_matrix import BidMatrix, BidMatrixGraphQL, BidMatrixWrite
 
 
 __all__ = [
-    "TotalBidMatrixCalculationOutput",
-    "TotalBidMatrixCalculationOutputWrite",
-    "TotalBidMatrixCalculationOutputApply",
-    "TotalBidMatrixCalculationOutputList",
-    "TotalBidMatrixCalculationOutputWriteList",
-    "TotalBidMatrixCalculationOutputApplyList",
-    "TotalBidMatrixCalculationOutputFields",
-    "TotalBidMatrixCalculationOutputTextFields",
+    "TotalBidMatrixCalculationInput",
+    "TotalBidMatrixCalculationInputWrite",
+    "TotalBidMatrixCalculationInputApply",
+    "TotalBidMatrixCalculationInputList",
+    "TotalBidMatrixCalculationInputWriteList",
+    "TotalBidMatrixCalculationInputApplyList",
+    "TotalBidMatrixCalculationInputFields",
+    "TotalBidMatrixCalculationInputTextFields",
 ]
 
 
-TotalBidMatrixCalculationOutputTextFields = Literal["process_id", "function_name", "function_call_id"]
-TotalBidMatrixCalculationOutputFields = Literal["process_id", "process_step", "function_name", "function_call_id"]
+TotalBidMatrixCalculationInputTextFields = Literal["process_id", "function_name", "function_call_id"]
+TotalBidMatrixCalculationInputFields = Literal[
+    "process_id", "process_step", "function_name", "function_call_id", "bid_date"
+]
 
-_TOTALBIDMATRIXCALCULATIONOUTPUT_PROPERTIES_BY_FIELD = {
+_TOTALBIDMATRIXCALCULATIONINPUT_PROPERTIES_BY_FIELD = {
     "process_id": "processId",
     "process_step": "processStep",
     "function_name": "functionName",
     "function_call_id": "functionCallId",
+    "bid_date": "bidDate",
 }
 
 
-class TotalBidMatrixCalculationOutputGraphQL(GraphQLCore):
-    """This represents the reading version of total bid matrix calculation output, used
+class TotalBidMatrixCalculationInputGraphQL(GraphQLCore):
+    """This represents the reading version of total bid matrix calculation input, used
     when data is retrieved from CDF using GraphQL.
 
     It is used when retrieving data from CDF using GraphQL.
 
     Args:
         space: The space where the node is located.
-        external_id: The external id of the total bid matrix calculation output.
-        data_record: The data record of the total bid matrix calculation output node.
+        external_id: The external id of the total bid matrix calculation input.
+        data_record: The data record of the total bid matrix calculation input node.
         process_id: The process associated with the function execution
         process_step: This is the step in the process.
         function_name: The name of the function
         function_call_id: The function call id
-        alerts: An array of calculation level Alerts.
-        bid_document: The bid document field.
-        input_: The previous step in the process.
+        bid_configuration: The bid configuration field.
+        bid_date: The bid date
+        partial_bid_matrices: The partial bid matrices that are used to calculate the total bid matrix.
     """
 
-    view_id = dm.ViewId("sp_powerops_models", "TotalBidMatrixCalculationOutput", "1")
+    view_id = dm.ViewId("sp_powerops_models_temp", "TotalBidMatrixCalculationInput", "1")
     process_id: Optional[str] = Field(None, alias="processId")
     process_step: Optional[int] = Field(None, alias="processStep")
     function_name: Optional[str] = Field(None, alias="functionName")
     function_call_id: Optional[str] = Field(None, alias="functionCallId")
-    alerts: Optional[list[AlertGraphQL]] = Field(default=None, repr=False)
-    bid_document: Optional[BidDocumentDayAheadGraphQL] = Field(None, repr=False, alias="bidDocument")
-    input_: Optional[TotalBidMatrixCalculationInputGraphQL] = Field(None, repr=False, alias="input")
+    bid_configuration: Optional[BidConfigurationGraphQL] = Field(None, repr=False, alias="bidConfiguration")
+    bid_date: Optional[datetime.date] = Field(None, alias="bidDate")
+    partial_bid_matrices: Optional[list[BidMatrixGraphQL]] = Field(default=None, repr=False, alias="partialBidMatrices")
 
     @model_validator(mode="before")
     def parse_data_record(cls, values: Any) -> Any:
         if not isinstance(values, dict):
             return values
         if "lastUpdatedTime" in values or "createdTime" in values:
             values["dataRecord"] = DataRecordGraphQL(
                 created_time=values.pop("createdTime", None),
                 last_updated_time=values.pop("lastUpdatedTime", None),
             )
         return values
 
-    @field_validator("alerts", "bid_document", "input_", mode="before")
+    @field_validator("bid_configuration", "partial_bid_matrices", mode="before")
     def parse_graphql(cls, value: Any) -> Any:
         if not isinstance(value, dict):
             return value
         if "items" in value:
             return value["items"]
         return value
 
-    def as_read(self) -> TotalBidMatrixCalculationOutput:
-        """Convert this GraphQL format of total bid matrix calculation output to the reading format."""
+    def as_read(self) -> TotalBidMatrixCalculationInput:
+        """Convert this GraphQL format of total bid matrix calculation input to the reading format."""
         if self.data_record is None:
             raise ValueError("This object cannot be converted to a read format because it lacks a data record.")
-        return TotalBidMatrixCalculationOutput(
+        return TotalBidMatrixCalculationInput(
             space=self.space,
             external_id=self.external_id,
             data_record=DataRecord(
                 version=0,
                 last_updated_time=self.data_record.last_updated_time,
                 created_time=self.data_record.created_time,
             ),
             process_id=self.process_id,
             process_step=self.process_step,
             function_name=self.function_name,
             function_call_id=self.function_call_id,
-            alerts=[alert.as_read() if isinstance(alert, GraphQLCore) else alert for alert in self.alerts or []],
-            bid_document=(
-                self.bid_document.as_read() if isinstance(self.bid_document, GraphQLCore) else self.bid_document
+            bid_configuration=(
+                self.bid_configuration.as_read()
+                if isinstance(self.bid_configuration, GraphQLCore)
+                else self.bid_configuration
             ),
-            input_=self.input_.as_read() if isinstance(self.input_, GraphQLCore) else self.input_,
+            bid_date=self.bid_date,
+            partial_bid_matrices=[
+                partial_bid_matrice.as_read() if isinstance(partial_bid_matrice, GraphQLCore) else partial_bid_matrice
+                for partial_bid_matrice in self.partial_bid_matrices or []
+            ],
         )
 
-    def as_write(self) -> TotalBidMatrixCalculationOutputWrite:
-        """Convert this GraphQL format of total bid matrix calculation output to the writing format."""
-        return TotalBidMatrixCalculationOutputWrite(
+    def as_write(self) -> TotalBidMatrixCalculationInputWrite:
+        """Convert this GraphQL format of total bid matrix calculation input to the writing format."""
+        return TotalBidMatrixCalculationInputWrite(
             space=self.space,
             external_id=self.external_id,
             data_record=DataRecordWrite(existing_version=0),
             process_id=self.process_id,
             process_step=self.process_step,
             function_name=self.function_name,
             function_call_id=self.function_call_id,
-            alerts=[alert.as_write() if isinstance(alert, DomainModel) else alert for alert in self.alerts or []],
-            bid_document=(
-                self.bid_document.as_write() if isinstance(self.bid_document, DomainModel) else self.bid_document
+            bid_configuration=(
+                self.bid_configuration.as_write()
+                if isinstance(self.bid_configuration, DomainModel)
+                else self.bid_configuration
             ),
-            input_=self.input_.as_write() if isinstance(self.input_, DomainModel) else self.input_,
+            bid_date=self.bid_date,
+            partial_bid_matrices=[
+                partial_bid_matrice.as_write() if isinstance(partial_bid_matrice, DomainModel) else partial_bid_matrice
+                for partial_bid_matrice in self.partial_bid_matrices or []
+            ],
         )
 
 
-class TotalBidMatrixCalculationOutput(DomainModel):
-    """This represents the reading version of total bid matrix calculation output.
+class TotalBidMatrixCalculationInput(FunctionInput):
+    """This represents the reading version of total bid matrix calculation input.
 
     It is used to when data is retrieved from CDF.
 
     Args:
         space: The space where the node is located.
-        external_id: The external id of the total bid matrix calculation output.
-        data_record: The data record of the total bid matrix calculation output node.
+        external_id: The external id of the total bid matrix calculation input.
+        data_record: The data record of the total bid matrix calculation input node.
         process_id: The process associated with the function execution
         process_step: This is the step in the process.
         function_name: The name of the function
         function_call_id: The function call id
-        alerts: An array of calculation level Alerts.
-        bid_document: The bid document field.
-        input_: The previous step in the process.
+        bid_configuration: The bid configuration field.
+        bid_date: The bid date
+        partial_bid_matrices: The partial bid matrices that are used to calculate the total bid matrix.
     """
 
-    space: str = DEFAULT_INSTANCE_SPACE
     node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference(
-        "sp_powerops_types", "TotalBidMatrixCalculationOutput"
+        "sp_powerops_types_temp", "TotalBidMatrixCalculationInput"
+    )
+    bid_configuration: Union[BidConfiguration, str, dm.NodeId, None] = Field(None, repr=False, alias="bidConfiguration")
+    bid_date: Optional[datetime.date] = Field(None, alias="bidDate")
+    partial_bid_matrices: Union[list[BidMatrix], list[str], list[dm.NodeId], None] = Field(
+        default=None, repr=False, alias="partialBidMatrices"
     )
-    process_id: str = Field(alias="processId")
-    process_step: int = Field(alias="processStep")
-    function_name: str = Field(alias="functionName")
-    function_call_id: str = Field(alias="functionCallId")
-    alerts: Union[list[Alert], list[str], list[dm.NodeId], None] = Field(default=None, repr=False)
-    bid_document: Union[BidDocumentDayAhead, str, dm.NodeId, None] = Field(None, repr=False, alias="bidDocument")
-    input_: Union[TotalBidMatrixCalculationInput, str, dm.NodeId, None] = Field(None, repr=False, alias="input")
-
-    def as_write(self) -> TotalBidMatrixCalculationOutputWrite:
-        """Convert this read version of total bid matrix calculation output to the writing version."""
-        return TotalBidMatrixCalculationOutputWrite(
+
+    def as_write(self) -> TotalBidMatrixCalculationInputWrite:
+        """Convert this read version of total bid matrix calculation input to the writing version."""
+        return TotalBidMatrixCalculationInputWrite(
             space=self.space,
             external_id=self.external_id,
             data_record=DataRecordWrite(existing_version=self.data_record.version),
             process_id=self.process_id,
             process_step=self.process_step,
             function_name=self.function_name,
             function_call_id=self.function_call_id,
-            alerts=[alert.as_write() if isinstance(alert, DomainModel) else alert for alert in self.alerts or []],
-            bid_document=(
-                self.bid_document.as_write() if isinstance(self.bid_document, DomainModel) else self.bid_document
+            bid_configuration=(
+                self.bid_configuration.as_write()
+                if isinstance(self.bid_configuration, DomainModel)
+                else self.bid_configuration
             ),
-            input_=self.input_.as_write() if isinstance(self.input_, DomainModel) else self.input_,
+            bid_date=self.bid_date,
+            partial_bid_matrices=[
+                partial_bid_matrice.as_write() if isinstance(partial_bid_matrice, DomainModel) else partial_bid_matrice
+                for partial_bid_matrice in self.partial_bid_matrices or []
+            ],
         )
 
-    def as_apply(self) -> TotalBidMatrixCalculationOutputWrite:
-        """Convert this read version of total bid matrix calculation output to the writing version."""
+    def as_apply(self) -> TotalBidMatrixCalculationInputWrite:
+        """Convert this read version of total bid matrix calculation input to the writing version."""
         warnings.warn(
             "as_apply is deprecated and will be removed in v1.0. Use as_write instead.",
             UserWarning,
             stacklevel=2,
         )
         return self.as_write()
 
 
-class TotalBidMatrixCalculationOutputWrite(DomainModelWrite):
-    """This represents the writing version of total bid matrix calculation output.
+class TotalBidMatrixCalculationInputWrite(FunctionInputWrite):
+    """This represents the writing version of total bid matrix calculation input.
 
     It is used to when data is sent to CDF.
 
     Args:
         space: The space where the node is located.
-        external_id: The external id of the total bid matrix calculation output.
-        data_record: The data record of the total bid matrix calculation output node.
+        external_id: The external id of the total bid matrix calculation input.
+        data_record: The data record of the total bid matrix calculation input node.
         process_id: The process associated with the function execution
         process_step: This is the step in the process.
         function_name: The name of the function
         function_call_id: The function call id
-        alerts: An array of calculation level Alerts.
-        bid_document: The bid document field.
-        input_: The previous step in the process.
+        bid_configuration: The bid configuration field.
+        bid_date: The bid date
+        partial_bid_matrices: The partial bid matrices that are used to calculate the total bid matrix.
     """
 
-    space: str = DEFAULT_INSTANCE_SPACE
     node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference(
-        "sp_powerops_types", "TotalBidMatrixCalculationOutput"
+        "sp_powerops_types_temp", "TotalBidMatrixCalculationInput"
+    )
+    bid_configuration: Union[BidConfigurationWrite, str, dm.NodeId, None] = Field(
+        None, repr=False, alias="bidConfiguration"
+    )
+    bid_date: Optional[datetime.date] = Field(None, alias="bidDate")
+    partial_bid_matrices: Union[list[BidMatrixWrite], list[str], list[dm.NodeId], None] = Field(
+        default=None, repr=False, alias="partialBidMatrices"
     )
-    process_id: str = Field(alias="processId")
-    process_step: int = Field(alias="processStep")
-    function_name: str = Field(alias="functionName")
-    function_call_id: str = Field(alias="functionCallId")
-    alerts: Union[list[AlertWrite], list[str], list[dm.NodeId], None] = Field(default=None, repr=False)
-    bid_document: Union[BidDocumentDayAheadWrite, str, dm.NodeId, None] = Field(None, repr=False, alias="bidDocument")
-    input_: Union[TotalBidMatrixCalculationInputWrite, str, dm.NodeId, None] = Field(None, repr=False, alias="input")
 
     def _to_instances_write(
         self,
         cache: set[tuple[str, str]],
         view_by_read_class: dict[type[DomainModelCore], dm.ViewId] | None,
         write_none: bool = False,
         allow_version_increase: bool = False,
     ) -> ResourcesWrite:
         resources = ResourcesWrite()
         if self.as_tuple_id() in cache:
             return resources
 
         write_view = (view_by_read_class or {}).get(
-            TotalBidMatrixCalculationOutput, dm.ViewId("sp_powerops_models", "TotalBidMatrixCalculationOutput", "1")
+            TotalBidMatrixCalculationInput, dm.ViewId("sp_powerops_models_temp", "TotalBidMatrixCalculationInput", "1")
         )
 
         properties: dict[str, Any] = {}
 
         if self.process_id is not None:
             properties["processId"] = self.process_id
 
@@ -255,27 +266,26 @@
 
         if self.function_name is not None:
             properties["functionName"] = self.function_name
 
         if self.function_call_id is not None:
             properties["functionCallId"] = self.function_call_id
 
-        if self.bid_document is not None:
-            properties["bidDocument"] = {
-                "space": self.space if isinstance(self.bid_document, str) else self.bid_document.space,
+        if self.bid_configuration is not None:
+            properties["bidConfiguration"] = {
+                "space": self.space if isinstance(self.bid_configuration, str) else self.bid_configuration.space,
                 "externalId": (
-                    self.bid_document if isinstance(self.bid_document, str) else self.bid_document.external_id
+                    self.bid_configuration
+                    if isinstance(self.bid_configuration, str)
+                    else self.bid_configuration.external_id
                 ),
             }
 
-        if self.input_ is not None:
-            properties["input"] = {
-                "space": self.space if isinstance(self.input_, str) else self.input_.space,
-                "externalId": self.input_ if isinstance(self.input_, str) else self.input_.external_id,
-            }
+        if self.bid_date is not None or write_none:
+            properties["bidDate"] = self.bid_date.isoformat() if self.bid_date else None
 
         if properties:
             this_node = dm.NodeApply(
                 space=self.space,
                 external_id=self.external_id,
                 existing_version=None if allow_version_increase else self.data_record.existing_version,
                 type=self.node_type,
@@ -285,90 +295,87 @@
                         properties=properties,
                     )
                 ],
             )
             resources.nodes.append(this_node)
             cache.add(self.as_tuple_id())
 
-        edge_type = dm.DirectRelationReference("sp_powerops_types", "calculationIssue")
-        for alert in self.alerts or []:
+        edge_type = dm.DirectRelationReference("sp_powerops_types_temp", "BidMatrix")
+        for partial_bid_matrice in self.partial_bid_matrices or []:
             other_resources = DomainRelationWrite.from_edge_to_resources(
                 cache,
                 start_node=self,
-                end_node=alert,
+                end_node=partial_bid_matrice,
                 edge_type=edge_type,
                 view_by_read_class=view_by_read_class,
                 write_none=write_none,
                 allow_version_increase=allow_version_increase,
             )
             resources.extend(other_resources)
 
-        if isinstance(self.bid_document, DomainModelWrite):
-            other_resources = self.bid_document._to_instances_write(cache, view_by_read_class)
-            resources.extend(other_resources)
-
-        if isinstance(self.input_, DomainModelWrite):
-            other_resources = self.input_._to_instances_write(cache, view_by_read_class)
+        if isinstance(self.bid_configuration, DomainModelWrite):
+            other_resources = self.bid_configuration._to_instances_write(cache, view_by_read_class)
             resources.extend(other_resources)
 
         return resources
 
 
-class TotalBidMatrixCalculationOutputApply(TotalBidMatrixCalculationOutputWrite):
-    def __new__(cls, *args, **kwargs) -> TotalBidMatrixCalculationOutputApply:
+class TotalBidMatrixCalculationInputApply(TotalBidMatrixCalculationInputWrite):
+    def __new__(cls, *args, **kwargs) -> TotalBidMatrixCalculationInputApply:
         warnings.warn(
-            "TotalBidMatrixCalculationOutputApply is deprecated and will be removed in v1.0. Use TotalBidMatrixCalculationOutputWrite instead."
+            "TotalBidMatrixCalculationInputApply is deprecated and will be removed in v1.0. Use TotalBidMatrixCalculationInputWrite instead."
             "The motivation for this change is that Write is a more descriptive name for the writing version of the"
-            "TotalBidMatrixCalculationOutput.",
+            "TotalBidMatrixCalculationInput.",
             UserWarning,
             stacklevel=2,
         )
         return super().__new__(cls)
 
 
-class TotalBidMatrixCalculationOutputList(DomainModelList[TotalBidMatrixCalculationOutput]):
-    """List of total bid matrix calculation outputs in the read version."""
+class TotalBidMatrixCalculationInputList(DomainModelList[TotalBidMatrixCalculationInput]):
+    """List of total bid matrix calculation inputs in the read version."""
 
-    _INSTANCE = TotalBidMatrixCalculationOutput
+    _INSTANCE = TotalBidMatrixCalculationInput
 
-    def as_write(self) -> TotalBidMatrixCalculationOutputWriteList:
-        """Convert these read versions of total bid matrix calculation output to the writing versions."""
-        return TotalBidMatrixCalculationOutputWriteList([node.as_write() for node in self.data])
+    def as_write(self) -> TotalBidMatrixCalculationInputWriteList:
+        """Convert these read versions of total bid matrix calculation input to the writing versions."""
+        return TotalBidMatrixCalculationInputWriteList([node.as_write() for node in self.data])
 
-    def as_apply(self) -> TotalBidMatrixCalculationOutputWriteList:
+    def as_apply(self) -> TotalBidMatrixCalculationInputWriteList:
         """Convert these read versions of primitive nullable to the writing versions."""
         warnings.warn(
             "as_apply is deprecated and will be removed in v1.0. Use as_write instead.",
             UserWarning,
             stacklevel=2,
         )
         return self.as_write()
 
 
-class TotalBidMatrixCalculationOutputWriteList(DomainModelWriteList[TotalBidMatrixCalculationOutputWrite]):
-    """List of total bid matrix calculation outputs in the writing version."""
+class TotalBidMatrixCalculationInputWriteList(DomainModelWriteList[TotalBidMatrixCalculationInputWrite]):
+    """List of total bid matrix calculation inputs in the writing version."""
 
-    _INSTANCE = TotalBidMatrixCalculationOutputWrite
+    _INSTANCE = TotalBidMatrixCalculationInputWrite
 
 
-class TotalBidMatrixCalculationOutputApplyList(TotalBidMatrixCalculationOutputWriteList): ...
+class TotalBidMatrixCalculationInputApplyList(TotalBidMatrixCalculationInputWriteList): ...
 
 
-def _create_total_bid_matrix_calculation_output_filter(
+def _create_total_bid_matrix_calculation_input_filter(
     view_id: dm.ViewId,
     process_id: str | list[str] | None = None,
     process_id_prefix: str | None = None,
     min_process_step: int | None = None,
     max_process_step: int | None = None,
     function_name: str | list[str] | None = None,
     function_name_prefix: str | None = None,
     function_call_id: str | list[str] | None = None,
     function_call_id_prefix: str | None = None,
-    bid_document: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
-    input_: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+    bid_configuration: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+    min_bid_date: datetime.date | None = None,
+    max_bid_date: datetime.date | None = None,
     external_id_prefix: str | None = None,
     space: str | list[str] | None = None,
     filter: dm.Filter | None = None,
 ) -> dm.Filter | None:
     filters = []
     if isinstance(process_id, str):
         filters.append(dm.filters.Equals(view_id.as_property_ref("processId"), value=process_id))
@@ -388,62 +395,48 @@
         filters.append(dm.filters.Prefix(view_id.as_property_ref("functionName"), value=function_name_prefix))
     if isinstance(function_call_id, str):
         filters.append(dm.filters.Equals(view_id.as_property_ref("functionCallId"), value=function_call_id))
     if function_call_id and isinstance(function_call_id, list):
         filters.append(dm.filters.In(view_id.as_property_ref("functionCallId"), values=function_call_id))
     if function_call_id_prefix is not None:
         filters.append(dm.filters.Prefix(view_id.as_property_ref("functionCallId"), value=function_call_id_prefix))
-    if bid_document and isinstance(bid_document, str):
+    if bid_configuration and isinstance(bid_configuration, str):
         filters.append(
             dm.filters.Equals(
-                view_id.as_property_ref("bidDocument"),
-                value={"space": DEFAULT_INSTANCE_SPACE, "externalId": bid_document},
+                view_id.as_property_ref("bidConfiguration"),
+                value={"space": DEFAULT_INSTANCE_SPACE, "externalId": bid_configuration},
             )
         )
-    if bid_document and isinstance(bid_document, tuple):
+    if bid_configuration and isinstance(bid_configuration, tuple):
         filters.append(
             dm.filters.Equals(
-                view_id.as_property_ref("bidDocument"), value={"space": bid_document[0], "externalId": bid_document[1]}
-            )
-        )
-    if bid_document and isinstance(bid_document, list) and isinstance(bid_document[0], str):
-        filters.append(
-            dm.filters.In(
-                view_id.as_property_ref("bidDocument"),
-                values=[{"space": DEFAULT_INSTANCE_SPACE, "externalId": item} for item in bid_document],
+                view_id.as_property_ref("bidConfiguration"),
+                value={"space": bid_configuration[0], "externalId": bid_configuration[1]},
             )
         )
-    if bid_document and isinstance(bid_document, list) and isinstance(bid_document[0], tuple):
+    if bid_configuration and isinstance(bid_configuration, list) and isinstance(bid_configuration[0], str):
         filters.append(
             dm.filters.In(
-                view_id.as_property_ref("bidDocument"),
-                values=[{"space": item[0], "externalId": item[1]} for item in bid_document],
-            )
-        )
-    if input_ and isinstance(input_, str):
-        filters.append(
-            dm.filters.Equals(
-                view_id.as_property_ref("input"), value={"space": DEFAULT_INSTANCE_SPACE, "externalId": input_}
+                view_id.as_property_ref("bidConfiguration"),
+                values=[{"space": DEFAULT_INSTANCE_SPACE, "externalId": item} for item in bid_configuration],
             )
         )
-    if input_ and isinstance(input_, tuple):
-        filters.append(
-            dm.filters.Equals(view_id.as_property_ref("input"), value={"space": input_[0], "externalId": input_[1]})
-        )
-    if input_ and isinstance(input_, list) and isinstance(input_[0], str):
+    if bid_configuration and isinstance(bid_configuration, list) and isinstance(bid_configuration[0], tuple):
         filters.append(
             dm.filters.In(
-                view_id.as_property_ref("input"),
-                values=[{"space": DEFAULT_INSTANCE_SPACE, "externalId": item} for item in input_],
+                view_id.as_property_ref("bidConfiguration"),
+                values=[{"space": item[0], "externalId": item[1]} for item in bid_configuration],
             )
         )
-    if input_ and isinstance(input_, list) and isinstance(input_[0], tuple):
+    if min_bid_date is not None or max_bid_date is not None:
         filters.append(
-            dm.filters.In(
-                view_id.as_property_ref("input"), values=[{"space": item[0], "externalId": item[1]} for item in input_]
+            dm.filters.Range(
+                view_id.as_property_ref("bidDate"),
+                gte=min_bid_date.isoformat() if min_bid_date else None,
+                lte=max_bid_date.isoformat() if max_bid_date else None,
             )
         )
     if external_id_prefix is not None:
         filters.append(dm.filters.Prefix(["node", "externalId"], value=external_id_prefix))
     if isinstance(space, str):
         filters.append(dm.filters.Equals(["node", "space"], value=space))
     if space and isinstance(space, list):
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_turbine_efficiency_curve.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_turbine_efficiency_curve.py`

 * *Files 2% similar despite different names*

```diff
@@ -52,15 +52,15 @@
         external_id: The external id of the turbine efficiency curve.
         data_record: The data record of the turbine efficiency curve node.
         head: The reference head values
         flow: The flow values
         efficiency: The turbine efficiency values
     """
 
-    view_id = dm.ViewId("sp_powerops_models", "TurbineEfficiencyCurve", "1")
+    view_id = dm.ViewId("sp_powerops_models_temp", "TurbineEfficiencyCurve", "1")
     head: Optional[float] = None
     flow: Optional[list[float]] = None
     efficiency: Optional[list[float]] = None
 
     @model_validator(mode="before")
     def parse_data_record(cls, values: Any) -> Any:
         if not isinstance(values, dict):
@@ -113,15 +113,15 @@
         head: The reference head values
         flow: The flow values
         efficiency: The turbine efficiency values
     """
 
     space: str = DEFAULT_INSTANCE_SPACE
     node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference(
-        "sp_powerops_types", "TurbineEfficiencyCurve"
+        "sp_powerops_types_temp", "TurbineEfficiencyCurve"
     )
     head: Optional[float] = None
     flow: Optional[list[float]] = None
     efficiency: Optional[list[float]] = None
 
     def as_write(self) -> TurbineEfficiencyCurveWrite:
         """Convert this read version of turbine efficiency curve to the writing version."""
@@ -156,15 +156,15 @@
         head: The reference head values
         flow: The flow values
         efficiency: The turbine efficiency values
     """
 
     space: str = DEFAULT_INSTANCE_SPACE
     node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference(
-        "sp_powerops_types", "TurbineEfficiencyCurve"
+        "sp_powerops_types_temp", "TurbineEfficiencyCurve"
     )
     head: Optional[float] = None
     flow: list[float]
     efficiency: list[float]
 
     def _to_instances_write(
         self,
@@ -174,15 +174,15 @@
         allow_version_increase: bool = False,
     ) -> ResourcesWrite:
         resources = ResourcesWrite()
         if self.as_tuple_id() in cache:
             return resources
 
         write_view = (view_by_read_class or {}).get(
-            TurbineEfficiencyCurve, dm.ViewId("sp_powerops_models", "TurbineEfficiencyCurve", "1")
+            TurbineEfficiencyCurve, dm.ViewId("sp_powerops_models_temp", "TurbineEfficiencyCurve", "1")
         )
 
         properties: dict[str, Any] = {}
 
         if self.head is not None or write_none:
             properties["head"] = self.head
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_water_partial_bid_calculation_input.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_task_dispatcher_output.py`

 * *Files 25% similar despite different names*

```diff
@@ -17,220 +17,228 @@
     DomainModelWrite,
     DomainModelWriteList,
     DomainModelList,
     DomainRelationWrite,
     GraphQLCore,
     ResourcesWrite,
 )
+from ._function_output import FunctionOutput, FunctionOutputWrite
 
 if TYPE_CHECKING:
-    from ._bid_calculation_task import BidCalculationTask, BidCalculationTaskGraphQL, BidCalculationTaskWrite
+    from ._alert import Alert, AlertGraphQL, AlertWrite
+    from ._function_input import FunctionInput, FunctionInputGraphQL, FunctionInputWrite
+    from ._task_dispatcher_input import TaskDispatcherInput, TaskDispatcherInputGraphQL, TaskDispatcherInputWrite
 
 
 __all__ = [
-    "WaterPartialBidCalculationInput",
-    "WaterPartialBidCalculationInputWrite",
-    "WaterPartialBidCalculationInputApply",
-    "WaterPartialBidCalculationInputList",
-    "WaterPartialBidCalculationInputWriteList",
-    "WaterPartialBidCalculationInputApplyList",
-    "WaterPartialBidCalculationInputFields",
-    "WaterPartialBidCalculationInputTextFields",
+    "TaskDispatcherOutput",
+    "TaskDispatcherOutputWrite",
+    "TaskDispatcherOutputApply",
+    "TaskDispatcherOutputList",
+    "TaskDispatcherOutputWriteList",
+    "TaskDispatcherOutputApplyList",
+    "TaskDispatcherOutputFields",
+    "TaskDispatcherOutputTextFields",
 ]
 
 
-WaterPartialBidCalculationInputTextFields = Literal["process_id", "function_name", "function_call_id"]
-WaterPartialBidCalculationInputFields = Literal["process_id", "process_step", "function_name", "function_call_id"]
+TaskDispatcherOutputTextFields = Literal["process_id", "function_name", "function_call_id"]
+TaskDispatcherOutputFields = Literal["process_id", "process_step", "function_name", "function_call_id"]
 
-_WATERPARTIALBIDCALCULATIONINPUT_PROPERTIES_BY_FIELD = {
+_TASKDISPATCHEROUTPUT_PROPERTIES_BY_FIELD = {
     "process_id": "processId",
     "process_step": "processStep",
     "function_name": "functionName",
     "function_call_id": "functionCallId",
 }
 
 
-class WaterPartialBidCalculationInputGraphQL(GraphQLCore):
-    """This represents the reading version of water partial bid calculation input, used
+class TaskDispatcherOutputGraphQL(GraphQLCore):
+    """This represents the reading version of task dispatcher output, used
     when data is retrieved from CDF using GraphQL.
 
     It is used when retrieving data from CDF using GraphQL.
 
     Args:
         space: The space where the node is located.
-        external_id: The external id of the water partial bid calculation input.
-        data_record: The data record of the water partial bid calculation input node.
+        external_id: The external id of the task dispatcher output.
+        data_record: The data record of the task dispatcher output node.
         process_id: The process associated with the function execution
         process_step: This is the step in the process.
         function_name: The name of the function
         function_call_id: The function call id
-        calculation_task: The calculation task field.
+        alerts: An array of calculation level Alerts.
+        input_: The previous step in the process.
+        process_sub_tasks: An array of input for process subtasks used for partial bid calculations.
     """
 
-    view_id = dm.ViewId("sp_powerops_models", "WaterPartialBidCalculationInput", "1")
+    view_id = dm.ViewId("sp_powerops_models_temp", "TaskDispatcherOutput", "1")
     process_id: Optional[str] = Field(None, alias="processId")
     process_step: Optional[int] = Field(None, alias="processStep")
     function_name: Optional[str] = Field(None, alias="functionName")
     function_call_id: Optional[str] = Field(None, alias="functionCallId")
-    calculation_task: Optional[BidCalculationTaskGraphQL] = Field(None, repr=False, alias="calculationTask")
+    alerts: Optional[list[AlertGraphQL]] = Field(default=None, repr=False)
+    input_: Optional[TaskDispatcherInputGraphQL] = Field(None, repr=False, alias="input")
+    process_sub_tasks: Optional[list[FunctionInputGraphQL]] = Field(default=None, repr=False, alias="processSubTasks")
 
     @model_validator(mode="before")
     def parse_data_record(cls, values: Any) -> Any:
         if not isinstance(values, dict):
             return values
         if "lastUpdatedTime" in values or "createdTime" in values:
             values["dataRecord"] = DataRecordGraphQL(
                 created_time=values.pop("createdTime", None),
                 last_updated_time=values.pop("lastUpdatedTime", None),
             )
         return values
 
-    @field_validator("calculation_task", mode="before")
+    @field_validator("alerts", "input_", "process_sub_tasks", mode="before")
     def parse_graphql(cls, value: Any) -> Any:
         if not isinstance(value, dict):
             return value
         if "items" in value:
             return value["items"]
         return value
 
-    def as_read(self) -> WaterPartialBidCalculationInput:
-        """Convert this GraphQL format of water partial bid calculation input to the reading format."""
+    def as_read(self) -> TaskDispatcherOutput:
+        """Convert this GraphQL format of task dispatcher output to the reading format."""
         if self.data_record is None:
             raise ValueError("This object cannot be converted to a read format because it lacks a data record.")
-        return WaterPartialBidCalculationInput(
+        return TaskDispatcherOutput(
             space=self.space,
             external_id=self.external_id,
             data_record=DataRecord(
                 version=0,
                 last_updated_time=self.data_record.last_updated_time,
                 created_time=self.data_record.created_time,
             ),
             process_id=self.process_id,
             process_step=self.process_step,
             function_name=self.function_name,
             function_call_id=self.function_call_id,
-            calculation_task=(
-                self.calculation_task.as_read()
-                if isinstance(self.calculation_task, GraphQLCore)
-                else self.calculation_task
-            ),
+            alerts=[alert.as_read() if isinstance(alert, GraphQLCore) else alert for alert in self.alerts or []],
+            input_=self.input_.as_read() if isinstance(self.input_, GraphQLCore) else self.input_,
+            process_sub_tasks=[
+                process_sub_task.as_read() if isinstance(process_sub_task, GraphQLCore) else process_sub_task
+                for process_sub_task in self.process_sub_tasks or []
+            ],
         )
 
-    def as_write(self) -> WaterPartialBidCalculationInputWrite:
-        """Convert this GraphQL format of water partial bid calculation input to the writing format."""
-        return WaterPartialBidCalculationInputWrite(
+    def as_write(self) -> TaskDispatcherOutputWrite:
+        """Convert this GraphQL format of task dispatcher output to the writing format."""
+        return TaskDispatcherOutputWrite(
             space=self.space,
             external_id=self.external_id,
             data_record=DataRecordWrite(existing_version=0),
             process_id=self.process_id,
             process_step=self.process_step,
             function_name=self.function_name,
             function_call_id=self.function_call_id,
-            calculation_task=(
-                self.calculation_task.as_write()
-                if isinstance(self.calculation_task, DomainModel)
-                else self.calculation_task
-            ),
+            alerts=[alert.as_write() if isinstance(alert, DomainModel) else alert for alert in self.alerts or []],
+            input_=self.input_.as_write() if isinstance(self.input_, DomainModel) else self.input_,
+            process_sub_tasks=[
+                process_sub_task.as_write() if isinstance(process_sub_task, DomainModel) else process_sub_task
+                for process_sub_task in self.process_sub_tasks or []
+            ],
         )
 
 
-class WaterPartialBidCalculationInput(DomainModel):
-    """This represents the reading version of water partial bid calculation input.
+class TaskDispatcherOutput(FunctionOutput):
+    """This represents the reading version of task dispatcher output.
 
     It is used to when data is retrieved from CDF.
 
     Args:
         space: The space where the node is located.
-        external_id: The external id of the water partial bid calculation input.
-        data_record: The data record of the water partial bid calculation input node.
+        external_id: The external id of the task dispatcher output.
+        data_record: The data record of the task dispatcher output node.
         process_id: The process associated with the function execution
         process_step: This is the step in the process.
         function_name: The name of the function
         function_call_id: The function call id
-        calculation_task: The calculation task field.
+        alerts: An array of calculation level Alerts.
+        input_: The previous step in the process.
+        process_sub_tasks: An array of input for process subtasks used for partial bid calculations.
     """
 
-    space: str = DEFAULT_INSTANCE_SPACE
     node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference(
-        "sp_powerops_types", "WaterPartialBidCalculationInput"
+        "sp_powerops_types_temp", "TaskDispatcherOutput"
     )
-    process_id: str = Field(alias="processId")
-    process_step: int = Field(alias="processStep")
-    function_name: str = Field(alias="functionName")
-    function_call_id: str = Field(alias="functionCallId")
-    calculation_task: Union[BidCalculationTask, str, dm.NodeId, None] = Field(None, repr=False, alias="calculationTask")
-
-    def as_write(self) -> WaterPartialBidCalculationInputWrite:
-        """Convert this read version of water partial bid calculation input to the writing version."""
-        return WaterPartialBidCalculationInputWrite(
+    input_: Union[TaskDispatcherInput, str, dm.NodeId, None] = Field(None, repr=False, alias="input")
+    process_sub_tasks: Union[list[FunctionInput], list[str], list[dm.NodeId], None] = Field(
+        default=None, repr=False, alias="processSubTasks"
+    )
+
+    def as_write(self) -> TaskDispatcherOutputWrite:
+        """Convert this read version of task dispatcher output to the writing version."""
+        return TaskDispatcherOutputWrite(
             space=self.space,
             external_id=self.external_id,
             data_record=DataRecordWrite(existing_version=self.data_record.version),
             process_id=self.process_id,
             process_step=self.process_step,
             function_name=self.function_name,
             function_call_id=self.function_call_id,
-            calculation_task=(
-                self.calculation_task.as_write()
-                if isinstance(self.calculation_task, DomainModel)
-                else self.calculation_task
-            ),
+            alerts=[alert.as_write() if isinstance(alert, DomainModel) else alert for alert in self.alerts or []],
+            input_=self.input_.as_write() if isinstance(self.input_, DomainModel) else self.input_,
+            process_sub_tasks=[
+                process_sub_task.as_write() if isinstance(process_sub_task, DomainModel) else process_sub_task
+                for process_sub_task in self.process_sub_tasks or []
+            ],
         )
 
-    def as_apply(self) -> WaterPartialBidCalculationInputWrite:
-        """Convert this read version of water partial bid calculation input to the writing version."""
+    def as_apply(self) -> TaskDispatcherOutputWrite:
+        """Convert this read version of task dispatcher output to the writing version."""
         warnings.warn(
             "as_apply is deprecated and will be removed in v1.0. Use as_write instead.",
             UserWarning,
             stacklevel=2,
         )
         return self.as_write()
 
 
-class WaterPartialBidCalculationInputWrite(DomainModelWrite):
-    """This represents the writing version of water partial bid calculation input.
+class TaskDispatcherOutputWrite(FunctionOutputWrite):
+    """This represents the writing version of task dispatcher output.
 
     It is used to when data is sent to CDF.
 
     Args:
         space: The space where the node is located.
-        external_id: The external id of the water partial bid calculation input.
-        data_record: The data record of the water partial bid calculation input node.
+        external_id: The external id of the task dispatcher output.
+        data_record: The data record of the task dispatcher output node.
         process_id: The process associated with the function execution
         process_step: This is the step in the process.
         function_name: The name of the function
         function_call_id: The function call id
-        calculation_task: The calculation task field.
+        alerts: An array of calculation level Alerts.
+        input_: The previous step in the process.
+        process_sub_tasks: An array of input for process subtasks used for partial bid calculations.
     """
 
-    space: str = DEFAULT_INSTANCE_SPACE
     node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference(
-        "sp_powerops_types", "WaterPartialBidCalculationInput"
+        "sp_powerops_types_temp", "TaskDispatcherOutput"
     )
-    process_id: str = Field(alias="processId")
-    process_step: int = Field(alias="processStep")
-    function_name: str = Field(alias="functionName")
-    function_call_id: str = Field(alias="functionCallId")
-    calculation_task: Union[BidCalculationTaskWrite, str, dm.NodeId, None] = Field(
-        None, repr=False, alias="calculationTask"
+    input_: Union[TaskDispatcherInputWrite, str, dm.NodeId, None] = Field(None, repr=False, alias="input")
+    process_sub_tasks: Union[list[FunctionInputWrite], list[str], list[dm.NodeId], None] = Field(
+        default=None, repr=False, alias="processSubTasks"
     )
 
     def _to_instances_write(
         self,
         cache: set[tuple[str, str]],
         view_by_read_class: dict[type[DomainModelCore], dm.ViewId] | None,
         write_none: bool = False,
         allow_version_increase: bool = False,
     ) -> ResourcesWrite:
         resources = ResourcesWrite()
         if self.as_tuple_id() in cache:
             return resources
 
         write_view = (view_by_read_class or {}).get(
-            WaterPartialBidCalculationInput, dm.ViewId("sp_powerops_models", "WaterPartialBidCalculationInput", "1")
+            TaskDispatcherOutput, dm.ViewId("sp_powerops_models_temp", "TaskDispatcherOutput", "1")
         )
 
         properties: dict[str, Any] = {}
 
         if self.process_id is not None:
             properties["processId"] = self.process_id
 
@@ -239,22 +247,18 @@
 
         if self.function_name is not None:
             properties["functionName"] = self.function_name
 
         if self.function_call_id is not None:
             properties["functionCallId"] = self.function_call_id
 
-        if self.calculation_task is not None:
-            properties["calculationTask"] = {
-                "space": self.space if isinstance(self.calculation_task, str) else self.calculation_task.space,
-                "externalId": (
-                    self.calculation_task
-                    if isinstance(self.calculation_task, str)
-                    else self.calculation_task.external_id
-                ),
+        if self.input_ is not None:
+            properties["input"] = {
+                "space": self.space if isinstance(self.input_, str) else self.input_.space,
+                "externalId": self.input_ if isinstance(self.input_, str) else self.input_.external_id,
             }
 
         if properties:
             this_node = dm.NodeApply(
                 space=self.space,
                 external_id=self.external_id,
                 existing_version=None if allow_version_increase else self.data_record.existing_version,
@@ -265,72 +269,98 @@
                         properties=properties,
                     )
                 ],
             )
             resources.nodes.append(this_node)
             cache.add(self.as_tuple_id())
 
-        if isinstance(self.calculation_task, DomainModelWrite):
-            other_resources = self.calculation_task._to_instances_write(cache, view_by_read_class)
+        edge_type = dm.DirectRelationReference("sp_powerops_types_temp", "calculationIssue")
+        for alert in self.alerts or []:
+            other_resources = DomainRelationWrite.from_edge_to_resources(
+                cache,
+                start_node=self,
+                end_node=alert,
+                edge_type=edge_type,
+                view_by_read_class=view_by_read_class,
+                write_none=write_none,
+                allow_version_increase=allow_version_increase,
+            )
+            resources.extend(other_resources)
+
+        edge_type = dm.DirectRelationReference("sp_powerops_types_temp", "processSubTasks")
+        for process_sub_task in self.process_sub_tasks or []:
+            other_resources = DomainRelationWrite.from_edge_to_resources(
+                cache,
+                start_node=self,
+                end_node=process_sub_task,
+                edge_type=edge_type,
+                view_by_read_class=view_by_read_class,
+                write_none=write_none,
+                allow_version_increase=allow_version_increase,
+            )
+            resources.extend(other_resources)
+
+        if isinstance(self.input_, DomainModelWrite):
+            other_resources = self.input_._to_instances_write(cache, view_by_read_class)
             resources.extend(other_resources)
 
         return resources
 
 
-class WaterPartialBidCalculationInputApply(WaterPartialBidCalculationInputWrite):
-    def __new__(cls, *args, **kwargs) -> WaterPartialBidCalculationInputApply:
+class TaskDispatcherOutputApply(TaskDispatcherOutputWrite):
+    def __new__(cls, *args, **kwargs) -> TaskDispatcherOutputApply:
         warnings.warn(
-            "WaterPartialBidCalculationInputApply is deprecated and will be removed in v1.0. Use WaterPartialBidCalculationInputWrite instead."
+            "TaskDispatcherOutputApply is deprecated and will be removed in v1.0. Use TaskDispatcherOutputWrite instead."
             "The motivation for this change is that Write is a more descriptive name for the writing version of the"
-            "WaterPartialBidCalculationInput.",
+            "TaskDispatcherOutput.",
             UserWarning,
             stacklevel=2,
         )
         return super().__new__(cls)
 
 
-class WaterPartialBidCalculationInputList(DomainModelList[WaterPartialBidCalculationInput]):
-    """List of water partial bid calculation inputs in the read version."""
+class TaskDispatcherOutputList(DomainModelList[TaskDispatcherOutput]):
+    """List of task dispatcher outputs in the read version."""
 
-    _INSTANCE = WaterPartialBidCalculationInput
+    _INSTANCE = TaskDispatcherOutput
 
-    def as_write(self) -> WaterPartialBidCalculationInputWriteList:
-        """Convert these read versions of water partial bid calculation input to the writing versions."""
-        return WaterPartialBidCalculationInputWriteList([node.as_write() for node in self.data])
+    def as_write(self) -> TaskDispatcherOutputWriteList:
+        """Convert these read versions of task dispatcher output to the writing versions."""
+        return TaskDispatcherOutputWriteList([node.as_write() for node in self.data])
 
-    def as_apply(self) -> WaterPartialBidCalculationInputWriteList:
+    def as_apply(self) -> TaskDispatcherOutputWriteList:
         """Convert these read versions of primitive nullable to the writing versions."""
         warnings.warn(
             "as_apply is deprecated and will be removed in v1.0. Use as_write instead.",
             UserWarning,
             stacklevel=2,
         )
         return self.as_write()
 
 
-class WaterPartialBidCalculationInputWriteList(DomainModelWriteList[WaterPartialBidCalculationInputWrite]):
-    """List of water partial bid calculation inputs in the writing version."""
+class TaskDispatcherOutputWriteList(DomainModelWriteList[TaskDispatcherOutputWrite]):
+    """List of task dispatcher outputs in the writing version."""
 
-    _INSTANCE = WaterPartialBidCalculationInputWrite
+    _INSTANCE = TaskDispatcherOutputWrite
 
 
-class WaterPartialBidCalculationInputApplyList(WaterPartialBidCalculationInputWriteList): ...
+class TaskDispatcherOutputApplyList(TaskDispatcherOutputWriteList): ...
 
 
-def _create_water_partial_bid_calculation_input_filter(
+def _create_task_dispatcher_output_filter(
     view_id: dm.ViewId,
     process_id: str | list[str] | None = None,
     process_id_prefix: str | None = None,
     min_process_step: int | None = None,
     max_process_step: int | None = None,
     function_name: str | list[str] | None = None,
     function_name_prefix: str | None = None,
     function_call_id: str | list[str] | None = None,
     function_call_id_prefix: str | None = None,
-    calculation_task: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
+    input_: str | tuple[str, str] | list[str] | list[tuple[str, str]] | None = None,
     external_id_prefix: str | None = None,
     space: str | list[str] | None = None,
     filter: dm.Filter | None = None,
 ) -> dm.Filter | None:
     filters = []
     if isinstance(process_id, str):
         filters.append(dm.filters.Equals(view_id.as_property_ref("processId"), value=process_id))
@@ -350,40 +380,35 @@
         filters.append(dm.filters.Prefix(view_id.as_property_ref("functionName"), value=function_name_prefix))
     if isinstance(function_call_id, str):
         filters.append(dm.filters.Equals(view_id.as_property_ref("functionCallId"), value=function_call_id))
     if function_call_id and isinstance(function_call_id, list):
         filters.append(dm.filters.In(view_id.as_property_ref("functionCallId"), values=function_call_id))
     if function_call_id_prefix is not None:
         filters.append(dm.filters.Prefix(view_id.as_property_ref("functionCallId"), value=function_call_id_prefix))
-    if calculation_task and isinstance(calculation_task, str):
+    if input_ and isinstance(input_, str):
         filters.append(
             dm.filters.Equals(
-                view_id.as_property_ref("calculationTask"),
-                value={"space": DEFAULT_INSTANCE_SPACE, "externalId": calculation_task},
+                view_id.as_property_ref("input"), value={"space": DEFAULT_INSTANCE_SPACE, "externalId": input_}
             )
         )
-    if calculation_task and isinstance(calculation_task, tuple):
+    if input_ and isinstance(input_, tuple):
         filters.append(
-            dm.filters.Equals(
-                view_id.as_property_ref("calculationTask"),
-                value={"space": calculation_task[0], "externalId": calculation_task[1]},
-            )
+            dm.filters.Equals(view_id.as_property_ref("input"), value={"space": input_[0], "externalId": input_[1]})
         )
-    if calculation_task and isinstance(calculation_task, list) and isinstance(calculation_task[0], str):
+    if input_ and isinstance(input_, list) and isinstance(input_[0], str):
         filters.append(
             dm.filters.In(
-                view_id.as_property_ref("calculationTask"),
-                values=[{"space": DEFAULT_INSTANCE_SPACE, "externalId": item} for item in calculation_task],
+                view_id.as_property_ref("input"),
+                values=[{"space": DEFAULT_INSTANCE_SPACE, "externalId": item} for item in input_],
             )
         )
-    if calculation_task and isinstance(calculation_task, list) and isinstance(calculation_task[0], tuple):
+    if input_ and isinstance(input_, list) and isinstance(input_[0], tuple):
         filters.append(
             dm.filters.In(
-                view_id.as_property_ref("calculationTask"),
-                values=[{"space": item[0], "externalId": item[1]} for item in calculation_task],
+                view_id.as_property_ref("input"), values=[{"space": item[0], "externalId": item[1]} for item in input_]
             )
         )
     if external_id_prefix is not None:
         filters.append(dm.filters.Prefix(["node", "externalId"], value=external_id_prefix))
     if isinstance(space, str):
         filters.append(dm.filters.Equals(["node", "space"], value=space))
     if space and isinstance(space, list):
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_water_partial_bid_calculation_output.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_water_partial_bid_calculation_output.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_watercourse.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_watercourse.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_generated/v1/data_classes/_watercourse_shop.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_generated/v1/data_classes/_price_area.py`

 * *Files 27% similar despite different names*

```diff
@@ -17,189 +17,204 @@
     DomainModelWrite,
     DomainModelWriteList,
     DomainModelList,
     DomainRelationWrite,
     GraphQLCore,
     ResourcesWrite,
 )
+from ._power_asset import PowerAsset, PowerAssetWrite
 
 
 __all__ = [
-    "WatercourseShop",
-    "WatercourseShopWrite",
-    "WatercourseShopApply",
-    "WatercourseShopList",
-    "WatercourseShopWriteList",
-    "WatercourseShopApplyList",
-    "WatercourseShopFields",
-    "WatercourseShopTextFields",
+    "PriceArea",
+    "PriceAreaWrite",
+    "PriceAreaApply",
+    "PriceAreaList",
+    "PriceAreaWriteList",
+    "PriceAreaApplyList",
+    "PriceAreaFields",
+    "PriceAreaTextFields",
 ]
 
 
-WatercourseShopTextFields = Literal["name", "display_name"]
-WatercourseShopFields = Literal["name", "display_name", "ordering"]
+PriceAreaTextFields = Literal["name", "display_name", "asset_type", "timezone"]
+PriceAreaFields = Literal["name", "display_name", "ordering", "asset_type", "timezone"]
 
-_WATERCOURSESHOP_PROPERTIES_BY_FIELD = {
+_PRICEAREA_PROPERTIES_BY_FIELD = {
     "name": "name",
     "display_name": "displayName",
     "ordering": "ordering",
+    "asset_type": "assetType",
+    "timezone": "timezone",
 }
 
 
-class WatercourseShopGraphQL(GraphQLCore):
-    """This represents the reading version of watercourse shop, used
+class PriceAreaGraphQL(GraphQLCore):
+    """This represents the reading version of price area, used
     when data is retrieved from CDF using GraphQL.
 
     It is used when retrieving data from CDF using GraphQL.
 
     Args:
         space: The space where the node is located.
-        external_id: The external id of the watercourse shop.
-        data_record: The data record of the watercourse shop node.
+        external_id: The external id of the price area.
+        data_record: The data record of the price area node.
         name: Name for the Asset
         display_name: Display name for the Asset.
         ordering: The ordering of the asset
+        asset_type: The type of the asset
+        timezone: The timezone of the price area
     """
 
-    view_id = dm.ViewId("sp_powerops_models", "WatercourseShop", "1")
+    view_id = dm.ViewId("sp_powerops_models_temp", "PriceArea", "1")
     name: Optional[str] = None
     display_name: Optional[str] = Field(None, alias="displayName")
     ordering: Optional[int] = None
+    asset_type: Optional[str] = Field(None, alias="assetType")
+    timezone: Optional[str] = None
 
     @model_validator(mode="before")
     def parse_data_record(cls, values: Any) -> Any:
         if not isinstance(values, dict):
             return values
         if "lastUpdatedTime" in values or "createdTime" in values:
             values["dataRecord"] = DataRecordGraphQL(
                 created_time=values.pop("createdTime", None),
                 last_updated_time=values.pop("lastUpdatedTime", None),
             )
         return values
 
-    def as_read(self) -> WatercourseShop:
-        """Convert this GraphQL format of watercourse shop to the reading format."""
+    def as_read(self) -> PriceArea:
+        """Convert this GraphQL format of price area to the reading format."""
         if self.data_record is None:
             raise ValueError("This object cannot be converted to a read format because it lacks a data record.")
-        return WatercourseShop(
+        return PriceArea(
             space=self.space,
             external_id=self.external_id,
             data_record=DataRecord(
                 version=0,
                 last_updated_time=self.data_record.last_updated_time,
                 created_time=self.data_record.created_time,
             ),
             name=self.name,
             display_name=self.display_name,
             ordering=self.ordering,
+            asset_type=self.asset_type,
+            timezone=self.timezone,
         )
 
-    def as_write(self) -> WatercourseShopWrite:
-        """Convert this GraphQL format of watercourse shop to the writing format."""
-        return WatercourseShopWrite(
+    def as_write(self) -> PriceAreaWrite:
+        """Convert this GraphQL format of price area to the writing format."""
+        return PriceAreaWrite(
             space=self.space,
             external_id=self.external_id,
             data_record=DataRecordWrite(existing_version=0),
             name=self.name,
             display_name=self.display_name,
             ordering=self.ordering,
+            asset_type=self.asset_type,
+            timezone=self.timezone,
         )
 
 
-class WatercourseShop(DomainModel):
-    """This represents the reading version of watercourse shop.
+class PriceArea(PowerAsset):
+    """This represents the reading version of price area.
 
     It is used to when data is retrieved from CDF.
 
     Args:
         space: The space where the node is located.
-        external_id: The external id of the watercourse shop.
-        data_record: The data record of the watercourse shop node.
+        external_id: The external id of the price area.
+        data_record: The data record of the price area node.
         name: Name for the Asset
         display_name: Display name for the Asset.
         ordering: The ordering of the asset
+        asset_type: The type of the asset
+        timezone: The timezone of the price area
     """
 
-    space: str = DEFAULT_INSTANCE_SPACE
     node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference(
-        "sp_powerops_types", "WatercourseShop"
+        "sp_powerops_types_temp", "PriceArea"
     )
-    name: str
-    display_name: Optional[str] = Field(None, alias="displayName")
-    ordering: Optional[int] = None
+    timezone: str
 
-    def as_write(self) -> WatercourseShopWrite:
-        """Convert this read version of watercourse shop to the writing version."""
-        return WatercourseShopWrite(
+    def as_write(self) -> PriceAreaWrite:
+        """Convert this read version of price area to the writing version."""
+        return PriceAreaWrite(
             space=self.space,
             external_id=self.external_id,
             data_record=DataRecordWrite(existing_version=self.data_record.version),
             name=self.name,
             display_name=self.display_name,
             ordering=self.ordering,
+            asset_type=self.asset_type,
+            timezone=self.timezone,
         )
 
-    def as_apply(self) -> WatercourseShopWrite:
-        """Convert this read version of watercourse shop to the writing version."""
+    def as_apply(self) -> PriceAreaWrite:
+        """Convert this read version of price area to the writing version."""
         warnings.warn(
             "as_apply is deprecated and will be removed in v1.0. Use as_write instead.",
             UserWarning,
             stacklevel=2,
         )
         return self.as_write()
 
 
-class WatercourseShopWrite(DomainModelWrite):
-    """This represents the writing version of watercourse shop.
+class PriceAreaWrite(PowerAssetWrite):
+    """This represents the writing version of price area.
 
     It is used to when data is sent to CDF.
 
     Args:
         space: The space where the node is located.
-        external_id: The external id of the watercourse shop.
-        data_record: The data record of the watercourse shop node.
+        external_id: The external id of the price area.
+        data_record: The data record of the price area node.
         name: Name for the Asset
         display_name: Display name for the Asset.
         ordering: The ordering of the asset
+        asset_type: The type of the asset
+        timezone: The timezone of the price area
     """
 
-    space: str = DEFAULT_INSTANCE_SPACE
     node_type: Union[dm.DirectRelationReference, None] = dm.DirectRelationReference(
-        "sp_powerops_types", "WatercourseShop"
+        "sp_powerops_types_temp", "PriceArea"
     )
-    name: str
-    display_name: Optional[str] = Field(None, alias="displayName")
-    ordering: Optional[int] = None
+    timezone: str
 
     def _to_instances_write(
         self,
         cache: set[tuple[str, str]],
         view_by_read_class: dict[type[DomainModelCore], dm.ViewId] | None,
         write_none: bool = False,
         allow_version_increase: bool = False,
     ) -> ResourcesWrite:
         resources = ResourcesWrite()
         if self.as_tuple_id() in cache:
             return resources
 
-        write_view = (view_by_read_class or {}).get(
-            WatercourseShop, dm.ViewId("sp_powerops_models", "WatercourseShop", "1")
-        )
+        write_view = (view_by_read_class or {}).get(PriceArea, dm.ViewId("sp_powerops_models_temp", "PriceArea", "1"))
 
         properties: dict[str, Any] = {}
 
         if self.name is not None:
             properties["name"] = self.name
 
         if self.display_name is not None or write_none:
             properties["displayName"] = self.display_name
 
         if self.ordering is not None or write_none:
             properties["ordering"] = self.ordering
 
+        if self.asset_type is not None or write_none:
+            properties["assetType"] = self.asset_type
+
+        if self.timezone is not None:
+            properties["timezone"] = self.timezone
+
         if properties:
             this_node = dm.NodeApply(
                 space=self.space,
                 external_id=self.external_id,
                 existing_version=None if allow_version_increase else self.data_record.existing_version,
                 type=self.node_type,
                 sources=[
@@ -211,62 +226,66 @@
             )
             resources.nodes.append(this_node)
             cache.add(self.as_tuple_id())
 
         return resources
 
 
-class WatercourseShopApply(WatercourseShopWrite):
-    def __new__(cls, *args, **kwargs) -> WatercourseShopApply:
+class PriceAreaApply(PriceAreaWrite):
+    def __new__(cls, *args, **kwargs) -> PriceAreaApply:
         warnings.warn(
-            "WatercourseShopApply is deprecated and will be removed in v1.0. Use WatercourseShopWrite instead."
+            "PriceAreaApply is deprecated and will be removed in v1.0. Use PriceAreaWrite instead."
             "The motivation for this change is that Write is a more descriptive name for the writing version of the"
-            "WatercourseShop.",
+            "PriceArea.",
             UserWarning,
             stacklevel=2,
         )
         return super().__new__(cls)
 
 
-class WatercourseShopList(DomainModelList[WatercourseShop]):
-    """List of watercourse shops in the read version."""
+class PriceAreaList(DomainModelList[PriceArea]):
+    """List of price areas in the read version."""
 
-    _INSTANCE = WatercourseShop
+    _INSTANCE = PriceArea
 
-    def as_write(self) -> WatercourseShopWriteList:
-        """Convert these read versions of watercourse shop to the writing versions."""
-        return WatercourseShopWriteList([node.as_write() for node in self.data])
+    def as_write(self) -> PriceAreaWriteList:
+        """Convert these read versions of price area to the writing versions."""
+        return PriceAreaWriteList([node.as_write() for node in self.data])
 
-    def as_apply(self) -> WatercourseShopWriteList:
+    def as_apply(self) -> PriceAreaWriteList:
         """Convert these read versions of primitive nullable to the writing versions."""
         warnings.warn(
             "as_apply is deprecated and will be removed in v1.0. Use as_write instead.",
             UserWarning,
             stacklevel=2,
         )
         return self.as_write()
 
 
-class WatercourseShopWriteList(DomainModelWriteList[WatercourseShopWrite]):
-    """List of watercourse shops in the writing version."""
+class PriceAreaWriteList(DomainModelWriteList[PriceAreaWrite]):
+    """List of price areas in the writing version."""
 
-    _INSTANCE = WatercourseShopWrite
+    _INSTANCE = PriceAreaWrite
 
 
-class WatercourseShopApplyList(WatercourseShopWriteList): ...
+class PriceAreaApplyList(PriceAreaWriteList): ...
 
 
-def _create_watercourse_shop_filter(
+def _create_price_area_filter(
     view_id: dm.ViewId,
     name: str | list[str] | None = None,
     name_prefix: str | None = None,
     display_name: str | list[str] | None = None,
     display_name_prefix: str | None = None,
     min_ordering: int | None = None,
     max_ordering: int | None = None,
+    asset_type: str | list[str] | None = None,
+    asset_type_prefix: str | None = None,
+    timezone: str | list[str] | None = None,
+    timezone_prefix: str | None = None,
     external_id_prefix: str | None = None,
     space: str | list[str] | None = None,
     filter: dm.Filter | None = None,
 ) -> dm.Filter | None:
     filters = []
     if isinstance(name, str):
         filters.append(dm.filters.Equals(view_id.as_property_ref("name"), value=name))
@@ -278,14 +297,26 @@
         filters.append(dm.filters.Equals(view_id.as_property_ref("displayName"), value=display_name))
     if display_name and isinstance(display_name, list):
         filters.append(dm.filters.In(view_id.as_property_ref("displayName"), values=display_name))
     if display_name_prefix is not None:
         filters.append(dm.filters.Prefix(view_id.as_property_ref("displayName"), value=display_name_prefix))
     if min_ordering is not None or max_ordering is not None:
         filters.append(dm.filters.Range(view_id.as_property_ref("ordering"), gte=min_ordering, lte=max_ordering))
+    if isinstance(asset_type, str):
+        filters.append(dm.filters.Equals(view_id.as_property_ref("assetType"), value=asset_type))
+    if asset_type and isinstance(asset_type, list):
+        filters.append(dm.filters.In(view_id.as_property_ref("assetType"), values=asset_type))
+    if asset_type_prefix is not None:
+        filters.append(dm.filters.Prefix(view_id.as_property_ref("assetType"), value=asset_type_prefix))
+    if isinstance(timezone, str):
+        filters.append(dm.filters.Equals(view_id.as_property_ref("timezone"), value=timezone))
+    if timezone and isinstance(timezone, list):
+        filters.append(dm.filters.In(view_id.as_property_ref("timezone"), values=timezone))
+    if timezone_prefix is not None:
+        filters.append(dm.filters.Prefix(view_id.as_property_ref("timezone"), value=timezone_prefix))
     if external_id_prefix is not None:
         filters.append(dm.filters.Prefix(["node", "externalId"], value=external_id_prefix))
     if isinstance(space, str):
         filters.append(dm.filters.Equals(["node", "space"], value=space))
     if space and isinstance(space, list):
         filters.append(dm.filters.In(["node", "space"], values=space))
     if filter:
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/_logger.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/_logger.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/data_set_api.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/data_set_api.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/powerops_client.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/powerops_client.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/shop/_api_client.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/shop/_api_client.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/shop/api/dayahead_trigger_api.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/shop/api/dayahead_trigger_api.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/shop/api/shop_result_files_api.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/shop/api/shop_result_files_api.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/shop/api/shop_results_api.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/shop/api/shop_results_api.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/shop/api/shop_run_api.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/shop/api/shop_run_api.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/shop/data_classes/__init__.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/shop/data_classes/__init__.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/shop/data_classes/case.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/shop/data_classes/case.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/shop/data_classes/dayahead_trigger.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/shop/data_classes/dayahead_trigger.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/shop/data_classes/helpers.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/shop/data_classes/helpers.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/shop/data_classes/plotting.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/shop/data_classes/plotting.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/shop/data_classes/shop_result_files.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/shop/data_classes/shop_result_files.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/shop/data_classes/shop_results.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/shop/data_classes/shop_results.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/shop/data_classes/shop_results_compare.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/shop/data_classes/shop_results_compare.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/shop/data_classes/shop_run.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/shop/data_classes/shop_run.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/shop/data_classes/shop_run_event.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/shop/data_classes/shop_run_event.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/shop/shop_case.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/shop/shop_case.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/shop/shop_file_reference.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/shop/shop_file_reference.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/shop/shop_run.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/shop/shop_run.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/shop/shop_run_api.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/shop/shop_run_api.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/shop/shop_run_filter.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/shop/shop_run_filter.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/client/shop/utils.py` & `cognite_power_ops-0.92.0/cognite/powerops/client/shop/utils.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/cognite_modules/_system.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/cognite_modules/_system.yaml`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/config.dev.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/config.dev.yaml`

 * *Files 15% similar despite different names*

```diff
@@ -4,53 +4,54 @@
   type: dev
   selected_modules_and_packages:
     - power_model_v1
 
 modules:
   custom_modules:
     power_model_v1:
-      powerops_type_space: 'sp_powerops_types'
-      powerops_instance_space: 'sp_powerops_instance'
-      powerops_models: 'sp_powerops_models'
-      version: '1'
+      # TODO: update space to not be temp
+      powerops_type_space: "sp_powerops_types_temp"
+      powerops_instance_space: "sp_powerops_instance_temp"
+      powerops_models: "sp_powerops_models_temp"
+      version: "1"
     power_model_v0:
       # variables used for substitution in other files in this directory
-      afrr_model_space: 'power-ops-afrr-bid'
-      dayahead_model_space: 'power-ops-day-ahead-bid'
-      shared_model_space: 'power-ops-shared'
-      type_space: 'power-ops-types'
-      powerops_instance_space: 'power-ops-instance'
-      asset_space: 'power-ops-assets'
+      afrr_model_space: "power-ops-afrr-bid"
+      dayahead_model_space: "power-ops-day-ahead-bid"
+      shared_model_space: "power-ops-shared"
+      type_space: "power-ops-types"
+      powerops_instance_space: "power-ops-instance"
+      asset_space: "power-ops-assets"
 
       # Shared Model Views
-      base_BidDocument_version: '1'
-      base_Alert_version: '1'
-      base_BidMethod_version: '1'
+      base_BidDocument_version: "1"
+      base_Alert_version: "1"
+      base_BidMethod_version: "1"
 
       # AFRR Bid Model Views
-      afrrbid_BidDocument_version: '1'
-      afrrbid_BidMethod_version: '1'
-      afrrbid_BidRow_version: '1'
-      afrrbid_PriceArea_version: '1'
+      afrrbid_BidDocument_version: "1"
+      afrrbid_BidMethod_version: "1"
+      afrrbid_BidRow_version: "1"
+      afrrbid_PriceArea_version: "1"
 
       # Day-Ahead Bid Model Views
-      dayaheadbid_BidDocument_version: '1'
-      dayaheadbid_BidMethod_version: '1'
-      dayaheadbid_BidMatrix_version: '1'
-      dayaheadbid_PriceArea_version: '1'
-      dayaheadbid_SHOPMultiScenarioMethod_version: '1'
-      dayaheadbid_MultiScenarioMatrix_version: '1'
-      dayaheadbid_CustomBidMethod_version: '1'
-      dayaheadbid_WaterValueBasedMethod_version: '1'
-      dayaheadbid_BasicBidMatrix_version: '1'
-      dayaheadbid_CustomBidMatrix_version: '1'
-      dayaheadbid_SHOPPriceScenario_version: '1'
-      dayaheadbid_SHOPPriceScenarioResult_version: '1'
+      dayaheadbid_BidDocument_version: "1"
+      dayaheadbid_BidMethod_version: "1"
+      dayaheadbid_BidMatrix_version: "1"
+      dayaheadbid_PriceArea_version: "1"
+      dayaheadbid_SHOPMultiScenarioMethod_version: "1"
+      dayaheadbid_MultiScenarioMatrix_version: "1"
+      dayaheadbid_CustomBidMethod_version: "1"
+      dayaheadbid_WaterValueBasedMethod_version: "1"
+      dayaheadbid_BasicBidMatrix_version: "1"
+      dayaheadbid_CustomBidMatrix_version: "1"
+      dayaheadbid_SHOPPriceScenario_version: "1"
+      dayaheadbid_SHOPPriceScenarioResult_version: "1"
 
       # Asset Model Views
-      asset_PriceArea_version: '1'
-      asset_Watercourse_version: '1'
-      asset_Plant_version: '1'
-      asset_Generator_version: '1'
-      asset_Reservoir_version: '1'
-      asset_TurbineEfficiency_version: '1'
-      asset_GeneratorEfficiency_version: '1'
+      asset_PriceArea_version: "1"
+      asset_Watercourse_version: "1"
+      asset_Plant_version: "1"
+      asset_Generator_version: "1"
+      asset_Reservoir_version: "1"
+      asset_TurbineEfficiency_version: "1"
+      asset_GeneratorEfficiency_version: "1"
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/AFRRBid.datamodel.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/AFRRBid.datamodel.yaml`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/Asset.datamodel.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/Asset.datamodel.yaml`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/DayAheadBid.datamodel.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/DayAheadBid.datamodel.yaml`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/containers/AFRRBid-BidRow.container.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/containers/AFRRBid-BidRow.container.yaml`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/containers/asset-EfficiencyCurve.container.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/containers/asset-EfficiencyCurve.container.yaml`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/containers/base-Alert.container.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/containers/base-Alert.container.yaml`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/containers/base-Asset.container.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/containers/base-Asset.container.yaml`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/containers/base-BidDocument.container.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/containers/base-BidDocument.container.yaml`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/containers/base-Generator.container.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/containers/base-Generator.container.yaml`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/containers/base-Plant.container.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/containers/base-Plant.container.yaml`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/containers/base-PriceArea.container.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/containers/base-PriceArea.container.yaml`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/containers/base-Watercourse.container.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/containers/base-Watercourse.container.yaml`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/containers/dayAheadBids-BidMatrix.container.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/containers/dayAheadBids-BidMatrix.container.yaml`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/containers/dayAheadBids-MultiScenarioMatrix.container.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/containers/dayAheadBids-MultiScenarioMatrix.container.yaml`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/containers/dayAheadBids-SHOPMultiScenario.container.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/containers/dayAheadBids-SHOPMultiScenario.container.yaml`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/node_types/power-ops-types.powerops_nodes.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/node_types/power-ops-types.powerops_nodes.yaml`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/views/AFRRBid-BidDocument.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/views/AFRRBid-BidDocument.view.yaml`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/views/AFRRBid-BidMethod.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/views/AFRRBid-BidMethod.view.yaml`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/views/AFRRBid-BidRow.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/views/AFRRBid-BidRow.view.yaml`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/views/AFRRBid-PriceArea.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/views/AFRRBid-PriceArea.view.yaml`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/views/asset-Generator.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/views/asset-Generator.view.yaml`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/views/asset-GeneratorEfficiency.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/views/asset-GeneratorEfficiency.view.yaml`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/views/asset-Plant.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/views/asset-Plant.view.yaml`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/views/asset-PriceArea.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/views/asset-PriceArea.view.yaml`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/views/asset-Reservoir.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/views/asset-Reservoir.view.yaml`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/views/asset-TurbineEfficiency.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/views/asset-TurbineEfficiency.view.yaml`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/views/asset-Watercourse.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/views/asset-Watercourse.view.yaml`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/views/baseBids-Alert.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/views/baseBids-Alert.view.yaml`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/views/baseBids-BidDocument.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/views/baseBids-BidDocument.view.yaml`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/views/baseBids-BidMethod.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/views/baseBids-BidMethod.view.yaml`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/views/dayAheadBids-BasicBidMatrix.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/views/dayAheadBids-BasicBidMatrix.view.yaml`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/views/dayAheadBids-BidDocument.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/views/dayAheadBids-BidDocument.view.yaml`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/views/dayAheadBids-BidMatrix.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/views/dayAheadBids-BidMatrix.view.yaml`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/views/dayAheadBids-BidMethod.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/views/dayAheadBids-BidMethod.view.yaml`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/views/dayAheadBids-CustomBidMatrix.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/views/dayAheadBids-CustomBidMatrix.view.yaml`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/views/dayAheadBids-CustomBidMethod.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/views/dayAheadBids-CustomBidMethod.view.yaml`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/views/dayAheadBids-MultiScenarioMatrix.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/views/dayAheadBids-MultiScenarioMatrix.view.yaml`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/views/dayAheadBids-PriceArea.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/views/dayAheadBids-PriceArea.view.yaml`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/views/dayAheadBids-SHOPMultiScenarioMethod.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/views/dayAheadBids-SHOPMultiScenarioMethod.view.yaml`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/views/dayAheadBids-SHOPPriceScenario.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/views/dayAheadBids-SHOPPriceScenario.view.yaml`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/views/dayAheadBids-SHOPPriceScenarioResult.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/views/dayAheadBids-SHOPPriceScenarioResult.view.yaml`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v0/data_models/views/dayAheadBids-WaterValueBasedMethod.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v0/data_models/views/dayAheadBids-WaterValueBasedMethod.view.yaml`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/all_PowerOps.datamodel.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/all_PowerOps.datamodel.yaml`

 * *Files 26% similar despite different names*

```diff
@@ -1,285 +1,193 @@
 externalId: all_PowerOps
-space: '{{powerops_models}}'
-version: '{{version}}'
+space: "{{powerops_models}}"
+version: "{{version}}"
 name: all_PowerOps
 views:
   - externalId: SHOPResult
-    space: '{{powerops_models}}'
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
+    version: "{{version}}"
   - externalId: BidRow
-    space: '{{powerops_models}}'
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
+    version: "{{version}}"
   - externalId: MarketConfiguration
-    space: '{{powerops_models}}'
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
-  - externalId: BidCalculationTask
-    space: '{{powerops_models}}'
-    type: view
-    version: '{{version}}'
+    version: "{{version}}"
   - externalId: Alert
-    space: '{{powerops_models}}'
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
+    version: "{{version}}"
   - externalId: ModelTemplate
-    space: '{{powerops_models}}'
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
+    version: "{{version}}"
   - externalId: Mapping
-    space: '{{powerops_models}}'
-    type: view
-    version: '{{version}}'
-  - externalId: BidConfigurationShop
-    space: '{{powerops_models}}'
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
+    version: "{{version}}"
   - externalId: BidConfiguration
-    space: '{{powerops_models}}'
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
-  - externalId: BidConfigurationAFRR
-    space: '{{powerops_models}}'
+    version: "{{version}}"
+  - externalId: PartialBidConfiguration
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
-  - externalId: BidConfigurationWater
-    space: '{{powerops_models}}'
+    version: "{{version}}"
+  - externalId: ShopBasedPartialBidConfiguration
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
+    version: "{{version}}"
+  - externalId: WaterValueBasedPartialBidConfiguration
+    space: "{{powerops_models}}"
+    type: view
+    version: "{{version}}"
   - externalId: SHOPTriggerInput
-    space: '{{powerops_models}}'
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
-  - externalId: WaterPartialBidCalculationInput
-    space: '{{powerops_models}}'
+    version: "{{version}}"
+  - externalId: WaterValueBasedPartialBidMatrixCalculationInput
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
+    version: "{{version}}"
   - externalId: PreprocessorInput
-    space: '{{powerops_models}}'
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
+    version: "{{version}}"
   - externalId: FunctionInput
-    space: '{{powerops_models}}'
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
+    version: "{{version}}"
   - externalId: TotalBidMatrixCalculationInput
-    space: '{{powerops_models}}'
-    type: view
-    version: '{{version}}'
-  - externalId: TaskDispatcherShopInput
-    space: '{{powerops_models}}'
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
-  - externalId: TaskDispatcherWaterInput
-    space: '{{powerops_models}}'
+    version: "{{version}}"
+  - externalId: TaskDispatcherInput
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
-  - externalId: PartialPostProcessingInput
-    space: '{{powerops_models}}'
+    version: "{{version}}"
+  - externalId: ShopPartialBidMatrixCalculationInput
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
-  - externalId: ShopPartialBidCalculationInput
-    space: '{{powerops_models}}'
-    type: view
-    version: '{{version}}'
+    version: "{{version}}"
   - externalId: Scenario
-    space: '{{powerops_models}}'
+    space: "{{powerops_models}}"
+    type: view
+    version: "{{version}}"
+  - externalId: ScenarioSet
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
+    version: "{{version}}"
   - externalId: PreprocessorOutput
-    space: '{{powerops_models}}'
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
-  - externalId: WaterPartialBidCalculationOutput
-    space: '{{powerops_models}}'
+    version: "{{version}}"
+  - externalId: PartialBidMatrixCalculationInput
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
-  - externalId: SHOPTriggerOutput
-    space: '{{powerops_models}}'
+    version: "{{version}}"
+  - externalId: PartialBidMatrixCalculationOutput
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
-  - externalId: PartialPostProcessingOutput
-    space: '{{powerops_models}}'
-    type: view
-    version: '{{version}}'
-  - externalId: ShopPartialBidCalculationOutput
-    space: '{{powerops_models}}'
+    version: "{{version}}"
+  - externalId: SHOPTriggerOutput
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
+    version: "{{version}}"
   - externalId: FunctionOutput
-    space: '{{powerops_models}}'
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
-  - externalId: TaskDispatcherWaterOutput
-    space: '{{powerops_models}}'
+    version: "{{version}}"
+  - externalId: TaskDispatcherOutput
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
+    version: "{{version}}"
   - externalId: TotalBidMatrixCalculationOutput
-    space: '{{powerops_models}}'
-    type: view
-    version: '{{version}}'
-  - externalId: TaskDispatcherShopOutput
-    space: '{{powerops_models}}'
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
+    version: "{{version}}"
   - externalId: BidDocument
-    space: '{{powerops_models}}'
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
+    version: "{{version}}"
   - externalId: BidDocumentDayAhead
-    space: '{{powerops_models}}'
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
+    version: "{{version}}"
   - externalId: BidDocumentAFRR
-    space: '{{powerops_models}}'
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
+    version: "{{version}}"
   - externalId: PowerAsset
-    space: '{{powerops_models}}'
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
+    version: "{{version}}"
   - externalId: Generator
-    space: '{{powerops_models}}'
-    type: view
-    version: '{{version}}'
-  - externalId: PriceAreaDayAhead
-    space: '{{powerops_models}}'
-    type: view
-    version: '{{version}}'
-  - externalId: WatercourseShop
-    space: '{{powerops_models}}'
-    type: view
-    version: '{{version}}'
-  - externalId: PriceAreaAsset
-    space: '{{powerops_models}}'
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
+    version: "{{version}}"
   - externalId: TurbineEfficiencyCurve
-    space: '{{powerops_models}}'
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
-  - externalId: Reservoir
-    space: '{{powerops_models}}'
-    type: view
-    version: '{{version}}'
+    version: "{{version}}"
   - externalId: Plant
-    space: '{{powerops_models}}'
-    type: view
-    version: '{{version}}'
-  - externalId: PlantShop
-    space: '{{powerops_models}}'
-    type: view
-    version: '{{version}}'
-  - externalId: Watercourse
-    space: '{{powerops_models}}'
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
+    version: "{{version}}"
   - externalId: PriceArea
-    space: '{{powerops_models}}'
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
+    version: "{{version}}"
   - externalId: GeneratorEfficiencyCurve
-    space: '{{powerops_models}}'
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
+    version: "{{version}}"
   - externalId: PriceAreaAFRR
-    space: '{{powerops_models}}'
-    type: view
-    version: '{{version}}'
-  - externalId: BidMethod
-    space: '{{powerops_models}}'
-    type: view
-    version: '{{version}}'
-  - externalId: BidMethodWaterValue
-    space: '{{powerops_models}}'
-    type: view
-    version: '{{version}}'
-  - externalId: BidMethodSHOPMultiScenario
-    space: '{{powerops_models}}'
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
-  - externalId: BidMethodCustom
-    space: '{{powerops_models}}'
-    type: view
-    version: '{{version}}'
-  - externalId: BidMethodDayAhead
-    space: '{{powerops_models}}'
-    type: view
-    version: '{{version}}'
-  - externalId: BidMethodAFRR
-    space: '{{powerops_models}}'
-    type: view
-    version: '{{version}}'
-  - externalId: BidMatrixRaw
-    space: '{{powerops_models}}'
-    type: view
-    version: '{{version}}'
+    version: "{{version}}"
   - externalId: BidMatrix
-    space: '{{powerops_models}}'
-    type: view
-    version: '{{version}}'
-  - externalId: CustomBidMatrix
-    space: '{{powerops_models}}'
-    type: view
-    version: '{{version}}'
-  - externalId: MultiScenarioMatrix
-    space: '{{powerops_models}}'
-    type: view
-    version: '{{version}}'
-  - externalId: MultiScenarioMatrixRaw
-    space: '{{powerops_models}}'
-    type: view
-    version: '{{version}}'
-  - externalId: BasicBidMatrix
-    space: '{{powerops_models}}'
-    type: view
-    version: '{{version}}'
-  - externalId: BasicBidMatrixRaw
-    space: '{{powerops_models}}'
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
-  - space: '{{powerops_models}}'
+    version: "{{version}}"
+  - space: "{{powerops_models}}"
     externalId: ShopObjectiveValue
-    version: '{{version}}'
+    version: "{{version}}"
     type: view
   - externalId: TaskDispatcherBenchmarkingInput
-    space: '{{powerops_models}}'
-    version: '{{version}}'
+    space: "{{powerops_models}}"
+    version: "{{version}}"
     type: view
   - externalId: TaskDispatcherBenchmarkingOutput
-    space: '{{powerops_models}}'
-    version: '{{version}}'
+    space: "{{powerops_models}}"
+    version: "{{version}}"
     type: view
   - externalId: BenchmarkingCollectorInput
-    space: '{{powerops_models}}'
-    version: '{{version}}'
+    space: "{{powerops_models}}"
+    version: "{{version}}"
     type: view
   - externalId: BenchmarkingCollectorOutput
-    space: '{{powerops_models}}'
-    version: '{{version}}'
+    space: "{{powerops_models}}"
+    version: "{{version}}"
     type: view
   - externalId: BidDocumentDayAheadSimple
-    space: '{{powerops_models}}'
-    version: '{{version}}'
+    space: "{{powerops_models}}"
+    version: "{{version}}"
     type: view
   - externalId: Case
-    space: '{{powerops_models}}'
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
+    version: "{{version}}"
   - externalId: Commands
-    space: '{{powerops_models}}'
-    type: view
-    version: '{{version}}'
-  - externalId: PriceProdCase
-    space: '{{powerops_models}}'
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
-  - externalId: SHOPResultPriceProd
-    space: '{{powerops_models}}'
+    version: "{{version}}"
+  - externalId: PriceProduction
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
+    version: "{{version}}"
   - externalId: SHOPTimeSeries
-    space: '{{powerops_models}}'
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
+    version: "{{version}}"
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/compute_DayAheadBenchmarking.datamodel.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/compute_DayAheadBenchmarking.datamodel.yaml`

 * *Files 19% similar despite different names*

```diff
@@ -1,50 +1,74 @@
 externalId: compute_DayAheadBenchmarking
-space: '{{powerops_models}}'
-version: '1'
+space: "{{powerops_models}}"
+version: "1"
 name: DayAheadBenchmarking
-description: 'The process of benchmarking day-ahead bids'
+description: "The process of benchmarking day-ahead bids"
 views:
   - externalId: TaskDispatcherBenchmarkingInput
-    space: '{{powerops_models}}'
-    version: '{{version}}'
+    space: "{{powerops_models}}"
+    version: "{{version}}"
     type: view
   - externalId: TaskDispatcherBenchmarkingOutput
-    space: '{{powerops_models}}'
-    version: '{{version}}'
+    space: "{{powerops_models}}"
+    version: "{{version}}"
     type: view
   - externalId: BenchmarkingCollectorInput
-    space: '{{powerops_models}}'
-    version: '{{version}}'
+    space: "{{powerops_models}}"
+    version: "{{version}}"
     type: view
   - externalId: BenchmarkingCollectorOutput
-    space: '{{powerops_models}}'
-    version: '{{version}}'
+    space: "{{powerops_models}}"
+    version: "{{version}}"
     type: view
   - externalId: BidDocumentDayAheadSimple
-    space: '{{powerops_models}}'
-    version: '{{version}}'
+    space: "{{powerops_models}}"
+    version: "{{version}}"
     type: view
-  - space: '{{powerops_models}}'
-    externalId: BidMethodDayAhead
-    version: '{{version}}'
-    type: view
-  - space: '{{powerops_models}}'
-    externalId: PriceAreaDayAhead
-    version: '{{version}}'
-    type: view
-  - space: '{{powerops_models}}'
+  - space: "{{powerops_models}}"
     externalId: BidMatrix
-    version: '{{version}}'
+    version: "{{version}}"
     type: view
-  - space: '{{powerops_models}}'
+  - space: "{{powerops_models}}"
     externalId: Alert
-    version: '{{version}}'
+    version: "{{version}}"
     type: view
-  - space: '{{powerops_models}}'
+  - space: "{{powerops_models}}"
     externalId: Scenario
-    version: '{{version}}'
+    version: "{{version}}"
     type: view
-  - space: '{{powerops_models}}'
+  - space: "{{powerops_models}}"
     externalId: ShopObjectiveValue
-    version: '{{version}}'
+    version: "{{version}}"
+    type: view
+  - space: "{{powerops_models}}"
+    externalId: FunctionInput
+    version: "{{version}}"
+    type: view
+  - space: "{{powerops_models}}"
+    externalId: FunctionOutput
+    version: "{{version}}"
+    type: view
+  - space: "{{powerops_models}}"
+    externalId: PriceArea
+    version: "{{version}}"
+    type: view
+  - space: "{{powerops_models}}"
+    externalId: ModelTemplate
+    version: "{{version}}"
+    type: view
+  - space: "{{powerops_models}}"
+    externalId: Commands
+    version: "{{version}}"
+    type: view
+  - space: "{{powerops_models}}"
+    externalId: Mapping
+    version: "{{version}}"
+    type: view
+  - space: "{{powerops_models}}"
+    externalId: PowerAsset
+    version: "{{version}}"
+    type: view
+  - space: "{{powerops_models}}"
+    externalId: PartialBidConfiguration
+    version: "{{version}}"
     type: view
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/compute_SHOPBasedDayAhead.datamodel.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/compute_TotalBidMatrixCalculation.datamodel.yaml`

 * *Files 12% similar despite different names*

```diff
@@ -1,110 +1,90 @@
-externalId: compute_SHOPBasedDayAhead
-space: '{{powerops_models}}'
-version: '1'
-name: SHOPBasedDayAheadBidProcess
-description: 'The process of going from BidConfig to a PartialBidMatrix'
+externalId: compute_TotalBidMatrixCalculation
+space: "{{powerops_models}}"
+version: "1"
+name: TotalBidMatrixCalculation
+description: "The process of going from PartialBidMatrix to TotalBidMatrix"
 views:
-  - externalId: TaskDispatcherShopInput
-    space: '{{powerops_models}}'
-    type: view
-    version: '{{version}}'
-  - externalId: TaskDispatcherShopOutput
-    space: '{{powerops_models}}'
-    type: view
-    version: '{{version}}'
-  - externalId: PreprocessorInput
-    space: '{{powerops_models}}'
-    type: view
-    version: '{{version}}'
-  - externalId: PreprocessorOutput
-    space: '{{powerops_models}}'
-    type: view
-    version: '{{version}}'
-  - externalId: SHOPTriggerInput
-    space: '{{powerops_models}}'
-    type: view
-    version: '{{version}}'
-  - externalId: SHOPTriggerOutput
-    space: '{{powerops_models}}'
-    type: view
-    version: '{{version}}'
-  - externalId: ShopPartialBidCalculationInput
-    space: '{{powerops_models}}'
-    type: view
-    version: '{{version}}'
-  - externalId: ShopPartialBidCalculationOutput
-    space: '{{powerops_models}}'
-    type: view
-    version: '{{version}}'
-  - externalId: BidMatrixRaw
-    space: '{{powerops_models}}'
-    type: view
-    version: '{{version}}'
-  - externalId: MultiScenarioMatrixRaw
-    space: '{{powerops_models}}'
+  - externalId: BidMatrix
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
-  - externalId: MarketConfiguration
-    space: '{{powerops_models}}'
+    version: "{{version}}"
+  - externalId: TotalBidMatrixCalculationInput
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
-  - externalId: BidMethodSHOPMultiScenario
-    space: '{{powerops_models}}'
+    version: "{{version}}"
+  - externalId: TotalBidMatrixCalculationOutput
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
-  - externalId: Scenario
-    space: '{{powerops_models}}'
+    version: "{{version}}"
+  - externalId: BidDocumentDayAhead
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
-  - externalId: Mapping
-    space: '{{powerops_models}}'
+    version: "{{version}}"
+  - externalId: PriceArea
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
-  - externalId: ModelTemplate
-    space: '{{powerops_models}}'
+    version: "{{version}}"
+  - externalId: Alert
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
+    version: "{{version}}"
   - externalId: SHOPResult
-    space: '{{powerops_models}}'
-    type: view
-    version: '{{version}}'
-  - externalId: Case
-    space: '{{powerops_models}}'
-    type: view
-    version: '{{version}}'
-  - externalId: SHOPResultPriceProd
-    space: '{{powerops_models}}'
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
-  - externalId: Alert
-    space: '{{powerops_models}}'
+    version: "{{version}}"
+  - externalId: MarketConfiguration
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
-  - externalId: PlantShop
-    space: '{{powerops_models}}'
+    version: "{{version}}"
+  - externalId: Scenario
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
-  - externalId: WatercourseShop
-    space: '{{powerops_models}}'
+    version: "{{version}}"
+  - externalId: ModelTemplate
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
-  - externalId: BidConfigurationShop
-    space: '{{powerops_models}}'
+    version: "{{version}}"
+  - externalId: Mapping
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
-  - externalId: PriceArea
+    version: "{{version}}"
+  - externalId: PriceProduction
     space: "{{powerops_models}}"
     type: view
     version: "{{version}}"
-  - externalId: PriceProdCase
-    space: '{{powerops_models}}'
+  - externalId: Case
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
+    version: "{{version}}"
   - externalId: SHOPTimeSeries
-    space: '{{powerops_models}}'
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
+    version: "{{version}}"
   - externalId: Commands
-    space: '{{powerops_models}}'
+    space: "{{powerops_models}}"
+    type: view
+    version: "{{version}}"
+  - space: "{{powerops_models}}"
+    externalId: FunctionInput
+    version: "{{version}}"
+    type: view
+  - space: "{{powerops_models}}"
+    externalId: FunctionOutput
+    version: "{{version}}"
+    type: view
+  - space: "{{powerops_models}}"
+    externalId: BidDocument
+    version: "{{version}}"
+    type: view
+  - space: "{{powerops_models}}"
+    externalId: PowerAsset
+    version: "{{version}}"
+    type: view
+  - space: "{{powerops_models}}"
+    externalId: PartialBidConfiguration
+    version: "{{version}}"
+    type: view
+  - space: "{{powerops_models}}"
+    externalId: BidConfiguration
+    version: "{{version}}"
     type: view
-    version: '{{version}}'
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/compute_TotalBidCalculation.datamodel.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/compute_SHOPBasedDayAhead.datamodel.yaml`

 * *Files 12% similar despite different names*

```diff
@@ -1,106 +1,118 @@
-externalId: compute_TotalBidCalculation
-space: '{{powerops_models}}'
-version: '1'
-name: TotalBidCalculation
-description: 'The process of going from PartialBidMatrix to TotalBidMatrix'
+externalId: compute_SHOPBasedDayAhead
+space: "{{powerops_models}}"
+version: "1"
+name: SHOPBasedDayAheadBidProcess
+description: "The process of going from BidConfig to a PartialBidMatrix"
 views:
-  - externalId: BidMatrixRaw
-    space: '{{powerops_models}}'
+  - externalId: TaskDispatcherInput
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
-  - externalId: MultiScenarioMatrixRaw
-    space: '{{powerops_models}}'
+    version: "{{version}}"
+  - externalId: TaskDispatcherOutput
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
-  - externalId: BidMatrix
-    space: '{{powerops_models}}'
+    version: "{{version}}"
+  - externalId: PreprocessorInput
+    space: "{{powerops_models}}"
+    type: view
+    version: "{{version}}"
+  - externalId: PreprocessorOutput
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
-  - externalId: MultiScenarioMatrix
-    space: '{{powerops_models}}'
+    version: "{{version}}"
+  - externalId: SHOPTriggerInput
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
-  - externalId: PartialPostProcessingInput
-    space: '{{powerops_models}}'
+    version: "{{version}}"
+  - externalId: SHOPTriggerOutput
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
-  - externalId: PartialPostProcessingOutput
-    space: '{{powerops_models}}'
+    version: "{{version}}"
+  - externalId: PartialBidMatrixCalculationInput
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
-  - externalId: TotalBidMatrixCalculationInput
-    space: '{{powerops_models}}'
+    version: "{{version}}"
+  - externalId: ShopPartialBidMatrixCalculationInput
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
-  - externalId: TotalBidMatrixCalculationOutput
-    space: '{{powerops_models}}'
+    version: "{{version}}"
+  - externalId: PartialBidMatrixCalculationOutput
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
-  - externalId: BidDocumentDayAhead
-    space: '{{powerops_models}}'
+    version: "{{version}}"
+  - externalId: BidMatrix
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
-  - externalId: PriceArea
-    space: '{{powerops_models}}'
+    version: "{{version}}"
+  - externalId: MarketConfiguration
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
-  - externalId: BidMethodDayAhead
-    space: '{{powerops_models}}'
+    version: "{{version}}"
+  - externalId: Scenario
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
-  - externalId: BidMethodWaterValue
-    space: '{{powerops_models}}'
+    version: "{{version}}"
+  - externalId: ScenarioSet
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
-  - externalId: BidMethodSHOPMultiScenario
-    space: '{{powerops_models}}'
+    version: "{{version}}"
+  - externalId: Mapping
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
-  - externalId: Alert
-    space: '{{powerops_models}}'
+    version: "{{version}}"
+  - externalId: ModelTemplate
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
+    version: "{{version}}"
   - externalId: SHOPResult
-    space: '{{powerops_models}}'
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
-  - externalId: SHOPResultPriceProd
-    space: '{{powerops_models}}'
+    version: "{{version}}"
+  - externalId: Case
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
-  - externalId: MarketConfiguration
+    version: "{{version}}"
+  - externalId: Alert
     space: "{{powerops_models}}"
     type: view
     version: "{{version}}"
-  - externalId: Scenario
-    space: '{{powerops_models}}'
+  - externalId: BidConfiguration
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
-  - externalId: ModelTemplate
-    space: '{{powerops_models}}'
+    version: "{{version}}"
+  - externalId: PriceArea
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
-  - externalId: Mapping
-    space: '{{powerops_models}}'
+    version: "{{version}}"
+  - externalId: PriceProduction
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
-  - externalId: WatercourseShop
-    space: '{{powerops_models}}'
+    version: "{{version}}"
+  - externalId: SHOPTimeSeries
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
-  - externalId: PriceProdCase
-    space: '{{powerops_models}}'
+    version: "{{version}}"
+  - externalId: Commands
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
-  - externalId: Case
-    space: '{{powerops_models}}'
+    version: "{{version}}"
+  - space: "{{powerops_models}}"
+    externalId: FunctionInput
+    version: "{{version}}"
     type: view
-    version: '{{version}}'
-  - externalId: SHOPTimeSeries
-    space: '{{powerops_models}}'
+  - space: "{{powerops_models}}"
+    externalId: FunctionOutput
+    version: "{{version}}"
     type: view
-    version: '{{version}}'
-  - externalId: Commands
-    space: '{{powerops_models}}'
+  - space: "{{powerops_models}}"
+    externalId: PowerAsset
+    version: "{{version}}"
+    type: view
+  - space: "{{powerops_models}}"
+    externalId: PartialBidConfiguration
+    version: "{{version}}"
+    type: view
+  - space: "{{powerops_models}}"
+    externalId: ShopBasedPartialBidConfiguration
+    version: "{{version}}"
     type: view
-    version: '{{version}}'
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/compute_WaterValueBasedDayAhead.datamodel.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/compute_WaterValueBasedDayAhead.datamodel.yaml`

 * *Files 12% similar despite different names*

```diff
@@ -1,74 +1,86 @@
 externalId: compute_WaterValueBasedDayAheadBid
 space: "{{powerops_models}}"
 version: "1"
 name: WaterValueBasedDayAheadBidProcess
 description: "The process of going from BidConfig to a PartialBidMatrix"
 views:
-  - externalId: TaskDispatcherWaterInput
+  - externalId: TaskDispatcherInput
     space: "{{powerops_models}}"
     type: view
     version: "{{version}}"
-  - externalId: TaskDispatcherWaterOutput
+  - externalId: TaskDispatcherOutput
     space: "{{powerops_models}}"
     type: view
     version: "{{version}}"
-  - space: "{{powerops_models}}"
-    externalId: BidCalculationTask
-    type: view
-    version: "{{version}}"
-  - externalId: WaterPartialBidCalculationInput
+  - externalId: PartialBidMatrixCalculationInput
     space: "{{powerops_models}}"
     type: view
     version: "{{version}}"
-  - externalId: WaterPartialBidCalculationOutput
+  - externalId: WaterValueBasedPartialBidMatrixCalculationInput
     space: "{{powerops_models}}"
     type: view
     version: "{{version}}"
-  - space: "{{powerops_models}}"
-    externalId: BidMatrixRaw
-    type: view
-    version: "{{version}}"
-  - externalId: BidMethodWaterValue
+  - externalId: PartialBidMatrixCalculationOutput
     space: "{{powerops_models}}"
     type: view
     version: "{{version}}"
   - externalId: Plant
     space: "{{powerops_models}}"
     type: view
     version: "{{version}}"
-  - externalId: Watercourse
-    space: "{{powerops_models}}"
-    type: view
-    version: "{{version}}"
   - externalId: Alert
     space: "{{powerops_models}}"
     type: view
     version: "{{version}}"
-  - externalId: BidConfigurationWater
+  - externalId: BidConfiguration
     space: "{{powerops_models}}"
     type: view
     version: "{{version}}"
   - externalId: PriceArea
     space: "{{powerops_models}}"
     type: view
     version: "{{version}}"
   - externalId: MarketConfiguration
     space: "{{powerops_models}}"
     type: view
     version: "{{version}}"
-  - externalId: Reservoir
-    space: "{{powerops_models}}"
-    type: view
-    version: "{{version}}"
   - externalId: Generator
     space: "{{powerops_models}}"
     type: view
     version: "{{version}}"
   - externalId: GeneratorEfficiencyCurve
     space: "{{powerops_models}}"
     type: view
     version: "{{version}}"
   - externalId: TurbineEfficiencyCurve
     space: "{{powerops_models}}"
     type: view
     version: "{{version}}"
+  - space: "{{powerops_models}}"
+    externalId: FunctionInput
+    version: "{{version}}"
+    type: view
+  - space: "{{powerops_models}}"
+    externalId: FunctionOutput
+    version: "{{version}}"
+    type: view
+  - space: "{{powerops_models}}"
+    externalId: BidMatrix
+    version: "{{version}}"
+    type: view
+  - space: "{{powerops_models}}"
+    externalId: PowerAsset
+    version: "{{version}}"
+    type: view
+  - space: "{{powerops_models}}"
+    externalId: PartialBidConfiguration
+    version: "{{version}}"
+    type: view
+  - space: "{{powerops_models}}"
+    externalId: WaterValueBasedPartialBidConfiguration
+    version: "{{version}}"
+    type: view
+  - externalId: Plant
+    space: "{{powerops_models}}"
+    type: view
+    version: "{{version}}"
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/config_DayAheadConfiguration.datamodel.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/config_DayAheadConfiguration.datamodel.yaml`

 * *Files 6% similar despite different names*

```diff
@@ -4,87 +4,67 @@
 name: DayAheadConfiguration
 description: "Settings for the day ahead bid process"
 views:
   - externalId: BidConfiguration
     space: "{{powerops_models}}"
     type: view
     version: "{{version}}"
-  - externalId: BidConfigurationShop
-    space: "{{powerops_models}}"
-    type: view
-    version: "{{version}}"
-  - externalId: BidConfigurationWater
-    space: "{{powerops_models}}"
-    type: view
-    version: "{{version}}"
   - externalId: MarketConfiguration
     space: "{{powerops_models}}"
     type: view
     version: "{{version}}"
-  - externalId: Scenario
+  - externalId: PriceArea
     space: "{{powerops_models}}"
     type: view
     version: "{{version}}"
-  - externalId: Mapping
+  - externalId: PartialBidConfiguration
     space: "{{powerops_models}}"
     type: view
     version: "{{version}}"
-  - externalId: ModelTemplate
+  - externalId: ShopBasedPartialBidConfiguration
     space: "{{powerops_models}}"
     type: view
     version: "{{version}}"
-  - externalId: Watercourse
+  - externalId: WaterValueBasedPartialBidConfiguration
     space: "{{powerops_models}}"
     type: view
     version: "{{version}}"
-  - externalId: WatercourseShop
+  - externalId: Scenario
     space: "{{powerops_models}}"
     type: view
     version: "{{version}}"
-  - externalId: Plant
+  - externalId: ScenarioSet
     space: "{{powerops_models}}"
     type: view
     version: "{{version}}"
-  - externalId: PlantShop
+  - externalId: Mapping
     space: "{{powerops_models}}"
     type: view
     version: "{{version}}"
-  - externalId: Generator
+  - externalId: ModelTemplate
     space: "{{powerops_models}}"
     type: view
     version: "{{version}}"
-  - externalId: Reservoir
+  - externalId: Generator
     space: "{{powerops_models}}"
     type: view
     version: "{{version}}"
   - externalId: TurbineEfficiencyCurve
     space: "{{powerops_models}}"
     type: view
     version: "{{version}}"
   - externalId: GeneratorEfficiencyCurve
     space: "{{powerops_models}}"
     type: view
     version: "{{version}}"
-  - externalId: BidMethodDayAhead
-    space: "{{powerops_models}}"
-    type: view
-    version: "{{version}}"
-  - externalId: BidMethodWaterValue
-    space: "{{powerops_models}}"
-    type: view
-    version: "{{version}}"
-  - externalId: BidMethodSHOPMultiScenario
-    space: "{{powerops_models}}"
-    type: view
-    version: "{{version}}"
-  - externalId: PriceArea
+  - externalId: Commands
     space: "{{powerops_models}}"
     type: view
     version: "{{version}}"
-  - externalId: BidMethod
+  - externalId: PowerAsset
     space: "{{powerops_models}}"
     type: view
     version: "{{version}}"
-  - externalId: Commands
+  - externalId: Plant
     space: "{{powerops_models}}"
     type: view
     version: "{{version}}"
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/containers/Alert.container.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/containers/Alert.container.yaml`

 * *Files 6% similar despite different names*

```diff
@@ -6,14 +6,23 @@
   time:
     type:
       list: false
       type: timestamp
     nullable: false
     autoIncrement: false
     name: time
+  processId:
+    type:
+      list: false
+      collation: ucs_basic
+      type: text
+    nullable: false
+    autoIncrement: false
+    name: processId
+    description: The process id
   title:
     type:
       list: false
       collation: ucs_basic
       type: text
     nullable: false
     autoIncrement: false
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/containers/Asset.container.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/containers/Asset.container.yaml`

 * *Files 12% similar despite different names*

```diff
@@ -33,13 +33,22 @@
     type:
       list: false
       type: int32
     nullable: true
     autoIncrement: false
     name: order
     description: The order of the Asset
+  assetType:
+    type:
+      list: false
+      collation: ucs_basic
+      type: text
+    nullable: true
+    autoIncrement: false
+    name: assetType
+    description: The type of the Asset
 indexes:
   time:
     properties:
       - name
     indexType: btree
     cursorable: false
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/containers/BidConfiguration.container.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/containers/PriceProduction.container.yaml`

 * *Files 20% similar despite different names*

```diff
@@ -1,44 +1,29 @@
 space: "{{powerops_models}}"
-externalId: BidConfiguration
-name: BidConfiguration
+externalId: PriceProduction
+name: PriceProduction
 usedFor: node
 properties:
-  name:
+  price:
     type:
       list: false
-      collation: ucs_basic
-      type: text
-    nullable: true
-    autoIncrement: false
-    name: name
-  method:
-    type:
-      type: direct # points to BidMethod container
-    nullable: true
+      type: timeseries
+    nullable: false
     autoIncrement: false
-    name: method
-  marketConfiguration:
+    name: price
+    description: TODO
+  production:
     type:
-      type: direct
       list: false
-    nullable: true
+      type: timeseries
+    nullable: false
     autoIncrement: false
-    name: marketConfiguration
-  priceArea:
+    name: production
+    description: TODO
+  shopResult:
     type:
-      type: direct
       list: false
+      type: direct
     nullable: true
     autoIncrement: false
-    name: priceArea
-indexes:
-  methodIndex:
-    properties:
-      - method
-    indexType: btree
-    cursorable: false
-  priceAreaIndex:
-    properties:
-      - priceArea
-    indexType: btree
-    cursorable: false
+    name: shopResult
+    description: TODO
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/containers/BidDocument.container.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/containers/BidDocument.container.yaml`

 * *Files 6% similar despite different names*

```diff
@@ -42,13 +42,22 @@
   priceArea:
     type:
       type: direct
     nullable: true
     autoIncrement: false
     name: priceArea
     description: The price area of the bid document.
+  processId:
+    type:
+      list: false
+      collation: ucs_basic
+      type: text
+    nullable: true
+    autoIncrement: false
+    name: processId
+    description: The process associated with the Bid calculation workflow.
 indexes:
   deliveryDate:
     properties:
       - deliveryDate
     indexType: btree
     cursorable: false
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/containers/BidMatrix.container.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/containers/BidMatrix.container.yaml`

 * *Files 24% similar despite different names*

```diff
@@ -1,48 +1,56 @@
 space: "{{powerops_models}}"
 externalId: BidMatrix
 name: BidMatrix
 usedFor: node
 properties:
-  resourceCost:
-    type:
-      list: false
-      collation: ucs_basic
-      type: text
-    nullable: true
-    autoIncrement: false
-    name: resourceCost
   matrix:
     type:
       list: false
       type: sequence
-    nullable: true
+    nullable: false
     autoIncrement: false
     name: matrix
-  assetType:
+  powerAsset:
     type:
       list: false
-      collation: ucs_basic
-      type: text
+      type: direct
     nullable: true
     autoIncrement: false
-    name: assetType
-  assetId:
+    name: powerAsset
+  state:
     type:
       list: false
       collation: ucs_basic
       type: text
-    nullable: true
+    nullable: false
     autoIncrement: false
-    name: assetId
-  method:
+    name: state
+  partialBidConfiguration:
     type:
+      list: false
       type: direct
     nullable: true
     autoIncrement: false
-    name: method
-  isProcessed:
-    type:
-      type: boolean
-    nullable: true
-    autoIncrement: false
-    name: isProcessed
+    name: partialBidConfiguration
+  # assetId:
+  #   type:
+  #     list: false
+  #     collation: ucs_basic
+  #     type: text
+  #   nullable: true
+  #   autoIncrement: false
+  #   name: assetId
+  # isProcessed:
+  #   type:
+  #     type: boolean
+  #   nullable: true
+  #   autoIncrement: false
+  #   name: isProcessed
+  # resourceCost:
+  #   type:
+  #     list: false
+  #     collation: ucs_basic
+  #     type: text
+  #   nullable: true
+  #   autoIncrement: false
+  #   name: resourceCost
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/containers/BidMethod.container.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/containers/FunctionMetadata.container.yaml`

 * *Files 23% similar despite different names*

```diff
@@ -1,55 +1,88 @@
 space: "{{powerops_models}}"
-externalId: BidMethod
-name: BidMethod
+externalId: FunctionMetadata
+name: FunctionMetadata
 usedFor: node
 properties:
-  name:
+  functionName:
     type:
       list: false
       collation: ucs_basic
       type: text
     nullable: false
     autoIncrement: false
-    name: name
-    description: The name of the bid method
-  mainScenario:
+    name: functionName
+    description: The name of the function
+  functionCallId:
     type:
-      type: direct
-    nullable: true
+      list: false
+      collation: ucs_basic
+      type: text
+    nullable: false
     autoIncrement: false
-    name: mainScenario
-    description: The main scenario to use when running the bid method
-  # Shop related as optional properties
-  shopStartSpecification:
+    name: functionCallId
+    description: The function call id
+  processId:
     type:
       list: false
       collation: ucs_basic
       type: text
+    nullable: false
+    autoIncrement: false
+    name: processId
+    description: The process id
+  processStep:
+    type:
+      list: false
+      type: int32
+    nullable: false
+    autoIncrement: false
+    name: processStep
+    description: The process step
+  linkedStep:
+    type:
+      list: false
+      type: direct
     nullable: true
     autoIncrement: false
-    name: name
-    description: The dynamic shop start specification
-  shopEndSpecification:
+    name: linkedStep
+    description: Typically the previous step
+  data:
     type:
       list: false
-      collation: ucs_basic
-      type: text
+      type: direct
     nullable: true
     autoIncrement: false
-    name: name
-    description: The dynamic shop end specification
-  shopBidDateSpecification:
+    name: data
+    description: The data
+  data2:
     type:
       list: false
-      collation: ucs_basic
-      type: text
+      type: direct
+    nullable: true
+    autoIncrement: false
+    name: data2
+    description: The data
+  data3:
+    type:
+      list: false
+      type: direct
     nullable: true
     autoIncrement: false
-    name: name
-    description: The dynamic bid date specification
+    name: data3
+    description: The data
 indexes:
-  name:
+  functionNameIndex:
+    properties:
+      - functionName
+    indexType: btree
+    cursorable: false
+  functionCallIdIndex:
+    properties:
+      - functionCallId
+    indexType: btree
+    cursorable: false
+  processIdIndex:
     properties:
-      - name
+      - processId
     indexType: btree
     cursorable: false
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/containers/BidRow.container.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/containers/BidRow.container.yaml`

 * *Files 4% similar despite different names*

```diff
@@ -74,36 +74,14 @@
       type: direct
       list: false
     name: linkedBid
     description: null
     nullable: true
     autoIncrement: false
     defaultValue: null
-  assetType:
-    type:
-      list: false
-      type: text
-      collation: ucs_basic
-    name: assetType
-    description: null
-    nullable: true
-    autoIncrement: false
-    defaultValue: null
-  assetId:
-    type:
-      list: false
-      type: text
-      collation: ucs_basic
-    name: assetId
-    description: null
-    nullable: true
-    autoIncrement: false
-    defaultValue: null
-  method:
+  powerAsset:
     type:
       type: direct
       list: false
-    name: method
-    description: null
     nullable: true
     autoIncrement: false
-    defaultValue: null
+    name: powerAsset
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/containers/Case.container.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/containers/Case.container.yaml`

 * *Files 15% similar despite different names*

```diff
@@ -49,20 +49,20 @@
     nullable: true
     autoIncrement: false
     name: cogShopFilesConfig
     description: Configuration for in what order to load the various files into pyshop
   startTime:
     type:
       list: false
-      type: date
+      type: timestamp
     nullable: true
     autoIncrement: false
     name: startTime
     description: The start of the optimisation period (for SHOP)
   endTime:
     type:
       list: false
-      type: date
+      type: timestamp
     nullable: true
     autoIncrement: false
     name: endTime
     description: The end of the optimisation period (for SHOP)
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/containers/EfficiencyCurve.container.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/containers/EfficiencyCurve.container.yaml`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/containers/FunctionData.container.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/containers/FunctionData.container.yaml`

 * *Files 6% similar despite different names*

```diff
@@ -34,21 +34,28 @@
   date2:
     type:
       list: false
       type: date
     nullable: true
     autoIncrement: false
     name: date2
-  date3:
+  timestamp1:
     type:
       list: false
-      type: date
+      type: timestamp
+    nullable: true
+    autoIncrement: false
+    name: timestamp1
+  timestamp2:
+    type:
+      list: false
+      type: timestamp
     nullable: true
     autoIncrement: false
-    name: date3
+    name: timestamp2
   text1:
     type:
       list: false
       type: text
     nullable: true
     autoIncrement: false
     name: text1
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/containers/Generator.container.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/containers/SHOPResult.container.yaml`

 * *Files 20% similar despite different names*

```diff
@@ -1,59 +1,52 @@
 space: "{{powerops_models}}"
-externalId: Generator
-name: Generator
+externalId: SHOPResult
+name: SHOPResult
 usedFor: node
 properties:
-  pMin:
+  case:
     type:
       list: false
-      type: float64
+      type: direct
     nullable: true
     autoIncrement: false
-    name: pMin
-    description: The minimum power output of the Generator
-  penstock:
+    name: case
+    description: The scenario that was used to create the shop result
+  objectiveSequence:
     type:
       list: false
-      type: int32
+      type: sequence
     nullable: true
     autoIncrement: false
-    name: penstock
-    description: The number of gates for water intakes to the Generator
-  startCost:
+    name: objectiveSequence
+  preRun:
     type:
       list: false
-      type: float64
+      type: file
     nullable: true
     autoIncrement: false
-    name: startCost
-    description: The start up cost, i.e., the cost to start the Generator
-  startStopCost:
+    name: preRun
+    description: The prerun file from the shop run
+  postRun:
     type:
       list: false
-      type: timeseries
+      type: file
     nullable: true
     autoIncrement: false
-    name: startStopCost
-    description: A timeseries with the start/stop cost of the Generator
-  isAvailable:
+    name: postRun
+    description: The post run file from the shop run
+  shopMessages:
     type:
       list: false
-      type: timeseries
+      type: file
     nullable: true
     autoIncrement: false
-    name: isAvailable
-    description: A binary timeseries indicating whether the Generator is available
-  efficiencyCurve:
+    name: shopMessages
+    description: The prerun file from the shop run
+  cplexLogs:
     type:
-      type: direct
+      list: false
+      type: file
     nullable: true
     autoIncrement: false
-    name: efficiencyCurve
-    description: The generator efficiency curve.
-constraints:
-  requiredAsset:
-    require:
-      space: "{{powerops_models}}"
-      externalId: Asset
-      type: container
-    constraintType: requires
+    name: cplexLogs
+    description: The cplex logs from the shop run
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/containers/Mapping.container.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/containers/Mapping.container.yaml`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/containers/MarketConfiguration.container.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/containers/MarketConfiguration.container.yaml`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/containers/ModelTemplate.container.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/containers/ModelTemplate.container.yaml`

 * *Files 18% similar despite different names*

```diff
@@ -1,12 +1,21 @@
 space: "{{powerops_models}}"
 externalId: ModelTemplate
 name: ModelTemplate
 usedFor: node
 properties:
+  name:
+    type:
+      list: false
+      collation: ucs_basic
+      type: text
+    nullable: false
+    autoIncrement: false
+    name: name
+    description: The name of the model template
   version:
     type:
       list: false
       collation: ucs_basic
       type: text
     nullable: true
     autoIncrement: false
@@ -17,22 +26,21 @@
       list: false
       collation: ucs_basic
       type: text
     nullable: false
     autoIncrement: false
     name: shopVersion
     description: The version of SHOP to run
-  watercourse:
+  penaltyLimit:
     type:
+      type: float64
       list: false
-      type: direct
     nullable: true
     autoIncrement: false
-    name: watercourse
-    description: The watercourse to run the model for
+    name: penaltyLimit
   model:
     type:
       list: false
       type: file
     nullable: false
     autoIncrement: false
     name: model
@@ -49,13 +57,7 @@
     type:
       list: true
       type: file
     nullable: true
     autoIncrement: false
     name: extraFiles
     description: Extra files to include in the model
-indexes:
-  watercourseIndex:
-    properties:
-      - watercourse
-    indexType: btree
-    cursorable: false
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/containers/Plant.container.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/containers/Plant.container.yaml`

 * *Files 14% similar despite different names*

```diff
@@ -6,69 +6,69 @@
   headLossFactor:
     type:
       type: float64
       list: false
     nullable: true
     autoIncrement: false
     name: headLossFactor
+  connectionLosses:
+    type:
+      type: float64
+      list: false
+    nullable: true
+    autoIncrement: false
+    name: connectionLosses
   outletLevel:
     type:
       type: float64
       list: false
     nullable: true
     autoIncrement: false
     name: outletLevel
-  pMax:
+  productionMax:
     type:
       type: float64
       list: false
     nullable: true
     autoIncrement: false
-    name: pMax
-  pMin:
+    name: productionMax
+  productionMin:
     type:
       type: float64
       list: false
     nullable: true
     autoIncrement: false
-    name: pMin
+    name: productionMin
   penstockHeadLossFactors:
     type:
       type: json
       list: false
     nullable: true
     autoIncrement: false
     name: penstockHeadLossFactors
-  watercourse:
-    type:
-      type: direct
-    nullable: true
-    autoIncrement: false
-    name: watercourse
-  connectionLosses:
-    type:
-      type: float64
-      list: false
-    nullable: true
-    autoIncrement: false
-    name: connectionLosses
-  pMaxTimeSeries:
+  # watercourse:
+  #   type:
+  #     type: direct
+  #   nullable: true
+  #   autoIncrement: false
+  #   name: watercourse
+  productionMaxTimeSeries:
     type:
       type: timeseries
       list: false
     nullable: true
     autoIncrement: false
-    name: pMaxTimeSeries
-  pMinTimeSeries:
+    name: productionMaxTimeSeries
+  productionMinTimeSeries:
     type:
       type: timeseries
       list: false
     nullable: true
     autoIncrement: false
-    name: pMinTimeSeries
+    name: productionMinTimeSeries
   waterValueTimeSeries:
     type:
       type: timeseries
       list: false
     nullable: true
     autoIncrement: false
     name: waterValueTimeSeries
@@ -96,20 +96,20 @@
   headDirectTimeSeries:
     type:
       type: timeseries
       list: false
     nullable: true
     autoIncrement: false
     name: headDirectTimeSeries
-  inletReservoir:
-    type:
-      type: direct
-    nullable: true
-    autoIncrement: false
-    name: inletReservoir
+    # inletReservoir:
+    #   type:
+    #     type: direct
+    #   nullable: true
+    #   autoIncrement: false
+    #   name: inletReservoir
 constraints:
   requiredAsset:
     require:
       space: "{{powerops_models}}"
       externalId: Asset
       type: container
     constraintType: requires
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/containers/PriceArea.container.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/containers/PriceArea.container.yaml`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/containers/SHOPTimeSeries.container.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/containers/SHOPTimeSeries.container.yaml`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/containers/Scenario.container.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/containers/Scenario.container.yaml`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/frontend_AFRRBid.datamodel.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/product_CogShop.datamodel.yaml`

 * *Files 20% similar despite different names*

```diff
@@ -1,26 +1,30 @@
-externalId: frontend_AFRRBid
-space: '{{powerops_models}}'
-version: '1'
-name: AFRRBid
-description: 'Stores results of AFRR bid calculations.'
+externalId: product_CogShop
+space: "{{powerops_models}}"
+version: "1"
+name: CogShop
+description: CogShop data model
 views:
-  - externalId: BidDocumentAFRR
-    space: '{{powerops_models}}'
+  - externalId: Case
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
-  - externalId: BidRow
-    space: '{{powerops_models}}'
-    type: view
-    version: '{{version}}'
-  - externalId: PriceAreaAFRR
-    space: '{{powerops_models}}'
-    type: view
-    version: '{{version}}'
-  - externalId: BidMethodAFRR
-    space: '{{powerops_models}}'
-    type: view
-    version: '{{version}}'
-  - externalId: Alert
-    space: '{{powerops_models}}'
+    version: "{{version}}"
+  - externalId: Scenario
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
+    version: "{{version}}"
+  - externalId: Commands
+    space: "{{powerops_models}}"
+    type: view
+    version: "{{version}}"
+  - externalId: Mapping
+    space: "{{powerops_models}}"
+    type: view
+    version: "{{version}}"
+  - externalId: ModelTemplate
+    space: "{{powerops_models}}"
+    type: view
+    version: "{{version}}"
+  - externalId: PowerAsset
+    space: "{{powerops_models}}"
+    type: view
+    version: "{{version}}"
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/frontend_Asset.datamodel.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_outputs/TotalBidMatrixCalculationOutput.view.yaml`

 * *Files 27% similar despite different names*

```diff
@@ -1,38 +1,54 @@
-externalId: frontend_Asset
 space: '{{powerops_models}}'
-version: '1'
-name: PowerAsset
-description: 'Describes the power operations asset model'
-views:
-  - externalId: PriceAreaAsset
-    space: '{{powerops_models}}'
-    type: view
-    version: '{{version}}'
-  - externalId: Watercourse
-    space: '{{powerops_models}}'
-    type: view
-    version: '{{version}}'
-  - externalId: Plant
-    space: '{{powerops_models}}'
-    type: view
-    version: '{{version}}'
-  - externalId: Generator
-    space: '{{powerops_models}}'
-    type: view
-    version: '{{version}}'
-  - externalId: Reservoir
-    space: '{{powerops_models}}'
-    type: view
-    version: '{{version}}'
-  - externalId: TurbineEfficiencyCurve
-    space: '{{powerops_models}}'
-    type: view
-    version: '{{version}}'
-  - externalId: GeneratorEfficiencyCurve
-    space: '{{powerops_models}}'
-    type: view
-    version: '{{version}}'
-  - externalId: BidMethodDayAhead
-    space: '{{powerops_models}}'
-    type: view
-    version: '{{version}}'
+externalId: TotalBidMatrixCalculationOutput
+name: TotalBidMatrixCalculationOutput
+description: The output of the total bid matrix calculation
+filter:
+  and:
+    - hasData:
+        - type: container
+          space: '{{powerops_models}}'
+          externalId: FunctionData
+    - equals:
+        property:
+          - node
+          - space
+        value: '{{powerops_instance_space}}'
+    - equals:
+        property:
+          - node
+          - type
+        value:
+          externalId: TotalBidMatrixCalculationOutput
+          space: '{{powerops_type_space}}'
+implements:
+  - space: '{{powerops_models}}'
+    externalId: FunctionOutput
+    version: '{{version}}'
+    type: view
+version: '{{version}}'
+properties:
+  bidDocument:
+    container:
+      space: '{{powerops_models}}'
+      externalId: FunctionData
+      type: container
+    containerPropertyIdentifier: direct1
+    name: bidDocument
+    source:
+      space: '{{powerops_models}}'
+      externalId: BidDocumentDayAhead
+      version: '{{version}}'
+      type: view
+  input:
+    container:
+      space: '{{powerops_models}}'
+      externalId: FunctionData
+      type: container
+    containerPropertyIdentifier: direct2
+    name: input
+    description: The previous step in the process.
+    source:
+      space: '{{powerops_models}}'
+      externalId: TotalBidMatrixCalculationInput
+      version: '{{version}}'
+      type: view
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/frontend_DayAheadBid.datamodel.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/frontend_DayAheadBid.datamodel.yaml`

 * *Files 15% similar despite different names*

```diff
@@ -1,78 +1,74 @@
 externalId: frontend_DayAheadBid
-space: '{{powerops_models}}'
-version: '1'
-name: 'DayAheadBid'
-description: 'Stores results of Day Ahead bid calculations.'
+space: "{{powerops_models}}"
+version: "1"
+name: "DayAheadBid"
+description: "Stores results of Day Ahead bid calculations."
 views:
   - externalId: BidDocumentDayAhead
-    space: '{{powerops_models}}'
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
-  - externalId: BidMatrix
-    space: '{{powerops_models}}'
-    type: view
-    version: '{{version}}'
-  - externalId: MultiScenarioMatrix
-    space: '{{powerops_models}}'
-    type: view
-    version: '{{version}}'
-  - externalId: BasicBidMatrix
-    space: '{{powerops_models}}'
-    type: view
-    version: '{{version}}'
-  - externalId: CustomBidMatrix
-    space: '{{powerops_models}}'
+    version: "{{version}}"
+  - externalId: BidDocument
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
-  - externalId: BidMethodCustom
-    space: '{{powerops_models}}'
-    type: view
-    version: '{{version}}'
-  - externalId: BidMethodDayAhead
-    space: '{{powerops_models}}'
+    version: "{{version}}"
+  - externalId: BidMatrix
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
+    version: "{{version}}"
   - externalId: PriceArea
-    space: '{{powerops_models}}'
-    type: view
-    version: '{{version}}'
-  - externalId: BidMethodSHOPMultiScenario
-    space: '{{powerops_models}}'
-    type: view
-    version: '{{version}}'
-  - externalId: BidMethodWaterValue
-    space: '{{powerops_models}}'
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
+    version: "{{version}}"
   - externalId: Alert
-    space: '{{powerops_models}}'
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
+    version: "{{version}}"
   - externalId: Scenario
-    space: '{{powerops_models}}'
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
+    version: "{{version}}"
   - externalId: ModelTemplate
-    space: '{{powerops_models}}'
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
+    version: "{{version}}"
   - externalId: Mapping
-    space: '{{powerops_models}}'
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
-  - externalId: WatercourseShop
-    space: '{{powerops_models}}'
+    version: "{{version}}"
+  - externalId: PriceProduction
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
-  - externalId: PriceProdCase
-    space: '{{powerops_models}}'
-    type: view
-    version: '{{version}}'
+    version: "{{version}}"
   - externalId: Commands
-    space: '{{powerops_models}}'
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
+    version: "{{version}}"
   - externalId: Case
-    space: '{{powerops_models}}'
+    space: "{{powerops_models}}"
+    type: view
+    version: "{{version}}"
+  - externalId: PowerAsset
+    space: "{{powerops_models}}"
+    type: view
+    version: "{{version}}"
+  - externalId: BidConfiguration
+    space: "{{powerops_models}}"
+    type: view
+    version: "{{version}}"
+  - externalId: PartialBidConfiguration
+    space: "{{powerops_models}}"
+    type: view
+    version: "{{version}}"
+  - externalId: SHOPResult
+    space: "{{powerops_models}}"
+    type: view
+    version: "{{version}}"
+  - externalId: MarketConfiguration
+    space: "{{powerops_models}}"
+    type: view
+    version: "{{version}}"
+  - externalId: SHOPTimeSeries
+    space: "{{powerops_models}}"
     type: view
-    version: '{{version}}'
+    version: "{{version}}"
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/node_types/power-ops-types.powerops_nodes.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/node_types/power-ops-types.powerops_nodes.yaml`

 * *Files 16% similar despite different names*

```diff
@@ -15,47 +15,33 @@
   externalId: isWatercourseOf
 
 # Types of nodes:
 ## AFRR Types
 - space: "{{powerops_type_space}}"
   externalId: AFRRBidDocument
 - space: "{{powerops_type_space}}"
-  externalId: BidMethodAFRR
-- space: "{{powerops_type_space}}"
   externalId: AFRRBidRow
 
 ## Assets Types
 - space: "{{powerops_type_space}}"
+  externalId: PowerAsset
+- space: "{{powerops_type_space}}"
+  externalId: PriceArea
+- space: "{{powerops_type_space}}"
   externalId: Generator
 - space: "{{powerops_type_space}}"
   externalId: GeneratorEfficiencyCurve
 - space: "{{powerops_type_space}}"
   externalId: Plant
 - space: "{{powerops_type_space}}"
-  externalId: PriceArea
-- space: "{{powerops_type_space}}"
-  externalId: PriceAreaDayAhead
-- space: "{{powerops_type_space}}"
   externalId: PriceAreaAFRR
 - space: "{{powerops_type_space}}"
-  externalId: Reservoir
-- space: "{{powerops_type_space}}"
   externalId: TurbineEfficiencyCurve
 - space: "{{powerops_type_space}}"
-  externalId: Watercourse
-- space: "{{powerops_type_space}}"
   externalId: Alert
-- space: "{{powerops_type_space}}"
-  externalId: PlantShop
-- space: "{{powerops_type_space}}"
-  externalId: WatercourseShop
-- space: "{{powerops_type_space}}"
-  externalId: PriceAreaAsset
-- space: "{{powerops_type_space}}"
-  externalId: PowerAsset
 
 ## CogShop types
 - space: "{{powerops_type_space}}"
   externalId: Case
 - space: "{{powerops_type_space}}"
   externalId: ModelTemplate
 - space: "{{powerops_type_space}}"
@@ -71,32 +57,26 @@
 - space: "{{powerops_type_space}}"
   externalId: PriceProdCase
 - space: "{{powerops_type_space}}"
   externalId: Commands
 - space: "{{powerops_type_space}}"
   externalId: SHOPTimeSeries
 - space: "{{powerops_type_space}}"
-  externalId: SHOPResultPriceProd.productionTimeseries
+  externalId: SHOPResult.outputTimeseries
+- space: "{{powerops_type_space}}"
+  externalId: PriceProduction
 - space: "{{powerops_type_space}}"
-  externalId: MultiScenarioMatrix.scenarioResults
+  externalId: PartialBidMatrixCalculationOutput
 
 ## DayAhead Types
 - space: "{{powerops_type_space}}"
-  externalId: DayAheadBasicBidMatrix
-- space: "{{powerops_type_space}}"
   externalId: DayAheadBidDocument
 - space: "{{powerops_type_space}}"
-  externalId: BidMethodDayAhead
-- space: "{{powerops_type_space}}"
   externalId: WaterValueBasedMethod
 - space: "{{powerops_type_space}}"
-  externalId: DayAheadMultiScenarioMatrix
-- space: "{{powerops_type_space}}"
-  externalId: DayAheadMultiScenarioMatrixRaw
-- space: "{{powerops_type_space}}"
   externalId: DayAheadSHOPMultiScenario
 - space: "{{powerops_type_space}}"
   externalId: DayAheadWaterValueBased
 
 ## Dayahead Backend types
 - space: "{{powerops_type_space}}"
   externalId: PreprocessorInput
@@ -105,89 +85,67 @@
 - space: "{{powerops_type_space}}"
   externalId: TotalBidMatrixCalculationInput.partialBidMatrices
 - space: "{{powerops_type_space}}"
   externalId: TotalBidMatrixCalculationInput
 - space: "{{powerops_type_space}}"
   externalId: TotalBidMatrixCalculationOutput
 - space: "{{powerops_type_space}}"
-  externalId: BidConfiguration.plants
-- space: "{{powerops_type_space}}"
-  externalId: BidConfiguration.watercourses
-- space: "{{powerops_type_space}}"
-  externalId: BidConfiguration.plantsShop
-- space: "{{powerops_type_space}}"
-  externalId: BidConfiguration.watercoursesShop
-- space: "{{powerops_type_space}}"
   externalId: BidConfiguration
 - space: "{{powerops_type_space}}"
-  externalId: BidConfigurationAFRR
-- space: "{{powerops_type_space}}"
-  externalId: BidConfigurationShop
+  externalId: partialBidMatrices
 - space: "{{powerops_type_space}}"
-  externalId: partialBidMatricesRaw
+  externalId: TaskDispatcherInput
 - space: "{{powerops_type_space}}"
-  externalId: TaskDispatcherShopInput
-- space: "{{powerops_type_space}}"
-  externalId: TaskDispatcherShopOutput
+  externalId: TaskDispatcherOutput
 - space: "{{powerops_type_space}}"
   externalId: PreprocessorInput
 - space: "{{powerops_type_space}}"
-  externalId: BidMatrixRaw
-- space: "{{powerops_type_space}}"
   externalId: BidMatrix
 - space: "{{powerops_type_space}}"
   externalId: SHOPTriggerInput
 - space: "{{powerops_type_space}}"
   externalId: SHOPTriggerOutput
 - space: "{{powerops_type_space}}"
-  externalId: WaterPartialBidCalculationOutput
+  externalId: PartialBidMatrixCalculationInput
 - space: "{{powerops_type_space}}"
-  externalId: BidCalculationTask
+  externalId: BidMatrixCalculationTask
 - space: "{{powerops_type_space}}"
-  externalId: TaskDispatcherWaterInput
+  externalId: ShopPartialBidMatrixCalculationInput
 - space: "{{powerops_type_space}}"
-  externalId: TaskDispatcherWaterOutput
+  externalId: ShopPartialBidMatrixCalculationOutput
 - space: "{{powerops_type_space}}"
-  externalId: PartialPostProcessingInput
+  externalId: WaterValueBasedPartialBidMatrixCalculationInput
 - space: "{{powerops_type_space}}"
-  externalId: PartialPostProcessingOutput
+  externalId: Water.partialBidMatrixCalculations
 - space: "{{powerops_type_space}}"
-  externalId: ShopPartialBidCalculationInput
-- space: "{{powerops_type_space}}"
-  externalId: ShopPartialBidCalculationOutput
-- space: "{{powerops_type_space}}"
-  externalId: WaterPartialBidCalculationInput
-- space: "{{powerops_type_space}}"
-  externalId: BidMethodDayAhead
-- space: "{{powerops_type_space}}"
-  externalId: BidMethodSHOPMultiScenario
-- space: "{{powerops_type_space}}"
-  externalId: BidMethodWaterValue
+  externalId: ModelTemplate.baseMappings
 - space: "{{powerops_type_space}}"
-  externalId: BidMethodDayahead.scenarios
+  externalId: ShopBasedPartialBidConfiguration
 - space: "{{powerops_type_space}}"
-  externalId: MultiScenarioMatrix.shopResults
+  externalId: ShopBasedPartialBidConfiguration.scenarios
 - space: "{{powerops_type_space}}"
-  externalId: Water.partialBidCalculations
+  externalId: WaterValueBasedPartialBidConfiguration
 - space: "{{powerops_type_space}}"
-  externalId: ModelTemplate.baseMappings
+  externalId: WaterValueBasedPartialBidConfiguration.generators
 - space: "{{powerops_type_space}}"
-  externalId: BidMethod
+  externalId: processSubTasks
 
 # Generic Types
 - space: "{{powerops_type_space}}"
   externalId: BidRow
 - space: "{{powerops_type_space}}"
   externalId: FunctionInput
 - space: "{{powerops_type_space}}"
   externalId: FunctionOutput
 - space: "{{powerops_type_space}}"
   externalId: MarketConfiguration
 - space: "{{powerops_type_space}}"
-  externalId: BidConfigurationWater
+  externalId: BidConfiguration
+- space: "{{powerops_type_space}}"
+  externalId: BidConfiguration.partials
 - space: "{{powerops_type_space}}"
   externalId: BidDocument
 
 # Benchmarking Types
 - space: "{{powerops_type_space}}"
   externalId: BenchmarkingCollectorInput
 - space: "{{powerops_type_space}}"
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/Alert.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/Alert.view.yaml`

 * *Files 14% similar despite different names*

```diff
@@ -1,89 +1,97 @@
-space: '{{powerops_models}}'
+space: "{{powerops_models}}"
 externalId: Alert
 name: Alert
 description: Alerts
 filter:
   and:
     - hasData:
         - type: container
-          space: '{{powerops_models}}'
+          space: "{{powerops_models}}"
           externalId: Alert
     - equals:
         property:
           - node
           - space
-        value: '{{powerops_instance_space}}'
+        value: "{{powerops_instance_space}}"
     - equals:
         property:
           - node
           - type
         value:
           externalId: Alert
-          space: '{{powerops_type_space}}'
+          space: "{{powerops_type_space}}"
 implements: []
-version: '{{version}}'
+version: "{{version}}"
 properties:
   time:
     container:
-      space: '{{powerops_models}}'
+      space: "{{powerops_models}}"
       externalId: Alert
       type: container
     containerPropertyIdentifier: time
     name: time
     description: Timestamp that the alert occurred (within the workflow)
+  processId:
+    container:
+      space: "{{powerops_models}}"
+      externalId: Alert
+      type: container
+    containerPropertyIdentifier: processId
+    name: processId
+    description: Process ID in the workflow that the alert is related to
   title:
     container:
-      space: '{{powerops_models}}'
+      space: "{{powerops_models}}"
       externalId: Alert
       type: container
     containerPropertyIdentifier: title
     name: title
     description: Summary description of the alert
   description:
     container:
-      space: '{{powerops_models}}'
+      space: "{{powerops_models}}"
       externalId: Alert
       type: container
     containerPropertyIdentifier: description
     name: description
     description: Detailed description of the alert
   severity:
     container:
-      space: '{{powerops_models}}'
+      space: "{{powerops_models}}"
       externalId: Alert
       type: container
     containerPropertyIdentifier: severity
     name: severity
     description: CRITICAL (calculation could not completed) WARNING  (calculation completed, with major issue) INFO     (calculation completed, with minor issues)
   alertType:
     container:
-      space: '{{powerops_models}}'
+      space: "{{powerops_models}}"
       externalId: Alert
       type: container
     containerPropertyIdentifier: alertType
     name: alertType
     description: Classification of the alert (not in current alerting implementation)
   statusCode:
     container:
-      space: '{{powerops_models}}'
+      space: "{{powerops_models}}"
       externalId: Alert
       type: container
     containerPropertyIdentifier: statusCode
     name: statusCode
     description: Unique status code for the alert. May be used by the frontend to avoid use of hardcoded description (i.e. like a translation)
   eventIds:
     container:
-      space: '{{powerops_models}}'
+      space: "{{powerops_models}}"
       externalId: Alert
       type: container
     containerPropertyIdentifier: eventIds
     name: eventIds
     description: An array of associated alert CDF Events (e.g. SHOP Run events)
   calculationRun:
     container:
-      space: '{{powerops_models}}'
+      space: "{{powerops_models}}"
       externalId: Alert
       type: container
     containerPropertyIdentifier: calculationRun
     name: calculationRun
     description: The identifier of the parent Bid Calculation (required so tha alerts can be created befor the BidDocument)
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/BidCalculationTask.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_outputs/PreprocessorOutput.view.yaml`

 * *Files 11% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 space: '{{powerops_models}}'
-externalId: BidCalculationTask
-name: BidCalculationTask
-description: The data for a bid calculation task
+externalId: PreprocessorOutput
+name: PreprocessorOutput
+description: Input to the task dispatcher in the shop bid process
 filter:
   and:
     - hasData:
         - type: container
           space: '{{powerops_models}}'
           externalId: FunctionData
     - equals:
@@ -14,45 +14,42 @@
           - space
         value: '{{powerops_instance_space}}'
     - equals:
         property:
           - node
           - type
         value:
-          externalId: BidCalculationTask
+          externalId: PreprocessorOutput
           space: '{{powerops_type_space}}'
-implements: []
+implements:
+  - space: '{{powerops_models}}'
+    externalId: FunctionOutput
+    version: '{{version}}'
+    type: view
 version: '{{version}}'
 properties:
-  plant:
+  case:
     container:
       space: '{{powerops_models}}'
       externalId: FunctionData
       type: container
     containerPropertyIdentifier: direct1
-    name: plant
+    name: case
+    description: The Case to trigger shop with
     source:
       space: '{{powerops_models}}'
-      externalId: Plant
+      externalId: Case
       version: '{{version}}'
       type: view
-  bidDate:
-    container:
-      space: '{{powerops_models}}'
-      externalId: FunctionData
-      type: container
-    containerPropertyIdentifier: date1
-    name: bidDate
-    description: The bid date that the task is for
-  priceArea:
+  input:
     container:
       space: '{{powerops_models}}'
       externalId: FunctionData
       type: container
     containerPropertyIdentifier: direct2
-    name: priceArea
-    description: The price area related to the bid calculation task
+    name: input
+    description: The prepped and processed scenario to send to shop trigger
     source:
       space: '{{powerops_models}}'
-      externalId: PriceArea
+      externalId: PreprocessorInput
       version: '{{version}}'
       type: view
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/BidRow.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/BidRow.view.yaml`

 * *Files 9% similar despite different names*

```diff
@@ -1,131 +1,118 @@
-space: '{{powerops_models}}'
+space: "{{powerops_models}}"
 externalId: BidRow
 name: BidRow
 description: One row in the BidDocument (usually referenced in industry as a Bid).
 filter:
   and:
     - hasData:
         - type: container
-          space: '{{powerops_models}}'
+          space: "{{powerops_models}}"
           externalId: BidRow
     - equals:
         property:
           - node
           - space
-        value: '{{powerops_instance_space}}'
+        value: "{{powerops_instance_space}}"
     - equals:
         property:
           - node
           - type
         value:
           externalId: BidRow
-          space: '{{powerops_type_space}}'
+          space: "{{powerops_type_space}}"
 implements: []
-version: '{{version}}'
+version: "{{version}}"
 properties:
   price:
     container:
-      space: '{{powerops_models}}'
+      space: "{{powerops_models}}"
       externalId: BidRow
       type: container
     containerPropertyIdentifier: price
     name: price
     description: Price in EUR/MW/h, rounded to nearest price step (0.1?)
   quantityPerHour:
     container:
-      space: '{{powerops_models}}'
+      space: "{{powerops_models}}"
       externalId: BidRow
       type: container
     containerPropertyIdentifier: quantityPerHour
     name: quantityPerHour
     description: The capacity offered, per hour, in MW, rounded to nearest step size (5?)
   product:
     container:
-      space: '{{powerops_models}}'
+      space: "{{powerops_models}}"
       externalId: BidRow
       type: container
     containerPropertyIdentifier: product
     name: product
   isDivisible:
     container:
-      space: '{{powerops_models}}'
+      space: "{{powerops_models}}"
       externalId: BidRow
       type: container
     containerPropertyIdentifier: isDivisible
     name: isDivisible
   minQuantity:
     container:
-      space: '{{powerops_models}}'
+      space: "{{powerops_models}}"
       externalId: BidRow
       type: container
     containerPropertyIdentifier: minQuantity
     name: minQuantity
     description: Min quantity, per hour. Only relevant for divisible Bids. The minimum capacity that must be accepted; this must be lower than capacityPerHour and is rounded to the nearest step (5 MW?)).
   isBlock:
     container:
-      space: '{{powerops_models}}'
+      space: "{{powerops_models}}"
       externalId: BidRow
       type: container
     containerPropertyIdentifier: isBlock
     name: isBlock
-    description: 'Indication if the row is part of a Block bid. If true: quantityPerHour must have the same value for consecutive hours (and no breaks). Block bids must be accepted for all hours or none.'
+    description: "Indication if the row is part of a Block bid. If true: quantityPerHour must have the same value for consecutive hours (and no breaks). Block bids must be accepted for all hours or none."
   exclusiveGroupId:
     container:
-      space: '{{powerops_models}}'
+      space: "{{powerops_models}}"
       externalId: BidRow
       type: container
     containerPropertyIdentifier: exclusiveGroupId
     name: exclusiveGroupId
     description: Other bids with the same ID are part of an exclusive group - only one of them can be accepted, and they must have the same direction (product). Not allowed for block bids.
   linkedBid:
     container:
-      space: '{{powerops_models}}'
+      space: "{{powerops_models}}"
       externalId: BidRow
       type: container
     containerPropertyIdentifier: linkedBid
     name: linkedBid
     description: The linked bid must have the opposite direction (link means that both or none must be accepted). Should be bi-directional.
     source:
-      space: '{{powerops_models}}'
+      space: "{{powerops_models}}"
       externalId: BidRow
-      version: '{{version}}'
+      version: "{{version}}"
       type: view
-  assetType:
+  powerAsset:
     container:
-      space: '{{powerops_models}}'
+      space: "{{powerops_models}}"
       externalId: BidRow
       type: container
-    containerPropertyIdentifier: assetType
-    name: assetType
-  assetId:
-    container:
-      space: '{{powerops_models}}'
-      externalId: BidRow
-      type: container
-    containerPropertyIdentifier: assetId
-    name: assetId
-  method:
-    container:
-      space: '{{powerops_models}}'
-      externalId: BidRow
-      type: container
-    containerPropertyIdentifier: method
-    name: method
+    containerPropertyIdentifier: powerAsset
+    name: powerAsset
+    description: TODO description
     source:
-      space: '{{powerops_models}}'
-      externalId: BidMethodAFRR
-      version: '{{version}}'
+      space: "{{powerops_models}}"
+      externalId: PowerAsset
+      version: "{{version}}"
       type: view
   alerts:
     type:
-      space: '{{powerops_type_space}}'
+      space: "{{powerops_type_space}}"
       externalId: calculationIssue
     source:
-      space: '{{powerops_models}}'
+      space: "{{powerops_models}}"
       externalId: Alert
-      version: '{{version}}'
+      version: "{{version}}"
       type: view
     direction: outwards
     name: alerts
     description: An array of associated alerts.
     connectionType: multi_edge_connection
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/MarketConfiguration.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/MarketConfiguration.view.yaml`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/assets/1-interface.PowerAsset.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/assets/1-interface.PowerAsset.view.yaml`

 * *Files 10% similar despite different names*

```diff
@@ -41,7 +41,15 @@
     container:
       space: "{{powerops_models}}"
       externalId: Asset
       type: container
     containerPropertyIdentifier: order
     name: ordering
     description: The ordering of the asset
+  assetType:
+    container:
+      space: "{{powerops_models}}"
+      externalId: Asset
+      type: container
+    containerPropertyIdentifier: assetType
+    name: assetType
+    description: The type of the asset
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/assets/1-interface.PriceArea.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/assets/1-interface.PriceArea.view.yaml`

 * *Files 16% similar despite different names*

```diff
@@ -3,41 +3,34 @@
 name: PriceArea
 description: Information about the Price Area that is only relevant for AFRR
 filter:
   and:
     - hasData:
         - type: container
           space: "{{powerops_models}}"
-          externalId: Asset
-        - type: container
-          space: "{{powerops_models}}"
           externalId: PriceArea
     - equals:
         property:
           - node
           - space
         value: "{{powerops_instance_space}}"
     - equals:
         property:
           - node
           - type
         value:
           externalId: PriceArea
           space: "{{powerops_type_space}}"
-implements: []
+implements:
+  - space: "{{powerops_models}}"
+    externalId: PowerAsset
+    version: "{{version}}"
+    type: view
 version: "{{version}}"
 properties:
-  name:
-    container:
-      space: "{{powerops_models}}"
-      externalId: Asset
-      type: container
-    containerPropertyIdentifier: name
-    name: name
-    description: The name of the price area
   timezone:
     container:
       space: "{{powerops_models}}"
       externalId: PriceArea
       type: container
     containerPropertyIdentifier: timezone
     name: timezone
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/assets/Generator.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/assets/Generator.view.yaml`

 * *Files 4% similar despite different names*

```diff
@@ -23,67 +23,67 @@
 implements:
   - space: "{{powerops_models}}"
     externalId: PowerAsset
     version: "{{version}}"
     type: view
 version: "{{version}}"
 properties:
-  pMin:
+  productionMin:
     container:
       space: "{{powerops_models}}"
       externalId: Generator
       type: container
-    containerPropertyIdentifier: pMin
-    name: pMin
-  penstock:
+    containerPropertyIdentifier: productionMin
+    name: productionMin
+  penstockNumber:
     container:
       space: "{{powerops_models}}"
       externalId: Generator
       type: container
-    containerPropertyIdentifier: penstock
-    name: penstock
+    containerPropertyIdentifier: penstockNumber
+    name: penstockNumber
   startCost:
     container:
       space: "{{powerops_models}}"
       externalId: Generator
       type: container
     containerPropertyIdentifier: startCost
     name: startCost
   startStopCost:
     container:
       space: "{{powerops_models}}"
       externalId: Generator
       type: container
     containerPropertyIdentifier: startStopCost
     name: startStopCost
-  isAvailableTimeSeries:
+  availabilityTimeSeries:
     container:
       space: "{{powerops_models}}"
       externalId: Generator
       type: container
-    containerPropertyIdentifier: isAvailable
-    name: isAvailableTimeSeries
-  efficiencyCurve:
+    containerPropertyIdentifier: availabilityTimeSeries
+    name: availabilityTimeSeries
+  generatorEfficiencyCurve:
     container:
       space: "{{powerops_models}}"
       externalId: Generator
       type: container
-    containerPropertyIdentifier: efficiencyCurve
-    name: generatorEfficiency
+    containerPropertyIdentifier: generatorEfficiencyCurve
+    name: generatorEfficiencyCurve
     source:
       space: "{{powerops_models}}"
       externalId: GeneratorEfficiencyCurve
       version: "{{version}}"
       type: view
-  turbineCurves:
+  turbineEfficiencyCurves:
     type:
       space: "{{powerops_type_space}}"
       externalId: isSubAssetOf
     source:
       space: "{{powerops_models}}"
       externalId: TurbineEfficiencyCurve
       version: "{{version}}"
       type: view
     direction: outwards
     name: turbineCurves
-    description: The watercourses that are connected to the PriceArea.
+    description: TODO description
     connectionType: multi_edge_connection
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/assets/GeneratorEfficiency.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/assets/GeneratorEfficiency.view.yaml`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/assets/Plant.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/assets/Plant.view.yaml`

 * *Files 27% similar despite different names*

```diff
@@ -37,68 +37,68 @@
   outletLevel:
     container:
       space: "{{powerops_models}}"
       externalId: Plant
       type: container
     containerPropertyIdentifier: outletLevel
     name: outletLevel
-  pMax:
+  productionMax:
     container:
       space: "{{powerops_models}}"
       externalId: Plant
       type: container
-    containerPropertyIdentifier: pMax
-    name: pMax
-  pMin:
+    containerPropertyIdentifier: productionMax
+    name: productionMax
+  productionMin:
     container:
       space: "{{powerops_models}}"
       externalId: Plant
       type: container
-    containerPropertyIdentifier: pMin
-    name: pMin
+    containerPropertyIdentifier: productionMin
+    name: productionMin
   penstockHeadLossFactors:
     container:
       space: "{{powerops_models}}"
       externalId: Plant
       type: container
     containerPropertyIdentifier: penstockHeadLossFactors
     name: penstockHeadLossFactors
-  watercourse:
-    container:
-      space: "{{powerops_models}}"
-      externalId: Plant
-      type: container
-    containerPropertyIdentifier: watercourse
-    name: watercourse
-    source:
-      space: "{{powerops_models}}"
-      externalId: Watercourse
-      version: "{{version}}"
-      type: view
+  # watercourse:
+  #   container:
+  #     space: "{{powerops_models}}"
+  #     externalId: Plant
+  #     type: container
+  #   containerPropertyIdentifier: watercourse
+  #   name: watercourse
+  #   source:
+  #     space: "{{powerops_models}}"
+  #     externalId: Watercourse
+  #     version: "{{version}}"
+  #     type: view
   connectionLosses:
     container:
       space: "{{powerops_models}}"
       externalId: Plant
       type: container
     containerPropertyIdentifier: connectionLosses
     name: connectionLosses
-  pMaxTimeSeries:
+  productionMaxTimeSeries:
     container:
       space: "{{powerops_models}}"
       externalId: Plant
       type: container
-    containerPropertyIdentifier: pMaxTimeSeries
-    name: pMaxTimeSeries
-  pMinTimeSeries:
+    containerPropertyIdentifier: productionMaxTimeSeries
+    name: productionMaxTimeSeries
+  productionMinTimeSeries:
     container:
       space: "{{powerops_models}}"
       externalId: Plant
       type: container
-    containerPropertyIdentifier: pMinTimeSeries
-    name: pMinTimeSeries
+    containerPropertyIdentifier: productionMinTimeSeries
+    name: productionMinTimeSeries
   waterValueTimeSeries:
     container:
       space: "{{powerops_models}}"
       externalId: Plant
       type: container
     containerPropertyIdentifier: waterValueTimeSeries
     name: waterValueTimeSeries
@@ -126,26 +126,26 @@
   headDirectTimeSeries:
     container:
       space: "{{powerops_models}}"
       externalId: Plant
       type: container
     containerPropertyIdentifier: headDirectTimeSeries
     name: headDirectTimeSeries
-  inletReservoir:
-    container:
-      space: "{{powerops_models}}"
-      externalId: Plant
-      type: container
-    containerPropertyIdentifier: inletReservoir
-    name: inletReservoir
-    source:
-      space: "{{powerops_models}}"
-      externalId: Reservoir
-      version: "{{version}}"
-      type: view
+  # inletReservoir:
+  #   container:
+  #     space: "{{powerops_models}}"
+  #     externalId: Plant
+  #     type: container
+  #   containerPropertyIdentifier: inletReservoir
+  #   name: inletReservoir
+  #   source:
+  #     space: "{{powerops_models}}"
+  #     externalId: Reservoir
+  #     version: "{{version}}"
+  #     type: view
   generators:
     type:
       space: "{{powerops_type_space}}"
       externalId: isSubAssetOf
     source:
       space: "{{powerops_models}}"
       externalId: Generator
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/assets/PlantShop.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/bid_configuration/ShopBasedPartialBidConfiguration.view.yaml`

 * *Files 26% similar despite different names*

```diff
@@ -1,29 +1,54 @@
 space: "{{powerops_models}}"
-externalId: PlantShop
-name: PlantShop
-description: A minimal representation of a Plant - only data that is needed for Shop
+externalId: ShopBasedPartialBidConfiguration
+name: ShopBasedPartialBidConfiguration
+description: A description of the ShopBasedPartialBidConfiguration
 filter:
   and:
     - hasData:
         - type: container
           space: "{{powerops_models}}"
-          externalId: Asset
+          externalId: PartialBidConfiguration
     - equals:
         property:
           - node
           - space
         value: "{{powerops_instance_space}}"
     - equals:
         property:
           - node
           - type
         value:
-          externalId: PlantShop
+          externalId: ShopBasedPartialBidConfiguration
           space: "{{powerops_type_space}}"
 implements:
   - space: "{{powerops_models}}"
-    externalId: PowerAsset
+    externalId: PartialBidConfiguration
     version: "{{version}}"
     type: view
 version: "{{version}}"
-properties: {}
+properties:
+  powerAsset:
+    container:
+      space: "{{powerops_models}}"
+      externalId: PartialBidConfiguration
+      type: container
+    containerPropertyIdentifier: powerAsset
+    name: powerAsset
+    description: TODO description
+    source:
+      space: "{{powerops_models}}"
+      externalId: PowerAsset
+      version: "{{version}}"
+      type: view
+  shopScenarios:
+    container:
+      space: "{{powerops_models}}"
+      externalId: PartialShopBased
+      type: container
+    containerPropertyIdentifier: shopScenarios
+    name: shopScenarios
+    source:
+      space: "{{powerops_models}}"
+      externalId: ScenarioSet
+      version: "{{version}}"
+      type: view
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/assets/PriceAreaAFRR.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/assets/PriceAreaAFRR.view.yaml`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/assets/PriceAreaAsset.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_outputs/TaskDispatcherOutput.view.yaml`

 * *Files 17% similar despite different names*

```diff
@@ -1,54 +1,55 @@
 space: "{{powerops_models}}"
-externalId: PriceAreaAsset
-name: PriceAreaAsset
+externalId: TaskDispatcherOutput
+name: TaskDispatcherOutput
+description: Input to the task dispatcher in the shop bid process
 filter:
   and:
     - hasData:
         - type: container
           space: "{{powerops_models}}"
-          externalId: PriceArea
+          externalId: FunctionData
     - equals:
         property:
           - node
           - space
         value: "{{powerops_instance_space}}"
     - equals:
         property:
           - node
           - type
         value:
-          externalId: PriceArea
+          externalId: TaskDispatcherOutput
           space: "{{powerops_type_space}}"
 implements:
   - space: "{{powerops_models}}"
-    externalId: PriceArea
+    externalId: FunctionOutput
     version: "{{version}}"
     type: view
 version: "{{version}}"
 properties:
-  watercourses:
-    type:
-      space: "{{powerops_type_space}}"
-      externalId: isWatercourseOf
+  input:
+    container:
+      space: "{{powerops_models}}"
+      externalId: FunctionData
+      type: container
+    containerPropertyIdentifier: direct1
+    name: input
+    description: The previous step in the process.
     source:
       space: "{{powerops_models}}"
-      externalId: Watercourse
+      externalId: TaskDispatcherInput
       version: "{{version}}"
       type: view
-    direction: outwards
-    name: watercourses
-    description: An array of associated watercourses.
-    connectionType: multi_edge_connection
-  plants:
+  processSubTasks:
     type:
       space: "{{powerops_type_space}}"
-      externalId: isPlantOf
+      externalId: processSubTasks
     source:
       space: "{{powerops_models}}"
-      externalId: Plant
+      externalId: FunctionInput
       version: "{{version}}"
       type: view
     direction: outwards
-    name: plants
-    description: An array of associated plants.
+    name: processSubTasks
+    description: An array of input for process subtasks used for partial bid calculations.
     connectionType: multi_edge_connection
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/assets/PriceAreaDayAhead.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_inputs/TaskDispatcherInput.view.yaml`

 * *Files 16% similar despite different names*

```diff
@@ -1,40 +1,49 @@
 space: "{{powerops_models}}"
-externalId: PriceAreaDayAhead
-name: PriceAreaDayAhead
+externalId: TaskDispatcherInput
+name: TaskDispatcherInput
+description: Input to the task dispatcher in the shop bid process
 filter:
   and:
     - hasData:
         - type: container
           space: "{{powerops_models}}"
-          externalId: PriceArea
+          externalId: FunctionData
     - equals:
         property:
           - node
           - space
         value: "{{powerops_instance_space}}"
     - equals:
         property:
           - node
           - type
         value:
-          externalId: PriceArea
+          externalId: TaskDispatcherInput
           space: "{{powerops_type_space}}"
 implements:
   - space: "{{powerops_models}}"
-    externalId: PriceArea
+    externalId: FunctionInput
     version: "{{version}}"
     type: view
 version: "{{version}}"
 properties:
-  defaultMethod:
+  bidConfiguration:
     container:
       space: "{{powerops_models}}"
-      externalId: PriceArea
+      externalId: FunctionData
       type: container
-    containerPropertyIdentifier: defaultMethodDayAhead
-    name: defaultMethod
+    containerPropertyIdentifier: direct1
+    name: bidConfiguration
     source:
       space: "{{powerops_models}}"
-      externalId: BidMethodDayAhead
+      externalId: BidConfiguration
       version: "{{version}}"
       type: view
+  bidDate:
+    container:
+      space: "{{powerops_models}}"
+      externalId: FunctionData
+      type: container
+    containerPropertyIdentifier: date1
+    name: bidDate
+    description: The bid date
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/assets/TurbineEfficiency.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/assets/TurbineEfficiency.view.yaml`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/assets/Watercourse.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_inputs/ShopPartialBidMatrixCalculationInput.view.yaml`

 * *Files 18% similar despite different names*

```diff
@@ -1,58 +1,55 @@
 space: "{{powerops_models}}"
-externalId: Watercourse
-name: Watercourse
-description: A description of the Watercourse Asset
+externalId: ShopPartialBidMatrixCalculationInput
+name: ShopPartialBidMatrixCalculationInput
+description: Input to the task dispatcher in the shop bid process
 filter:
   and:
     - hasData:
         - type: container
           space: "{{powerops_models}}"
-          externalId: Watercourse
+          externalId: FunctionData
     - equals:
         property:
           - node
           - space
         value: "{{powerops_instance_space}}"
     - equals:
         property:
           - node
           - type
         value:
-          externalId: Watercourse
+          externalId: ShopPartialBidMatrixCalculationInput
           space: "{{powerops_type_space}}"
 implements:
   - space: "{{powerops_models}}"
-    externalId: PowerAsset
+    externalId: PartialBidMatrixCalculationInput
     version: "{{version}}"
     type: view
 version: "{{version}}"
 properties:
-  productionObligation:
-    container:
-      space: "{{powerops_models}}"
-      externalId: Watercourse
-      type: container
-    containerPropertyIdentifier: productionObligationTimeSeries
-    name: productionObligation
-    description: The production obligation for the Watercourse.
-  plants:
+  priceProduction:
     type:
       space: "{{powerops_type_space}}"
-      externalId: isSubAssetOf
+      externalId: PriceProduction
     source:
       space: "{{powerops_models}}"
-      externalId: Plant
+      externalId: PriceProduction
       version: "{{version}}"
       type: view
     direction: outwards
-    name: plants
-    description: The plants that are connected to the Watercourse.
+    name: priceProduction
+    description: An array of shop results with price/prod timeseries pairs for all plants included in the respective shop scenario
     connectionType: multi_edge_connection
-  penaltyLimit:
+  partialBidConfiguration:
     container:
       space: "{{powerops_models}}"
-      externalId: Watercourse
+      externalId: FunctionData
       type: container
-    containerPropertyIdentifier: penaltyLimit
-    name: penaltyLimit
-    description: The penalty limit for the watercourse (used by SHOP).
+    containerPropertyIdentifier: direct3
+    name: partialBidConfiguration
+    description: The partial bid configuration related to the bid calculation task
+    source:
+      space: "{{powerops_models}}"
+      externalId: ShopBasedPartialBidConfiguration
+      version: "{{version}}"
+      type: view
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/assets/WatercourseShop.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/frontend_Asset.datamodel.yaml`

 * *Files 26% similar despite different names*

```diff
@@ -1,29 +1,30 @@
+externalId: frontend_Asset
 space: "{{powerops_models}}"
-externalId: WatercourseShop
-name: WatercourseShop
-description: A minimal representation of a Watercourse which is needed for Shop
-filter:
-  and:
-    - hasData:
-        - type: container
-          space: "{{powerops_models}}"
-          externalId: Asset
-    - equals:
-        property:
-          - node
-          - space
-        value: "{{powerops_instance_space}}"
-    - equals:
-        property:
-          - node
-          - type
-        value:
-          externalId: WatercourseShop
-          space: "{{powerops_type_space}}"
-implements:
-  - space: "{{powerops_models}}"
-    externalId: PowerAsset
+version: "1"
+name: PowerAsset
+description: "Describes the power operations asset model"
+views:
+  - externalId: PriceArea
+    space: "{{powerops_models}}"
+    type: view
+    version: "{{version}}"
+  - externalId: PowerAsset
+    space: "{{powerops_models}}"
+    type: view
+    version: "{{version}}"
+  - externalId: Plant
+    space: "{{powerops_models}}"
+    type: view
     version: "{{version}}"
+  - externalId: Generator
+    space: "{{powerops_models}}"
     type: view
-version: "{{version}}"
-properties: {}
+    version: "{{version}}"
+  - externalId: TurbineEfficiencyCurve
+    space: "{{powerops_models}}"
+    type: view
+    version: "{{version}}"
+  - externalId: GeneratorEfficiencyCurve
+    space: "{{powerops_models}}"
+    type: view
+    version: "{{version}}"
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/bid_configuration/1-interface.BidConfiguration.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_inputs/TaskDispatcherBenchmarkingInput.view.yaml`

 * *Files 26% similar despite different names*

```diff
@@ -1,31 +1,41 @@
-space: "{{powerops_models}}"
-externalId: BidConfiguration
-name: BidConfiguration
-description: ""
+space: '{{powerops_models}}'
+externalId: TaskDispatcherBenchmarkingInput
+name: TaskDispatcherBenchmarkingInput
+description: The task dispatcher input data for benchmarking
 filter:
   and:
     - hasData:
         - type: container
-          space: "{{powerops_models}}"
-          externalId: BidConfiguration
+          space: '{{powerops_models}}'
+          externalId: FunctionMetadata
     - equals:
         property:
           - node
           - space
-        value: "{{powerops_instance_space}}"
-implements: []
-version: "{{version}}"
+        value: '{{powerops_instance_space}}'
+    - equals:
+        property:
+          - node
+          - type
+        value:
+          externalId: TaskDispatcherBenchmarkingInput
+          space: '{{powerops_type_space}}'
+implements:
+  - space: '{{powerops_models}}'
+    externalId: FunctionInput
+    version: '{{version}}'
+    type: view
+version: '{{version}}'
 properties:
-  marketConfiguration:
+  bidDocument:
     container:
-      space: "{{powerops_models}}"
-      externalId: BidConfiguration
+      space: '{{powerops_models}}'
+      externalId: FunctionMetadata
       type: container
-    containerPropertyIdentifier: marketConfiguration
-    name: marketConfiguration
-    description: The bid method related to the bid configuration
+    containerPropertyIdentifier: data
+    name: bidDocument
     source:
-      space: "{{powerops_models}}"
-      externalId: MarketConfiguration
-      version: "{{version}}"
+      space: '{{powerops_models}}'
+      externalId: BidDocumentDayAheadSimple
+      version: '{{version}}'
       type: view
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/bid_configuration/BidConfigurationAFRR.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_inputs/ShopTriggerInput.view.yaml`

 * *Files 14% similar despite different names*

```diff
@@ -1,80 +1,63 @@
 space: "{{powerops_models}}"
-externalId: BidConfigurationAFRR
-name: BidConfigurationAFRR
-description: ""
+externalId: SHOPTriggerInput
+name: SHOPTriggerInput
+description: Base class for all function inputs
 filter:
   and:
     - hasData:
         - type: container
           space: "{{powerops_models}}"
-          externalId: BidConfiguration
+          externalId: FunctionData
     - equals:
         property:
           - node
           - space
         value: "{{powerops_instance_space}}"
     - equals:
         property:
           - node
           - type
         value:
+          externalId: SHOPTriggerInput
           space: "{{powerops_type_space}}"
-          externalId: BidConfigurationAFRR
 implements:
   - space: "{{powerops_models}}"
-    externalId: BidConfiguration
+    externalId: FunctionInput
     version: "{{version}}"
     type: view
 version: "{{version}}"
 properties:
-  method:
+  cogShopTag:
     container:
       space: "{{powerops_models}}"
-      externalId: BidConfiguration
+      externalId: FunctionData
       type: container
-    containerPropertyIdentifier: method
-    name: method
-    description: The bid method related to the bid configuration
-    source:
-      space: "{{powerops_models}}"
-      externalId: BidMethodAFRR
-      version: "{{version}}"
-      type: view
-  priceArea:
+    containerPropertyIdentifier: text1
+    name: cogShopTag
+    description: Optionally specify cogshop tag to trigger
+  case:
     container:
       space: "{{powerops_models}}"
-      externalId: BidConfiguration
+      externalId: FunctionData
       type: container
-    containerPropertyIdentifier: priceArea
-    name: priceArea
+    containerPropertyIdentifier: direct2
+    name: case
+    description: The scenario that is used in the shop run
     source:
       space: "{{powerops_models}}"
-      externalId: PriceAreaAFRR
+      externalId: Case
       version: "{{version}}"
       type: view
-  plants:
-    type:
-      space: "{{powerops_type_space}}"
-      externalId: BidConfiguration.plants
-    source:
+  preProcessorInput:
+    container:
       space: "{{powerops_models}}"
-      externalId: Plant
-      version: "{{version}}"
-      type: view
-    direction: outwards
-    name: plants
-    description: The plants
-    connectionType: multi_edge_connection
-  watercourses:
-    type:
-      space: "{{powerops_type_space}}"
-      externalId: BidConfiguration.watercourses
+      externalId: FunctionData
+      type: container
+    containerPropertyIdentifier: direct3
+    name: preProcessorInput
+    description: The preprocessor input to the shop run
     source:
       space: "{{powerops_models}}"
-      externalId: Watercourse
+      externalId: PreprocessorInput
       version: "{{version}}"
       type: view
-    direction: outwards
-    name: watercourses
-    description: The watercourses
-    connectionType: multi_edge_connection
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/bid_configuration/BidConfigurationSHOP.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/bid_configuration/BidConfiguration.view.yaml`

 * *Files 10% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 space: "{{powerops_models}}"
-externalId: BidConfigurationShop
-name: BidConfigurationShop
+externalId: BidConfiguration
+name: BidConfiguration
 description: ""
 filter:
   and:
     - hasData:
         - type: container
           space: "{{powerops_models}}"
           externalId: BidConfiguration
@@ -15,75 +15,58 @@
         value: "{{powerops_instance_space}}"
     - equals:
         property:
           - node
           - type
         value:
           space: "{{powerops_type_space}}"
-          externalId: BidConfigurationShop
-implements:
-  - space: "{{powerops_models}}"
-    externalId: BidConfiguration
-    version: "{{version}}"
-    type: view
+          externalId: BidConfiguration
+implements: []
 version: "{{version}}"
 properties:
   name:
     container:
       space: "{{powerops_models}}"
       externalId: BidConfiguration
       type: container
     containerPropertyIdentifier: name
     name: name
     description: The name of the bid configuration
-  method:
+  marketConfiguration:
     container:
       space: "{{powerops_models}}"
       externalId: BidConfiguration
       type: container
-    containerPropertyIdentifier: method
-    name: method
-    description: The bid method related to the bid configuration
+    containerPropertyIdentifier: marketConfiguration
+    name: marketConfiguration
+    description: The market configuration related to the bid configuration
     source:
       space: "{{powerops_models}}"
-      externalId: BidMethodSHOPMultiScenario
+      externalId: MarketConfiguration
       version: "{{version}}"
       type: view
-  plantsShop:
-    type:
-      space: "{{powerops_type_space}}"
-      externalId: BidConfiguration.plantsShop
+  priceArea:
+    container:
+      space: "{{powerops_models}}"
+      externalId: BidConfiguration
+      type: container
+    containerPropertyIdentifier: priceArea
+    name: priceArea
+    description: The price area related to the bid calculation task
     source:
       space: "{{powerops_models}}"
-      externalId: PlantShop
+      externalId: PriceArea
       version: "{{version}}"
       type: view
-    direction: outwards
-    name: plants
-    description: The plants modelled in the shop runs
-    connectionType: multi_edge_connection
-  watercoursesShop:
+  partials:
     type:
       space: "{{powerops_type_space}}"
-      externalId: BidConfiguration.watercoursesShop
+      externalId: BidConfiguration.partials
     source:
       space: "{{powerops_models}}"
-      externalId: WatercourseShop
+      externalId: PartialBidConfiguration
       version: "{{version}}"
       type: view
     direction: outwards
-    name: watercourses
-    description: The watercourses modelled in the shop runs
+    name: partials
+    description: Configuration of the partial bids that make up the total bid
     connectionType: multi_edge_connection
-  priceArea:
-    container:
-      space: "{{powerops_models}}"
-      externalId: BidConfiguration
-      type: container
-    containerPropertyIdentifier: priceArea
-    name: priceArea
-    description: The price area related to the bid configuration
-    source:
-      space: "{{powerops_models}}"
-      externalId: PriceArea # consider removing PriceAreaDayahead view... not needed for this config
-      version: "{{version}}"
-      type: view
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/bid_configuration/BidConfigurationWater.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/bid_document/BidDocumentDayAhead.view.yaml`

 * *Files 24% similar despite different names*

```diff
@@ -1,81 +1,67 @@
 space: "{{powerops_models}}"
-externalId: BidConfigurationWater
-name: BidConfigurationWater
-description: ""
+externalId: BidDocumentDayAhead
+name: BidDocumentDayAhead
 filter:
   and:
     - hasData:
         - type: container
           space: "{{powerops_models}}"
-          externalId: BidConfiguration
+          externalId: BidDocument
+        - type: container
+          space: "{{powerops_models}}"
+          externalId: BidDocumentDayAhead
     - equals:
         property:
           - node
           - space
         value: "{{powerops_instance_space}}"
     - equals:
         property:
           - node
           - type
         value:
+          externalId: DayAheadBidDocument
           space: "{{powerops_type_space}}"
-          externalId: BidConfigurationWater
 implements:
   - space: "{{powerops_models}}"
-    externalId: BidConfiguration
+    externalId: BidDocument
     version: "{{version}}"
     type: view
 version: "{{version}}"
 properties:
-  method:
+  bidConfiguration:
     container:
       space: "{{powerops_models}}"
-      externalId: BidConfiguration
+      externalId: BidDocumentDayAhead
       type: container
-    containerPropertyIdentifier: method
-    name: method
-    description: The bid method related to the bid configuration
+    containerPropertyIdentifier: bidConfiguration
+    name: bidConfiguration
     source:
       space: "{{powerops_models}}"
-      externalId: BidMethodWaterValue
+      externalId: BidConfiguration
       version: "{{version}}"
       type: view
-  plants:
-    type:
-      space: "{{powerops_type_space}}"
-      externalId: BidConfiguration.plants
+  total:
+    container:
+      space: "{{powerops_models}}"
+      externalId: BidDocumentDayAhead
+      type: container
+    containerPropertyIdentifier: total
+    name: total
     source:
       space: "{{powerops_models}}"
-      externalId: Plant
+      externalId: BidMatrix
       version: "{{version}}"
       type: view
-    direction: outwards
-    name: plants
-    description: The plants and related information needed to run the water value based method
-    connectionType: multi_edge_connection
-  watercourses:
+  partials:
     type:
       space: "{{powerops_type_space}}"
-      externalId: BidConfiguration.watercourses
+      externalId: partialBid
     source:
       space: "{{powerops_models}}"
-      externalId: Watercourse
+      externalId: BidMatrix
       version: "{{version}}"
       type: view
     direction: outwards
-    name: watercourses
-    description: The watercourses and related information needed to run the water value based method
+    name: partials
     connectionType: multi_edge_connection
-  priceArea:
-    container:
-      space: "{{powerops_models}}"
-      externalId: BidConfiguration
-      type: container
-    containerPropertyIdentifier: priceArea
-    name: priceArea
-    description: The price area related to the bid configuration
-    source:
-      space: "{{powerops_models}}"
-      externalId: PriceArea
-      version: "{{version}}"
-      type: view
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/bid_document/1-interface.BidDocument.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/bid_document/1-interface.BidDocument.view.yaml`

 * *Files 6% similar despite different names*

```diff
@@ -20,14 +20,22 @@
     container:
       space: "{{powerops_models}}"
       externalId: BidDocument
       type: container
     containerPropertyIdentifier: name
     name: name
     description: Unique name for a given instance of a Bid Document. A combination of name, priceArea, date and startCalculation.
+  processId:
+    container:
+      space: "{{powerops_models}}"
+      externalId: BidDocument
+      type: container
+    containerPropertyIdentifier: processId
+    name: processId
+    description: The process associated with the Bid calculation workflow.
   deliveryDate:
     container:
       space: "{{powerops_models}}"
       externalId: BidDocument
       type: container
     containerPropertyIdentifier: deliveryDate
     name: deliveryDate
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/bid_document/BidDocumentAFRR.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/bid_document/BidDocumentAFRR.view.yaml`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/bid_document/BidDocumentDayAhead.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/bid_document/BidDocumentDayAheadSimple.view.yaml`

 * *Files 13% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 space: "{{powerops_models}}"
-externalId: BidDocumentDayAhead
-name: BidDocumentDayAhead
+externalId: BidDocumentDayAheadSimple
+name: BidDocumentDayAheadSimple
 filter:
   and:
     - hasData:
         - type: container
           space: "{{powerops_models}}"
           externalId: BidDocument
         - type: container
@@ -18,45 +18,29 @@
     - equals:
         property:
           - node
           - type
         value:
           externalId: DayAheadBidDocument
           space: "{{powerops_type_space}}"
-implements:
-  - space: "{{powerops_models}}"
-    externalId: BidDocument
-    version: "{{version}}"
-    type: view
+implements: []
 version: "{{version}}"
 properties:
   priceArea:
     container:
       space: "{{powerops_models}}"
       externalId: BidDocument
       type: container
     containerPropertyIdentifier: priceArea
     name: priceArea
     source:
       space: "{{powerops_models}}"
       externalId: PriceArea
       version: "{{version}}"
       type: view
-  method:
-    container:
-      space: "{{powerops_models}}"
-      externalId: BidDocumentDayAhead
-      type: container
-    containerPropertyIdentifier: method
-    name: method
-    source:
-      space: "{{powerops_models}}"
-      externalId: BidMethodDayAhead
-      version: "{{version}}"
-      type: view
   total:
     container:
       space: "{{powerops_models}}"
       externalId: BidDocumentDayAhead
       type: container
     containerPropertyIdentifier: total
     name: total
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/bid_document/BidDocumentDayAheadSimple.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/bid_matrix/BidMatrix.view.yaml`

 * *Files 14% similar despite different names*

```diff
@@ -1,75 +1,68 @@
 space: "{{powerops_models}}"
-externalId: BidDocumentDayAheadSimple
-name: BidDocumentDayAheadSimple
+externalId: BidMatrix
+name: BidMatrix
 filter:
   and:
     - hasData:
         - type: container
           space: "{{powerops_models}}"
-          externalId: BidDocument
-        - type: container
-          space: "{{powerops_models}}"
-          externalId: BidDocumentDayAhead
+          externalId: BidMatrix
     - equals:
         property:
           - node
           - space
         value: "{{powerops_instance_space}}"
-    - equals:
-        property:
-          - node
-          - type
-        value:
-          externalId: DayAheadBidDocument
-          space: "{{powerops_type_space}}"
 implements: []
 version: "{{version}}"
 properties:
-  priceArea:
+  matrix:
     container:
       space: "{{powerops_models}}"
-      externalId: BidDocument
+      externalId: BidMatrix
       type: container
-    containerPropertyIdentifier: priceArea
-    name: priceArea
-    source:
-      space: "{{powerops_models}}"
-      externalId: PriceAreaDayAhead
-      version: "{{version}}"
-      type: view
-  method:
+    containerPropertyIdentifier: matrix
+    name: matrix
+  powerAsset:
     container:
       space: "{{powerops_models}}"
-      externalId: BidDocumentDayAhead
+      externalId: BidMatrix
       type: container
-    containerPropertyIdentifier: method
-    name: method
     source:
       space: "{{powerops_models}}"
-      externalId: BidMethodDayAhead
+      externalId: PowerAsset
       version: "{{version}}"
       type: view
-  total:
+    containerPropertyIdentifier: powerAsset
+    name: powerAsset
+  state:
     container:
       space: "{{powerops_models}}"
-      externalId: BidDocumentDayAhead
+      externalId: BidMatrix
       type: container
-    containerPropertyIdentifier: total
-    name: total
-    source:
+    containerPropertyIdentifier: state
+    name: state
+  partialBidConfiguration:
+    container:
       space: "{{powerops_models}}"
       externalId: BidMatrix
+      type: container
+    source:
+      space: "{{powerops_models}}"
+      externalId: PartialBidConfiguration
       version: "{{version}}"
       type: view
-  partials:
+    containerPropertyIdentifier: partialBidConfiguration
+    name: partialBidConfiguration
+  alerts:
     type:
       space: "{{powerops_type_space}}"
-      externalId: partialBid
+      externalId: calculationIssue
     source:
       space: "{{powerops_models}}"
-      externalId: BidMatrix
+      externalId: Alert
       version: "{{version}}"
       type: view
     direction: outwards
-    name: partials
+    name: alerts
+    description: An array of calculation level Alerts.
     connectionType: multi_edge_connection
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/bid_matrix/1-interface.BidMatrix.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_inputs/1-interface.FunctionInput.view.yaml`

 * *Files 22% similar despite different names*

```diff
@@ -1,65 +1,56 @@
 space: "{{powerops_models}}"
-externalId: BidMatrix
-name: BidMatrix
+externalId: FunctionInput
+name: FunctionInput
+description: Base class for all function inputs
 filter:
   and:
     - hasData:
         - type: container
           space: "{{powerops_models}}"
-          externalId: BidMatrix
+          externalId: FunctionMetadata
     - equals:
         property:
           - node
           - space
         value: "{{powerops_instance_space}}"
+        # - not:
+        #     in:
+        #       property:
+        #         - node
+        #         - type
+        #       value: FunctionOutput
 implements: []
 version: "{{version}}"
 properties:
-  resourceCost:
+  processId:
     container:
       space: "{{powerops_models}}"
-      externalId: BidMatrix
+      externalId: FunctionMetadata
       type: container
-    containerPropertyIdentifier: resourceCost
-    name: resourceCost
-  matrix:
+    containerPropertyIdentifier: processId
+    name: processId
+    description: The process associated with the function execution
+  processStep:
     container:
       space: "{{powerops_models}}"
-      externalId: BidMatrix
+      externalId: FunctionMetadata
       type: container
-    containerPropertyIdentifier: matrix
-    name: matrix
-  assetType:
+    containerPropertyIdentifier: processStep
+    name: processStep
+    description: This is the step in the process.
+  functionName:
     container:
       space: "{{powerops_models}}"
-      externalId: BidMatrix
+      externalId: FunctionMetadata
       type: container
-    containerPropertyIdentifier: assetType
-    name: assetType
-  assetId:
+    containerPropertyIdentifier: functionName
+    name: functionName
+    description: The name of the function
+  functionCallId:
     container:
       space: "{{powerops_models}}"
-      externalId: BidMatrix
+      externalId: FunctionMetadata
       type: container
-    containerPropertyIdentifier: assetId
-    name: assetId
-  alerts:
-    type:
-      space: "{{powerops_type_space}}"
-      externalId: calculationIssue
-    source:
-      space: "{{powerops_models}}"
-      externalId: Alert
-      version: "{{version}}"
-      type: view
-    direction: outwards
-    name: alerts
-    connectionType: multi_edge_connection
-  isProcessed:
-    container:
-      space: "{{powerops_models}}"
-      externalId: BidMatrix
-      type: container
-    containerPropertyIdentifier: isProcessed
-    name: isProcessed
-    description: Whether the bid matrix has been processed by the bid matrix processor or not
+    containerPropertyIdentifier: functionCallId
+    name: functionCallId
+    description: The function call id
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/bid_matrix/CustomBidMatrix.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_outputs/PartialBidMatrixCalculationOutput.view.yaml`

 * *Files 21% similar despite different names*

```diff
@@ -1,49 +1,65 @@
-space: '{{powerops_models}}'
-externalId: CustomBidMatrix
-name: CustomBidMatrix
+space: "{{powerops_models}}"
+externalId: PartialBidMatrixCalculationOutput
+name: PartialBidMatrixCalculationOutput
+description: The output of a bid calculation method
 filter:
   and:
     - hasData:
         - type: container
-          space: '{{powerops_models}}'
-          externalId: BidMatrix
+          space: "{{powerops_models}}"
+          externalId: FunctionData
     - equals:
         property:
           - node
           - space
-        value: '{{powerops_instance_space}}'
+        value: "{{powerops_instance_space}}"
     - equals:
         property:
-          - '{{powerops_models}}'
-          - BidMatrix
-          - isProcessed
-        value: 'true'
-    - not:
-        in:
-          property:
-            - node
-            - type
-          values:
-            - space: '{{powerops_type_space}}'
-              externalId: DayAheadBasicBidMatrix
-            - space: '{{powerops_type_space}}'
-              externalId: DayAheadMultiScenarioMatrix
+          - node
+          - type
+        value:
+          externalId: PartialBidMatrixCalculationOutput
+          space: "{{powerops_type_space}}"
 implements:
-  - space: '{{powerops_models}}'
-    externalId: BidMatrix
-    version: '{{version}}'
+  - space: "{{powerops_models}}"
+    externalId: FunctionOutput
+    version: "{{version}}"
     type: view
-version: '{{version}}'
+version: "{{version}}"
 properties:
-  method:
+  partialMatrix:
     container:
-      space: '{{powerops_models}}'
+      space: "{{powerops_models}}"
+      externalId: FunctionData
+      type: container
+    containerPropertyIdentifier: direct1
+    name: partialMatrix
+    source:
+      space: "{{powerops_models}}"
       externalId: BidMatrix
+      version: "{{version}}"
+      type: view
+  bidConfiguration:
+    container:
+      space: "{{powerops_models}}"
+      externalId: FunctionData
+      type: container
+    containerPropertyIdentifier: direct2
+    name: bidConfiguration
+    source:
+      space: "{{powerops_models}}"
+      externalId: BidConfiguration
+      version: "{{version}}"
+      type: view
+  input:
+    container:
+      space: "{{powerops_models}}"
+      externalId: FunctionData
       type: container
-    containerPropertyIdentifier: method
-    name: method
+    containerPropertyIdentifier: direct3
+    name: input
     source:
-      space: '{{powerops_models}}'
-      externalId: BidMethodCustom
-      version: '{{version}}'
+      space: "{{powerops_models}}"
+      externalId: PartialBidMatrixCalculationInput
+      version: "{{version}}"
       type: view
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/bid_matrix/MultiScenarioMatrix.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_outputs/SHOPTriggerOutput.view.yaml`

 * *Files 17% similar despite different names*

```diff
@@ -1,59 +1,54 @@
-space: '{{powerops_models}}'
-externalId: MultiScenarioMatrix
-name: MultiScenarioMatrix
+space: "{{powerops_models}}"
+externalId: SHOPTriggerOutput
+name: SHOPTriggerOutput
+description: Base class for all function inputs
 filter:
   and:
     - hasData:
         - type: container
-          space: '{{powerops_models}}'
-          externalId: BidMatrix
+          space: "{{powerops_models}}"
+          externalId: FunctionData
     - equals:
         property:
           - node
           - space
-        value: '{{powerops_instance_space}}'
+        value: "{{powerops_instance_space}}"
     - equals:
         property:
           - node
           - type
         value:
-          externalId: DayAheadMultiScenarioMatrix
-          space: '{{powerops_type_space}}'
-    - equals:
-        property:
-          - '{{powerops_models}}'
-          - BidMatrix
-          - isProcessed
-        value: 'true'
+          externalId: SHOPTriggerOutput
+          space: "{{powerops_type_space}}"
 implements:
-  - space: '{{powerops_models}}'
-    externalId: BidMatrix
-    version: '{{version}}'
+  - space: "{{powerops_models}}"
+    externalId: FunctionOutput
+    version: "{{version}}"
     type: view
-version: '{{version}}'
+version: "{{version}}"
 properties:
-  method:
+  shopResult:
     container:
-      space: '{{powerops_models}}'
-      externalId: BidMatrix
+      space: "{{powerops_models}}"
+      externalId: FunctionData
       type: container
-    containerPropertyIdentifier: method
-    name: method
+    containerPropertyIdentifier: direct1
+    name: shopResult
     source:
-      space: '{{powerops_models}}'
-      externalId: BidMethodSHOPMultiScenario
-      version: '{{version}}'
+      space: "{{powerops_models}}"
+      externalId: SHOPResult
+      version: "{{version}}"
       type: view
-  scenarioResults:
-    type:
-      space: '{{powerops_type_space}}'
-      externalId: MultiScenarioMatrix.scenarioResults
+  input:
+    container:
+      space: "{{powerops_models}}"
+      externalId: FunctionData
+      type: container
+    containerPropertyIdentifier: direct2
+    name: input
+    description: The prepped and processed scenario to send to shop trigger
     source:
-      space: '{{powerops_models}}'
-      externalId: PriceProdCase
-      version: '{{version}}'
+      space: "{{powerops_models}}"
+      externalId: SHOPTriggerInput
+      version: "{{version}}"
       type: view
-    direction: outwards
-    name: scenarioResults
-    description: An array of price/prod pairs, one for each scenario/case - this is needed for the frontend
-    connectionType: multiEdgeConnection
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/bid_matrix/MultiScenarioMatrixRaw.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_inputs/PreprocessorInput.view.yaml`

 * *Files 19% similar despite different names*

```diff
@@ -1,59 +1,58 @@
-space: '{{powerops_models}}'
-externalId: MultiScenarioMatrixRaw
-name: MultiScenarioMatrixRaw
+space: "{{powerops_models}}"
+externalId: PreprocessorInput
+name: PreprocessorInput
+description: Input to the task dispatcher in the shop bid process
 filter:
   and:
     - hasData:
         - type: container
-          space: '{{powerops_models}}'
-          externalId: BidMatrix
+          space: "{{powerops_models}}"
+          externalId: FunctionData
     - equals:
         property:
           - node
           - space
-        value: '{{powerops_instance_space}}'
+        value: "{{powerops_instance_space}}"
     - equals:
         property:
           - node
           - type
         value:
-          externalId: DayAheadMultiScenarioMatrix
-          space: '{{powerops_type_space}}'
-    - equals:
-        property:
-          - '{{powerops_models}}'
-          - BidMatrix
-          - isProcessed
-        value: 'false'
+          externalId: PreprocessorInput
+          space: "{{powerops_type_space}}"
 implements:
-  - space: '{{powerops_models}}'
-    externalId: BidMatrixRaw
-    version: '{{version}}'
+  - space: "{{powerops_models}}"
+    externalId: FunctionInput
+    version: "{{version}}"
     type: view
-version: '{{version}}'
+version: "{{version}}"
 properties:
-  method:
+  scenario:
     container:
-      space: '{{powerops_models}}'
-      externalId: BidMatrix
+      space: "{{powerops_models}}"
+      externalId: FunctionData
       type: container
-    containerPropertyIdentifier: method
-    name: method
-    source:
-      space: '{{powerops_models}}'
-      externalId: BidMethodSHOPMultiScenario
-      version: '{{version}}'
-      type: view
-  shopResults:
-    type:
-      space: '{{powerops_type_space}}'
-      externalId: MultiScenarioMatrix.shopResults
+    containerPropertyIdentifier: direct1
+    name: scenario
+    description: The scenario to run shop with
     source:
-      space: '{{powerops_models}}'
-      externalId: SHOPResultPriceProd
-      version: '{{version}}'
+      space: "{{powerops_models}}"
+      externalId: Scenario
+      version: "{{version}}"
       type: view
-    direction: outwards
-    name: shopResults
-    description: An array of results, one for each scenario.
-    connectionType: multi_edge_connection
+  shopStart:
+    container:
+      space: "{{powerops_models}}"
+      externalId: FunctionData
+      type: container
+    containerPropertyIdentifier: timestamp1
+    name: shopStart
+    description: Start date of bid period
+  shopEnd:
+    container:
+      space: "{{powerops_models}}"
+      externalId: FunctionData
+      type: container
+    containerPropertyIdentifier: timestamp2
+    name: shopEnd
+    description: End date of bid period
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/bid_method/BidMethodSHOPMultiScenario.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/cogshop/ScenarioSet.view.yaml`

 * *Files 13% similar despite different names*

```diff
@@ -1,65 +1,63 @@
-space: '{{powerops_models}}'
-externalId: BidMethodSHOPMultiScenario
-name: BidMethodSHOPMultiScenario
+space: "{{powerops_models}}"
+externalId: ScenarioSet
+name: ScenarioSet
+description: The Scenario Set to run shop with
 filter:
   and:
     - hasData:
         - type: container
-          space: '{{powerops_models}}'
-          externalId: BidMethod
+          space: "{{powerops_models}}"
+          externalId: ScenarioSet
     - equals:
         property:
           - node
           - space
-        value: '{{powerops_instance_space}}'
-    - equals:
-        property:
-          - node
-          - type
-        value:
-          externalId: BidMethodSHOPMultiScenario
-          space: '{{powerops_type_space}}'
-implements:
-  - space: '{{powerops_models}}'
-    externalId: BidMethodDayAhead
-    version: '{{version}}'
-    type: view
-version: '{{version}}'
+        value: "{{powerops_instance_space}}"
+implements: []
+version: "{{version}}"
 properties:
-  scenarios:
-    type:
-      space: '{{powerops_type_space}}'
-      externalId: BidMethodDayahead.scenarios
-    source:
-      space: '{{powerops_models}}'
-      externalId: Scenario
-      version: '{{version}}'
-      type: view
-    direction: outwards
-    name: scenarios
-    description: The scenarios to run this bid method with (includes incremental mappings and base mappings)
-    connectionType: multi_edge_connection
+  name:
+    container:
+      space: "{{powerops_models}}"
+      externalId: ScenarioSet
+      type: container
+    containerPropertyIdentifier: name
+    name: name
+    description: The name of the scenario set to run
   shopStartSpecification:
     container:
-      space: '{{powerops_models}}'
-      externalId: BidMethod
+      space: "{{powerops_models}}"
+      externalId: ScenarioSet
       type: container
     containerPropertyIdentifier: shopStartSpecification
     name: shopStartSpecification
-    description: The shop start specification
+    description: TODO definition
   shopEndSpecification:
     container:
-      space: '{{powerops_models}}'
-      externalId: BidMethod
+      space: "{{powerops_models}}"
+      externalId: ScenarioSet
       type: container
     containerPropertyIdentifier: shopEndSpecification
     name: shopEndSpecification
-    description: The shop end specification
+    description: TODO definition
   shopBidDateSpecification:
     container:
-      space: '{{powerops_models}}'
-      externalId: BidMethod
+      space: "{{powerops_models}}"
+      externalId: ScenarioSet
       type: container
     containerPropertyIdentifier: shopBidDateSpecification
     name: shopBidDateSpecification
-    description: The shop bid date specification
+    description: TODO definition
+  shopScenarios:
+    type:
+      space: "{{powerops_type_space}}"
+      externalId: ScenarioSet.scenarios
+    source:
+      space: "{{powerops_models}}"
+      externalId: Scenario
+      version: "{{version}}"
+      type: view
+    direction: outwards
+    name: shopScenarios
+    description: Configuration of the partial bids that make up the total bid configuration
+    connectionType: multi_edge_connection
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/cogshop/Case.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/cogshop/Case.view.yaml`

 * *Files 6% similar despite different names*

```diff
@@ -1,93 +1,93 @@
-space: '{{powerops_models}}'
+space: "{{powerops_models}}"
 externalId: Case
 name: Case
 description: A case that links a Scenario and shop dates to run shop with
 filter:
   and:
     - hasData:
         - type: container
-          space: '{{powerops_models}}'
+          space: "{{powerops_models}}"
           externalId: Case
     - equals:
         property:
           - node
           - space
-        value: '{{powerops_instance_space}}'
+        value: "{{powerops_instance_space}}"
     - equals:
         property:
           - node
           - type
         value:
           externalId: Case
-          space: '{{powerops_type_space}}'
+          space: "{{powerops_type_space}}"
 implements: []
-version: '{{version}}'
+version: "{{version}}"
 properties:
   scenario:
     container:
-      space: '{{powerops_models}}'
+      space: "{{powerops_models}}"
       externalId: Case
       type: container
     containerPropertyIdentifier: scenario
     name: scenario
     description: The Shop scenario that was used to produce this result
     source:
-      space: '{{powerops_models}}'
+      space: "{{powerops_models}}"
       externalId: Scenario
-      version: '{{version}}'
+      version: "{{version}}"
       type: view
   caseFile:
     container:
-      space: '{{powerops_models}}'
+      space: "{{powerops_models}}"
       externalId: Case
       type: container
     containerPropertyIdentifier: caseFile
     name: caseFile
     description: The case file used
   reservoirMapping:
     container:
-      space: '{{powerops_models}}'
+      space: "{{powerops_models}}"
       externalId: Case
       type: container
     containerPropertyIdentifier: reservoirMapping
     name: reservoirMapping
     description: The cut file reservoir mapping
   cutOrderFiles:
     container:
-      space: '{{powerops_models}}'
+      space: "{{powerops_models}}"
       externalId: Case
       type: container
     containerPropertyIdentifier: cutOrderFiles
     name: cutOrderFiles
     description: Cut order files (Module series in PRODRISK)
   extraFiles:
     container:
-      space: '{{powerops_models}}'
+      space: "{{powerops_models}}"
       externalId: Case
       type: container
     containerPropertyIdentifier: extraFiles
     name: extraFiles
   cogShopFilesConfig:
     container:
-      space: '{{powerops_models}}'
+      space: "{{powerops_models}}"
       externalId: Case
       type: container
     containerPropertyIdentifier: cogShopFilesConfig
     name: cogShopFilesConfig
     description: Configuration for in what order to load the various files into pyshop
   startTime:
     container:
-      space: '{{powerops_models}}'
+      space: "{{powerops_models}}"
       externalId: Case
       type: container
     containerPropertyIdentifier: startTime
     name: startTime
     description: The start time of the case
   endTime:
     container:
-      space: '{{powerops_models}}'
+      space: "{{powerops_models}}"
       externalId: Case
       type: container
     containerPropertyIdentifier: endTime
     name: endTime
     description: The end time of the case
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/cogshop/Commands.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/cogshop/Commands.view.yaml`

 * *Files 18% similar despite different names*

```diff
@@ -1,33 +1,41 @@
-space: '{{powerops_models}}'
+space: "{{powerops_models}}"
 externalId: Commands
 name: Commands
 description: The commands to use in the shop model file
 filter:
   and:
     - hasData:
         - type: container
-          space: '{{powerops_models}}'
+          space: "{{powerops_models}}"
           externalId: CommandsConfig
     - equals:
         property:
           - node
           - space
-        value: '{{powerops_instance_space}}'
+        value: "{{powerops_instance_space}}"
     - equals:
         property:
           - node
           - type
         value:
           externalId: Commands
-          space: '{{powerops_type_space}}'
+          space: "{{powerops_type_space}}"
 implements: []
-version: '{{version}}'
+version: "{{version}}"
 properties:
+  name:
+    container:
+      space: "{{powerops_models}}"
+      externalId: CommandsConfig
+      type: container
+    containerPropertyIdentifier: name
+    name: name
+    description: Name for the Commands
   commands:
     container:
-      space: '{{powerops_models}}'
+      space: "{{powerops_models}}"
       externalId: CommandsConfig
       type: container
     containerPropertyIdentifier: commands
     name: commands
     description: The commands used in the shop model file
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/cogshop/Mapping.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/cogshop/Mapping.view.yaml`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/cogshop/ModelTemplate.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/cogshop/ModelTemplate.view.yaml`

 * *Files 23% similar despite different names*

```diff
@@ -1,91 +1,94 @@
-space: '{{powerops_models}}'
+space: "{{powerops_models}}"
 externalId: ModelTemplate
 name: ModelTemplate
 description: The template for SHOP models
 filter:
   and:
     - hasData:
         - type: container
-          space: '{{powerops_models}}'
+          space: "{{powerops_models}}"
           externalId: ModelTemplate
     - equals:
         property:
           - node
           - space
-        value: '{{powerops_instance_space}}'
+        value: "{{powerops_instance_space}}"
     - equals:
         property:
           - node
           - type
         value:
           externalId: ModelTemplate
-          space: '{{powerops_type_space}}'
+          space: "{{powerops_type_space}}"
 implements: []
-version: '{{version}}'
+version: "{{version}}"
 properties:
+  name:
+    container:
+      space: "{{powerops_models}}"
+      externalId: ModelTemplate
+      type: container
+    containerPropertyIdentifier: name
+    name: name
+    description: TODO
   version:
     container:
-      space: '{{powerops_models}}'
+      space: "{{powerops_models}}"
       externalId: ModelTemplate
       type: container
     containerPropertyIdentifier: version
     name: version
     description: The version of the model
   shopVersion:
     container:
-      space: '{{powerops_models}}'
+      space: "{{powerops_models}}"
       externalId: ModelTemplate
       type: container
     containerPropertyIdentifier: shopVersion
     name: shopVersion
     description: The version of SHOP to run
-  watercourse:
+  penaltyLimit:
     container:
-      space: '{{powerops_models}}'
+      space: "{{powerops_models}}"
       externalId: ModelTemplate
       type: container
-    containerPropertyIdentifier: watercourse
-    name: watercourse
-    description: The watercourse to run the model for
-    source:
-      space: '{{powerops_models}}'
-      externalId: WatercourseShop
-      version: '{{version}}'
-      type: view
+    containerPropertyIdentifier: penaltyLimit
+    name: penaltyLimit
+    description: TODO
   model:
     container:
-      space: '{{powerops_models}}'
+      space: "{{powerops_models}}"
       externalId: ModelTemplate
       type: container
     containerPropertyIdentifier: model
     name: model
     description: The shop model file to use as template before applying base mapping
   cogShopFilesConfig:
     container:
-      space: '{{powerops_models}}'
+      space: "{{powerops_models}}"
       externalId: ModelTemplate
       type: container
     containerPropertyIdentifier: cogShopFilesConfig
     name: cogShopFilesConfig
     description: Configuration for in what order to load the various files into pyshop
   extraFiles:
     container:
-      space: '{{powerops_models}}'
+      space: "{{powerops_models}}"
       externalId: ModelTemplate
       type: container
     containerPropertyIdentifier: extraFiles
     name: extraFiles
     description: Extra files related to a model template
   baseMappings:
     type:
-      space: '{{powerops_type_space}}'
+      space: "{{powerops_type_space}}"
       externalId: ModelTemplate.baseMappings
     source:
-      space: '{{powerops_models}}'
+      space: "{{powerops_models}}"
       externalId: Mapping
-      version: '{{version}}'
+      version: "{{version}}"
       type: view
     direction: outwards
     name: baseMappings
     description: The base mappings for the model
     connectionType: multi_edge_connection
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/cogshop/Scenario.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/cogshop/Scenario.view.yaml`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_inputs/1-interface.FunctionInput.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/shop_result/PriceProduction.view.yaml`

 * *Files 22% similar despite different names*

```diff
@@ -1,50 +1,51 @@
-space: '{{powerops_models}}'
-externalId: FunctionInput
-name: FunctionInput
-description: Base class for all function inputs
+space: "{{powerops_models}}"
+externalId: PriceProduction
+name: PriceProdution
+description: TODO
 filter:
   and:
     - hasData:
         - type: container
-          space: '{{powerops_models}}'
-          externalId: FunctionMetadata
+          space: "{{powerops_models}}"
+          externalId: PriceProduction
     - equals:
         property:
           - node
           - space
-        value: '{{powerops_instance_space}}'
+        value: "{{powerops_instance_space}}"
+    - equals:
+        property:
+          - node
+          - type
+        value:
+          externalId: PriceProduction
+          space: "{{powerops_type_space}}"
 implements: []
-version: '{{version}}'
+version: "{{version}}"
 properties:
-  processId:
-    container:
-      space: '{{powerops_models}}'
-      externalId: FunctionMetadata
-      type: container
-    containerPropertyIdentifier: processId
-    name: processId
-    description: The process associated with the function execution
-  processStep:
+  price:
     container:
-      space: '{{powerops_models}}'
-      externalId: FunctionMetadata
+      space: "{{powerops_models}}"
+      externalId: PriceProduction
       type: container
-    containerPropertyIdentifier: processStep
-    name: processStep
-    description: This is the step in the process.
-  functionName:
+    containerPropertyIdentifier: price
+    name: price
+  production:
     container:
-      space: '{{powerops_models}}'
-      externalId: FunctionMetadata
+      space: "{{powerops_models}}"
+      externalId: PriceProduction
       type: container
-    containerPropertyIdentifier: functionName
-    name: functionName
-    description: The name of the function
-  functionCallId:
+    containerPropertyIdentifier: production
+    name: production
+  shopResult:
     container:
-      space: '{{powerops_models}}'
-      externalId: FunctionMetadata
+      space: "{{powerops_models}}"
+      externalId: PriceProduction
       type: container
-    containerPropertyIdentifier: functionCallId
-    name: functionCallId
-    description: The function call id
+    source:
+      space: "{{powerops_models}}"
+      externalId: SHOPResult
+      version: "{{version}}"
+      type: view
+    containerPropertyIdentifier: shopResult
+    name: shopResult
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_inputs/BenchmarkingCollectorInput.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_inputs/BenchmarkingCollectorInput.view.yaml`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_inputs/PartialPostProcessingInput.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_outputs/BenchmarkingCollectorOutput.view.yaml`

 * *Files 17% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 space: '{{powerops_models}}'
-externalId: PartialPostProcessingInput
-name: PartialPostProcessingInput
-description: Partial post processing input
+externalId: BenchmarkingCollectorOutput
+name: BenchmarkingCollectorOutput
+description: The benchmarking collector output data.
 filter:
   and:
     - hasData:
         - type: container
           space: '{{powerops_models}}'
           externalId: FunctionMetadata
     - equals:
@@ -14,41 +14,28 @@
           - space
         value: '{{powerops_instance_space}}'
     - equals:
         property:
           - node
           - type
         value:
-          externalId: PartialPostProcessingInput
+          externalId: BenchmarkingCollectorOutput
           space: '{{powerops_type_space}}'
 implements:
   - space: '{{powerops_models}}'
-    externalId: FunctionInput
+    externalId: FunctionOutput
     version: '{{version}}'
     type: view
 version: '{{version}}'
 properties:
-  marketConfig:
+  input:
     container:
       space: '{{powerops_models}}'
       externalId: FunctionMetadata
       type: container
-    containerPropertyIdentifier: data
-    name: marketConfig
+    containerPropertyIdentifier: linkedStep
+    name: input
     source:
       space: '{{powerops_models}}'
-      externalId: MarketConfiguration
+      externalId: BenchmarkingCollectorInput
       version: '{{version}}'
       type: view
-  partialBidMatricesRaw:
-    type:
-      space: '{{powerops_type_space}}'
-      externalId: partialBidMatricesRaw
-    source:
-      space: '{{powerops_models}}'
-      externalId: BidMatrixRaw
-      version: '{{version}}'
-      type: view
-    direction: outwards
-    name: partialBidMatricesRaw
-    description: The partial bid matrices that needs post processing.
-    connectionType: multi_edge_connection
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_inputs/PreprocessorInput.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_outputs/1-interface.FunctionOutput.view.yaml`

 * *Files 20% similar despite different names*

```diff
@@ -1,58 +1,69 @@
-space: '{{powerops_models}}'
-externalId: PreprocessorInput
-name: PreprocessorInput
-description: Input to the task dispatcher in the shop bid process
+space: "{{powerops_models}}"
+externalId: FunctionOutput
+name: FunctionOutput
+description: Base class for all function inputs
 filter:
   and:
     - hasData:
         - type: container
-          space: '{{powerops_models}}'
-          externalId: FunctionData
+          space: "{{powerops_models}}"
+          externalId: FunctionMetadata
     - equals:
         property:
           - node
           - space
-        value: '{{powerops_instance_space}}'
-    - equals:
-        property:
-          - node
-          - type
-        value:
-          externalId: PreprocessorInput
-          space: '{{powerops_type_space}}'
-implements:
-  - space: '{{powerops_models}}'
-    externalId: FunctionInput
-    version: '{{version}}'
-    type: view
-version: '{{version}}'
+        value: "{{powerops_instance_space}}"
+        # - not:
+        #     in:
+        #       property:
+        #         - node
+        #         - type
+        #       value: FunctionInput
+implements: []
+version: "{{version}}"
 properties:
-  scenario:
+  processId:
+    container:
+      space: "{{powerops_models}}"
+      externalId: FunctionMetadata
+      type: container
+    containerPropertyIdentifier: processId
+    name: processId
+    description: The process associated with the function execution
+  processStep:
     container:
-      space: '{{powerops_models}}'
-      externalId: FunctionData
+      space: "{{powerops_models}}"
+      externalId: FunctionMetadata
       type: container
-    containerPropertyIdentifier: direct1
-    name: scenario
-    description: The scenario to run shop with
+    containerPropertyIdentifier: processStep
+    name: processStep
+    description: This is the step in the process.
+  alerts:
+    type:
+      space: "{{powerops_type_space}}"
+      externalId: calculationIssue
     source:
-      space: '{{powerops_models}}'
-      externalId: Scenario
-      version: '{{version}}'
+      space: "{{powerops_models}}"
+      externalId: Alert
+      version: "{{version}}"
       type: view
-  shopStart:
+    direction: outwards
+    name: alerts
+    description: An array of calculation level Alerts.
+    connectionType: multi_edge_connection
+  functionName:
     container:
-      space: '{{powerops_models}}'
-      externalId: FunctionData
+      space: "{{powerops_models}}"
+      externalId: FunctionMetadata
       type: container
-    containerPropertyIdentifier: date1
-    name: shopStart
-    description: Start date of bid period
-  shopEnd:
+    containerPropertyIdentifier: functionName
+    name: functionName
+    description: The name of the function
+  functionCallId:
     container:
-      space: '{{powerops_models}}'
-      externalId: FunctionData
+      space: "{{powerops_models}}"
+      externalId: FunctionMetadata
       type: container
-    containerPropertyIdentifier: date2
-    name: shopEnd
-    description: End date of bid period
+    containerPropertyIdentifier: functionCallId
+    name: functionCallId
+    description: The function call id
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_inputs/ShopPartialBidCalculationInput.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/shop_result/SHOPResult.view.yaml`

 * *Files 26% similar despite different names*

```diff
@@ -1,84 +1,98 @@
-space: '{{powerops_models}}'
-externalId: ShopPartialBidCalculationInput
-name: ShopPartialBidCalculationInput
-description: Input to the task dispatcher in the shop bid process
+space: "{{powerops_models}}"
+externalId: SHOPResult
+name: SHOPResult
+description: A generic shop result type that collects all time series outputs from SHOP. This type replaces the POWEROPS_SHOP_RUN event in cdf today
+#TODO: Consider adding a filter for SHOPResult and SHOPResultPriceProd to exclude PriceProdCase instances from this view that also uses the SHOPResult container
 filter:
   and:
     - hasData:
         - type: container
-          space: '{{powerops_models}}'
-          externalId: FunctionData
+          space: "{{powerops_models}}"
+          externalId: SHOPResult
     - equals:
         property:
           - node
           - space
-        value: '{{powerops_instance_space}}'
-    - equals:
-        property:
-          - node
-          - type
-        value:
-          externalId: ShopPartialBidCalculationInput
-          space: '{{powerops_type_space}}'
-implements:
-  - space: '{{powerops_models}}'
-    externalId: FunctionInput
-    version: '{{version}}'
-    type: view
-version: '{{version}}'
+        value: "{{powerops_instance_space}}"
+implements: []
+version: "{{version}}"
 properties:
-  plant:
+  case:
     container:
-      space: '{{powerops_models}}'
-      externalId: FunctionData
+      space: "{{powerops_models}}"
+      externalId: SHOPResult
       type: container
-    containerPropertyIdentifier: direct1
-    name: plant
-    description: The plant to calculate the partial bid for. Extract price/prod timeseries from Shop Results
+    containerPropertyIdentifier: case
+    name: case
+    description: The case that was used to produce this result
     source:
-      space: '{{powerops_models}}'
-      externalId: PlantShop
-      version: '{{version}}'
+      space: "{{powerops_models}}"
+      externalId: Case
+      version: "{{version}}"
       type: view
-  shopResultPriceProd:
+  objectiveSequence:
+    container:
+      space: "{{powerops_models}}"
+      externalId: SHOPResult
+      type: container
+    containerPropertyIdentifier: objectiveSequence
+    name: objectiveSequence
+    description: The sequence of the objective function
+  preRun:
+    container:
+      space: "{{powerops_models}}"
+      externalId: SHOPResult
+      type: container
+    containerPropertyIdentifier: preRun
+    name: preRun
+    description: The pre-run data for the SHOP run
+  postRun:
+    container:
+      space: "{{powerops_models}}"
+      externalId: SHOPResult
+      type: container
+    containerPropertyIdentifier: postRun
+    name: postRun
+    description: The post-run data for the SHOP run
+  shopMessages:
+    container:
+      space: "{{powerops_models}}"
+      externalId: SHOPResult
+      type: container
+    containerPropertyIdentifier: shopMessages
+    name: shopMessages
+    description: The messages from the SHOP run
+  cplexLogs:
+    container:
+      space: "{{powerops_models}}"
+      externalId: SHOPResult
+      type: container
+    containerPropertyIdentifier: cplexLogs
+    name: cplexLogs
+    description: The logs from CPLEX
+  outputTimeseries:
     type:
-      space: '{{powerops_type_space}}'
-      externalId: SHOPResultPriceProd
+      space: "{{powerops_type_space}}"
+      externalId: SHOPResult.outputTimeseries
     source:
-      space: '{{powerops_models}}'
-      externalId: SHOPResultPriceProd
-      version: '{{version}}'
+      space: "{{powerops_models}}"
+      externalId: SHOPTimeSeries
+      version: "{{version}}"
       type: view
     direction: outwards
-    name: shopResults
-    description: An array of shop results with price/prod timeserires pairs for all plants included in the respective shop scenario
+    name: outputTimeseries
+    description: TODO
     connectionType: multi_edge_connection
-  marketConfiguration:
-    container:
-      space: '{{powerops_models}}'
-      externalId: FunctionData
-      type: container
-    containerPropertyIdentifier: direct2
-    name: marketConfiguration
-    description: The market configuration to be used to generate the partial bid matrix
+  alerts:
+    type:
+      space: "{{powerops_type_space}}"
+      externalId: calculationIssue
     source:
-      space: '{{powerops_models}}'
-      externalId: MarketConfiguration
-      version: '{{version}}'
+      space: "{{powerops_models}}"
+      externalId: Alert
+      version: "{{version}}"
       type: view
-  stepEnabled:
-    container:
-      space: '{{powerops_models}}'
-      externalId: FunctionData
-      type: container
-    containerPropertyIdentifier: flag
-    name: stepEnabled
-    description: Whether the step is enabled or not
-  bidDate:
-    container:
-      space: '{{powerops_models}}'
-      externalId: FunctionData
-      type: container
-    containerPropertyIdentifier: date1
-    name: bidDate
-    description: The bid date
+    direction: outwards
+    name: alerts
+    description: An array of calculation level Alerts.
+    connectionType: multi_edge_connection
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_inputs/ShopTriggerInput.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_inputs/WaterPartialBidMatrixCalculationInput.view.yaml`

 * *Files 24% similar despite different names*

```diff
@@ -1,50 +1,42 @@
-space: '{{powerops_models}}'
-externalId: SHOPTriggerInput
-name: SHOPTriggerInput
-description: Base class for all function inputs
+space: "{{powerops_models}}"
+externalId: WaterValueBasedPartialBidMatrixCalculationInput
+name: WaterValueBasedPartialBidMatrixCalculation
+description: The data for a bid calculation for water value based method
 filter:
   and:
     - hasData:
         - type: container
-          space: '{{powerops_models}}'
+          space: "{{powerops_models}}"
           externalId: FunctionData
     - equals:
         property:
           - node
           - space
-        value: '{{powerops_instance_space}}'
+        value: "{{powerops_instance_space}}"
     - equals:
         property:
           - node
           - type
         value:
-          externalId: SHOPTriggerInput
-          space: '{{powerops_type_space}}'
+          externalId: WaterValueBasedPartialBidMatrixCalculationInput
+          space: "{{powerops_type_space}}"
 implements:
-  - space: '{{powerops_models}}'
-    externalId: FunctionInput
-    version: '{{version}}'
+  - space: "{{powerops_models}}"
+    externalId: PartialBidMatrixCalculationInput
+    version: "{{version}}"
     type: view
-version: '{{version}}'
+version: "{{version}}"
 properties:
-  cogShopTag:
+  partialBidConfiguration:
     container:
-      space: '{{powerops_models}}'
+      space: "{{powerops_models}}"
       externalId: FunctionData
       type: container
-    containerPropertyIdentifier: text1
-    name: cogShopTag
-    description: Optionally specify cogshop tag to trigger
-  scenario:
-    container:
-      space: '{{powerops_models}}'
-      externalId: FunctionData
-      type: container
-    containerPropertyIdentifier: direct2
-    name: scenario
-    description: The scenario that is used in the shop run
+    containerPropertyIdentifier: direct3
+    name: partialBidConfiguration
+    description: The partial bid configuration related to the bid calculation task
     source:
-      space: '{{powerops_models}}'
-      externalId: Scenario
-      version: '{{version}}'
+      space: "{{powerops_models}}"
+      externalId: WaterValueBasedPartialBidConfiguration
+      version: "{{version}}"
       type: view
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_inputs/TaskDispatcherBenchmarkingInput.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_outputs/TaskDispatcherBenchmarkingOutput.view.yaml`

 * *Files 14% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 space: '{{powerops_models}}'
-externalId: TaskDispatcherBenchmarkingInput
-name: TaskDispatcherBenchmarkingInput
-description: The task dispatcher input data for benchmarking
+externalId: TaskDispatcherBenchmarkingOutput
+name: TaskDispatcherBenchmarkingOutput
+description: The task dispatcher output data for benchmarking
 filter:
   and:
     - hasData:
         - type: container
           space: '{{powerops_models}}'
           externalId: FunctionMetadata
     - equals:
@@ -14,28 +14,28 @@
           - space
         value: '{{powerops_instance_space}}'
     - equals:
         property:
           - node
           - type
         value:
-          externalId: TaskDispatcherBenchmarkingInput
+          externalId: TaskDispatcherBenchmarkingOutput
           space: '{{powerops_type_space}}'
 implements:
   - space: '{{powerops_models}}'
-    externalId: FunctionInput
+    externalId: FunctionOutput
     version: '{{version}}'
     type: view
 version: '{{version}}'
 properties:
-  bidDocument:
+  input:
     container:
       space: '{{powerops_models}}'
       externalId: FunctionMetadata
       type: container
-    containerPropertyIdentifier: data
-    name: bidDocument
+    containerPropertyIdentifier: linkedStep
+    name: input
     source:
       space: '{{powerops_models}}'
-      externalId: BidDocumentDayAheadSimple
+      externalId: TaskDispatcherBenchmarkingInput
       version: '{{version}}'
       type: view
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_inputs/TaskDispatcherWaterInput.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_inputs/1-interface.PartialBidMatrixCalculationInput.view.yaml`

 * *Files 15% similar despite different names*

```diff
@@ -1,49 +1,50 @@
-space: '{{powerops_models}}'
-externalId: TaskDispatcherWaterInput
-name: TaskDispatcherWaterInput
-description: Input to the task dispatcher in the shop bid process
+space: "{{powerops_models}}"
+externalId: PartialBidMatrixCalculationInput
+name: PartialBidMatrixCalculation
+description: The data for a bid calculation for water value based method
 filter:
   and:
     - hasData:
         - type: container
-          space: '{{powerops_models}}'
+          space: "{{powerops_models}}"
           externalId: FunctionData
     - equals:
         property:
           - node
           - space
-        value: '{{powerops_instance_space}}'
-    - equals:
-        property:
-          - node
-          - type
-        value:
-          externalId: TaskDispatcherWaterInput
-          space: '{{powerops_type_space}}'
+        value: "{{powerops_instance_space}}"
+        # - in:
+        #     property:
+        #       - node
+        #       - type
+        #     value:
+        #       - WaterValueBasedPartialBidMatrixCalculationInput
+        #       - ShopPartialBidMatrixCalculationInput
 implements:
-  - space: '{{powerops_models}}'
+  - space: "{{powerops_models}}"
     externalId: FunctionInput
-    version: '{{version}}'
+    version: "{{version}}"
     type: view
-version: '{{version}}'
+version: "{{version}}"
 properties:
+  bidDate:
+    container:
+      space: "{{powerops_models}}"
+      externalId: FunctionData
+      type: container
+    containerPropertyIdentifier: date1
+    name: bidDate
+    description: The bid date
   bidConfiguration:
     container:
-      space: '{{powerops_models}}'
+      space: "{{powerops_models}}"
       externalId: FunctionData
       type: container
     containerPropertyIdentifier: direct1
     name: bidConfiguration
+    description: TODO description
     source:
-      space: '{{powerops_models}}'
-      externalId: BidConfigurationWater
-      version: '{{version}}'
+      space: "{{powerops_models}}"
+      externalId: BidConfiguration
+      version: "{{version}}"
       type: view
-  bidDate:
-    container:
-      space: '{{powerops_models}}'
-      externalId: FunctionData
-      type: container
-    containerPropertyIdentifier: date1
-    name: bidDate
-    description: The bid date
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_outputs/1-interface.FunctionOutput.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/shop_result/SHOPTimeSeries.view.yaml`

 * *Files 25% similar despite different names*

```diff
@@ -1,63 +1,57 @@
 space: '{{powerops_models}}'
-externalId: FunctionOutput
-name: FunctionOutput
-description: Base class for all function inputs
+externalId: SHOPTimeSeries
+name: SHOPTimeSeries
+description: A wrapper around a timeseries object from the output of a successful SHOP run
 filter:
   and:
     - hasData:
         - type: container
           space: '{{powerops_models}}'
-          externalId: FunctionMetadata
+          externalId: SHOPTimeSeries
     - equals:
         property:
           - node
           - space
         value: '{{powerops_instance_space}}'
+    - equals:
+        property:
+          - node
+          - type
+        value:
+          externalId: SHOPTimeSeries
+          space: '{{powerops_type_space}}'
 implements: []
 version: '{{version}}'
 properties:
-  processId:
+  objectType:
     container:
       space: '{{powerops_models}}'
-      externalId: FunctionMetadata
+      externalId: SHOPTimeSeries
       type: container
-    containerPropertyIdentifier: processId
-    name: processId
-    description: The process associated with the function execution
-  processStep:
+    containerPropertyIdentifier: objectType
+    name: objectType
+    description: The type of the object
+  objectName:
     container:
       space: '{{powerops_models}}'
-      externalId: FunctionMetadata
+      externalId: SHOPTimeSeries
       type: container
-    containerPropertyIdentifier: processStep
-    name: processStep
-    description: This is the step in the process.
-  alerts:
-    type:
-      space: '{{powerops_type_space}}'
-      externalId: calculationIssue
-    source:
-      space: '{{powerops_models}}'
-      externalId: Alert
-      version: '{{version}}'
-      type: view
-    direction: outwards
-    name: alerts
-    description: An array of calculation level Alerts.
-    connectionType: multi_edge_connection
-  functionName:
+    containerPropertyIdentifier: objectName
+    name: objectName
+    description: The name of the object
+  attributeName:
     container:
       space: '{{powerops_models}}'
-      externalId: FunctionMetadata
+      externalId: SHOPTimeSeries
       type: container
-    containerPropertyIdentifier: functionName
-    name: functionName
-    description: The name of the function
-  functionCallId:
+    containerPropertyIdentifier: attributeName
+    name: attributeName
+    description: The name of the attribute
+  timeseries:
     container:
       space: '{{powerops_models}}'
-      externalId: FunctionMetadata
+      externalId: SHOPTimeSeries
       type: container
-    containerPropertyIdentifier: functionCallId
-    name: functionCallId
-    description: The function call id
+    containerPropertyIdentifier: timeseries
+    name: timeseries
+    description: Timeseries object from output of SHOP stored as a timeseries in cdf
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_outputs/PartiallPostProcessingOutput.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_inputs/TotalBidMatrixCalculationInput.view.yaml`

 * *Files 18% similar despite different names*

```diff
@@ -1,54 +1,62 @@
-space: '{{powerops_models}}'
-externalId: PartialPostProcessingOutput
-name: PartialPostProcessingOutput
-description: The output of the function that post-processes all raw partial bid matrices
+space: "{{powerops_models}}"
+externalId: TotalBidMatrixCalculationInput
+name: TotalBidMatrixCalculationInput
+description: Input for the total bid matrix calculation
 filter:
   and:
     - hasData:
         - type: container
-          space: '{{powerops_models}}'
-          externalId: FunctionData
+          space: "{{powerops_models}}"
+          externalId: FunctionMetadata
     - equals:
         property:
           - node
           - space
-        value: '{{powerops_instance_space}}'
+        value: "{{powerops_instance_space}}"
     - equals:
         property:
           - node
           - type
         value:
-          externalId: PartialPostProcessingOutput
-          space: '{{powerops_type_space}}'
+          externalId: TotalBidMatrixCalculationInput
+          space: "{{powerops_type_space}}"
 implements:
-  - space: '{{powerops_models}}'
-    externalId: FunctionOutput
-    version: '{{version}}'
+  - space: "{{powerops_models}}"
+    externalId: FunctionInput
+    version: "{{version}}"
     type: view
-version: '{{version}}'
+version: "{{version}}"
 properties:
-  partialMatrices:
-    type:
-      space: '{{powerops_type_space}}'
-      externalId: BidMatrix
+  bidConfiguration:
+    container:
+      space: "{{powerops_models}}"
+      externalId: FunctionData
+      type: container
+    containerPropertyIdentifier: direct1
+    name: bidConfiguration
     source:
-      space: '{{powerops_models}}'
-      externalId: BidMatrix
-      version: '{{version}}'
+      space: "{{powerops_models}}"
+      externalId: BidConfiguration
+      version: "{{version}}"
       type: view
-    direction: outwards
-    name: partialMatrices
-    description: The processed partial bid matrices that are used to calculate the total bid matrix.
-    connectionType: multi_edge_connection
-  input:
+  bidDate:
     container:
-      space: '{{powerops_models}}'
+      space: "{{powerops_models}}"
       externalId: FunctionData
       type: container
-    containerPropertyIdentifier: direct1
-    name: input
+    containerPropertyIdentifier: date1
+    name: bidDate
+    description: The bid date
+  partialBidMatrices:
+    type:
+      space: "{{powerops_type_space}}"
+      externalId: BidMatrix
     source:
-      space: '{{powerops_models}}'
-      externalId: PartialPostProcessingInput
-      version: '{{version}}'
+      space: "{{powerops_models}}"
+      externalId: BidMatrix
+      version: "{{version}}"
       type: view
+    direction: outwards
+    name: partialBidMatrices
+    description: The partial bid matrices that are used to calculate the total bid matrix.
+    connectionType: multi_edge_connection
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/function_outputs/PreprocessorOutput.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/bid_configuration/1-interface.PartialBidConfiguration.view.yaml`

 * *Files 26% similar despite different names*

```diff
@@ -1,55 +1,41 @@
-space: '{{powerops_models}}'
-externalId: PreprocessorOutput
-name: PreprocessorOutput
-description: Input to the task dispatcher in the shop bid process
+space: "{{powerops_models}}"
+externalId: PartialBidConfiguration
+name: PartialBidConfiguration
+description: A partial bid configuration
 filter:
   and:
     - hasData:
         - type: container
-          space: '{{powerops_models}}'
-          externalId: FunctionData
+          space: "{{powerops_models}}"
+          externalId: PartialBidConfiguration
     - equals:
         property:
           - node
           - space
-        value: '{{powerops_instance_space}}'
-    - equals:
-        property:
-          - node
-          - type
-        value:
-          externalId: PreprocessorOutput
-          space: '{{powerops_type_space}}'
-implements:
-  - space: '{{powerops_models}}'
-    externalId: FunctionOutput
-    version: '{{version}}'
-    type: view
-version: '{{version}}'
+        value: "{{powerops_instance_space}}"
+version: "{{version}}"
 properties:
-  case:
+  name:
+    container:
+      space: "{{powerops_models}}"
+      externalId: PartialBidConfiguration
+      type: container
+    containerPropertyIdentifier: name
+    name: name
+    description: Name for the PartialBidConfiguration
+  method:
     container:
-      space: '{{powerops_models}}'
-      externalId: FunctionData
+      space: "{{powerops_models}}"
+      externalId: PartialBidConfiguration
       type: container
-    containerPropertyIdentifier: direct1
-    name: case
-    description: The Case to trigger shop with
-    source:
-      space: '{{powerops_models}}'
-      externalId: Case
-      version: '{{version}}'
-      type: view
-  input:
+    containerPropertyIdentifier: method
+    name: method
+    description: Name of the method used for the bid calculation
+  addSteps:
     container:
-      space: '{{powerops_models}}'
-      externalId: FunctionData
+      space: "{{powerops_models}}"
+      externalId: PartialBidConfiguration
       type: container
-    containerPropertyIdentifier: direct2
-    name: input
-    description: The prepped and processed scenario to send to shop trigger
-    source:
-      space: '{{powerops_models}}'
-      externalId: PreprocessorInput
-      version: '{{version}}'
-      type: view
+    containerPropertyIdentifier: addSteps
+    name: addSteps
+    description: TODO definition
```

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/custom_modules/power_model_v1/data_models/views/shop_result/ShopObjectiveValue.view.yaml` & `cognite_power_ops-0.92.0/cognite/powerops/custom_modules/power_model_v1/data_models/views/shop_result/ShopObjectiveValue.view.yaml`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/prerun_transformations/transformations.py` & `cognite_power_ops-0.92.0/cognite/powerops/prerun_transformations/transformations.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/resync/config/__init__.py` & `cognite_power_ops-0.92.0/cognite/powerops/resync/config/__init__.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/resync/config/_main.py` & `cognite_power_ops-0.92.0/cognite/powerops/resync/config/_main.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/resync/config/_settings.py` & `cognite_power_ops-0.92.0/cognite/powerops/resync/config/_settings.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/resync/config/_shared.py` & `cognite_power_ops-0.92.0/cognite/powerops/resync/config/_shared.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/resync/config/cogshop/shop_file_config.py` & `cognite_power_ops-0.92.0/cognite/powerops/resync/config/cogshop/shop_file_config.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/resync/config/market/__init__.py` & `cognite_power_ops-0.92.0/cognite/powerops/resync/config/market/__init__.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/resync/config/market/_core.py` & `cognite_power_ops-0.92.0/cognite/powerops/resync/config/market/_core.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/resync/config/market/benchmarking.py` & `cognite_power_ops-0.92.0/cognite/powerops/resync/config/market/benchmarking.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/resync/config/market/dayahead.py` & `cognite_power_ops-0.92.0/cognite/powerops/resync/config/market/dayahead.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/resync/config/market/market.py` & `cognite_power_ops-0.92.0/cognite/powerops/resync/config/market/market.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/resync/config/market/rkom.py` & `cognite_power_ops-0.92.0/cognite/powerops/resync/config/market/rkom.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/resync/config/production/connections.py` & `cognite_power_ops-0.92.0/cognite/powerops/resync/config/production/connections.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/resync/config/production/generator.py` & `cognite_power_ops-0.92.0/cognite/powerops/resync/config/production/generator.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/resync/config/production/plant.py` & `cognite_power_ops-0.92.0/cognite/powerops/resync/config/production/plant.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/resync/config/production/watercourse.py` & `cognite_power_ops-0.92.0/cognite/powerops/resync/config/production/watercourse.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/resync/core/cdf.py` & `cognite_power_ops-0.92.0/cognite/powerops/resync/core/cdf.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/resync/core/main.py` & `cognite_power_ops-0.92.0/cognite/powerops/resync/core/main.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/resync/core/transform.py` & `cognite_power_ops-0.92.0/cognite/powerops/resync/core/transform.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/resync/core/validation.py` & `cognite_power_ops-0.92.0/cognite/powerops/resync/core/validation.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/resync/diff/core.py` & `cognite_power_ops-0.92.0/cognite/powerops/resync/diff/core.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/resync/diff/data_classes.py` & `cognite_power_ops-0.92.0/cognite/powerops/resync/diff/data_classes.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/resync/models/_shared_v1_v2/_to_instances.py` & `cognite_power_ops-0.92.0/cognite/powerops/resync/models/_shared_v1_v2/_to_instances.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/resync/models/_shared_v1_v2/cogshop_model.py` & `cognite_power_ops-0.92.0/cognite/powerops/resync/models/_shared_v1_v2/cogshop_model.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/resync/models/_shared_v1_v2/market_model.py` & `cognite_power_ops-0.92.0/cognite/powerops/resync/models/_shared_v1_v2/market_model.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/resync/models/_shared_v1_v2/production_model.py` & `cognite_power_ops-0.92.0/cognite/powerops/resync/models/_shared_v1_v2/production_model.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/resync/models/base/__init__.py` & `cognite_power_ops-0.92.0/cognite/powerops/resync/models/base/__init__.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/resync/models/base/asset_model.py` & `cognite_power_ops-0.92.0/cognite/powerops/resync/models/base/asset_model.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/resync/models/base/asset_type.py` & `cognite_power_ops-0.92.0/cognite/powerops/resync/models/base/asset_type.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/resync/models/base/cdf_resources.py` & `cognite_power_ops-0.92.0/cognite/powerops/resync/models/base/cdf_resources.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/resync/models/base/data_model.py` & `cognite_power_ops-0.92.0/cognite/powerops/resync/models/base/data_model.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/resync/models/base/dms_models.py` & `cognite_power_ops-0.92.0/cognite/powerops/resync/models/base/dms_models.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/resync/models/base/graph_ql.py` & `cognite_power_ops-0.92.0/cognite/powerops/resync/models/base/graph_ql.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/resync/models/base/model.py` & `cognite_power_ops-0.92.0/cognite/powerops/resync/models/base/model.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/resync/models/base/resource_type.py` & `cognite_power_ops-0.92.0/cognite/powerops/resync/models/base/resource_type.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/resync/models/v1/__init__.py` & `cognite_power_ops-0.92.0/cognite/powerops/resync/models/v1/__init__.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/resync/models/v1/cogshop.py` & `cognite_power_ops-0.92.0/cognite/powerops/resync/models/v1/cogshop.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/resync/models/v1/config_to_model/to_cogshop_model.py` & `cognite_power_ops-0.92.0/cognite/powerops/resync/models/v1/config_to_model/to_cogshop_model.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/resync/models/v1/config_to_model/to_market_model.py` & `cognite_power_ops-0.92.0/cognite/powerops/resync/models/v1/config_to_model/to_market_model.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/resync/models/v1/config_to_model/to_production_model.py` & `cognite_power_ops-0.92.0/cognite/powerops/resync/models/v1/config_to_model/to_production_model.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/resync/models/v1/graphql_schemas/cogshop1.graphql` & `cognite_power_ops-0.92.0/cognite/powerops/resync/models/v1/graphql_schemas/cogshop1.graphql`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/resync/models/v1/market/__init__.py` & `cognite_power_ops-0.92.0/cognite/powerops/resync/models/v1/market/__init__.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/resync/models/v1/market/base.py` & `cognite_power_ops-0.92.0/cognite/powerops/resync/models/v1/market/base.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/resync/models/v1/market/benchmark.py` & `cognite_power_ops-0.92.0/cognite/powerops/resync/models/v1/market/benchmark.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/resync/models/v1/market/dayahead.py` & `cognite_power_ops-0.92.0/cognite/powerops/resync/models/v1/market/dayahead.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/resync/models/v1/market/rkom.py` & `cognite_power_ops-0.92.0/cognite/powerops/resync/models/v1/market/rkom.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/resync/models/v1/production.py` & `cognite_power_ops-0.92.0/cognite/powerops/resync/models/v1/production.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/resync/models/v2/config_to_model/to_powerasset_model.py` & `cognite_power_ops-0.92.0/cognite/powerops/resync/models/v2/config_to_model/to_powerasset_model.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/resync/models/v2/powerops_models.py` & `cognite_power_ops-0.92.0/cognite/powerops/resync/models/v2/powerops_models.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/resync/models/v2/production_dm.py` & `cognite_power_ops-0.92.0/cognite/powerops/resync/models/v2/production_dm.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/resync/time_series_mapping/mapping.py` & `cognite_power_ops-0.92.0/cognite/powerops/resync/time_series_mapping/mapping.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/resync/time_series_mapping/static_mapping.py` & `cognite_power_ops-0.92.0/cognite/powerops/resync/time_series_mapping/static_mapping.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/resync/validation.py` & `cognite_power_ops-0.92.0/cognite/powerops/resync/validation.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/utils/cdf/_cdf_auth.py` & `cognite_power_ops-0.92.0/cognite/powerops/utils/cdf/_cdf_auth.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/utils/cdf/_settings.py` & `cognite_power_ops-0.92.0/cognite/powerops/utils/cdf/_settings.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/utils/cdf/calls.py` & `cognite_power_ops-0.92.0/cognite/powerops/utils/cdf/calls.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/utils/cdf/extraction_pipelines.py` & `cognite_power_ops-0.92.0/cognite/powerops/utils/cdf/extraction_pipelines.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/utils/cdf/resource_creation.py` & `cognite_power_ops-0.92.0/cognite/powerops/utils/cdf/resource_creation.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/utils/lookup.py` & `cognite_power_ops-0.92.0/cognite/powerops/utils/lookup.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/utils/navigation.py` & `cognite_power_ops-0.92.0/cognite/powerops/utils/navigation.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/utils/require.py` & `cognite_power_ops-0.92.0/cognite/powerops/utils/require.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/utils/retry/api.py` & `cognite_power_ops-0.92.0/cognite/powerops/utils/retry/api.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/utils/serialization.py` & `cognite_power_ops-0.92.0/cognite/powerops/utils/serialization.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/cognite/powerops/utils/time.py` & `cognite_power_ops-0.92.0/cognite/powerops/utils/time.py`

 * *Files identical despite different names*

### Comparing `cognite_power_ops-0.91.3/pyproject.toml` & `cognite_power_ops-0.92.0/pyproject.toml`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 [tool.poetry]
 name = "cognite-power-ops"
-version = "0.91.3"
+version = "0.92.0"
 description = "SDK for power markets operations on Cognite Data Fusion"
 readme = "README.md"
 authors = ["Cognite <support@cognite.com>"]
 license = "Apache 2.0"
 documentation = "https://cognite-power-ops-sdk.readthedocs-hosted.com/en/latest/"
 homepage = "https://cognite-power-ops-sdk.readthedocs-hosted.com/en/latest/"
 repository = "https://github.com/cognitedata/power-ops-sdk"
```

### Comparing `cognite_power_ops-0.91.3/PKG-INFO` & `cognite_power_ops-0.92.0/PKG-INFO`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: cognite-power-ops
-Version: 0.91.3
+Version: 0.92.0
 Summary: SDK for power markets operations on Cognite Data Fusion
 Home-page: https://cognite-power-ops-sdk.readthedocs-hosted.com/en/latest/
 License: Apache 2.0
 Author: Cognite
 Author-email: support@cognite.com
 Requires-Python: >=3.9,<3.13
 Classifier: License :: Other/Proprietary License
```

